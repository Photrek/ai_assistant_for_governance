{
  "content": "Cardano Improvement Proposals\n\nActiveMeta#1CIP ProcessFrederic Johnson, Sebastien Guillemot, Matthias Benkort, Duncan Coutts, Michael Peyton Jones, and Robert PhairActiveMetaCreated on March 21, 2020\n#1\nCIP Process\nFrederic Johnson, Sebastien Guillemot, Matthias Benkort, Duncan Coutts, Michael Peyton Jones, and Robert Phair\nCreated on March 21, 2020\nActiveWallets#2Coin Selection Algorithms for CardanoJonathan KnowlesActiveWalletsCreated on May 4, 2020\n#2\nCoin Selection Algorithms for Cardano\nJonathan Knowles\nCreated on May 4, 2020\nActiveWallets#3Wallet Key GenerationMatthias Benkort, and Sebastien GuillemotActiveWalletsCreated on May 7, 2020\n#3\nWallet Key Generation\nMatthias Benkort, and Sebastien Guillemot\nCreated on May 7, 2020\nProposedWallets#4Wallet ChecksumsRuslan Dudin, and Sebastien GuillemotProposedWalletsCreated on May 1, 2019\n#4\nWallet Checksums\nRuslan Dudin, and Sebastien Guillemot\nCreated on May 1, 2019\nActiveTools#5Common Bech32 PrefixesMatthias BenkortActiveToolsCreated on May 28, 2020\n#5\nCommon Bech32 Prefixes\nMatthias Benkort\nCreated on May 28, 2020\nActiveTools#6Stake Pool Extended MetadataMarkus Gufler, and Mike FullmanActiveToolsCreated on July 20, 2020\n#6\nStake Pool Extended Metadata\nMarkus Gufler, and Mike Fullman\nCreated on July 20, 2020\nProposedLedger#7Curve Pledge BenefitShawn McMurdoProposedLedgerCreated on August 11, 2020\n#7\nCurve Pledge Benefit\nShawn McMurdo\nCreated on August 11, 2020\nActiveTools#8Message SigningSebastien GuillemotActiveToolsCreated on September 28, 2020\n#8\nMessage Signing\nSebastien Guillemot\nCreated on September 28, 2020\nActiveLedger#9Protocol Parameters (Shelley Era)Kevin HammondActiveLedgerCreated on January 29, 2021\n#9\nProtocol Parameters (Shelley Era)\nKevin Hammond\nCreated on January 29, 2021\nActiveMetadata#10Transaction Metadata Label RegistrySebastien GuillemotActiveMetadataCreated on October 31, 2020\n#10\nTransaction Metadata Label Registry\nSebastien Guillemot\nCreated on October 31, 2020\nActiveWallets#11Staking key chain for HD walletsSebastien Guillemot, and Matthias BenkortActiveWalletsCreated on November 4, 2020\n#11\nStaking key chain for HD wallets\nSebastien Guillemot, and Matthias Benkort\nCreated on November 4, 2020\nProposedMetadata#12On-chain stake pool operator to delegates communicationMarek Mahut, Sebastien Guillemot, and J n HrnkoProposedMetadataCreated on November 7, 2020\n#12\nOn-chain stake pool operator to delegates communication\nMarek Mahut, Sebastien Guillemot, and J n Hrnko\nCreated on November 7, 2020\nProposedWallets#13Cardano URI SchemeRobert Phair, Sebastien Guillemot, and Vicente AlmonacidProposedWalletsCreated on September 22, 2020\n#13\nCardano URI Scheme\nRobert Phair, Sebastien Guillemot, and Vicente Almonacid\nCreated on September 22, 2020\nActiveTokens#14User-Facing Asset FingerprintMatthias Benkort, and Rodney LorrimarActiveTokensCreated on February 1, 2020\n#14\nUser-Facing Asset Fingerprint\nMatthias Benkort, and Rodney Lorrimar\nCreated on February 1, 2020\nActiveMetadata#15Registration Transaction Metadata FormatSebastien Guillemot, Rinor Hoxha, and Mikhail ZabaluevActiveMetadataCreated on January 5, 2020\n#15\nRegistration Transaction Metadata Format\nSebastien Guillemot, Rinor Hoxha, and Mikhail Zabaluev\nCreated on January 5, 2020\nActiveTools#16Cryptographic Key Serialisation FormatsLuke NadurActiveToolsCreated on December 21, 2020\n#16\nCryptographic Key Serialisation Formats\nLuke Nadur\nCreated on December 21, 2020\nInactive (abandoned for lack of interest)Tools#17Cardano Delegation PortfolioMatthias BenkortInactive (abandoned for lack of interest)ToolsCreated on April 2, 2020\n#17\nCardano Delegation Portfolio\nMatthias Benkort\nCreated on April 2, 2020\nProposedWallets#18Multi-Stake-Keys WalletsMatthias BenkortProposedWalletsCreated on March 18, 2020\n#18\nMulti-Stake-Keys Wallets\nMatthias Benkort\nCreated on March 18, 2020\nActiveLedger#19Cardano AddressesMatthias BenkortActiveLedgerCreated on March 25, 2020\n#19\nCardano Addresses\nMatthias Benkort\nCreated on March 25, 2020\nActiveMetadata#20Transaction message/comment metadataMartin Lang, Ola Ahlman, and Andrew WestbergActiveMetadataCreated on June 13, 2021\n#20\nTransaction message/comment metadata\nMartin Lang, Ola Ahlman, and Andrew Westberg\nCreated on June 13, 2021\nActiveWallets#21Transaction requirements for interoperability with hardware walletsGabriel Kerekes, Rafael Korbas, and Jan MazakActiveWalletsCreated on June 15, 2021\n#21\nTransaction requirements for interoperability with hardware wallets\nGabriel Kerekes, Rafael Korbas, and Jan Mazak\nCreated on June 15, 2021\nActiveTools#22Pool operator verificationAndrew Westberg, Martin Lang, and Ola AhlmanActiveToolsCreated on June 21, 2021\n#22\nPool operator verification\nAndrew Westberg, Martin Lang, and Ola Ahlman\nCreated on June 21, 2021\nProposedLedger#23Fair Min FeesShawn McMurdoProposedLedgerCreated on February 4, 2021\n#23\nFair Min Fees\nShawn McMurdo\nCreated on February 4, 2021\nProposedWallets#24Non-Centralizing RankingsShawn McMurdoProposedWalletsCreated on September 15, 2020\n#24\nNon-Centralizing Rankings\nShawn McMurdo\nCreated on September 15, 2020\nActiveTokens#25Media Token Metadata StandardAlessandro Konrad, and SmaugActiveTokensCreated on April 8, 2021\n#25\nMedia Token Metadata Standard\nAlessandro Konrad, and Smaug\nCreated on April 8, 2021\nActiveMetadata#26Cardano Off-Chain MetadataMatthias Benkort, Michael Peyton Jones, and Polina VinogradovaActiveMetadataCreated on August 10, 2021\n#26\nCardano Off-Chain Metadata\nMatthias Benkort, Michael Peyton Jones, and Polina Vinogradova\nCreated on August 10, 2021\nActiveTokens#27CNFT Community Royalties StandardHuth S0lo, and TheRealAdamDeanActiveTokensCreated on August 29, 2021\n#27\nCNFT Community Royalties Standard\nHuth S0lo, and TheRealAdamDean\nCreated on August 29, 2021\nActiveLedger#28Protocol Parameters (Alonzo Era)Kevin HammondActiveLedgerCreated on October 14, 2021\n#28\nProtocol Parameters (Alonzo Era)\nKevin Hammond\nCreated on October 14, 2021\nActiveTools#29Phase-1 Monetary Scripts Serialization FormatsMatthias BenkortActiveToolsCreated on August 17, 2020\n#29\nPhase-1 Monetary Scripts Serialization Formats\nMatthias Benkort\nCreated on August 17, 2020\nActiveWallets#30Cardano dApp-Wallet Web BridgerooooooooobActiveWalletsCreated on April 29, 2021\n#30\nCardano dApp-Wallet Web Bridge\nrooooooooob\nCreated on April 29, 2021\nActivePlutus#31Reference inputsMichael Peyton JonesActivePlutusCreated on November 29, 2021\n#31\nReference inputs\nMichael Peyton Jones\nCreated on November 29, 2021\nActivePlutus#32Inline datumsMichael Peyton JonesActivePlutusCreated on November 29, 2021\n#32\nInline datums\nMichael Peyton Jones\nCreated on November 29, 2021\nActivePlutus#33Reference scriptsMichael Peyton JonesActivePlutusCreated on November 29, 2021\n#33\nReference scripts\nMichael Peyton Jones\nCreated on November 29, 2021\nProposedTools#34Chain ID RegistrySebastien GuillemotProposedToolsCreated on November 24, 2021\n#34\nChain ID Registry\nSebastien Guillemot\nCreated on November 24, 2021\nActiveMeta#35Changes to Plutus CoreMichael Peyton JonesActiveMetaCreated on February 9, 2022\n#35\nChanges to Plutus Core\nMichael Peyton Jones\nCreated on February 9, 2022\nProposedTools#36Catalyst Registration Transaction Metadata Format (Updated)Giacomo Pasini, Kevin Hammond, and Mark StopkaProposedToolsCreated on December 6, 2021\n#36\nCatalyst Registration Transaction Metadata Format (Updated)\nGiacomo Pasini, Kevin Hammond, and Mark Stopka\nCreated on December 6, 2021\nProposedLedger#37Dynamic Saturation Based on PledgeCasey GibsonProposedLedgerCreated on December 3, 2021\n#37\nDynamic Saturation Based on Pledge\nCasey Gibson\nCreated on December 3, 2021\nActiveLedger#40Collateral OutputSebastien Guillemot, Jared Corduan, and Andre KnispelActiveLedgerCreated on February 10, 2022\n#40\nCollateral Output\nSebastien Guillemot, Jared Corduan, and Andre Knispel\nCreated on February 10, 2022\nActivePlutus#42New Plutus Builtin serialiseDataMatthias Benkort, and Sebastian NagelActivePlutusCreated on February 9, 2022\n#42\nNew Plutus Builtin serialiseData\nMatthias Benkort, and Sebastian Nagel\nCreated on February 9, 2022\nActiveWallets#45Decentralized WebRTC dApp-Wallet CommunicationFabian Bormann, and Jaime CasoActiveWalletsCreated on November 29, 2022\n#45\nDecentralized WebRTC dApp-Wallet Communication\nFabian Bormann, and Jaime Caso\nCreated on November 29, 2022\nActivePlutus#49ECDSA and Schnorr signatures in Plutus CoreKoz Ross, Michael Peyton-Jones, and I igo Querejeta AzurmendiActivePlutusCreated on April 27, 2022\n#49\nECDSA and Schnorr signatures in Plutus Core\nKoz Ross, Michael Peyton-Jones, and I igo Querejeta Azurmendi\nCreated on April 27, 2022\nProposedLedger#50Pledge Leverage-Based Staking RewardsMichael LiesenfeltProposedLedgerCreated on April 4, 2022\n#50\nPledge Leverage-Based Staking Rewards\nMichael Liesenfelt\nCreated on April 4, 2022\nProposedMeta#52Cardano audit best practice guidelinesSimon ThompsonProposedMetaCreated on April 25, 2022\n#52\nCardano audit best practice guidelines\nSimon Thompson\nCreated on April 25, 2022\nProposedTokens#54Cardano Smart NFTsKieran SimkinProposedTokensCreated on May 18, 2022\n#54\nCardano Smart NFTs\nKieran Simkin\nCreated on May 18, 2022\nActiveLedger#55Protocol Parameters (Babbage Era)Jared CorduanActiveLedgerCreated on May 19, 2022\n#55\nProtocol Parameters (Babbage Era)\nJared Corduan\nCreated on May 19, 2022\nActiveTools#57Plutus Contract BlueprintKtorZ, and scarmuegaActiveToolsCreated on May 15, 2022\n#57\nPlutus Contract Blueprint\nKtorZ, and scarmuega\nCreated on May 15, 2022\nInactive (superseded by CIP-0121 and CIP-0122)Plutus#58Plutus Bitwise PrimitivesKoz Ross, and Maximilian K nigInactive (superseded by CIP-0121 and CIP-0122)PlutusCreated on May 27, 2022\n#58\nPlutus Bitwise Primitives\nKoz Ross, and Maximilian K nig\nCreated on May 27, 2022\nActiveMeta#59Terminology Surrounding Core FeaturesJared CorduanActiveMetaCreated on June 9, 2022\n#59\nTerminology Surrounding Core Features\nJared Corduan\nCreated on June 9, 2022\nActiveMetadata#60Music Token MetadataAndrew Westberg, Ryan Jones, Justin Morgan, Ian Singer, Anthony Eizmendiz, Session Cruz, Jimmy Londo, Gudbrand Tokerud, Kevin St.Clair, Brandon Loyche, Andrew Donovan, The Finest LLC (DBA So Litty Records), Cristhian Escobar, and Gabriel Stephan TalamantesActiveMetadataCreated on July 26, 2022\n#60\nMusic Token Metadata\nAndrew Westberg, Ryan Jones, Justin Morgan, Ian Singer, Anthony Eizmendiz, Session Cruz, Jimmy Londo, Gudbrand Tokerud, Kevin St.Clair, Brandon Loyche, Andrew Donovan, The Finest LLC (DBA So Litty Records), Cristhian Escobar, and Gabriel Stephan Talamantes\nCreated on July 26, 2022\nProposedTokens#67Asset Name Label RegistryAlessandro Konrad, and Thomas VellekoopProposedTokensCreated on July 13, 2022\n#67\nAsset Name Label Registry\nAlessandro Konrad, and Thomas Vellekoop\nCreated on July 13, 2022\nActiveTokens#68Datum Metadata StandardAlessandro Konrad, and Thomas VellekoopActiveTokensCreated on July 13, 2022\n#68\nDatum Metadata Standard\nAlessandro Konrad, and Thomas Vellekoop\nCreated on July 13, 2022\nActivePlutus#69Script Signature UnificationMaksymilian 'zygomeb' BrodowiczActivePlutusCreated on August 23, 2022\n#69\nScript Signature Unification\nMaksymilian 'zygomeb' Brodowicz\nCreated on August 23, 2022\nProposedTools#71Non-Fungible Token (NFT) Proxy Voting StandardThaddeus DiamondProposedToolsCreated on October 11, 2022\n#71\nNon-Fungible Token (NFT) Proxy Voting Standard\nThaddeus Diamond\nCreated on October 11, 2022\nProposedMetadata#72Cardano dApp Registration & DiscoveryBruno Martins, Mateusz Czeladka, and Daniel MainProposedMetadataCreated on October 18, 2022\n#72\nCardano dApp Registration & Discovery\nBruno Martins, Mateusz Czeladka, and Daniel Main\nCreated on October 18, 2022\nProposedLedger#74Set minPoolCost to 0Robin of LoxleyProposedLedgerCreated on October 17, 2022\n#74\nSet minPoolCost to 0\nRobin of Loxley\nCreated on October 17, 2022\nProposedLedger#75Fair Stake Pool RewardsTobias FanceeProposedLedgerCreated on October 21, 2022\n#75\nFair Stake Pool Rewards\nTobias Fancee\nCreated on October 21, 2022\nActiveLedger#80Transaction Serialization Deprecation CycleJared CorduanActiveLedgerCreated on November 9, 2022\n#80\nTransaction Serialization Deprecation Cycle\nJared Corduan\nCreated on November 9, 2022\nProposedLedger#82Improved Rewards Scheme ParametersTobias FanceeProposedLedgerCreated on January 3, 2022\n#82\nImproved Rewards Scheme Parameters\nTobias Fancee\nCreated on January 3, 2022\nActiveMetadata#83Encrypted Transaction message/comment metadata (Addendum to CIP-0020)Martin Lang, Ola Ahlman, Andrew Westberg, and Adam DeanActiveMetadataCreated on December 8, 2022\n#83\nEncrypted Transaction message/comment metadata (Addendum to CIP-0020)\nMartin Lang, Ola Ahlman, Andrew Westberg, and Adam Dean\nCreated on December 8, 2022\nActiveMeta#84Cardano Ledger EvolutionJared CorduanActiveMetaCreated on January 30, 2023\n#84\nCardano Ledger Evolution\nJared Corduan\nCreated on January 30, 2023\nActivePlutus#85Sums-of-products in Plutus CoreMichael Peyton JonesActivePlutusCreated on January 30, 2023\n#85\nSums-of-products in Plutus Core\nMichael Peyton Jones\nCreated on January 30, 2023\nProposedMetadata#86NFT Metadata Update OraclesNicolas Ayotte, George Flerovsky, Samuel Williams, and Nathaniel LaneProposedMetadataCreated on November 1, 2022\n#86\nNFT Metadata Update Oracles\nNicolas Ayotte, George Flerovsky, Samuel Williams, and Nathaniel Lane\nCreated on November 1, 2022\nProposedTokens#88Token Policy RegistrationAdam DeanProposedTokensCreated on February 27, 2023\n#88\nToken Policy Registration\nAdam Dean\nCreated on February 27, 2023\nProposedPlutus#91Don't force Built-In functionsNiels M ndlerProposedPlutusCreated on February 5, 2023\n#91\nDon't force Built-In functions\nNiels M ndler\nCreated on February 5, 2023\nProposedTools#93Authenticated Web3 HTTP requestsJuan Salvador Mag n ValeroProposedToolsCreated on December 27, 2022\n#93\nAuthenticated Web3 HTTP requests\nJuan Salvador Mag n Valero\nCreated on December 27, 2022\nActiveTools#94On-chain SPO pollsMatthias Benkort, and Markus GuflerActiveToolsCreated on March 21, 2023\n#94\nOn-chain SPO polls\nMatthias Benkort, and Markus Gufler\nCreated on March 21, 2023\nActiveWallets#95Web-Wallet Bridge - Conway ledger eraRyan WilliamsActiveWalletsCreated on February 24, 2022\n#95\nWeb-Wallet Bridge - Conway ledger era\nRyan Williams\nCreated on February 24, 2022\nActiveWallets#99Proof of OnboardingAdam Dean, Carl, and Alex DochioiuActiveWalletsCreated on June 20, 2023\n#99\nProof of Onboarding\nAdam Dean, Carl, and Alex Dochioiu\nCreated on June 20, 2023\nProposedMetadata#100Governance MetadataPi LanninghamProposedMetadataCreated on April 9, 2023\n#100\nGovernance Metadata\nPi Lanningham\nCreated on April 9, 2023\nProposedPlutus#101Integration of keccak256 into PlutusSergei Patrikeev, and I igo Querejeta-AzurmendiProposedPlutusCreated on June 13, 2023\n#101\nIntegration of keccak256 into Plutus\nSergei Patrikeev, and I igo Querejeta-Azurmendi\nCreated on June 13, 2023\nProposedTokens#102Royalty Datum MetadataSam DelaneyProposedTokensCreated on August 8, 2023\n#102\nRoyalty Datum Metadata\nSam Delaney\nCreated on August 8, 2023\nActiveWallets#103Web-Wallet Bridge - Bulk transaction signingMart n Schere, and Ola AhlmanActiveWalletsCreated on September 3, 2023\n#103\nWeb-Wallet Bridge - Bulk transaction signing\nMart n Schere, and Ola Ahlman\nCreated on September 3, 2023\nProposedWallets#104Web-Wallet Bridge - Account public keyOla Ahlman, and Andrew WestbergProposedWalletsCreated on September 3, 2023\n#104\nWeb-Wallet Bridge - Account public key\nOla Ahlman, and Andrew Westberg\nCreated on September 3, 2023\nActiveWallets#105Conway era Key Chains for HD WalletsRyan WilliamsActiveWalletsCreated on September 22, 2023\n#105\nConway era Key Chains for HD Wallets\nRyan Williams\nCreated on September 22, 2023\nProposedWallets#106Web-Wallet Bridge - Multisig walletsLeoProposedWalletsCreated on October 12, 2023\n#106\nWeb-Wallet Bridge - Multisig wallets\nLeo\nCreated on October 12, 2023\nProposedTools#107URI Scheme - Block and transaction objectsPi LanninghamProposedToolsCreated on September 22, 2020\n#107\nURI Scheme - Block and transaction objects\nPi Lanningham\nCreated on September 22, 2020\nProposedMetadata#108Governance Metadata - Governance ActionsRyan WilliamsProposedMetadataCreated on November 23, 2023\n#108\nGovernance Metadata - Governance Actions\nRyan Williams\nCreated on November 23, 2023\nProposedPlutus#109Modular Exponentiation Built-in for Plutus CoreKenneth MacKenzie, Michael Peyton Jones, I igo Querejeta-Azurmendi, and Thomas VellekoopProposedPlutusCreated on October 5, 2023\n#109\nModular Exponentiation Built-in for Plutus Core\nKenneth MacKenzie, Michael Peyton Jones, I igo Querejeta-Azurmendi, and Thomas Vellekoop\nCreated on October 5, 2023\nActivePlutus#110Plutus v1 Script ReferencesPi LanninghamActivePlutusCreated on December 20, 2023\n#110\nPlutus v1 Script References\nPi Lanningham\nCreated on December 20, 2023\nProposedPlutus#112Observe Script TypePhilip DiSarroProposedPlutusCreated on January 8, 2024\n#112\nObserve Script Type\nPhilip DiSarro\nCreated on January 8, 2024\nProposedTools#114CBOR Tags RegistrySteven JohnsonProposedToolsCreated on January 25, 2020\n#114\nCBOR Tags Registry\nSteven Johnson\nCreated on January 25, 2020\nProposedTools#115CBOR tag definition - ED25519-BIP32 KeysSteven JohnsonProposedToolsCreated on January 19, 2024\n#115\nCBOR tag definition - ED25519-BIP32 Keys\nSteven Johnson\nCreated on January 19, 2024\nProposedTools#116Standard JSON encoding for Domain TypesVladimir KalnitskyProposedToolsCreated on February 22, 2024\n#116\nStandard JSON encoding for Domain Types\nVladimir Kalnitsky\nCreated on February 22, 2024\nActivePlutus#117Explicit script return valuesMichael Peyton JonesActivePlutusCreated on January 22, 2024\n#117\nExplicit script return values\nMichael Peyton Jones\nCreated on January 22, 2024\nProposedMetadata#119Governance metadata - DRepsThomas UpfieldProposedMetadataCreated on February 7, 2024\n#119\nGovernance metadata - DReps\nThomas Upfield\nCreated on February 7, 2024\nProposedMetadata#120Constitution specificationRyan Williams, and Danielle StankoProposedMetadataCreated on March 19, 2024\n#120\nConstitution specification\nRyan Williams, and Danielle Stanko\nCreated on March 19, 2024\nActivePlutus#121Integer-ByteString conversionsKoz Ross, Ilia Rodionov, and Jeff CheahActivePlutusCreated on November 17, 2023\n#121\nInteger-ByteString conversions\nKoz Ross, Ilia Rodionov, and Jeff Cheah\nCreated on November 17, 2023\nProposedPlutus#122Logical operations over BuiltinByteStringKoz RossProposedPlutusCreated on May 3, 2024\n#122\nLogical operations over BuiltinByteString\nKoz Ross\nCreated on May 3, 2024\nProposedPlutus#123Bitwise operations over BuiltinByteStringKoz RossProposedPlutusCreated on May 16, 2024\n#123\nBitwise operations over BuiltinByteString\nKoz Ross\nCreated on May 16, 2024\nProposedMetadata#124Extend token metadata for translationsVito Melchionna, Aaron Schmid, and Carolina Isler @LaPetiteADAProposedMetadataCreated on March 19, 2023\n#124\nExtend token metadata for translations\nVito Melchionna, Aaron Schmid, and Carolina Isler @LaPetiteADA\nCreated on March 19, 2023\nProposedPlutus#127ripemd-160 hashing in Plutus CoreTomasz RybarczyProposedPlutusCreated on May 22, 2024\n#127\nripemd-160 hashing in Plutus Core\nTomasz Rybarczy\nCreated on May 22, 2024\nProposedLedger#128Preserving Order of Transaction InputsJonathan RodriguezProposedLedgerCreated on February 1, 2024\n#128\nPreserving Order of Transaction Inputs\nJonathan Rodriguez\nCreated on February 1, 2024\nProposedTools#129Governance IdentifiersAshish PrajapatiProposedToolsCreated on July 15, 2024\n#129\nGovernance Identifiers\nAshish Prajapati\nCreated on July 15, 2024\nProposedPlutus#133Plutus support for Multi-Scalar Multiplication over BLS12-381Dmytro Kaidalov, Adam Smolarek, and Thomas VellekoopProposedPlutusCreated on August 22, 2024\n#133\nPlutus support for Multi-Scalar Multiplication over BLS12-381\nDmytro Kaidalov, Adam Smolarek, and Thomas Vellekoop\nCreated on August 22, 2024\nProposedWallets#134Cardano URIs - Address RepresentationSteven JohnsonProposedWalletsCreated on August 23, 2024\n#134\nCardano URIs - Address Representation\nSteven Johnson\nCreated on August 23, 2024\nActiveTools#135Disaster Recovery Plan for Cardano networksKevin Hammond, Sam Leathers, Alex Moser, Steve Wagendorp, Andrew Westberg, and Nicholas ClarkeActiveToolsCreated on June 17, 2024\n#135\nDisaster Recovery Plan for Cardano networks\nKevin Hammond, Sam Leathers, Alex Moser, Steve Wagendorp, Andrew Westberg, and Nicholas Clarke\nCreated on June 17, 2024\nProposedMetadata#136Governance metadata - Constitutional Committee votesRyan Williams, and Eystein Magnus HansenProposedMetadataCreated on July 17, 2024\n#136\nGovernance metadata - Constitutional Committee votes\nRyan Williams, and Eystein Magnus Hansen\nCreated on July 17, 2024\nProposedNetwork#137Decentralized Message QueueJean-Philippe Raynaud, Arnaud Bailly, Marcin Szamotulski, Armando Santos, Neil Davies, and Sebastian NagelProposedNetworkCreated on August 2, 2024\n#137\nDecentralized Message Queue\nJean-Philippe Raynaud, Arnaud Bailly, Marcin Szamotulski, Armando Santos, Neil Davies, and Sebastian Nagel\nCreated on August 2, 2024\nProposedPlutus#138Plutus Core Builtin Type - `Array`Michael Peyton Jones, and Ana PantilieProposedPlutusCreated on September 18, 2024\n#138\nPlutus Core Builtin Type - `Array`\nMichael Peyton Jones, and Ana Pantilie\nCreated on September 18, 2024\nActivePlutus#381Plutus support for Pairings over BLS12-381I igo Querejeta-AzurmendiActivePlutusCreated on February 11, 2022\n#381\nPlutus support for Pairings over BLS12-381\nI igo Querejeta-Azurmendi\nCreated on February 11, 2022\nActiveLedger#1694A First Step Towards On-Chain Decentralized GovernanceJared Corduan, Andre Knispel, Matthias Benkort, Kevin Hammond, Charles Hoskinson, and Samuel LeathersActiveLedgerCreated on November 18, 2022\n#1694\nA First Step Towards On-Chain Decentralized Governance\nJared Corduan, Andre Knispel, Matthias Benkort, Kevin Hammond, Charles Hoskinson, and Samuel Leathers\nCreated on November 18, 2022\nActiveWallets#1852HD (Hierarchy for Deterministic) Wallets for CardanoSebastien Guillemot, and Matthias BenkortActiveWalletsCreated on October 28, 2019\n#1852\nHD (Hierarchy for Deterministic) Wallets for Cardano\nSebastien Guillemot, and Matthias Benkort\nCreated on October 28, 2019\nProposedWallets#1853HD (Hierarchy for Deterministic) Stake Pool Cold Keys for CardanoRafael KorbasProposedWalletsCreated on December 14, 2020\n#1853\nHD (Hierarchy for Deterministic) Stake Pool Cold Keys for Cardano\nRafael Korbas\nCreated on December 14, 2020\nActiveWallets#1854Multi-signatures HD WalletsMatthias Benkort, and Pawel JakubasActiveWalletsCreated on February 23, 2021\n#1854\nMulti-signatures HD Wallets\nMatthias Benkort, and Pawel Jakubas\nCreated on February 23, 2021\nProposedWallets#1855Forging policy keys for HD WalletsSamuel Leathers, John Lotoski, Michael Bishop, and David ArnoldProposedWalletsCreated on June 2, 2021\n#1855\nForging policy keys for HD Wallets\nSamuel Leathers, John Lotoski, Michael Bishop, and David Arnold\nCreated on June 2, 2021\nActiveMeta#9999Cardano Problem StatementsMatthias Benkort, and Michael Peyton JonesActiveMetaCreated on October 14, 2022\n#9999\nCardano Problem Statements\nMatthias Benkort, and Michael Peyton Jones\nCreated on October 14, 2022\n2023 Cardano Foundation\n\n---\n\nCIP-0001 | CIP Process\n\nA Cardano Improvement Proposal (CIP) is a formalised design document for the Cardano community and the name of the process by which such documents are produced and listed. A CIP provides information or describes a change to the Cardano ecosystem, processes, or environment concisely and in sufficient technical detail. In this CIP, we explain what a CIP is, how the CIP process functions, the role of the CIP Editors, and how users should go about proposing, discussing, and structuring a CIP.\nThe Cardano Foundation intends CIPs to be the primary mechanisms for proposing new features, collecting community input on an issue, and documenting design decisions that have gone into Cardano. Plus, because CIPs are text files in a versioned repository, their revision history is the historical record of significant changes affecting Cardano.\nCIPs aim to address two challenges mainly:\nThe need for various parties to agree on a common approach to ease the interoperability of tools or interfaces. The need to propose and discuss changes to the protocol or established practice of the ecosystem.\nThe need for various parties to agree on a common approach to ease the interoperability of tools or interfaces.\nThe need to propose and discuss changes to the protocol or established practice of the ecosystem.\nThe CIP process does not by itself offer any form of governance. For example, it does not govern the process by which proposed changes to the Cardano protocol are implemented and deployed. Yet, it is a crucial, community-driven component of the governance decision pipeline as it helps to collect thoughts and proposals in an organised fashion. Additionally, specific projects may choose to actively engage with the CIP process for some or all changes to their project.\nThis document outlines the technical structure of the CIP and the technical requirements of the submission and review process. The history, social features and human elements of the CIP process are described the CIP repository Wiki.\nDocument Structure Header Preamble Translations Repository Organization Versioning Licensing Statuses Status: Proposed Status: Active Status: Inactive Path to Active Categories Project Enlisting\nStructure Header Preamble Translations Repository Organization Versioning Licensing\nHeader Preamble\nTranslations\nRepository Organization\nVersioning\nLicensing\nStatuses Status: Proposed Status: Active Status: Inactive\nStatus: Proposed\nStatus: Active\nStatus: Inactive\nPath to Active\nCategories\nProject Enlisting\nProcess 1. Early Stage 1.a. Authors open a pull request Naming CIPs with similar subjects 1.b. Authors seek feedback 2. Editors' role 2.a. Triage in bi-weekly meetings 2.b. Reviews 3. Merging CIPs in the repository 4. Implementors work towards Active status following their 'Implementation Plan'\n1. Early Stage 1.a. Authors open a pull request Naming CIPs with similar subjects 1.b. Authors seek feedback\n1.a. Authors open a pull request Naming CIPs with similar subjects\nNaming CIPs with similar subjects\n1.b. Authors seek feedback\n2. Editors' role 2.a. Triage in bi-weekly meetings 2.b. Reviews\n2.a. Triage in bi-weekly meetings\n2.b. Reviews\n3. Merging CIPs in the repository\n4. Implementors work towards Active status following their 'Implementation Plan'\nEditors Missions Reviews Nomination\nMissions\nReviews\nNomination\nA CIP is, first and foremost, a document which proposes a solution to a well-defined problem. Documents are Markdown files with a Preamble and a set of pre-defined sections. CIPs authors must abide by the general structure, though they are free to organise each section as they see fit.\nThe structure of a CIP file is summarised in the table below:\nMotivation\nN/A\nNote Each of these section titles (Abstract onward) should be an H2 heading (beginning with markdown ##). Subsections like Versioning or Acceptance Criteria should be H3 headings (e.g. ### Versioning). Don't include a H1 title heading (markdown #): for web friendly contexts, this will be generated from the Preamble.\n##\n### Versioning\n#\nEach CIP must begin with a YAML key:value style header preamble (also known as front matter data), preceded and followed by three hyphens (---).\n---\nCIP\nTitle\n-\nStatus\nCategory\nAuthors\nImplementors\nN/A\n[]\nDiscussions\nSolution-To\nCreated\nLicense\nFor example:\n--- CIP: 1 Title: CIP Process Status: Active Category: Meta Authors: - Frederic Johnson <frederic.johnson@cardanofoundation.org> - Sebastien Guillemot <sebastien@dcspark.io> - Matthias Benkort <matthias.benkort@cardanofoundation.org> - Duncan Coutts <duncan.coutts@iohk.io> Implementors: N/A Discussions: - https://github.com/cardano-foundation/CIPs/pull/366 Created: 2020-03-21 License: CC-BY-4.0 ---\n--- CIP: 1 Title: CIP Process Status: Active Category: Meta Authors: - Frederic Johnson <frederic.johnson@cardanofoundation.org> - Sebastien Guillemot <sebastien@dcspark.io> - Matthias Benkort <matthias.benkort@cardanofoundation.org> - Duncan Coutts <duncan.coutts@iohk.io> Implementors: N/A Discussions: - https://github.com/cardano-foundation/CIPs/pull/366 Created: 2020-03-21 License: CC-BY-4.0 ---\nEspecially because Markdown link syntax is not supported in the header preamble, labels can be added to clarify list items; e.g.:\nDiscussions: - Original-PR: https://github.com/cardano-foundation/CIPs/pull/366\nDiscussions: - Original-PR: https://github.com/cardano-foundation/CIPs/pull/366\nNote A reference template is available in .github/CIP-TEMPLATE.md\nA CIP must be stored in a specific folder named after its number (4-digit, left-padded with 0) and in a file called README.md. Before a number is assigned, a placeholder folder name should be used (e.g. CIP-my-new-feature). After a number has been assigned, rename the folder.\n0\nREADME.md\nCIP-my-new-feature\nAdditional supporting files (such as diagrams, binary specifications, dialect grammars, JSON schemas etc.) may be added to the CIP's folder under freely chosen names.\nFor example:\nCIP-0010 ├── README.md ├── registry.json └── registry.schema.json\nCIP-0010 ├── README.md ├── registry.json └── registry.schema.json\nWhile CIPs are mainly technical documents meant to be read primarily by developers -- and thus often written in English; some may be translated into various languages to increase their outreach. Any file in a CIP folder may also include translated content satisfying the following rules:\nAny translated file shall share a common basename with its original source.\nAny translated file shall share a common basename with its original source.\nTranslation file basenames must have a suffix using an ISO 639-1 language code, separated by a dot . character. (e.g. README.fr.md).\nTranslation file basenames must have a suffix using an ISO 639-1 language code, separated by a dot . character. (e.g. README.fr.md).\n.\nREADME.fr.md\nWhen no language code is provided as suffix, the default language for the document is assumed to be English (UK/US).\nWhen no language code is provided as suffix, the default language for the document is assumed to be English (UK/US).\nTranslated CIPs (i.e. README files), must not include the original preamble. They must, however, include the following preamble as yaml frontmatter data:\nTranslated CIPs (i.e. README files), must not include the original preamble. They must, however, include the following preamble as yaml frontmatter data:\nREADME\nCIP\nSource\nTitle\nRevision\n12ab34c\nTranslators\nJohn Doe <john.doe@email.domain>\nLanguage\nfr\nTranslated CIPs inherit the same licensing terms as their original sources.\nCIPs must indicate how the defined Specification is versioned. Note this does not apply to the CIP text, for which annotated change logs are automatically generated and available through the GitHub UI as a history of CIP files and directories.\nAuthors are free to describe any approach to versioning that allows versioned alterations to be added without author oversight. Stipulating that the proposal must be superseded by another is also considered to be valid versioning.\nA single Versioning scheme can be placed either as a subsection of the Specification section or in an optional Versioning top-level section near the end. If the Specification contains multiple specification subsections, each of these can have a Versioning subsection within it.\nCIPs are licensed in the public domain. More so, they must be licensed under one of the following licenses. Each new CIP must identify at least one acceptable license in its preamble. In addition, each license must be referenced by its respective abbreviation below in the \"Copyright\" section.\nWarning\nAll licenses not explicitly included in the above lists are not acceptable terms for a Cardano Improvement Proposal unless a later CIP extends this one to add them.\nCIPs can have three statuses: Proposed, Active or Inactive. The CIP Process section highlights how CIPs move through these statuses; no CIP should be given one of these statuses without satisfying the criteria described here below.\nProposed\nActive\nInactive\nNote There is no \"draft\" status: a proposal which has not been merged (and hence exists in a PR) is a draft CIP. Draft CIPs should include the status they are aiming for on acceptance. Typically, but not always, this will be 'Proposed'.\nA 'Proposed' CIP is any CIP that meets the essential CIP criteria but is not yet 'Active'. The criteria that must meet a CIP to be merged as 'Proposed' are:\nIt must contain all the sections described in Structure.\nThe quality of the content must be to the Editors satisfaction. That means it must be grammatically sound, well-articulated and demonstrates noticeable efforts in terms of completeness and level of detail.\nIts technical soundness should have been established. Where necessary, this may require review by particular experts and addressing their concerns. Note that the requirement is that the proposal makes sense (i.e. be technically sound), yet no consulted experts need to think it is a good idea.\nIt must have a valid Path to Active as defined below.\nAn 'Active' CIP has taken effect according to the criteria defined in its 'Path to Active' section. Said differently, each CIP defines by which criteria it can become 'Active' and those criteria are included in the review process. Exact criteria thereby depends on the nature of the CIP, typically:\nFor CIPs that relate to particular projects or pieces of technology, it becomes 'Active' by being implemented and released;\nFor changes to the Cardano protocol, a CIP becomes 'Active' by being live on the Cardano mainnet;\nFor ecosystem standards, it means gaining sufficient and visible adoption in the community.\nIt must have a valid Path to Active as defined below: even the CIP is already acknowledged as Active or being documented retroactively (after acceptance and implementation).\nActive\nA proposal that is 'Active' is considered complete and is synonymous with \"production readiness\" when it comes to the maturity of a solution. 'Active' CIPs will not be updated substantially (apart from minor edits, proofreading and added precisions). They can, nevertheless, be challenged through new proposals if need be.\nAn 'Inactive' CIP describes any proposal that does not fit into the other types. A CIP can therefore be 'Inactive' for various reasons (e.g. obsolete, superseded, abandoned). Hence the status must indicate a justification between parentheses; for example:\nStatus: Inactive (superseded by CIP-0001)\nStatus: Inactive (superseded by CIP-0001)\nThis must be subdivided into two sub-sections:\n'Acceptance Criteria' This sub-section must define a list of criteria by which the proposal can become active. Criteria must relate to observable metrics or deliverables and be reviewed by editors and project maintainers when applicable. For example: \"The changes to the ledger rules are implemented and deployed on Cardano mainnet by a majority of the network\", or \"The following key projects have implemented support for this standard\".\n'Acceptance Criteria'\nThis sub-section must define a list of criteria by which the proposal can become active. Criteria must relate to observable metrics or deliverables and be reviewed by editors and project maintainers when applicable. For example: \"The changes to the ledger rules are implemented and deployed on Cardano mainnet by a majority of the network\", or \"The following key projects have implemented support for this standard\".\n'Implementation Plan' This sub-section should define the plan by which a proposal will meet its acceptance criteria, when applicable. More, proposals that require implementation work in a specific project may indicate one or more implementors. Implementors must sign off on the plan and be referenced in the document's preamble. In particular, an implementation that requires a hard-fork should explicitly mention it in its 'Implementation Plan'.\n'Implementation Plan'\nThis sub-section should define the plan by which a proposal will meet its acceptance criteria, when applicable. More, proposals that require implementation work in a specific project may indicate one or more implementors. Implementors must sign off on the plan and be referenced in the document's preamble.\nIn particular, an implementation that requires a hard-fork should explicitly mention it in its 'Implementation Plan'.\nNote the statuses of Proposed and Active both require a Path to Active section, making this a required section for all viable proposals. Even if a CIP is edited or submitted with an Inactive status, it may still be helpful to have a Path to Active if there are conditions that might lead to its acceptance or implementation.\nProposed\nActive\nInactive\nPath to Active\nCIPs are classified into distinct categories that help organise (and thus, find) them. Categories are meant to be flexible and evolve as new domains emerge. Authors may leave the category as ? should they not be sure under which category their proposal falls; editors will eventually assign one or reject the proposal altogether should it relate to an area where the CIP process does not apply.\n?\nSubmissions can be made to these categories whenever relevant, without following any particular submission guidelines:\nAdditionally, representatives of ecosystem categories may explicitly enlist their categories (see next section) to suggest a closer relationship with the CIP process. The following categories are confirmed as enlisted according to CIPs which define that relationship:\nThese tentatively enlisted categories await CIPs to describe any enlistment relationship:\nProject representatives intending to follow an \"enlisted\" category above agree to coordinate with related development by sharing efforts to review and validate new proposals. It should be noted that single organisations can no longer represent any ecosystem or development category, which makes these enlistment guidelines both decentralised and cooperative, including whenever possible:\nallocating time to review proposals from actors of the community when solicited by editors (i.e. after one first round of reviews);\ndefining additional rules and processes whereby external actors can engage with their project as part of the CIP process;\ndefining boundaries within their project for which the CIP process does apply;\nestablishing points of contact and any designated reviews for a category;\nagreeing upon how proposals move from the state of idea (i.e. CIP) to actual implementation work;\nwriting CIPs for significant changes introduced in their projects when it applies.\nAny guidelines for this cooperation should be described by a dedicated CIP whenever possible. When such a CIP is posted or supersedes another one, it will be entered into the above table in the Categories section. Participants of enlisted categories should follow the requirements outlined in that CIP and should update such proposals whenever these requirements or relationships change.\nWarning A positive review by any enlisted project representative does not constitute a commitment to implement the CIP. It is still the CIP author's responsibility to create an implementation plan and identify implementors.\nEditors occasionally invite representatives from enlisted categories to speak during review meetings and solicit them for ultimate approvals of proposals in their area of expertise.\nNote Optionally, projects may show their enlisting using the following badge on their introductory README:\n![https://github.com/cardano-foundation/CIPs](https://raw.githubusercontent.com/cardano-foundation/CIPs/master/.github/badge.svg)\n![https://github.com/cardano-foundation/CIPs](https://raw.githubusercontent.com/cardano-foundation/CIPs/master/.github/badge.svg)\nProposals must be submitted to the cardano-foundation/CIPs repository as a pull request named after the proposal's title. The pull request title should not include a CIP number (and use ? instead as number); the editors will assign one. Discussions may precede a proposal. Early reviews and discussions streamline the process down the line.\n?\nNote Pull requests should not include implementation code: any code bases should instead be provided as links to a code repository.\nNote Proposals addressing a specific CPS should also be listed in the corresponding CPS header, in 'Proposed Solutions', to keep track of ongoing work.\nWhen a CIP title and subject matter share a common element, begin the CIP title with that common element and end it with the specific portion, delimited with the - character. Example (CIP-0095):\n-\nWeb-Wallet Bridge - Governance\nCIP editors will help determine these common elements and, whenever necessary, rename both CIP document titles and PR titles accordingly. The objective is to provide commonly recognisable names for similar developments (e.g. multiple extensions to another CIP or scheme).\nIn the original comment for your pull request, please include a link to the directory or the README.md for the CIP in your working branch, so readers and reviewers can easily follow your work. This makes it easier for editors and the community to read and review your proposal.\nREADME.md\nNote If this link changes (e.g. from the CIP directory being renamed), please keep this link updated.\nAuthors shall champion their proposals. The CIP process is a collaborative effort, which implies discussions between different groups of individuals. While editors may provide specific inputs and help reach out to experts, authors shall actively seek feedback from the community to help move their proposal forward.\nDiscussions and comments shall mainly happen on Github in pull requests. When discussed on other mediums, we expect authors or participants to report back a summary of their discussions to the original pull request to keep track of the most critical conversations in a written form and all in one place.\nAs much as possible, commenters/reviewers shall remain unbiased in their judgement and assess proposals in good faith. Authors have the right to reject comments or feedback but are strongly encouraged to address concerns in their 'Rationale' section. Ultimately, CIP editors shall make the last call concerning the various statements made on a proposal and their treatment by the author(s).\nBy opening pull requests or posting comments, commenters and authors agree to our Code of Conduct. Any comment infringing this code of conduct shall be removed or altered without prior notice.\nNote For acceptability guidelines, including a concise review checklist, see CIP Wiki CIPs for Reviewers & Authors.\nCIP editors meet regularly in a public Discord server to go through newly proposed ideas in a triage phase. As a result of a triage, editors acknowledge new CIPs, and briefly review their preamble. Editors also assign numbers to newly proposed CIPs during this phase. Incidentally, the triage allows new CIPs to get visibility for potential reviews.\nIn every meeting, editors will also review in more depth some chosen CIPs (based on their readiness and the stability of the discussions) and assess if they meet the criteria to be merged in their aimed status.\nDuring review sessions, editors will regularly invite project maintainers or actors from the ecosystem who are deemed relevant to the meeting's agenda. However, meetings are open and held in public to encourage anyone interested in participating.\nA dedicated Discord channel may also be created for some long-running discussions to support quick chats and progress on particular topics (while still being regularly summarised on the repository).\nOnce a proposal has reached all requirements for its target status (as explained in Statuses) and has been sufficiently and faithfully discussed by community members, it is merged with its target status.\nWarning Ideas deemed unsound shall be rejected with justifications or withdrawn by the authors. Similarly, proposals that appear abandoned by their authors shall be rejected until resurrected by their authors or another community member.\nCIPs are generally merged with the status 'Proposed' until they meet their 'Path to Active' requirements. In some rare cases (mainly when written after the facts and resulting in a broad consensus), proposals may be merged as 'Active' immediately.\nEach proposal is unique and has a bespoke 'Path to Active', which must be reviewed case-by-case. There must be sufficient time between the first appearance of a proposal and its merge into the repository to ensure enough opportunity for community members to review it.\nOnce merged, implementors shall execute the CIP's 'Implementation Plan', if any. If a proposal has no implementors or no 'Implementation Plan', it may simply remain as 'Proposed' in the repository.\nWarning It is perfectly fine to submit ideas in the repository with no concrete implementation plan, yet they should be treated as such: ideas.\nBesides, once all of the 'Path to Active' requirements have been met, authors shall make another pull request to change their CIP's status to 'Active'. Editors may also do this on occasion.\nFor a full, current description of Editor workflow, see CIP Wiki CIPs for Editors.\nCIP Editors safeguard the CIP process: they form a group enforcing the process described in this document and facilitating conversations between community actors. CIP editors should strive to keep up to date with general technical discussions and Cardano proposals. For each new draft proposal submitted on cardano-foundation/CIPs an editor might review it as follows:\nRead the proposal to check if it is ready, sound, and complete.\nCheck if it has been properly formatted.\nCheck if sufficient time has been allowed for proper discussion amongst the community.\nEnsure the motivation behind the CIP is valid and that design choices have relevant justifications or rationale.\nConfirm licensing terms are acceptable.\nAssign a CIP number\nAssign a given category to help with searching\nRequest wording/grammar adjustments\nCIPs that do not meet a sufficient level of quality or don't abide by the process described in this document will be rejected until their authors address review comments.\nNote that editors may provide technical feedback on proposals in some cases, although they aren't expected to be the sole technical reviewers of proposals. CIPs are, before anything, a community-driven effort. While editors are here to facilitate the discussion and mediate debates, they aren't necessarily technical experts on all subjects covered by CIPs.\nTherefore, CIPs authors are encouraged to reach out to known experts to demonstrate their good faith and openness when they champion a proposal. Editors may help with such efforts but cannot be expected to do this alone.\nExisting editors or the community may nominate new editors, provided they have proven to be already existing and active contributors to the CIP process and are ready to commit some of their time to the CIP process regularly.\nThe missions of an editor include, but aren't exclusively limited to, any of the tasks listed above. Active members that seek to become listed editors may also come forth and let it be known. Any application will take the form of a pull request updating this document with a justification as the pull request's description.\nCurrent editors are listed here below:\nEmeritus editors:\nFrederic Johnson - @crptmppt\nSebastien Guillemot - @SebastienGllmt\nMatthias Benkort - @KtorZ\nDuncan Coutts - @dcoutts\nA significant friction point regarding complex CIPs is often how the main problem is stated. The 'Motivation' is often insufficient (or simply underused) to describe various aspects of a problem, its scope, and its constraints. This lack of clarity leads, in the end, to poorly defined issues and debates over solutions that feel unclear amongst different participants.\nThe introduction of CIP-9999: Cardano Problem Statements addresses this gap by introducing a formal template and structure around problem statements. However, requiring any CIP to be preceded by a CPS would likely be overkill and become an obstacle to the overall adoption of the CIP process for more straightforward problems. At this stage, it is reasonable to think either (a) that CIP authors would foresee the complexity and state their problem as a CPS beforehand or (b) that editors or reviewers will require authors to write a CPS to clarify a perhaps ambiguous motivation on complex CIPs.\nWe also anticipate project maintainers or community actors will write standalone CPS to document well-known issues for which the design space is still to be explored.\nA recurring pain point with the previous CIP process was the lack of clear ownership/accountability of some proposals affecting remote parts of the ecosystem. On several occasions, proposals from community members have concerned, for example, core components of the Cardano architecture. However, those proposals have been hard to move forward with and to either reject or turn into concrete action steps. Authors usually do not have the technical proficiency required to implement them and rely on the core engineering team in charge of projects to do so. Thus, explicit compliance and collaboration of those engineering teams are necessary to propose changes affecting their work.\nBy asking teams to explicitly state their compliance with the CIP process with clear, accountable owners (as individuals), it becomes plausible to now establish a dialogue between community members and technical leadership responsible for specific ecosystem projects. Furthermore, projects that, on the contrary, do not seek to participate in CIP or receive contributions in the form of CIP/CPS are automatically taken out of this process, saving time and energy for both reviewers and authors.\nThe 'Editors' section now details how to become a CIP editor. The process aims to be simple and define those involved the most with editorship tasks as editors. Thus, being an active contributor to the CIP process as a prerequisite only makes sense. We want to leave the room open for either existing editors to refer an existing community as an editor or for community members to formulate that request explicitly.\nThere are no delays or number of contributions necessary to pretend to become an editor. Those criteria are often less relevant than others and more subjective, such as the quality of one's participation or their relevance. Since editors also need to work with one another in the end, it seems logical that existing editors have their final say about whom they intend to work with.\ntype\nThe type field in the header has shown to be:\ntype\nconfusing (often authors are getting it wrong);\nnot-too-useful (as a type tells you very little about the nature of the CIP).\ntype\nAn ad-hoc classification by non-rigid categories, which may evolve over time to reflect ecosystem areas, seems better suited. Therefore, we do not require authors to categorise their CIPs; instead, categories will be added and maintained by editors as a side task atop the CIP process.\nOver time we've learnt that the valuable information a status should convey is really about the readiness of a CIP, especially regarding standards. For a long time, many CIPs have lived as Draft despite some being used in dozens of systems. Consequently, the status has lost a bit of its meaning. The frontier between Draft and Proposed hasn't always been clear, and it has proven challenging to come up with good statuses to describe all possible rejections. So instead, the current division of statuses is as simple-as-it-can-be and remains flexible regarding rejections.\nDraft\nstatus\nDraft\nProposed\nThe choice of a code of conduct follows other popular open source initiatives. It serves as a good, unilaterally accepted foundation which can be later revisited if necessary.\nThe proposal has been reviewed by the community and sufficiently advertised on various channels. Cardano Forum IOG Technical Community Discord Twitter Reddit Cardano Summit 2022 IO ScotFest 2022\nThe proposal has been reviewed by the community and sufficiently advertised on various channels.\nCardano Forum\nIOG Technical Community Discord\nTwitter\nReddit\nCardano Summit 2022\nIO ScotFest 2022\nAll major concerns or feedback have been addressed. The document is as unambiguous as it can be and it outlines a process that a supermajority of reviewers is happy to follow.\nAll major concerns or feedback have been addressed. The document is as unambiguous as it can be and it outlines a process that a supermajority of reviewers is happy to follow.\nRework existing draft CIPs to adopt this new format and process. In particular, CIPs affecting enlisted projects should be brought to the attention of the respective project maintainers.\nEdit / align old CIPs preambles and sections to at least reflect also this new format.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0002 | Coin Selection Algorithms for Cardano\n\nThis article describes, in human-readable terms, the coin selection algorithms used by Cardano Wallet and other parts of the Cardano ecosystem.\nIn the context of this article, coin selection refers to the process of choosing unspent coins from a wallet (or UTxO set) in order to pay money to one or more recipients.\nThis document was written to help people gain an understanding of the coin selection algorithms used in Cardano without having to read through and understand Cardano source code.\nWe aim to provide descriptions of algorithms that:\ndon't require prior experience with any particular programming language;\nare understandable for people who are unfamiliar with coin selection;\nare precise enough for software engineers to be able to reimplement these algorithms in their preferred programming languages.\nWhere appropriate, we also provide mathematical descriptions, for added clarity.\nCoin selection is a large, complex topic, with many difficult problems to solve. However, all software that performs coin selection must ultimately deal with at least the following pair of problems:\nHow to generate a coin selection, by choosing unspent coins from a wallet (or UTxO set) in order to pay money to one or more recipients.\nHow to adjust a coin selection in order to pay for a network fee, so that the coin selection can be published as a transaction on the blockchain.\nThis article concerns itself with the former problem of how to generate a coin selection.\nProblems relating to network fees, and how to adjust coin selections to pay for such fees, are outside the scope of this article.\nThis section introduces the fundamental concepts behind coin selection, provides a discussion of why coin selection is a non-trivial problem, and describes important goals of coin selection algorithms.\nCoin selection is the process of choosing unspent coins from a wallet in order to pay money to one or more recipients.\nIn the familiar world of physical money, our wallets hold value in the form of coins and banknotes.\nWhen making a payment to someone, we typically select a combination of coins and banknotes from a wallet that, when added together, have enough value to cover the amount required.\nIdeally, we'd always be able to select just enough to cover the exact amount. However, given that coins and banknotes have fixed values (and cannot be subdivided without destroying their value), it's often impossible to select the exact amount required. In such cases, we typically give the recipient more than the required amount, and then receive the excess value back as change.\n:bulb: Example\nAlice wishes to pay for her lunch.\nThe total price comes to 2.50 (2 euros and 50 cents). In her wallet, she has five one-euro coins, and one ten-euro note.\nNote that there is no combination of coins (or notes) in her wallet that when added together give a total of 2.50, but there are several possible combinations that exceed the total.\nTo solve this problem, Alice selects one of these combinations: three one-euro coins. She uses the coins to make the payment, and then receives one 50-cent coin as change.\nSimilarly to how a physical wallet holds value in the form of unspent coins and banknotes, a Cardano wallet holds value in the form of unspent transaction outputs. An unspent transaction output is the result of a previous transaction that transferred money to the wallet, where the value has not yet been spent by another transaction. Each unspent transaction output has an associated coin value, and the total value of a wallet is the sum of these coin values. Collectively, the set of unspent transaction outputs is known as the UTxO set.\nWhen using a Cardano wallet to make a payment, the wallet software must select a combination of unspent outputs from the wallet's UTxO set, so that the total value of selected outputs is enough to cover the target amount.\nJust as with physical coins and notes, unspent outputs from the UTxO set cannot be subdivided, and must either be spent completely in a given transaction, or not be spent at all. Similarly to a transaction with physical money, the wallet software must select a combination of unspent outputs whose total value is greater than the target amount, and then arrange that change is paid back to the wallet.\nCoin selection refers to the process of selecting a combination of unspent outputs from a wallet's UTxO set in order to make one or more payments, and computing the set of change to be paid back to the wallet.\nThere are a number of issues which make the problem of coin selection more complicated than would initially appear.\nEach transaction has a maximum size, as defined by the protocol. The size of a transaction increases as we add more inputs or outputs.\nTherefore, there's a practical limit on the number of coins we can select for any given transaction.\nOne simple strategy for selecting coins might be to mimic what we do when making a payment with coins and banknotes in the physical world. By giving the recipient an amount that's as close as possible to the amount they're expecting, we can minimize the amount of change they need to return to us.\nHowever, trying to replicate this strategy with a UTxO-based wallet has an undesirable effect: minimizing the total value of selected coins will also minimize the value of change returned to the wallet. When repeated over time, this strategy will tend to cause an accumulation of tiny outputs in the wallet's UTxO set known as dust.\nDust outputs are a problem, because even if the total value of dust in a wallet is more than enough to cover a given target amount, if we attempt to include that dust in a given transaction, we may run out of space (by reaching the transaction size limit) before we can cover the target amount.\nFor more information on dust avoidance, see Self Organisation in Coin Selection.\nOne simple strategy for generating change might be to mimic what a shop assistant does when accepting a payment in the real world, which is to minimize the number of coins and banknotes that they return to the customer. This is beneficial for the shop, as it reduces the chances of them running out of change, and beneficial for the customer, as it reduces the amount of change that they have to carry around in their wallet.\nAnalogously, when generating change for a UTxO-based wallet, we might be tempted to use the simple strategy of just creating a single change output with the exact excess value.\nHowever, using this strategy has an undesirable effect: the repeated act of minimizing the number of change outputs will tend (over time) to reduce the number of entries in a wallet's UTxO set. This is bad for two reasons:\nHaving a small UTxO set limits the number of future payments that we can make in parallel. The approach of coalescing all change into a single output is widely considered to have negative privacy implications.\nHaving a small UTxO set limits the number of future payments that we can make in parallel.\nHaving a small UTxO set limits the number of future payments that we can make in parallel.\nThe approach of coalescing all change into a single output is widely considered to have negative privacy implications.\nThe approach of coalescing all change into a single output is widely considered to have negative privacy implications.\nIn light of the issues described above, we'd ideally like for our coin selection algorithms to be able to:\nlimit, over the course of time, the amount of dust that accumulates in the UTxO set.\nlimit, over the course of time, the amount of dust that accumulates in the UTxO set.\nmaintain, over the course of time, a UTxO set with useful outputs: that is, outputs that allow us to process future payments with a reasonably small number of inputs.\nmaintain, over the course of time, a UTxO set with useful outputs: that is, outputs that allow us to process future payments with a reasonably small number of inputs.\nThe Background section introduces the fundamental concepts behind coin selection, provides a discussion of why coin selection is a non-trivial problem, and describes the goals of coin selection algorithms.\nThe Interface section gives a description of the common interface unifying all coin selection algorithms used within Cardano Wallet, the standard parameter types, result types, and error types used by this interface, and a description of the properties that all conforming implementations are expected to satisfy.\nThe Algorithms section gives detailed descriptions of each of the individual coin selection algorithms used in Cardano Wallet, along with step-by-step descriptions of the computations involved.\nThe Reference Implementations section provides links to reference implementations of each algorithm in various languages.\nAbstract\nMotivation Scope Background What is Coin Selection? Coin Selection in the Physical World Coin Selection in Cardano Why is Coin Selection Non-Trivial? The Transaction Size Limitation The Problem of Dust The Problem of Concurrency Goals of Coin Selection Algorithms\nScope\nBackground What is Coin Selection? Coin Selection in the Physical World Coin Selection in Cardano Why is Coin Selection Non-Trivial? The Transaction Size Limitation The Problem of Dust The Problem of Concurrency Goals of Coin Selection Algorithms\nWhat is Coin Selection? Coin Selection in the Physical World Coin Selection in Cardano\nCoin Selection in the Physical World\nCoin Selection in Cardano\nWhy is Coin Selection Non-Trivial? The Transaction Size Limitation The Problem of Dust The Problem of Concurrency\nThe Transaction Size Limitation\nThe Problem of Dust\nThe Problem of Concurrency\nGoals of Coin Selection Algorithms\nSpecification Structure Definitions Address Coin Value Transaction Transaction Input Transaction Output Spent Transaction Output Unspent Transaction Output UTxO Set Change Output Dust Output Interface Parameters Requested Output Set Initial UTxO Set Maximum Input Count Results Coin Selection Remaining UTxO Set Properties Coverage of Payments Correctness of Change Conservation of UTxO Conservation of Outputs Failure Modes UTxO Balance Insufficient UTxO Not Fragmented Enough UTxO Fully Depleted Maximum Input Count Exceeded Algorithms Largest-First State Available UTxO List Unpaid Output List Accumulated Coin Selection Computation Random-Improve Cardinality State Available UTxO Set Accumulated Coin Selection Computation Phase 1: Random Selection Phase 2: Improvement Termination\nStructure\nDefinitions Address Coin Value Transaction Transaction Input Transaction Output Spent Transaction Output Unspent Transaction Output UTxO Set Change Output Dust Output\nAddress\nCoin Value\nTransaction\nTransaction Input\nTransaction Output\nSpent Transaction Output\nUnspent Transaction Output\nUTxO Set\nChange Output\nDust Output\nInterface Parameters Requested Output Set Initial UTxO Set Maximum Input Count Results Coin Selection Remaining UTxO Set Properties Coverage of Payments Correctness of Change Conservation of UTxO Conservation of Outputs Failure Modes UTxO Balance Insufficient UTxO Not Fragmented Enough UTxO Fully Depleted Maximum Input Count Exceeded\nParameters Requested Output Set Initial UTxO Set Maximum Input Count\nRequested Output Set\nInitial UTxO Set\nMaximum Input Count\nResults Coin Selection Remaining UTxO Set\nCoin Selection\nRemaining UTxO Set\nProperties Coverage of Payments Correctness of Change Conservation of UTxO Conservation of Outputs\nCoverage of Payments\nCorrectness of Change\nConservation of UTxO\nConservation of Outputs\nFailure Modes UTxO Balance Insufficient UTxO Not Fragmented Enough UTxO Fully Depleted Maximum Input Count Exceeded\nUTxO Balance Insufficient\nUTxO Not Fragmented Enough\nUTxO Fully Depleted\nMaximum Input Count Exceeded\nAlgorithms Largest-First State Available UTxO List Unpaid Output List Accumulated Coin Selection Computation Random-Improve Cardinality State Available UTxO Set Accumulated Coin Selection Computation Phase 1: Random Selection Phase 2: Improvement Termination\nLargest-First State Available UTxO List Unpaid Output List Accumulated Coin Selection Computation\nState Available UTxO List Unpaid Output List Accumulated Coin Selection\nAvailable UTxO List\nUnpaid Output List\nAccumulated Coin Selection\nComputation\nRandom-Improve Cardinality State Available UTxO Set Accumulated Coin Selection Computation Phase 1: Random Selection Phase 2: Improvement Termination\nCardinality\nState Available UTxO Set Accumulated Coin Selection\nAvailable UTxO Set\nAccumulated Coin Selection\nComputation Phase 1: Random Selection Phase 2: Improvement\nPhase 1: Random Selection\nPhase 2: Improvement\nTermination\nRationale: how does this CIP achieve its goals? Motivating Principles LargestFirst Random-Improve Principle 1: Dust Management Principle 2: Change Management Principle 3: Performance Management External Resources Self Organisation in Coin Selection\nMotivating Principles LargestFirst Random-Improve Principle 1: Dust Management Principle 2: Change Management Principle 3: Performance Management\nLargestFirst\nRandom-Improve Principle 1: Dust Management Principle 2: Change Management Principle 3: Performance Management\nPrinciple 1: Dust Management\nPrinciple 2: Change Management\nPrinciple 3: Performance Management\nExternal Resources Self Organisation in Coin Selection\nSelf Organisation in Coin Selection\nPath to Active Acceptance Criteria Implementation Plan Reference Implementations Largest-First Random-Improve\nAcceptance Criteria\nImplementation Plan Reference Implementations Largest-First Random-Improve\nReference Implementations Largest-First Random-Improve\nLargest-First\nRandom-Improve\nCopyright\nThis section defines common terms that are used throughout this document.\nAn address is a unique identifier that represents a payment recipient, a destination for a payment.\nAddresses are typically owned (and generated) by individual wallets.\nIn general, coin selection algorithms are agnostic to the type of addresses used to identify payment recipients. Any address type may be used, so long as the set of possible addresses is totally-ordered.\nA coin value is a non-negative integer value that represents a number of Lovelace.\nOne Ada is exactly equal to one million Lovelace.\nIn a UTxO-based blockchain, a transaction is a binding between inputs and outputs.\ninput #1 >---+ +---> output #1 \\ / input #2 >-----+------+ / \\ input #3 >---+ +---> output #2\ninput #1 >---+ +---> output #1 \\ / input #2 >-----+------+ / \\ input #3 >---+ +---> output #2\nA transaction input is a unique reference to a single output from a previous transaction.\nIn general, coin selection algorithms are agnostic to the type of references used to identify outputs from previous transactions. Any type may be used, so long as the set of possible references is totally-ordered, and so long as it is possible to determine the coin value associated with any given reference.\nIn the case of Cardano and other UTxO-based blockchains, this reference generally consists of a pair of values (h, n), where:\nh is a unique identifier for an existing transaction t;\nn is a 0-based integer index into the output list of transaction t.\nA transaction output consists of a pair of values (a, v), where:\na is the address of a recipient.\nv is the coin value to pay to the recipient.\nA spent transaction output is an output from an existing transaction that has already been referenced as an input within a later transaction on the blockchain.\nIn effect, the coin value associated with that transaction output has been spent, and cannot be reused.\nAn unspent transaction output is an output from an existing transaction that has not yet been referenced as an input within a later transaction.\nIn effect, the coin value associated with that transaction output has not yet been spent, and is still available.\nA UTxO set is a set of unspent transaction outputs.\nThis term is commonly used in two ways:\nTo describe the complete set of all unspent transaction outputs within a blockchain.\nTo describe the complete set of all unspent transaction outputs within a blockchain.\nTo describe the subset of unspent transaction outputs associated with a wallet. The UTxO set of a wallet represents the total unspent value associated with that wallet.\nTo describe the subset of unspent transaction outputs associated with a wallet. The UTxO set of a wallet represents the total unspent value associated with that wallet.\nFrom the point of view of a coin selection algorithm, each member of a UTxO set can be represented as a pair of the form (u, v), where:\nu is a unique reference to an unspent output from a previous transaction.\nv is the coin value associated with u.\nIn general, coin selection algorithms are agnostic to the type of references used to identify unspent outputs from previous transactions. Any type may be used, so long as the set of possible references is totally-ordered.\nIn practice however, the type of each unique reference u is equivalent to the type of a transaction input, as transaction inputs are simply references to unspent outputs from previous transactions.\nIn the context of a wallet, a change output is a transaction output that transfers value back to the wallet, rather than to an external payment recipient. The address associated with a change output is generated by the wallet, and belongs to the wallet.\nChange ouputs are necessary in a UTxO-based blockchain, as the value associated with any given transaction input must be spent entirely by the transaction that includes it.\nWhen selecting entries from a UTxO set to include as inputs in a transaction, a coin selection algorithm will generally not be able to select inputs that precisely match the total value of all payments to external recipients, and will therefore need to select more than is strictly required. To avoid the destruction of value, selection algorithms create change outputs to return the excess value back to the wallet.\nA dust output is a transaction output with an associated coin value that is:\nsmall in comparison to payments typically made by the user of the wallet;\nsmall in comparison to the marginal fee associated with including it in a transaction.\nDust outputs are a problem, because even if the total value of dust in a wallet is more than enough to cover a given payment amount, if we attempt to include that dust in a given transaction, we may run out of space (by reaching the transaction size limit) before we can cover the target amount.\nAll coin selection algorithms used by Cardano Wallet implement a common interface.\nAt the most fundamental level, a coin selection algorithm is a mathematical function that when applied to a requested output set and an initial UTxO set, will produce a coin selection: the basis for a transaction in a UTxO-based blockchain.\nThis section describes:\nthe parameters accepted by all coin selection algorithms;\nthe results they produce when successful;\nthe error conditions that may occur on failure;\nthe properties that apply to all coin selection algorithms: mathematical laws governing the relationships between parameters and results.\nIn this section, the terms coin selection algorithm and coin selection function will be used interchangeably.\nAll coin selection functions accept the following parameters:\nRequested Output Set A list of payments to be made to recipient addresses, encoded as a list of transaction outputs. Initial UTxO Set A UTxO set from which the coin selection algorithm can select entries, to cover payments listed in the requested output set. In the context of a wallet, this parameter would normally be assigned with the wallet's complete UTxO set, giving the coin selection algorithm access to the total value associated with that wallet. Maximum Input Count An upper bound on the number of UTxO entries that the coin selection algorithm is permitted to select from the initial UTxO set. This parameter is necessary for blockchains that impose an upper limit on the size of transactions.\nRequested Output Set A list of payments to be made to recipient addresses, encoded as a list of transaction outputs.\nRequested Output Set\nA list of payments to be made to recipient addresses, encoded as a list of transaction outputs.\nInitial UTxO Set A UTxO set from which the coin selection algorithm can select entries, to cover payments listed in the requested output set. In the context of a wallet, this parameter would normally be assigned with the wallet's complete UTxO set, giving the coin selection algorithm access to the total value associated with that wallet.\nInitial UTxO Set\nA UTxO set from which the coin selection algorithm can select entries, to cover payments listed in the requested output set.\nIn the context of a wallet, this parameter would normally be assigned with the wallet's complete UTxO set, giving the coin selection algorithm access to the total value associated with that wallet.\nMaximum Input Count An upper bound on the number of UTxO entries that the coin selection algorithm is permitted to select from the initial UTxO set. This parameter is necessary for blockchains that impose an upper limit on the size of transactions.\nMaximum Input Count\nAn upper bound on the number of UTxO entries that the coin selection algorithm is permitted to select from the initial UTxO set.\nThis parameter is necessary for blockchains that impose an upper limit on the size of transactions.\nAll coin selection functions produce the following result values:\nCoin Selection A coin selection is the basis for a transaction in a UTxO-based blockchain. It is a record with three fields: A set of inputs, equivalent to a subset of the initial UTxO set. From the point of view of a wallet, this represents the value that has been selected from the wallet in order to cover the total payment value. A set of outputs (see transaction output). Represents the set of payments to be made to recipient addresses. A set of change values (see change output), where each change value is simply a coin value. From the point of view of a wallet, this represents the change to be returned to the wallet. Remaining UTxO Set The remaining UTxO set is a subset of the initial UTxO set. It represents the set of values that remain after the coin selection algorithm has removed values to pay for entries in the requested output set. In the context of a wallet, if a coin selection algorithm is applied to the wallet's complete UTxO set, then the remaining UTxO set represents the updated UTxO set of that wallet.\nCoin Selection A coin selection is the basis for a transaction in a UTxO-based blockchain. It is a record with three fields: A set of inputs, equivalent to a subset of the initial UTxO set. From the point of view of a wallet, this represents the value that has been selected from the wallet in order to cover the total payment value. A set of outputs (see transaction output). Represents the set of payments to be made to recipient addresses. A set of change values (see change output), where each change value is simply a coin value. From the point of view of a wallet, this represents the change to be returned to the wallet.\nCoin Selection\nA coin selection is the basis for a transaction in a UTxO-based blockchain.\nIt is a record with three fields:\nA set of inputs, equivalent to a subset of the initial UTxO set. From the point of view of a wallet, this represents the value that has been selected from the wallet in order to cover the total payment value.\nA set of inputs, equivalent to a subset of the initial UTxO set.\nFrom the point of view of a wallet, this represents the value that has been selected from the wallet in order to cover the total payment value.\nA set of outputs (see transaction output). Represents the set of payments to be made to recipient addresses.\nA set of outputs (see transaction output).\nRepresents the set of payments to be made to recipient addresses.\nA set of change values (see change output), where each change value is simply a coin value. From the point of view of a wallet, this represents the change to be returned to the wallet.\nA set of change values (see change output), where each change value is simply a coin value.\nFrom the point of view of a wallet, this represents the change to be returned to the wallet.\nRemaining UTxO Set The remaining UTxO set is a subset of the initial UTxO set. It represents the set of values that remain after the coin selection algorithm has removed values to pay for entries in the requested output set. In the context of a wallet, if a coin selection algorithm is applied to the wallet's complete UTxO set, then the remaining UTxO set represents the updated UTxO set of that wallet.\nRemaining UTxO Set\nThe remaining UTxO set is a subset of the initial UTxO set.\nIt represents the set of values that remain after the coin selection algorithm has removed values to pay for entries in the requested output set.\nIn the context of a wallet, if a coin selection algorithm is applied to the wallet's complete UTxO set, then the remaining UTxO set represents the updated UTxO set of that wallet.\nAll coin selection algorithms satisfy a common set of properties: general rules that govern the relationship between the parameters supplied to coin selection functions and the results they are allowed to produce.\nThis property states that the total value of inputs in the resulting coin selection result is sufficient to cover the total value of the requested output set.\nIn particular:\nvselected vrequested\nWhere:\nvrequested is the total value of the requested output set\nvrequested\nis the total value of the requested output set\nvselected is the total value of the inputs field of the coin selection result.\nvselected\nis the total value of the inputs field of the coin selection result.\nThis property states that the correct amount of change was generated.\nIn particular:\nvselected = vrequested + vchange\nWhere:\nvchange is the total value of the change field of the coin selection result.\nvchange\nis the total value of the change field of the coin selection result.\nvrequested is the total value of the requested output set\nvrequested\nis the total value of the requested output set\nvselected is the total value of the inputs field of the coin selection result.\nvselected\nis the total value of the inputs field of the coin selection result.\nThis property states that every entry in the initial UTxO set is included in either the inputs set of the generated coin selection, or in the remaining UTxO set, but not both.\nIf a UTxO entry is selected by the coin selection algorithm, it is included in the coin selection inputs set.\nIf a UTxO entry is selected by the coin selection algorithm, it is included in the coin selection inputs set.\nIf a UTxO entry is not selected by the coin selection algorithm, it is included in the remaining UTxO set.\nIf a UTxO entry is not selected by the coin selection algorithm, it is included in the remaining UTxO set.\nThe following laws hold:\nUinitial Uremaining\nUinitial Uselected\nAnd:\nUremaining Uselected =\nUremaining Uselected = Uinitial\nWhere:\nUinitial is the initial UTxO set.\nUinitial\nis the initial UTxO set.\nUremaining is the remaining UTxO set.\nUremaining\nis the remaining UTxO set.\nUselected is the value of the inputs field of the coin selection result.\nUselected\nis the value of the inputs field of the coin selection result.\nThis property states that the requested output set is conserved in the coin selection result.\nIn particular, the outputs field of the coin selection result should be equal to the requested output set.\nThere are a number of ways in which a coin selection algorithm can fail:\nUTxO Balance Insufficient This failure occurs when the total value of the entries within the initial UTxO set (the amount of money available) is less than the the total value of all entries in the requested output set (the amount of money required).\nUTxO Balance Insufficient\nThis failure occurs when the total value of the entries within the initial UTxO set (the amount of money available) is less than the the total value of all entries in the requested output set (the amount of money required).\nUTxO Not Fragmented Enough This failure occurs when the number of entries in the initial UTxO set is smaller than the number of entries in the requested output set, for algorithms that impose the restriction that a single UTxO entry can only be used to pay for at most one output.\nUTxO Not Fragmented Enough\nThis failure occurs when the number of entries in the initial UTxO set is smaller than the number of entries in the requested output set, for algorithms that impose the restriction that a single UTxO entry can only be used to pay for at most one output.\nUTxO Fully Depleted This failure occurs if the algorithm depletes all entries from the initial UTxO set before it is able to pay for all outputs in the requested output set. This can happen even if the total value of entries within the initial UTxO set is greater than the total value of all entries in the requested output set, due to various restrictions that coin selection algorithms impose on themselves when selecting UTxO entries.\nUTxO Fully Depleted\nThis failure occurs if the algorithm depletes all entries from the initial UTxO set before it is able to pay for all outputs in the requested output set.\nThis can happen even if the total value of entries within the initial UTxO set is greater than the total value of all entries in the requested output set, due to various restrictions that coin selection algorithms impose on themselves when selecting UTxO entries.\nMaximum Input Count Exceeded This failure occurs when another input must be selected by the algorithm in order to continue making progress, but doing so will increase the size of the resulting selection beyond an acceptable limit, specified by the maximum input count parameter.\nMaximum Input Count Exceeded\nThis failure occurs when another input must be selected by the algorithm in order to continue making progress, but doing so will increase the size of the resulting selection beyond an acceptable limit, specified by the maximum input count parameter.\nThis section describes the coin selection algorithms used by Cardano Wallet, along with step-by-step descriptions of the computations involved.\nAll algorithms implement a common interface, as described in the Interface section.\nThere are two main algorithms used by Cardano Wallet:\nLargest-First\nRandom-Improve\nIn general, Cardano Wallet gives priority to the Random-Improve algorithm, as experimental evidence shows that it performs better at minimising dust and maintaining a UTxO set with useful outputs. (See Self Organisation in Coin Selection for more details.)\nHowever, in rare cases, the Random-Improve algorithm may fail to produce a result. In such cases, Cardano Wallet will fall back to the Largest-First algorithm.\nThe Largest-First coin selection algorithm considers UTxO set entries in descending order of value, from largest to smallest.\nWhen applied to a set of requested outputs, the algorithm repeatedly selects entries from the initial UTxO set until the total value of selected entries is greater than or equal to the total value of requested outputs.\nThe name of the algorithm is taken from the idea that the largest UTxO entry is always selected first. Specifically:\nA given UTxO entry u1 with value v1 can be selected if and only if there is no other unselected entry u2 with value v2 where v2 v1.\nAt all stages of processing, the algorithm maintains the following pieces of state:\nAvailable UTxO List This is initially equal to the initial UTxO set, sorted into descending order of coin value. The head of the list is always the remaining UTxO entry with the largest coin value. Entries are incrementally removed from the head of the list as the algorithm proceeds, until enough value has been selected. Selected UTxO Set This is initially equal to the empty set.\nAvailable UTxO List This is initially equal to the initial UTxO set, sorted into descending order of coin value. The head of the list is always the remaining UTxO entry with the largest coin value. Entries are incrementally removed from the head of the list as the algorithm proceeds, until enough value has been selected.\nAvailable UTxO List\nThis is initially equal to the initial UTxO set, sorted into descending order of coin value.\nThe head of the list is always the remaining UTxO entry with the largest coin value.\nEntries are incrementally removed from the head of the list as the algorithm proceeds, until enough value has been selected.\nSelected UTxO Set This is initially equal to the empty set.\nSelected UTxO Set\nThis is initially equal to the empty set.\nThe algorithm proceeds according to the following sequence of steps:\nStep 1 If the available UTxO list is empty: Terminate with a UTxO Balance Insufficient error. If the available UTxO list is not empty: Remove an UTxO entry from the head of the available UTxO list and add it to the selected UTxO set.\nStep 1\nIf the available UTxO list is empty:\nTerminate with a UTxO Balance Insufficient error.\nIf the available UTxO list is not empty:\nRemove an UTxO entry from the head of the available UTxO list and add it to the selected UTxO set.\nStep 2 Compare the total size nselected of the selected UTxO set with the maximum input count nmax. If nselected nmax then: Terminate with a Maximum Input Count Exceeded error. If nselected nmax then: Go to step 3.\nStep 2\nCompare the total size nselected of the selected UTxO set with the maximum input count nmax.\nIf nselected nmax then: Terminate with a Maximum Input Count Exceeded error.\nIf nselected nmax then:\nTerminate with a Maximum Input Count Exceeded error.\nIf nselected nmax then: Go to step 3.\nIf nselected nmax then:\nGo to step 3.\nStep 3 Compare the total value vselected of the selected UTxO set to the total value vrequested of the requested output set: If vselected vrequested then go to step 1. If vselected vrequested then go to step 4.\nStep 3\nCompare the total value vselected of the selected UTxO set to the total value vrequested of the requested output set:\nIf vselected vrequested then go to step 1.\nIf vselected vrequested then go to step 4.\nStep 4 Return a coin selection result where: The inputs set is equal to the selected UTxO set. The outputs set is equal to the requested output set. If vselected vrequested then: The change set contains just a single coin of value (vselected vrequested). If vselected = vrequested then: The change set is empty.\nStep 4\nReturn a coin selection result where:\nThe inputs set is equal to the selected UTxO set.\nThe inputs set is equal to the selected UTxO set.\nThe outputs set is equal to the requested output set.\nThe outputs set is equal to the requested output set.\nIf vselected vrequested then: The change set contains just a single coin of value (vselected vrequested).\nIf vselected vrequested then:\nThe change set contains just a single coin of value (vselected vrequested).\nIf vselected = vrequested then: The change set is empty.\nIf vselected = vrequested then:\nThe change set is empty.\nThe Random-Improve coin selection algorithm works in two phases:\nIn the first phase, the algorithm iterates through each of the requested outputs in descending order of coin value, from largest to smallest. For each output, the algorithm repeatedly selects entries at random from the initial UTxO set, until each requested output has been associated with a set of UTxO entries whose total value is enough to pay for that ouput.\nIn the first phase, the algorithm iterates through each of the requested outputs in descending order of coin value, from largest to smallest. For each output, the algorithm repeatedly selects entries at random from the initial UTxO set, until each requested output has been associated with a set of UTxO entries whose total value is enough to pay for that ouput.\nIn the second phase, the algorithm attempts to expand each existing UTxO selection with additional values taken at random from the initial UTxO set, to the point where the total value of each selection is as close as possible to twice the value of its associated output.\nIn the second phase, the algorithm attempts to expand each existing UTxO selection with additional values taken at random from the initial UTxO set, to the point where the total value of each selection is as close as possible to twice the value of its associated output.\nAfter the above phases are complete, for each output of value voutput and accompanying UTxO selection of value vselection, the algorithm generates a single change output of value vchange, where:\nvchange = vselection voutput\nSince the goal of the second phase was to expand each selection to the point where its total value is approximately twice the value of its associated output, this corresponds to a change output whose target value is approximately equal to the value of the output itself:\nvchange = vselection voutput\nvchange 2voutput voutput\nvchange voutput\nThe Random-Improve algorithm imposes the following cardinality restriction:\nEach entry from the initial UTxO set is used to pay for at most one output from the requested output set.\nAs a result of this restriction, the algorithm will fail with a UTxO Not Fragmented Enough error if the number of entries in the initial UTxO set is smaller than the number of entries in the requested output set.\nAt all stages of processing, the algorithm maintains the following pieces of state:\nAvailable UTxO Set This is initially equal to the initial UTxO set. Accumulated Coin Selection The accumulated coin selection is a coin selection where all fields are initially equal to the empty set.\nAvailable UTxO Set This is initially equal to the initial UTxO set.\nAvailable UTxO Set\nThis is initially equal to the initial UTxO set.\nAccumulated Coin Selection The accumulated coin selection is a coin selection where all fields are initially equal to the empty set.\nAccumulated Coin Selection\nThe accumulated coin selection is a coin selection where all fields are initially equal to the empty set.\nThe algorithm proceeds in two phases.\nPhase 1: Random Selection\nIn this phase, the algorithm iterates through each of the requested outputs in descending order of coin value, from largest to smallest.\nFor each output of value v, the algorithm repeatedly selects entries at random from the available UTxO set, until the total value of selected entries is greater than or equal to v. The selected entries are then associated with that output, and removed from the available UTxO set.\nThis phase ends when every output has been associated with a selection of UTxO entries.\nPhase 2: Improvement\nIn this phase, the algorithm attempts to improve upon each of the UTxO selections made in the previous phase, by conservatively expanding the selection made for each output in order to generate improved change values.\nDuring this phase, the algorithm:\nprocesses outputs in ascending order of coin value.\nprocesses outputs in ascending order of coin value.\ncontinues to select values from the available UTxO set.\ncontinues to select values from the available UTxO set.\nincrementally populates the accumulated coin selection.\nincrementally populates the accumulated coin selection.\nFor each output of value v, the algorithm:\nCalculates a target range for the total value of inputs used to pay for that output, defined by the triplet: (minimum, ideal, maximum) = (v, 2v, 3v) Attempts to improve upon the existing UTxO selection for that output, by repeatedly selecting additional entries at random from the available UTxO set, stopping when the selection can be improved upon no further. A selection with value v1 is considered to be an improvement over a selection with value v0 if all of the following conditions are satisfied: Condition 1: we have moved closer to the ideal value: abs (ideal v1) abs (ideal v0) Condition 2: we have not exceeded the maximum value: v1 maximum Condition 3: when counting cumulatively across all outputs considered so far, we have not selected more than the maximum number of UTxO entries specified by Maximum Input Count. Creates a change value for the output, equal to the total value of the improved UTxO selection for that output minus the value v of that output. Updates the accumulated coin selection: Adds the output to the outputs field; Adds the improved UTxO selection to the inputs field; Adds the change value to the change values field.\nCalculates a target range for the total value of inputs used to pay for that output, defined by the triplet: (minimum, ideal, maximum) = (v, 2v, 3v)\nCalculates a target range for the total value of inputs used to pay for that output, defined by the triplet:\n(minimum, ideal, maximum) = (v, 2v, 3v)\nAttempts to improve upon the existing UTxO selection for that output, by repeatedly selecting additional entries at random from the available UTxO set, stopping when the selection can be improved upon no further. A selection with value v1 is considered to be an improvement over a selection with value v0 if all of the following conditions are satisfied: Condition 1: we have moved closer to the ideal value: abs (ideal v1) abs (ideal v0) Condition 2: we have not exceeded the maximum value: v1 maximum Condition 3: when counting cumulatively across all outputs considered so far, we have not selected more than the maximum number of UTxO entries specified by Maximum Input Count.\nAttempts to improve upon the existing UTxO selection for that output, by repeatedly selecting additional entries at random from the available UTxO set, stopping when the selection can be improved upon no further.\nA selection with value v1 is considered to be an improvement over a selection with value v0 if all of the following conditions are satisfied:\nCondition 1: we have moved closer to the ideal value: abs (ideal v1) abs (ideal v0)\nCondition 1: we have moved closer to the ideal value:\nabs (ideal v1) abs (ideal v0)\nCondition 2: we have not exceeded the maximum value: v1 maximum\nCondition 2: we have not exceeded the maximum value:\nv1 maximum\nCondition 3: when counting cumulatively across all outputs considered so far, we have not selected more than the maximum number of UTxO entries specified by Maximum Input Count.\nCondition 3: when counting cumulatively across all outputs considered so far, we have not selected more than the maximum number of UTxO entries specified by Maximum Input Count.\nCreates a change value for the output, equal to the total value of the improved UTxO selection for that output minus the value v of that output.\nCreates a change value for the output, equal to the total value of the improved UTxO selection for that output minus the value v of that output.\nUpdates the accumulated coin selection: Adds the output to the outputs field; Adds the improved UTxO selection to the inputs field; Adds the change value to the change values field.\nUpdates the accumulated coin selection:\nAdds the output to the outputs field;\nAdds the improved UTxO selection to the inputs field;\nAdds the change value to the change values field.\nThis phase ends when every output has been processed, or when the available UTxO set has been exhausted, whichever occurs sooner.\nWhen both phases are complete, the algorithm terminates.\nThe accumulated coin selection is returned to the caller as the coin selection result.\nThe available UTxO set is returned to the caller as the remaining UTxO set result.\n-\nThere are several motivating principles behind the design of the algorithm.\nThe probability that random selection will choose dust entries from a UTxO set increases with the proportion of dust in the set.\nTherefore, for a UTxO set with a large amount of dust, there's a high probability that a random subset will include a large amount of dust.\nOver time, selecting entries randomly in this way will tend to limit the amount of dust that accumulates in the UTxO set.\nAs mentioned in the Goals section, it is desirable that coin selection algorithms, over time, are able to create UTxO sets that have useful outputs: outputs that will allow us to process future payments with a reasonably small number of inputs.\nIf for each payment request of value v we create a change output of roughly the same value v, then we will end up with a distribution of change values that matches the typical value distribution of payment requests.\n:bulb: Example\nAlice often buys bread and other similar items that cost around 1.00 each.\nWhen she instructs her wallet software to make a payment for around 1.00, the software attempts to select a set of unspent transaction outputs with a total value of around 2.00.\nAs she frequently makes payments for similar amounts, transactions created by her wallet will also frequently produce change coins of around 1.00 in value.\nOver time, her wallet will self-organize to contain multiple coins of around 1.00, which are useful for the kinds of payments that Alice frequently makes.\nSearching the UTxO set for additional entries to improve our change outputs is only useful if the UTxO set contains entries that are sufficiently small enough. But it is precisely when the UTxO set contains many small entries that it is less likely for a randomly-chosen UTxO entry to push the total above the upper bound.\nThis article introduces the Random-Improve coin selection algorithm, invented by Edsko de Vries.\nIt describes the three principles of self-organisation that inform the algorithm's design, and provides experimental evidence to demonstrate the algorithm's effectiveness at maintaining healthy UTxO sets over time.\nThere exists one or more reference implementations with appropriate testing illustrating the various properties of coin-selection stated in this document.\nReference implementations of the Largest-First algorithm are available in the following languages:\nReference implementations of the Random-Improve algorithm are available in the following languages:\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0003 | Wallet Key Generation\n\nMany wallets utilize some way of mapping a sentence of words (easy to read and write for humans) uniquely back and forth to a sized binary data (harder to remember).\nThis document outlines the various mapping algorithms used in the Cardano ecosystem.\nThe philosophy of cryptocurrencies is that you are in charge of your own finances. Therefore, it is very anti-thematic for wallet software to lock in a user by explicitly describing the algorithm used to derive keys for a wallet (both the master key and key derivation)\nTo this end, this document outlines all the relevant key generation algorithms used in the Cardano ecosystem.\nConversion from a recovery phrase to entropy is the same as described in BIP39.\nIn Cardano, hierarchical deterministic (abbrev. HD) wallets are similar to those described in BIP-0032. Notably, we use a variation called ED25519-BIP32. A reference implementation can be found here.\nThe master key generation is the mean by which on turns an initial entropy into a secure cryptographic key.\nMore specifically, the generation is a function from an initial seed to an extended private key (abbrev. XPrv) composed of:\n64 bytes: an extended Ed25519 secret key composed of: 32 bytes: Ed25519 curve scalar from which few bits have been tweaked according to ED25519-BIP32 32 bytes: Ed25519 binary blob used as IV for signing\n32 bytes: Ed25519 curve scalar from which few bits have been tweaked according to ED25519-BIP32\n32 bytes: Ed25519 binary blob used as IV for signing\n32 bytes: chain code for allowing secure child key derivation\nThroughout the years, Cardano has used different styles of master key generation:\nThis CIP is merely to document the existing standards and not to provide rationales for the various methods used.\nHowever, you can learn more at the following links:\nAdrestia documentation\nSLIP-0010\nSLIP-0023\nEach generation method is documented and provides test vectors in a language-agnostic way.\nThere exists reference implementations in various languages for each method.\nAt least 2 Cardano wallets (e.g. Yoroi & Daedalus) implement these methods.\nImplementation of each algorithm will be carried out in Yoroi and Daedalus (via cardano-wallet) by Emurgo and Input Output respectively.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0004 | Wallet Checksums\n\nWe introduce a checksum algorithm to help users verify they are restoring the right wallet before the restoration actually takes place.\nUsers occasionally enter the wrong mnemonic for their wallet. In this case, they simply see a 0 ADA wallet after syncing is over. This not only wastes the user's time, in the worst case it makes them think they either lost all their ADA or think there is a bug in the wallet implementation.\nTo solve this, we introduce a checksum that can be computed without having to perform wallet restoration.\nFirst, it's important to note that the method for generating a checksum is heavily dependent on the type of wallet (ex: BIP44, etc.). We outline an algorithm that works with most, but not all, types of wallet.\nEasily recomputed without access to mnemonic, private key or other similarly sensitive data Does not reveal anything about the wallet (irreversible -- cannot tell addresses, private key, etc. from just seeing the checksum) Negligible chance of collision Easy to memorize for the user Can be easily saved both digitally or on paper\nEasily recomputed without access to mnemonic, private key or other similarly sensitive data\nDoes not reveal anything about the wallet (irreversible -- cannot tell addresses, private key, etc. from just seeing the checksum)\nNegligible chance of collision\nEasy to memorize for the user\nCan be easily saved both digitally or on paper\nTo satisfy (1), the checksum SHOULD be seeded from the public key for the wallet. Notably, in the BIP44 case, it should come from the bip44 account derivation level's public key. Note: For HD wallets, the public key used SHOULD contain the chaincode also because we need to make sure that not just the public key, but all its child keys also, are properly generated.\nTo satisfy (2) and (3), the a hash of the public key is used\nTo satisfy (4) and (5), we generate for an ImagePart and a TextPart. The brain can roughly remember images allowing you to quickly dismiss checksums that look totally different. However, since images can sometimes be similar, a TextPart is also provided for double-checking. Additionally, if the user does not have access to a printer, the text part can be easily written down by hand on a piece of paper to satisfy (5).\nWe first provide a template for the code, explain the template and then provide the parameterization we use for Cardano\nfunction calculateChecksum(publicKeyHash: string /* note: lowercase hex representation */) { const hash = hash1(publicKeyHash); const [a, b, c, d] = hash_to_32(hash); // get a 4 byte value from the hash const alpha = `ABCDEJHKLNOPSTXZ`; // take 16 letters from the alphabet that are easy to distinguish // construct the TextPart from a group of letters and a group of numbers const letters = x => `${alpha[Math.floor(x / 16)]}${alpha[x % 16]}`; const numbers = `${((c << 8) + d) % 10000}`.padStart(4, '0'); const id = `${letters(a)}${letters(b)}-${numbers}`; return { hash, // used to generate the ImagePart id, // used as the TextPart }; }\nfunction calculateChecksum(publicKeyHash: string /* note: lowercase hex representation */) { const hash = hash1(publicKeyHash); const [a, b, c, d] = hash_to_32(hash); // get a 4 byte value from the hash const alpha = `ABCDEJHKLNOPSTXZ`; // take 16 letters from the alphabet that are easy to distinguish // construct the TextPart from a group of letters and a group of numbers const letters = x => `${alpha[Math.floor(x / 16)]}${alpha[x % 16]}`; const numbers = `${((c << 8) + d) % 10000}`.padStart(4, '0'); const id = `${letters(a)}${letters(b)}-${numbers}`; return { hash, // used to generate the ImagePart id, // used as the TextPart }; }\nFor ease of perception it seems that short alphanumeric sequences are the best for humans to remember, especially when letters and numbers are separated and not mixed together.\nFor letters, we render the bytes in hex, but replace the alphanumeric used in hex with this letter-only alphabet:\nA B C D E J H K L N O P S T X Z\nA B C D E J H K L N O P S T X Z\nThis alphabet satisfies the following requirements:\nHas exactly 16 letters (one-to-one mapping with 2 bytes in HEX) Does not contain characters that look too much like each other Minimizes occurrences of undesirable words in this list.\nHas exactly 16 letters (one-to-one mapping with 2 bytes in HEX)\nDoes not contain characters that look too much like each other\nMinimizes occurrences of undesirable words in this list.\nThe last two bytes are compressed to a 4-digit number. For this we will simply take the last 4 digits of the 16-bit integer number constructed from 2 bytes as ((A 8) + B) 10000 (zero-padded).\n((A << 8) + B) % 10000\nThis above produces 10000 unique values across all possible values of A and B and giving maximum of 7 potential collisions per value and 6.5 average collisions per value, which is the minimum, given the fact that we reduce maximum potential number 65536 to 4 digits. Note: resulting number is zero-padded to 4 digits.\nFor the image, we take the result of hash1 and use it as the seed for the blockies library.\nhash1\nThis library in particular has the following benefits:\nHas been audited\nUsed by other blockchains and therefore has common libraries for implementation\nNote: This library internally re-hashes its input to a 128-bit entropy string\nFor hash1, we use blake2b512. Blake2b is a standardized hash function that is used in Cardano for other purposes like key derivations. Reusing blake2b means one less dependency. We use 512 bytes of output to try and future-proof this algorithm (better to spread the entropy across more bits than needed than end up not capturing the full entropy in the future).\nhash1\nblake2b512\n512\nFor hash_to_32 we use CRC32. We hash a second time for the following:\nhash_to_32\nThe TextPart is constructed from 4 bytes (2 for letters, 2 for numbers) and so we need to project the result of hash1 down to 4 bytes. We don't want to simply take the last 4 bytes of hash1 because that would reveal part of the input used to generate the ImagePart. Although strictly speaking this should not be of a concern (since the result of hash1 doesn't reveal any information about the original key), we take this as a precaution. CRC32 is used in the Byron implementation of Cardano as a checksum for addresses, meaning no additional dependency has to be added.\nThe TextPart is constructed from 4 bytes (2 for letters, 2 for numbers) and so we need to project the result of hash1 down to 4 bytes.\nhash1\nWe don't want to simply take the last 4 bytes of hash1 because that would reveal part of the input used to generate the ImagePart. Although strictly speaking this should not be of a concern (since the result of hash1 doesn't reveal any information about the original key), we take this as a precaution.\nhash1\nhash1\nCRC32 is used in the Byron implementation of Cardano as a checksum for addresses, meaning no additional dependency has to be added.\nCRC32\nAlthough there is no specification for CRC32 and many variations exist, in Cardano we use the CRC-32-IEEE variation. You can find a C implementation here\nFor hash1, we still use blake2b512 but we now set the blake2b personalization to the the utf-8 byte equivalent of wallets checksum (exactly 16 utf-8 bytes in length) to avoid collision with any other standard that decides to hash a public key. For hash_to_32, we no longer use crc32 for the following reasons:\nFor hash1, we still use blake2b512 but we now set the blake2b personalization to the the utf-8 byte equivalent of wallets checksum (exactly 16 utf-8 bytes in length) to avoid collision with any other standard that decides to hash a public key.\nhash1\nblake2b512\npersonalization\nwallets checksum\nFor hash_to_32, we no longer use crc32 for the following reasons:\nhash_to_32\ncrc32\nIt has multiple competing implementations all called crc32 (easily to get the wrong implementation library)\ncrc32\nIt requires building a lookup table, making it slower than other hashing algorithms for similar safety\nCardano no longer uses crc32 in the Shelley mainnet as addresses now use BIP173 - bech32 which has its own checksum algorithm.\ncrc32\nInstead, we replace it with FNV-1a in 32-bit mode. FNV-1a is fast, easy to implement inline and gives good empirical distribution.\nNote that a different construction is needed for wallet types which do not have a public key (such as a balance tracking application which simply manages a set of addresses). In the balanace tracking case, simply hashing the set of addresses used is possible, but it means that adding & removing an address would change the checksum (possibly unintuitive). Since the checksum is meant to represent the wallet itself, we also cannot run a checksum on the name of the wallet or any other user-inputted data.\nThere exists a reference implementation with test vectors.\nChecksums are adopted by two or more wallets. Yoroi\nYoroi\nReference implementations: Javascript\nJavascript\nThis CIP is licensed under Apache-2.0\n2023 Cardano Foundation\n\n---\n\nCIP-0005 | Common Bech32 Prefixes\n\nThis CIP defines a set of common prefixes (or so-called human-readable part in the bech32) encoding format) for various bech32-encoded binary data across the Cardano eco-system.\nMany tools used within the Cardano eco-system are manipulating binary data. Binary data are typically encoded as hexadecimal text strings when shown in a user interface (might it be a console, a url or a structured document from a server). From the user perspective, it can be difficult to distinguish between various encoded data. From the tools developer perspective, it can also be difficult to validate inputs based only on raw bytes (in particular when encoded data often have the same length).\nTherefore, we can leverage bech32 for binary data encoding, with a set of common prefixes that can be used across tools and software to disambiguate payloads.\nWe define the following set of common prefixes with their corresponding semantic. Any software willing to represent binary data in a human-friendly way should abide by these guidelines. Should a data-type be missing, we encourage developers to update this CIP and register a new prefix.\nacct_sk\nacct_vk\nacct_xsk\nacct_xvk\nacct_shared_sk\nacct_shared_vk\nacct_shared_xsk\nacct_shared_xvk\naddr_sk\naddr_vk\naddr_xsk\naddr_xvk\naddr_shared_sk\naddr_shared_vk\naddr_shared_xsk\naddr_shared_xvk\ncc_cold_sk\ncc_cold_vk\ncc_cold_xsk\ncc_cold_xvk\ncc_hot_sk\ncc_hot_vk\ncc_hot_xsk\ncc_hot_xvk\ncvote_sk\ncvote_vk\ndrep_sk\ndrep_vk\ndrep_xsk\ndrep_xvk\nkes_sk\nkes_vk\npolicy_sk\npolicy_vk\npool_sk\npool_vk\nroot_sk\nroot_vk\nroot_xsk\nroot_xvk\nroot_shared_sk\nroot_shared_vk\nroot_shared_xsk\nroot_shared_xvk\nstake_sk\nstake_vk\nstake_xsk\nstake_xvk\nstake_shared_sk\nstake_shared_vk\nstake_shared_xsk\nstake_shared_xvk\nvrf_sk\nvrf_vk\nasset\npool\nscript\naddr_vkh\naddr_shared_vkh\npolicy_vkh\nstake_vkh\nstake_shared_vkh\nreq_signer_vkh\nvrf_vkh\ndatum\nscript_data\ndrep_vkh\ndrep_script\ncc_cold_vkh\ncc_cold_script\ncc_hot_vkh\ncc_hot_script\naddr\naddr_test\nstake\nstake_test\ndrep\ncc_cold\ncc_hot\ngov_action\nThe prefixes above are the version defined by CIP-0129 and should be used at this time. The prefixes below were previously defined by CIP-0105, and are deprecated. Please see CIP-0105 for the detailed deprecation information, and details to upgrade to CIP-0129.\nFor detailed information on the new specification and the rationale behind the upgrade, please refer to CIP-0129.\n| drep | Delegate representative verification key hash (DRep ID) | blake2b_224 digest of a delegate representative verification key | | drep_script | Delegate representative script hash (DRep ID) | blake2b_224 digest of a serialized delegate representative script | | cc_cold | Constitutional committee cold verification key hash (cold credential) | blake2b_224 digest of a consitutional committee cold verification key | | cc_cold_script | Constitutional committee cold script hash (cold credential) | blake2b_224 digest of a serialized constitutional committee cold script | | cc_hot | Constitutional committee hot verification key hash (hot credential) | blake2b_224 digest of a consitutional committee hot verification key | | cc_hot_script | Constitutional committee hot script hash (hot credential) | blake2b_224 digest of a serialized constitutional committee hot script |\ndrep\ndrep_script\ncc_cold\ncc_cold_script\ncc_hot\ncc_hot_script\n_test\nAddress already contains a discriminant tag, yet it requires one to peek at the internal binary payload. With Base58-encoded addresses, people have been used to look at first few characters and distinguish address this way. Not only this is cumbersome, but it is also made harder with both Shelley and Bech32-encoded addresses. On the one hand, the \"common\" part of the internal payload is much less than in Byron addresses and thus, the first bytes of the payload are varying much more. Plus, the bech32 prefix which can now be fixed makes it even more error-prone.\nTherefore, having a clear human-readable indicator regarding the network discrimination is useful.\naddr\nAddresses probably are the most user-facing object in the current Cardano eco-system. Being able to clearly identify them\n:bulb: Open question: with side-chains and multi-currencies coming soon, would it make sense to include the currency in the bech32 prefix? e.g. ada1... or ada_addr1.\nada1...\nada_addr1.\nstake\nStake addresses are references to reward account. They are used in many manipulation involving rewards (registering stake key, delegating, fetching reward balance etc..). We therefore make it a \"first-class\" object and assign it a dedicated prefix.\nsk\nvk\nBoth are rather transparent abbreviations for signing key and verification key.\nxsk\nxvk\nThe prefix x is typically used in cryptography to refer to extended keys (e.g. xpub, xprv ...). Following this convention, we prefix sk and vk as such when they refer to extended keys.\nx\nxpub\nxprv\nsk\nvk\nvkh\nAn abbreviation for verification key hash.\nVerification key hashes are commonly utilized throughout the Cardano eco-system. For example, they're used in stake pool registration and retirement certificates, stake key registration, delegation, and deregistration certificates, etc. As a result, it seems useful to have a human-readable prefix by which one can discern the different kinds of verification key hashes.\nThe only prior work done towards that direction has been jcli. Historically, prefixes evolved very organically and without any agreed-upon standard. jcli is however only compatible with J rmungandr and is not intended to be compatible with the cardano-node. Therefore, there's little concern regarding compatibility here.\nThere is a variety of tools and services utilizing this standard: Trezor, Ledger cardano-cli cardano-wallet Blockfrost cardanoscan, cexplorer ... and more\nTrezor, Ledger\ncardano-cli\ncardano-wallet\nBlockfrost\ncardanoscan, cexplorer\n... and more\nAvailable JavaScript library: cip5-js\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0006 | Stake Pool Extended Metadata\n\nThis CIP defines the concept of extended metadata for pools that is referenced from the pool registration data stored on chain.\nAs the ecosystem around Cardano stake pools proliferate so will the desire to slice, organize and search pool information dynamically. Currently the metadata referenced on chain provides 512 bytes that can be allocated across the four information categories (delegation-design-specification Section 4.2):\nticker\ndescription\nhomepage\nname\nMany additional attributes can be envisioned for future wallets, pool explorers, and information aggregators. The proposal below outlines an initial strategy for capturing this extended metadata.\nNote Updated: 2020-11-24 2nd key-pair for validation 2021-02-08 json schema\nWe define two more fields for the on chain referenced metadata file that references another JSON file on a URL with the extended metadata. The proposed metadata is as follows:\nticker\ndescription\nhomepage\nname\nextDataUrl\nextSigUrl\nextVkey\nIn order to include the additional ext Field data, we suggest increasing the maximum size of the main metadata file from currently 512 to 1024 bytes.\nIn difference to the main metadata, the extended metadata should be updateable without having to use the cold key of the pool and without having to perform an on-chain transaction. The consumer of these data should still have the possibility to verify the authenticity of the data.\nThe operator notes all his additional pool information in the extended metadata (extData.json).\nextData.json\nWe propose the pool operator generate a new public-secret key-pair (extData.skey and extData.vkey)\nextData.skey\nextData.vkey\ncardano-cli node key-gen --cold-verification-key-file extData.vkey --cold-signing-key-file extData.skey --operational-certificate-issue-counter-file extData.counter\ncardano-cli node key-gen --cold-verification-key-file extData.vkey --cold-signing-key-file extData.skey --operational-certificate-issue-counter-file extData.counter\nThen a new (not available yet) cardano-cli command generate the signature (extData.sign) .\ncardano-cli\nextData.sign\ncardano-cli stake-pool rawdata-sign --raw-metadata-file extData.json --signing-key-file extData.skey --out-file extData.sign\ncardano-cli stake-pool rawdata-sign --raw-metadata-file extData.json --signing-key-file extData.skey --out-file extData.sign\nThe operator now:\nhas the extData.json and extData.sign files\nextData.json\nextData.sign\nwill publish them at some https:// URL (probably same host as the main metadata)\nset the published extData.json URL in the main metadata extDataUrl field\nextData.json\nextDataUrl\nset the published extData.sign URL in then main metadata extSigUrl field\nextData.sign\nextSigUrl\nset the extData.vkey string in the main metadata extVkey field\nextData.vkey\nextVkey\nre-register the extended main metadata file on chain\nThis re-registration of the main metadata file with the extData.vkey and the two URLs is only necessary once. Afterwards, the operator can update his extended metadata at any time, compute the new signature with the cardano-cli stake-pool rawdata-sign command, and publish both files at the existing extDataUrl and extSigUrl.\nextData.vkey\ncardano-cli stake-pool rawdata-sign\nextDataUrl\nextSigUrl\nIn the following we describe a first minimal version of the extended JSON file format.\nSince this extended metadata file can be updated at any time by the pool operator, a serial number is useful for consuming applications and services to identify updates.\nThere are main thematic sections with respective subordinate data fields:\nthe itn section is about the verifiable linking of an ITN pool ticker with its counterpart in Mainnet to identify fraudulent duplicates. (already used as not standardized extension)\nthe pool section contains additional information about the pool instance the pool.contact section contains information for additional information and contact data the pool.media_assets section contains additional information about the pools media files and colors the pool.itn section is an optional section for ITN pool operators\nthe pool.contact section contains information for additional information and contact data\nthe pool.media_assets section contains additional information about the pools media files and colors\nthe pool.itn section is an optional section for ITN pool operators\nThe full schema is given in annexe as schema.json\n{ \"serial\": 870, \"pool\": { \"id\": \"69579373ec20f2f82d2dc2360410350b308112f2939f92a\", \"country\": \"JPN\", \"status\": \"active\", \"contact\": { \"primary\": \"email\", \"email\": \"info@demopool.org\", \"facebook\": \"demopool12\", \"github\": \"demopooldev\", \"feed\": \"https://www.demopool.org/feed.xml\", \"telegram\": \"demopool\", \"twitter\": \"demopoolbird\" }, \"media_assets\": { \"icon_png_64x64\": \"https://www.demopool.org/media/icon64.png\", \"logo_png\": \"https://www.demopool.org/media/logo.png\", \"logo_svg\": \"https://www.demopool.org/media/logo.svg\", \"color_fg\": \"#AABBCC\", \"color_bg\": \"#C0C0C0\" }, \"itn\": { \"owner\": \"ed25519_pk1...\", \"witness\": \"ed25519_sig1...\" } } }\n{ \"serial\": 870, \"pool\": { \"id\": \"69579373ec20f2f82d2dc2360410350b308112f2939f92a\", \"country\": \"JPN\", \"status\": \"active\", \"contact\": { \"primary\": \"email\", \"email\": \"info@demopool.org\", \"facebook\": \"demopool12\", \"github\": \"demopooldev\", \"feed\": \"https://www.demopool.org/feed.xml\", \"telegram\": \"demopool\", \"twitter\": \"demopoolbird\" }, \"media_assets\": { \"icon_png_64x64\": \"https://www.demopool.org/media/icon64.png\", \"logo_png\": \"https://www.demopool.org/media/logo.png\", \"logo_svg\": \"https://www.demopool.org/media/logo.svg\", \"color_fg\": \"#AABBCC\", \"color_bg\": \"#C0C0C0\" }, \"itn\": { \"owner\": \"ed25519_pk1...\", \"witness\": \"ed25519_sig1...\" } } }\nNo fields are removed or changed in the current on chain metadata. The new ext... fields are optional and not necessary to parse for any entities that do not need additional information about a pool\next...\nThere exist at least two explorers which make use of this extended metadata structure or very close equivalent: pooltool.io cexplorer.io\npooltool.io\ncexplorer.io\nProvide direct support for this specification in stake pool explorers and other tools.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0007 | Curve Pledge Benefit\n\nModifying the current rewards calculation equation by substituting a n-root curved relationship between pledge and pledge benefit rewards for the current linear relationship will better achieve the original design goal of incentivizing pledge to help prevent Sybil attacks. This also reduces the unfortunate side effect in the current equation that over rewards private pools which provide no additional security benefit.\nThere are two main reasons for changing the current linear a0 pledge benefit factor in the rewards equation.\nPools pledging less than 1 million ADA see very little reward benefit. This is not a strong incentive for pool operators as at current prices that is approximately $150,000 USD. Private pools get massive reward benefit without providing any additional protection against Sybil attacks. Why should a private pool make 29 more rewards than a pool with 5m ADA pledge while doing the same work?\nPools pledging less than 1 million ADA see very little reward benefit. This is not a strong incentive for pool operators as at current prices that is approximately $150,000 USD.\nPools pledging less than 1 million ADA see very little reward benefit. This is not a strong incentive for pool operators as at current prices that is approximately $150,000 USD.\nPrivate pools get massive reward benefit without providing any additional protection against Sybil attacks. Why should a private pool make 29 more rewards than a pool with 5m ADA pledge while doing the same work?\nPrivate pools get massive reward benefit without providing any additional protection against Sybil attacks. Why should a private pool make 29 more rewards than a pool with 5m ADA pledge while doing the same work?\nThis is a modification of the maxPool function defined in section 11.8 Rewards Distribution Calculation of A Formal Specification of the Cardano Ledger .\nmaxPool = (R / (1 + a0)) * (o + (s * a0 * ((o - (s * ((z0 - o) / z0))) / z0)))\nwhere: R = ((reserve * rho) + fees) * (1 - tau) o = min(pool_stake / total_stake, z0) = z0 for fully saturated pool s = pledge / total_stake z0 = 1 / k and the following are current protocol parameters: k = 150 rho = 0.0022 a0 = 0.3 tau = .05\nThe idea is to replace s in the above equation with an n-root curve expression of pledge rather than the linear pledge value.\nWe use an expression called crossover to represent the point where the curve crosses the line and the benefit in the new and original equations is identical. Because the a0 pledge benefit is spread over the pledge range from 0 to saturation there is a dependence on k and total_stake. Since k and total_stake will likely change over time it is best to express crossover in terms of k and total_stake as follows:\ncrossover = total_stake / (k * crossover_factor)\nwhere crossover_factor is any real number greater than or equal to 1. So crossover_factor is essentially a divisor of the pool saturation amount. For example, setting crossover_factor to 20 with k = 150 and total_stake = 31 billion gives a crossover of approximately 10.3 million.\nAlso, we can parameterize the n-root curve exponent. This gives us:\ns = pow(pledge, (1 / curve_root)) * pow(crossover, ((curve_root - 1) / curve_root)) / total_stake\nThe curve_root could be set to any integer greater than 0 and when set to 1 produces the current rewards equation. The curve_root is n in n-root. For example, 1 = linear, 2 = square root, 3 = cube root, 4 = fourth root, etc.\nBy making this modification to the rewards equation we introduce two new protocol parameters, crossover_factor and curve_root, that need to be set thoughtfully.\nSee rewards.php for some simple PHP code that allows you to try different values for crossover_factor and curve_root and compare the resulting rewards to the current equation. For usage, run \"php -f rewards.php help\".\nAn interesting set of parameters as an example is:\ncurve_root = 3 crossover_factor = 8\nRunning \"php -f rewards.php 3 8\" produces:\nAssumptions Reserve: 14b Total stake: 31.7b Tx fees: 0 Fully Saturated Pool Rewards available in epoch: 29.3m Pool saturation: 211.3m\nCurve root: 3 Crossover factor: 8 Crossover: 26.4m\nPledge Rewards Benefit Alt Rwd Alt Bnft 0k 150051 0 150051 0 10k 150053 0 150458 0.27 50k 150062 0.01 150747 0.46 100k 150073 0.01 150928 0.58 200k 150094 0.03 151156 0.74 500k 150158 0.07 151551 1 1m 150264 0.14 151941 1.26 2m 150477 0.28 152432 1.59 5m 151116 0.71 153282 2.15 10m 152181 1.42 154122 2.71 20m 154311 2.84 155180 3.42 50m 160702 7.1 157012 4.64 100m 171352 14.2 158821 5.84 211.3m 195067 30 161305 7.5\nAs you can see this gives meaningful pledge benefit rewards to pools pledging less than 1m ADA.\nUsing the n-root curve pledge benefit shows a much more reasonable distribution of pledge related rewards which will encourage meaningful pledges from more pool operators thus making the network more secure against Sybil attacks. It also provides higher rewards for higher pledge without disproportionately rewarding a very few private pool operators who provide no additional security value to the network. This modification maintains the general principles of the current rewards equation and does not introduce any hard limits. It improves the incentives that were originally designed to make them more meaningful for the majority of pool operators.\nThis proposal is backwards compatible with the current reward function by setting the curve_root parameter to 1.\nThe new equation is implemented in the ledger and enacted through a hard-fork.\nAgreement by the Ledger team as defined in CIP-0084 under Expectations for ledger CIPs including \"expert opinion\" on changes to rewards & incentives.\nAgreement by the Ledger team as defined in CIP-0084 under Expectations for ledger CIPs including \"expert opinion\" on changes to rewards & incentives.\nAuthor has offered to produce an implementation of this change as a pull request if shown where the current maxPool reward equation is implemented in the code.\nAuthor has offered to produce an implementation of this change as a pull request if shown where the current maxPool reward equation is implemented in the code.\n2020 Shawn McMurdo. This CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0008 | Message Signing\n\nPrivate keys can be used to sign arbitrary data. If you have the public key, you can verify the data was signed by the owner of the private key. This is how transaction signing works internally but its utility is not limited to transactions. This document tries to set a standard for how to represent and verify signed messages for Cardano.\nMost common use cases:\nProving ownership of a set addresses (possibly to prove ownership of more then X Ada) Proving ownership of addresses used in a transaction Proving ownership of an identity or other off-chain data with a public key attached to it\nProving ownership of a set addresses (possibly to prove ownership of more then X Ada)\nProving ownership of addresses used in a transaction\nProving ownership of an identity or other off-chain data with a public key attached to it\nFirst we will show a very basic example of the structure to help the reader understand the definitions that comes after\n[ bstr, ; protected header { * label => any }, ; unprotected header bstr / nil, ; message to sign [ ; signature array [ ; first signature bstr ; protected { * label => any }, ; unprotected bstr ; signature ] ] ]\n[ bstr, ; protected header { * label => any }, ; unprotected header bstr / nil, ; message to sign [ ; signature array [ ; first signature bstr ; protected { * label => any }, ; unprotected bstr ; signature ] ] ]\nYou can see the structure has two layers -- both containing a protected and an unprotected section. Items inside protected are part of the data signed while unprotected is meant to annotate the COSE structure (for example add annotations as you pass a message across a stack). Items MUST NOT be duplicated.\nprotected\nunprotected\nprotected\nunprotected\nYou can find more complete definitions of protected and unprotected in RFC 8152 section-3 and you can see some the reserved entries (called Generic_Headers) in the maps in RFC 8152 section-3.1.\nprotected\nunprotected\nGeneric_Headers\nNote: payload can be nil. This means that the payload is known by both the signer and the verifier and therefore doesn't need to be encoded.\npayload\nnil\nFor your convenience, the structure is provided here:\nempty_or_serialized_map = bstr .cbor header_map / bstr .size 0 header_map = { Generic_Headers, ; reserved headers (see COSE section 3.1) * label => values ; any number of int/string labels for application-specific purpose. } Headers = ( protected : empty_or_serialized_map, unprotected : header_map ) ; signature layer COSE_Signature = [ Headers, signature : bstr ] ; if signing with just ONE key COSE_Sign1 = [ Headers, payload : bstr / nil, signature : bstr ] # if signing with >1 key COSE_Sign = [ Headers, payload : bstr / nil, signatures : [+ COSE_Signature] ] signed_message = COSE_SIGN / COSE_Sign1\nempty_or_serialized_map = bstr .cbor header_map / bstr .size 0 header_map = { Generic_Headers, ; reserved headers (see COSE section 3.1) * label => values ; any number of int/string labels for application-specific purpose. } Headers = ( protected : empty_or_serialized_map, unprotected : header_map ) ; signature layer COSE_Signature = [ Headers, signature : bstr ] ; if signing with just ONE key COSE_Sign1 = [ Headers, payload : bstr / nil, signature : bstr ] # if signing with >1 key COSE_Sign = [ Headers, payload : bstr / nil, signatures : [+ COSE_Signature] ] signed_message = COSE_SIGN / COSE_Sign1\nInstead of signing the full structure, we instead sign the following type which is derived from the structure\nSig_structure = [ context : \"Signature\" / \"Signature1\" / \"CounterSignature\", ; explained below body_protected : empty_or_serialized_map, ; protected from layer 1 ? sign_protected : empty_or_serialized_map, ; not present in Sign1 case external_aad : bstr, ; explanation below payload : bstr ]\nSig_structure = [ context : \"Signature\" / \"Signature1\" / \"CounterSignature\", ; explained below body_protected : empty_or_serialized_map, ; protected from layer 1 ? sign_protected : empty_or_serialized_map, ; not present in Sign1 case external_aad : bstr, ; explanation below payload : bstr ]\nThe external_aad allows an application to ask the user to sign some extra data but NOT put it inside the COSE structure (only as part of the data to sign). Defaults to h''. You can read more about this at RFC 8152 section-4.3.\nexternal_aad\nh''\nThe context is meant to encode what structure was used for the COSE request. CounterSignature is explained in a later section of this specification.\ncontext\nCounterSignature\nTo be able to effectively verify we need two things:\nP1 - (optional) knowledge of the relation of a public key and a Cardano address P2 - Knowledge of which algorithm was used to sign\nP1 - (optional) knowledge of the relation of a public key and a Cardano address\nP2 - Knowledge of which algorithm was used to sign\nFor P1, the mapping of public keys to addresses has two main cases:\nP1\nfor Shelley addresses, one payment key maps to many addresses (base, pointer, enterprise) for Byron addresses, chaincode and address attributes need to be combined with the public key to generate an address\nfor Shelley addresses, one payment key maps to many addresses (base, pointer, enterprise)\nfor Byron addresses, chaincode and address attributes need to be combined with the public key to generate an address\nTo resolve this, one SHOULD add the full address to the protected header when using a v2 address. The v2 addresses contain the chaincode + attributes and we can verify the address matches combining it with the verification public key.\n? address: bstr,\n? address: bstr,\nFor P2, we use the alg header and to specify which public key was used to sign, use the cwt protected header.\nP2\nalg\nAlthough COSE defines multiple ways to encrypt, we simplify our spec to the two following cases:\nEncrypted with the recipient's public key (called key transport in COSE spec) Encrypted with a user-chosen password (called passwords in COSE spec)\nEncrypted with the recipient's public key (called key transport in COSE spec)\nkey transport\nEncrypted with a user-chosen password (called passwords in COSE spec)\npasswords\nIn order to facilitate implementations in wallets, we limit the usage of these to the following\nChachaPoly Ed25519PubKey\nChachaPoly Ed25519PubKey\nWe will explain what this means shortly but you can find the full list of the types of encryption allowed by COSE at RFC 8152 section 5.1.1\nThe COSE specification is made to be composable -- that is you can have a plaintext that you wrap with a signature, then wrap with an encryption, then wrap with a signature again (and so on).\nThat means that for encryption in particular, it can either\nBe used to encrypt plaintext directly Be used to encrypt another COSE message\nBe used to encrypt plaintext directly\nBe used to encrypt another COSE message\nIn this spec, we care about the case where you encrypt a signed_message\nsigned_message\nHere is the overall CBOR structure\n; holds encrypted content itself COSE_Encrypt = [ Headers, ciphertext : bstr / nil, ; contains encrypted signed_message recipients : [+COSE_recipient] ] ; holds encrypted keys the receiver can use to decrypt the content COSE_recipient = [ Headers, ciphertext : bstr / nil, ; contains encrypted key to decrypt the COSE_Encrypt ciphertext ? recipients : [+COSE_recipient] ; in case you need multiple rounds to get decryption key ]\n; holds encrypted content itself COSE_Encrypt = [ Headers, ciphertext : bstr / nil, ; contains encrypted signed_message recipients : [+COSE_recipient] ] ; holds encrypted keys the receiver can use to decrypt the content COSE_recipient = [ Headers, ciphertext : bstr / nil, ; contains encrypted key to decrypt the COSE_Encrypt ciphertext ? recipients : [+COSE_recipient] ; in case you need multiple rounds to get decryption key ]\nTo encrypt the structure as a whole, we call our encryption method once for each level (root, recipient, etc.) recursively. For example, we encrypt the signed_message and put it in the COSE_Encrypt ciphertext, then we encrypt the decryption key and put it in the COSE_recipient ciphertext.\nsigned_message\nCOSE_Encrypt\nCOSE_recipient\nFor the Headers,\nHeaders\nThe protected fields MUST be empty. These are meant to be used with AEAD which we don't need in this specification (you can read more about it at RFC 8152 section 5.3).\nprotected\nWe define two ways to encrypt content:\nFor password-based encryption we don't need a receiver field (anybody who knows the password can decrypt) so we instead use the following (simplified) structure\nCOSE_Encrypt0 = [ Headers, ciphertext : bstr / nil, ] PasswordEncryption = 16 (COSE_Encrypt0)\nCOSE_Encrypt0 = [ Headers, ciphertext : bstr / nil, ] PasswordEncryption = 16 (COSE_Encrypt0)\nThe COSE spec uses the following parameters for ChaCha20/Poly1305 as specified in 10.3:\n256-bit key\n128-bit tag\n96-bit nonce\nWe RECOMMEND using 19162 iterations as this matches the existing password encryption in the Yoroi encryption spec`.\n19162\nWe only allow encrypting based on ED25519 public keys (the ones used for Cardano). To encrypt based on these public keys, you must\nED25519\nCompute a password consisting of 22 case-sensitive alphanumeric (a z, A Z, 0 9) characters (this gives you ~128 bits of entropy)\nCompute a password consisting of 22 case-sensitive alphanumeric (a z, A Z, 0 9) characters (this gives you ~128 bits of entropy)\nNow, for each receiver, you must\nCompute an ephemeral key pair using ED25519 extended Compute the shared DH secret between the private key from step (1) and the public key received (using the exchange functionality) Use this as the password to encrypt the password in (1) using the Yoroi encryption spec\nCompute an ephemeral key pair using ED25519 extended\nED25519 extended\nCompute the shared DH secret between the private key from step (1) and the public key received (using the exchange functionality)\nexchange\nUse this as the password to encrypt the password in (1) using the Yoroi encryption spec\nThe structure will look like the following:\nCOSE_Encrypt = [ Headers, ciphertext : bstr / nil, ; contained signed_message encrypted with random password recipients : [+COSE_recipient] ] COSE_recipient = [ Headers, ciphertext : bstr / nil, ; contains random password encrypted with shared secret ] PubKeyEncryption = 96 (COSE_Encrypt)\nCOSE_Encrypt = [ Headers, ciphertext : bstr / nil, ; contained signed_message encrypted with random password recipients : [+COSE_recipient] ] COSE_recipient = [ Headers, ciphertext : bstr / nil, ; contains random password encrypted with shared secret ] PubKeyEncryption = 96 (COSE_Encrypt)\nThe Headers for the recipient MUST have a epk label containing the public key of the ephemeral keypair as described in the CBOR Encoded Message Syntax.\nHeaders\nepk\nThe Headers for the body MUST have version: uint in the unprotected field. See Versions table for possible version numbers.\nHeaders\nversion: uint\nunprotected\nTo solve E3, signed_message body header MUST contain hashed: bool as an unprotected header which defines whether or not we signed the payload OR the Blake2b224 hash of the payload. The hash MUST be used in the following two cases\nE3\nsigned_message\nhashed: bool\nunprotected\npayload\nBlake2b224\npayload\nThe size of the raw payload would otherwise be too big to fit in hardware wallet memory (see E1). Note that the exact size for which this is the case depends on the device. The payload characters (ex: non-ASCII) that cannot be displayed on the hardware wallet device (see E3)\nThe size of the raw payload would otherwise be too big to fit in hardware wallet memory (see E1). Note that the exact size for which this is the case depends on the device.\npayload\nThe payload characters (ex: non-ASCII) that cannot be displayed on the hardware wallet device (see E3)\nWe RECOMMEND showing the user the full payload on the device if possible because it lowers the attack surface (otherwise the user has to trust that the hash of the payload was calculated correctly).\nBlake2b224 was chosen specifically because 224 bits is already a long string for hardware wallets.\nBlake2b224\n224\nOnce we have our top-level encrypted_message or signed_message we need to encode them in way that can be displayed to users (doesn't need to be stored and can be inferred just from the data)\nencrypted_message\nsigned_message\nWe define the encoding in three parts prefix || data || checksum (where || means append)\nprefix || data || checksum\nWe need a human-readable prefix. We use \"CM\" for \"Cardano Message\" followed by the message type:\nencrypted_message: cme_\nencrypted_message\ncme_\nsigned_message: cms_\nsigned_message\ncms_\nmac_message: cmm_ (unused in this spec)\nmac_message\ncmm_\nData is simply the base64url encoding of the message\nbase64url\nWe use fnv32a on the data for the checksum and store it as the base64url encoding of its network byte order representation.\nbase64url\nWe recommend usage of unprotected headers vs protected headers when possible. This is because we have to limit the amount of data passed to a hardware wallet to satisfy E1. If the only effect of an adversary changing an unprotected header only leads to the signature not matching, then it's best to leave it unprotected.\nThe public key SHOULD NOT contain any chaincode information, as it could compromise child non-hardened keys. It is both a privacy and a security risk (see here for more detail).\nOn top being usable for all cases mentioned in Motivation, we also desire the following to ensure it works well with hardware wallets:\nE1 - Low runtime memory environment\nE2 - Low app size environment (cannot implement every cryptographic algorithm on the device or app size would be too big)\nE3 - Works well with limited display (some hardware wallets cannot display long text and cannot display UTF8)\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"NOT RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.\nCBOR is a way to serialize structured data in a more compact way than what is allowed by JSON. It is widely used across the Cardano ecosystem and so we use it to encode the data for this specification.\nCDDL is a human-readable CBOR notation format. CBOR schemas defined in this document are defined uinsg CDDL. We use label = int / tstr in several places.\nlabel = int / tstr\nThis is a standard for how to use CBOR for message signing. It is based on the JSON equivalent JSON Object Signing and Encryption RFC 7520.\nWe base our construction on COSE because all Cardano libraries already depend on CBOR due to its use in the base protocol (which means we don't need to introduce a new library). It is also more compact, which is useful in case data generated by this standard ever needs to be stored on-chain.\nThis is a standard for pre-defined header elements for message signing based on the equivalent standard for JSON (JWT [RFC 7519]). This allows us to standardize notions of concepts like expiration time of a signed message.\nBECH32 (BIP-173) is a standard for encoding addresses such that they\nAre human readable (both in length and contain a common prefix)\nCan easily be displayed in a QR code\nContain error detection through a BCH checksum\nCardano has several address types based on the era they were created in:\nv1 - Legacy Daedalus (starts with Dd)\nv1\nv2 - Legacy Icarus (starts with Ae2)\nv2\nv3 - Shelley (bech32)\nv3\nAlthough v3 is relevant to us for encoding public keys into addresses, we do not use bech32's scheme for encoding in this specification. This is because\nv3\nbech32\nThe payload may be too big to reasonably encode in a QR code so the benefits of using base32 are limited.\nBCH checksums are not made for large payloads and additionally the polynomial used has to be fine-tuned for the expected length (but the length of our payload varies too much in this spec)\nAlthough Cardano Byron addresses use CRC32 (IEEE variation), due to E1 and E2 we use fnv32a for checksums.\nE1\nE2\nThis is because CRC32:\nHas fairly few collisions compared to other hashes Is moderately slower than other hashes Requires more memory than alternatives (need to build a lookup table) Is somewhat complex implementation (many alternatives exist)\nHas fairly few collisions compared to other hashes\nIs moderately slower than other hashes\nRequires more memory than alternatives (need to build a lookup table)\nIs somewhat complex implementation (many alternatives exist)\nwhile fnv32a:\nStill has fairly few collisions Is faster than CRC32 in general Needs only O(1) memory requirement Is simple to implement\nStill has fairly few collisions\nIs faster than CRC32 in general\nNeeds only O(1) memory requirement\nIs simple to implement\nIn particular, using fnv32a over CRC32 frees up 1024 bytes of memory due to not having a lookup table which is significant on hardware wallets.\nBlake2b is a hash algorithm used commonly in Cardano. Notably, Blake2b-224, Blake2b-256 and Blake2b512 are used depending on the context.\nbase64url allows encoding bytes in a human-readable format that is also safe to pass in URLs.\nbase64url\nOther blockchains have existing specifications for message signing, but they mostly revolve around scripts trying to validate messages. We don't leverage any of their work in particular but it may be of interest.\nBIP-137 - simply scheme for message signing that works with P2PKH, P2PSH and bech32\nBIP-137 - simply scheme for message signing that works with P2PKH, P2PSH and bech32\nBIP-322 - reuses Bitcoin script to process a generic signed message format\nBIP-322 - reuses Bitcoin script to process a generic signed message format\nEIP-191 - encode data for Ethereum contracts\nEIP-191 - encode data for Ethereum contracts\nEIP-712 - encode structs for Ethereum contracts\nEIP-712 - encode structs for Ethereum contracts\nCardano already allows message signing within the WASM bindings. Notably,\nsign verify\nsign\nverify\nYou can see an example of these two functions here\nEven if you use cryptographically secure sign and verify functions, you still have the following problems:\nsign\nverify\nNo human-recognizable prefix No error detection User could accidentally sign a transaction or a block thinking it's harmless data\nNo human-recognizable prefix\nNo error detection\nUser could accidentally sign a transaction or a block thinking it's harmless data\nWe also have a risk of a few different kinds of replay attacks\nA dapp asks person A to sign \"BOB\" and then another dapp asks user B to sign \"BOB\". B can just use the signature from A A dapp asks person A to sign \"BOB\" on a testnet chain. Person B then sends this signed message to the same dapp running on mainnet (same argument applies to sidechains)\nA dapp asks person A to sign \"BOB\" and then another dapp asks user B to sign \"BOB\". B can just use the signature from A\nA dapp asks person A to sign \"BOB\" on a testnet chain. Person B then sends this signed message to the same dapp running on mainnet (same argument applies to sidechains)\nCOSE-JS\nRust message signing\nThis specification provides no means of Revocation.\nThere are wallets supporting creation of signed messages as per this protocol (enumerated 2023-12-19 as per CardanoBallot list of wallets supporting CIP-8 signed messages): Flint Eternl Nami Typhon Yoroi Nufi Gerowallet Lace\nFlint\nEternl\nNami\nTyphon\nYoroi\nNufi\nGerowallet\nLace\nThere exist one or more implementations in commonly used development libraries: Mesh @emurgo/cardano-message-signing-asmjs\nMesh\n@emurgo/cardano-message-signing-asmjs\n@emurgo/cardano-message-signing-asmjs\nThere exist CLI Tools supporting creation and verification of signed messages: cardano-signer\ncardano-signer\nThere exist one or more implementations in web sites and other tools: SundaeSwap Governance voting\nSundaeSwap Governance voting\nMake this standard available as well-supported means of message signing across Cardano wallets, dApps, and CLI tools.\nSupport this standard in a usable reference implemtation (@emurgo/cardano-message-signing-asmjs).\n@emurgo/cardano-message-signing-asmjs\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0009 | Protocol Parameters (Shelley Era)\n\nThis CIP is an informational CIP that describes the initial protocol parameter settings for the Shelley era of the Cardano blockchain, plus the changes that have been made. It is intended to serve as a historic record, allowing protocol parameter changes to be tracked back to the original settings.\nWe need to provide a concise description of the initial protocol parameter choices, that can be used by the community as the base for future proposed protocol changes, and that document the chain of changes to the parameters.\nThis CIP records only the changes to the protocol parameters that have actually been made. Suggested changes to protocol parameters should be proposed by preparing and submitting a new CIP, rather than editing this CIP. The following information should be included.\nWhere necessary, the summary rationale should be supported by a few paragraphs of text giving the full rationale, plus references to any external documents that are needed to understand the proposal.\nProtocol parameters are used to affect the operation of the Cardano Protocol. They may be either updatable or non-updatable. Updatable parameters can be tuned to vary the operation of the block producing protocol, impacting the proportion of pools that are federated/non-federated, how much influence the \"pledge\" has etc. Non-updatable parameters affect the fundamentals of the blockchain protocol, including defining the genesis block, basic security properties etc. Some non-updatable parameters may be embedded within the source code or implemented as software. Each major protocol version defines its own sets of updatable/non-updatable parameters.\nThe initial updatable protocol parameter values are given below (in JSON format). Any of these parameters may be changed by submitting a parameter update proposal. A change to the major protocol parameter version triggers a \"hard fork\" event. This will require stake pool operators to upgrade to a new software version that complies with the new chain production protocol as well as being able to verify the construction of the chain.\n{ \"protocolVersion\": { \"major\": 2, \"minor\": 0 }, \"nOpt\": 150, \"a0\": 0.3, \"minPoolCost\": 340000000, \"decentralisationParam\": 1.0, \"maxBlockBodySize\": 65536, \"maxBlockHeaderSize\": 1100, \"maxTxSize\": 16384, \"tau\": 0.2, \"rho\": 3.0e-3, \"poolDeposit\": 500000000, \"keyDeposit\": 2000000, \"minFeeB\": 155381, \"minFeeA\": 44, \"minUTxOValue\": 1000000, \"extraEntropy\": { \"tag\": \"NeutralNonce\" }, \"eMax\": 18 }\n{ \"protocolVersion\": { \"major\": 2, \"minor\": 0 }, \"nOpt\": 150, \"a0\": 0.3, \"minPoolCost\": 340000000, \"decentralisationParam\": 1.0, \"maxBlockBodySize\": 65536, \"maxBlockHeaderSize\": 1100, \"maxTxSize\": 16384, \"tau\": 0.2, \"rho\": 3.0e-3, \"poolDeposit\": 500000000, \"keyDeposit\": 2000000, \"minFeeB\": 155381, \"minFeeA\": 44, \"minUTxOValue\": 1000000, \"extraEntropy\": { \"tag\": \"NeutralNonce\" }, \"eMax\": 18 }\nThe meaning of the fields is:\nprotocolVersion\": { \"major\": 2, \"minor\": 2 }\n{ \"tag\": \"NeutralNonce\" }\nThe initial non-updatable protocol parameters are given below (in JSON format):\n\"activeSlotsCoeff\": 0.05, ... \"genDelegs\": { \"ad5463153dc3d24b9ff133e46136028bdc1edbb897f5a7cf1b37950c\": { \"delegate\": \"d9e5c76ad5ee778960804094a389f0b546b5c2b140a62f8ec43ea54d\", \"vrf\": \"64fa87e8b29a5b7bfbd6795677e3e878c505bc4a3649485d366b50abadec92d7\" }, ... } }, \"updateQuorum\": 5, \"networkId\": \"Mainnet\", \"initialFunds\": {}, \"maxLovelaceSupply\": 45000000000000000e, \"networkMagic\": 764824073, \"epochLength\": 432000, \"systemStart\": \"2017-09-23T21:44:51Z\", \"slotsPerKESPeriod\": 129600, \"slotLength\": 1, \"maxKESEvolutions\": 62, \"securityParam\": 2160 }\n\"activeSlotsCoeff\": 0.05, ... \"genDelegs\": { \"ad5463153dc3d24b9ff133e46136028bdc1edbb897f5a7cf1b37950c\": { \"delegate\": \"d9e5c76ad5ee778960804094a389f0b546b5c2b140a62f8ec43ea54d\", \"vrf\": \"64fa87e8b29a5b7bfbd6795677e3e878c505bc4a3649485d366b50abadec92d7\" }, ... } }, \"updateQuorum\": 5, \"networkId\": \"Mainnet\", \"initialFunds\": {}, \"maxLovelaceSupply\": 45000000000000000e, \"networkMagic\": 764824073, \"epochLength\": 432000, \"systemStart\": \"2017-09-23T21:44:51Z\", \"slotsPerKESPeriod\": 129600, \"slotLength\": 1, \"maxKESEvolutions\": 62, \"securityParam\": 2160 }\nThe meaning of the fields is:\nThe original protocol parameters are given in the Byron genesis file. These parameters need to be included in any operational stake pool so that the Byron portion of the chain can be verified, but they can no longer be altered.\n{ \"avvmDistr\": { ... }, \"blockVersionData\": { ... }, \"ftsSeed\": \"76617361206f7061736120736b6f766f726f64612047677572646120626f726f64612070726f766f6461\", \"protocolConsts\": { ... }, \"startTime\": 1506203091, \"bootStakeholders\": { ... }, \"heavyDelegation\": { ... } }, \"nonAvvmBalances\": {}, \"vssCerts\": { ... }\n{ \"avvmDistr\": { ... }, \"blockVersionData\": { ... }, \"ftsSeed\": \"76617361206f7061736120736b6f766f726f64612047677572646120626f726f64612070726f766f6461\", \"protocolConsts\": { ... }, \"startTime\": 1506203091, \"bootStakeholders\": { ... }, \"heavyDelegation\": { ... } }, \"nonAvvmBalances\": {}, \"vssCerts\": { ... }\nChanges will affect many stakeholders and must therefore be subject to open community debate and discussion.\nUltimately, the Voltaire protocol voting mechanism will be used to achieve fully automated, decentralised and transparent governance. In the interim, the CIP process will be used.\nChanges to the parameters need to be signalled to the community well in advance, so that they can take appropriate action. For the most significant parameters, a minimum of 4-6 weeks elapsed time between announcement and enactment is appropriate. This period must be included in the CIP. Announcements will be made as soon as practical after the conclusion of the vote.\nProtocol parameter changes must be submitted and endorsed within the first 24 hours of the epoch before they are required to come into effect. For example, a change that is intended for epoch 300 must be submitted and endorsed in the first 24 hours of epoch 299. Once a change has been submitted and endorsed by a sufficient quorum of keyholders (currently 5 of the 7 genesis keys), it cannot be revoked.\nOnce a protocol parameter change has been announced, it can only be overridden through the voting process (CIP, Voltaire etc.). Any vote must be completed before the start of the epoch in which the change is to be submitted.\nFollowing the Shelley hard fork event, the decentralisationParam parameter has been gradually decreased from 1.0 to 0.3, with the goal of ultimately decreasing it to 0 (at which point it can be removed entirely as an updatable parameter). This has gradually reduced the impact of the federated block producing nodes, so ensuring that the network moves to become a distributed collection of increasingly decentralised stake pools. The parameter was frozen at 0.32 between epochs 234 and 240. The nOpt parameter was changed from 150 to 500 in epoch 234.\ndecentralisationParam\n1.0\n0.3\n0\n0.32\nnOpt\n150\n500\nThe Allegra Hard Fork Event on 2020-12-16 (epoch 236) introduced token locking capabilities plus some other small changes to the protocol. No parameters were added or removed.\n{ \"poolDeposit\": 500000000, \"protocolVersion\": { \"minor\": 0, \"major\": 3 }, \"minUTxOValue\": 1000000, \"decentralisationParam\": 0.32, \"maxTxSize\": 16384, \"minPoolCost\": 340000000, \"minFeeA\": 44, \"maxBlockBodySize\": 65536, \"minFeeB\": 155381, \"eMax\": 18, \"extraEntropy\": { \"tag\": \"NeutralNonce\" }, \"maxBlockHeaderSize\": 1100, \"keyDeposit\": 2000000, \"nOpt\": 500, \"rho\": 3.0e-3, \"tau\": 0.2, \"a0\": 0.3 }\n{ \"poolDeposit\": 500000000, \"protocolVersion\": { \"minor\": 0, \"major\": 3 }, \"minUTxOValue\": 1000000, \"decentralisationParam\": 0.32, \"maxTxSize\": 16384, \"minPoolCost\": 340000000, \"minFeeA\": 44, \"maxBlockBodySize\": 65536, \"minFeeB\": 155381, \"eMax\": 18, \"extraEntropy\": { \"tag\": \"NeutralNonce\" }, \"maxBlockHeaderSize\": 1100, \"keyDeposit\": 2000000, \"nOpt\": 500, \"rho\": 3.0e-3, \"tau\": 0.2, \"a0\": 0.3 }\nThe Mary Hard Fork Event will introduce multi-asset token capability. It is not expected that any parameter will be added or removed.\n{ \"poolDeposit\": 500000000, \"protocolVersion\": { \"minor\": 0, \"major\": 4 }, \"minUTxOValue\": 1000000, \"decentralisationParam\": 0.32, \"maxTxSize\": 16384, \"minPoolCost\": 340000000, \"minFeeA\": 44, \"maxBlockBodySize\": 65536, \"minFeeB\": 155381, \"eMax\": 18, \"extraEntropy\": { \"tag\": \"NeutralNonce\" }, \"maxBlockHeaderSize\": 1100, \"keyDeposit\": 2000000, \"nOpt\": 500, \"rho\": 3.0e-3, \"tau\": 0.2, \"a0\": 0.3 }\n{ \"poolDeposit\": 500000000, \"protocolVersion\": { \"minor\": 0, \"major\": 4 }, \"minUTxOValue\": 1000000, \"decentralisationParam\": 0.32, \"maxTxSize\": 16384, \"minPoolCost\": 340000000, \"minFeeA\": 44, \"maxBlockBodySize\": 65536, \"minFeeB\": 155381, \"eMax\": 18, \"extraEntropy\": { \"tag\": \"NeutralNonce\" }, \"maxBlockHeaderSize\": 1100, \"keyDeposit\": 2000000, \"nOpt\": 500, \"rho\": 3.0e-3, \"tau\": 0.2, \"a0\": 0.3 }\nSee CIP-0028: Protocol Parameters (Alonzo Era).\nThe initial parameter settings were chosen based on information from the Incentivised Testnet, the Haskell Testnet, Stake Pool Operators plus benchmarking and security concerns. This parameter choice was deliberately conservative, in order to avoid throttling rewards in the initial stages of the Cardano mainnet, and to support a wide range of possible stake pool operator (professional, amateur, self, etc.). Some parameter choices (systemStart, securityParam) were required to be backwards compatible with the Byron chain.\nsystemStart\nsecurityParam\nThe key parameters that govern the behaviour of the system are nOpt, a0, decentralisationParam and minPoolCost. Changes to these parameters need to be considered as a package -- there can be unintended consequences when changing a single parameter in isolation.\nnOpt\na0\ndecentralisationParam\nminPoolCost\nIt is expected that the following changes to these parameters are likely in the near to medium term:\nincreasing nOpt to align more closely with the number of active pools\nnOpt\nincreasing a0 to increase the pledge effect\na0\ndecreasing minPoolCost (e.g. in line with growth with the Ada value)\nminPoolCost\ndecreasing decentralisationParam to 0 (to enable full decentralisation of block production)\ndecentralisationParam\nFurther adjustments are likely to be required to tune the system as it evolves.\nFour parameters govern the economics of the system: tau, rho, minFeeA and minFeeB. The first two concern the rate of rewards that are provided to stake pools, delegators and the treasury. The others concern transaction costs.\ntau\nrho\nminFeeA\nminFeeB\nThree parameters govern block and transaction sizes: maxBlockBodySize, maxBlockHeaderSize, maxTxSize. Their settings have been chosen to ensure the required levels of functionality, within constrained resource restrictions (including long-term blockchain size and real-time worldwide exchange of blocks). Changes to these parameters may impact functionality, network reliability and performance.\nmaxBlockBodySize\nmaxBlockHeaderSize\nmaxTxSize\nThis CIP describes the initial set of protocol parameters and the changes to date, so backwards compatibility is not an issue. Future proposals may change any or all of these parameters. A change to the major protocol version indicates a major change in the node software. Such a change may involve adding/removing parameters or changing their semantics/formats. In contrast, minor protocol changes are used to ensure key software updates without changing the meaning of any protocol parameters.\nThe Shelley ledger era is activated.\nDocumented parameters are in operational use by Cardano Node and Ledger.\nOriginal (Shelley) and subsequent ledger era parameters are deemed correct by working groups at IOG.\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0010 | Transaction Metadata Label Registry\n\nCardano transaction metadata forces metadata entries to namespace their content using an unsigned integer key. This specification is a registry of which use cases has allocated which number to avoid collisions.\nThe top level of the transaction metadata CBOR object is a mapping of transaction_metadatum_label to the actual metadata where the transaction_metadatum_label represents an (ideally unique) key for a metadata use case. This allows enables the following:\ntransaction_metadatum_label\ntransaction_metadatum_label\nFast lookup for nodes to query all transactions containing metadata that uses a specific key Allows a single transaction to include multiple metadata entries for different standards\nFast lookup for nodes to query all transactions containing metadata that uses a specific key\nAllows a single transaction to include multiple metadata entries for different standards\nTransaction metadata refers to an optional CBOR object in every transaction since the start of the Shelley era. It is defined as the follow CDDL data structure\ntransaction_metadatum = { * transaction_metadatum => transaction_metadatum } / [ * transaction_metadatum ] / int / bytes .size (0..64) / text .size (0..64) transaction_metadatum_label = uint transaction_metadata = { * transaction_metadatum_label => transaction_metadatum }\ntransaction_metadatum = { * transaction_metadatum => transaction_metadatum } / [ * transaction_metadatum ] / int / bytes .size (0..64) / text .size (0..64) transaction_metadatum_label = uint transaction_metadata = { * transaction_metadatum_label => transaction_metadatum }\nThese are the reserved transaction_metadatum_label values\ntransaction_metadatum_label\ntransaction_metadatum_label\nFor the registry itself, please see registry.json in the machine-readable format. Please open your pull request against this file.\n* It's best to avoid using 0 or any a similar number like 1 that other people are very likely to use. Prefer instead to generate a random number\n0\n1\nCreating a registry for transaction_metadatum_label values has the following benefit:\ntransaction_metadatum_label\nIt makes it easy for developers to know which transaction_metadatum_label to use to query their node if looking for transactions that use a standard It makes it easy to avoid collisions with other standards that use transaction metadata\nIt makes it easy for developers to know which transaction_metadatum_label to use to query their node if looking for transactions that use a standard\ntransaction_metadatum_label\nIt makes it easy to avoid collisions with other standards that use transaction metadata\nConsistent, long-term use by Cardano implementors of the metadata label registry by all applications requiring a universally acknowledged metadata label.\nConsistent, long-term use in the CIP editing process: tagging, verifying, and merging new label requirements.\nConfirmed interest and cooperation in this metadata labelling standard and its registry.json convention by Cardano implementors: including NFT creators, data aggregators, and sidechains.\nregistry.json\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0011 | Staking key chain for HD wallets\n\nStarting with the Shelley hardfork, Cardano makes use of both the UTXO model and the account model. To support both transaction models from the same master key, we allocate a new chain for CIP-1852.\nGenerally it's best to only use a cryptographic key for a single purpose, and so it's best to make the staking key be separate from any key used for UTXO addresses.\nNote The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"NOT RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.\nRecall that CIP-1852 specifies the following derivation path:\nm / purpose' / coin_type' / account' / chain / address_index\nm / purpose' / coin_type' / account' / chain / address_index\nWe set chain=2 to indicate the staking key chain. Keys in this chain MUST follow the accounting model for transactions and SHOULD be used for reward addresses\nchain=2\nWe RECOMMEND wallets only use address_index=0 for compatibility with existing software. This also avoids the need for staking key discovery.\naddress_index=0\nWallets that use multiple staking keys are REQUIRED to use sequential indexing with no gaps. This is to make detection of mangle addresses (addresses where the payment key belongs to the user, but the staking key doesn't) easier.\nNote: an observer looking at the blockchain will be able to tell if two staking keys belong to the same user if they are generated from the same wallet with different address_index values because the payment keys inside the base addresses will be the same.\naddress_index\nrecovery phrase\nprevent company field green slot measure chief hero apple task eagle sunset endorse dress seed\nprevent company field green slot measure chief hero apple task eagle sunset endorse dress seed\nprivate key (including chaincode) for m / 1852' / 1815' / 0' / 2 / 0\nm / 1852' / 1815' / 0' / 2 / 0\nb8ab42f1aacbcdb3ae858e3a3df88142b3ed27a2d3f432024e0d943fc1e597442d57545d84c8db2820b11509d944093bc605350e60c533b8886a405bd59eed6dcf356648fe9e9219d83e989c8ff5b5b337e2897b6554c1ab4e636de791fe5427\nb8ab42f1aacbcdb3ae858e3a3df88142b3ed27a2d3f432024e0d943fc1e597442d57545d84c8db2820b11509d944093bc605350e60c533b8886a405bd59eed6dcf356648fe9e9219d83e989c8ff5b5b337e2897b6554c1ab4e636de791fe5427\nreward address (with network_id=1)\nnetwork_id=1\nstake1uy8ykk8dzmeqxm05znz65nhr80m0k3gxnjvdngf8azh6sjc6hyh36\nstake1uy8ykk8dzmeqxm05znz65nhr80m0k3gxnjvdngf8azh6sjc6hyh36\nThe term \"account\" is unfortunately an overloaded term so we clarify all its uses here:\nBIP44 uses the term \"account\" as one derivation level to mean the following\nThis level splits the key space into independent user identities, so the wallet never mixes the coins across different accounts. To differentiate this from other usage, we sometimes refer to it as an account' (the bip32 notation) or a BIP44 Account.\naccount'\nBlockchains like Ethereum does not use the UTXO model and instead uses the Account model for transactions.\nAll notable wallet and tooling providers follow this method of key derivation.\nThis method of key derivation has been agreed as canonical and has been included in CIP-1852.\nThis method of key derivation has been supported by all wallet and tool providers beginning with the Shelley ledger era.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0012 | On-chain stake pool operator to delegates communication\n\nStandard format for metadata used in an on-chain communication of stake pool owner towards their delegates.\nStake pool owners and their delegates lack an on-chain communication standard between them.\nCIP-0006 already defines an external feed of a stake pool within the extended metadata. However, there is need for a more verifiable on-chain communication standard that will also provide additional cost associated with such communication to prevent its abuse.\nWe define two types of communication metadata, which are distinguished by transaction metadata label as defined in CIP-0010: Transaction metadata label registry:\nMessage board communication is a type of metadata that has been included in an on-chain transaction between two base addresses associated with a stake pool operator owner address. Given the onetime fee for this communication, we are considering this as a message board of a stake pool, as it also enables delegates to easier access historical metadata communication.\nMessage board communication is a type of metadata that has been included in an on-chain transaction between two base addresses associated with a stake pool operator owner address. Given the onetime fee for this communication, we are considering this as a message board of a stake pool, as it also enables delegates to easier access historical metadata communication.\nDirect delegate communication is a type of metadata that has been included in an on-chain transaction between a stake pool owner account and a delegate's account. This type of communication is more expensive for the stake pool owner, preventing higher abuse and therefore enables wallets to implement notification granularity. It might be suitable for targeting specific delegates, such as messaging only new joined delegates, loyal delegates, high-amount delegates etc.\nDirect delegate communication is a type of metadata that has been included in an on-chain transaction between a stake pool owner account and a delegate's account. This type of communication is more expensive for the stake pool owner, preventing higher abuse and therefore enables wallets to implement notification granularity. It might be suitable for targeting specific delegates, such as messaging only new joined delegates, loyal delegates, high-amount delegates etc.\nAs per CIP-0010, we assign:\nMessage board communication transaction metadata label 1990,\n1990\nDirect delegate communication transaction metadata label 1991.\n1991\nMetadata are written in JSON format and maximum size of metadata around 16KB.\nThe root object property is a 3 bytes UTF-8 encoded string representing the ISO 639-3 language code of the content.\ntitle\ncontent\nvalid\nexpires\nThe schema.json file defines the metadata.\n{ \"1991\": [ { \"lat\": { \"title\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do\", \"content\": [ \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do \", \"eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut e\", \"nim ad minim veniam, quis nostrud exercitation ullamco laboris n\", \"isi ut aliquip ex ea commodo consequat. Duis aute irure dolor in\", \" reprehenderit in voluptate velit esse cillum dolore eu fugiat n\", \"ulla pariatur. Excepteur sint occaecat cupidatat non proident, s\", \"unt in culpa qui officia deserunt mollit anim id est laborum.\" ], \"valid\": 10661033, \"expire\": 10669033 } }, { \"eng\": { \"title\": \"But I must explain to you how all this mistaken idea\", \"content\": [ \"But I must explain to you how all this mistaken idea of denounci\", \"ng of a pleasure and praising pain was born and I will give you \", \"a complete account of the system, and expound the actual teachin\", \"gs of the great explorer of the truth, the master-builder of hum\", \"an happiness. No one rejects, dislikes, or avoids pleasure itsel\", \"f, because it is pleasure, but because those who do not know how\", \" to pursue pleasure rationally encounter consequences.\" ], \"valid\": 10661033, \"expire\": 10669033 } } ] }\n{ \"1991\": [ { \"lat\": { \"title\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do\", \"content\": [ \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do \", \"eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut e\", \"nim ad minim veniam, quis nostrud exercitation ullamco laboris n\", \"isi ut aliquip ex ea commodo consequat. Duis aute irure dolor in\", \" reprehenderit in voluptate velit esse cillum dolore eu fugiat n\", \"ulla pariatur. Excepteur sint occaecat cupidatat non proident, s\", \"unt in culpa qui officia deserunt mollit anim id est laborum.\" ], \"valid\": 10661033, \"expire\": 10669033 } }, { \"eng\": { \"title\": \"But I must explain to you how all this mistaken idea\", \"content\": [ \"But I must explain to you how all this mistaken idea of denounci\", \"ng of a pleasure and praising pain was born and I will give you \", \"a complete account of the system, and expound the actual teachin\", \"gs of the great explorer of the truth, the master-builder of hum\", \"an happiness. No one rejects, dislikes, or avoids pleasure itsel\", \"f, because it is pleasure, but because those who do not know how\", \" to pursue pleasure rationally encounter consequences.\" ], \"valid\": 10661033, \"expire\": 10669033 } } ] }\nThe format of the content field is required to be an array of 64 bytes chunks, as this is the maximum size of a JSON field in the Cardano ledger. Tools, such as wallets, are required to recompose the content of the message.\ncontent\nThe current Cardano protocol parameter for maximum transaction size, that will hold the metadata, is around 16KB.\nNo backwards compatibility breaking changes are introduced.\nIndications that more than one wallet or backend supports this standard, including: Yoroi (in progress from Implement CIP12 to Yoroi backends)\nYoroi (in progress from Implement CIP12 to Yoroi backends)\nDevelop reference implementation (CIP12 communication tool examples)\nOffer this standard for implementation in downstream tools and wallets: pending their own decisions about whether and how to display communication messages.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0013 | Cardano URI Scheme\n\nThis describes a general standard URI scheme with two specific protocols to handle Ada transfers and links to weighted lists of stake pools.\nDevelopers of protocols that use URI schemes should be able to choose unique protocol keywords indicating how these links are handled by applications.\nBeyond the two earliest defined protocols below, protocols using distinct keywords (e.g. //stake) can be defined in other CIPs and implemented without ambiguity by applications which interpret those particular URI protocols.\n//stake\nUsers who create community content often want donations as a financial incentive. However, forcing users to open their wallet and copy-paste an address lowers the amount of people likely to send tokens (especially if they have to sync their wallet first).\nIf donating was as simple as clicking a link that opens a light wallet with pre-populated fields, users may be more willing to send tokens. URI schemes would enable users to easily make payments by simply clicking links on webpages or scanning QR Codes.\nCentralised sources of information have led a growing amount of stake to be disproportionately assigned to pools pushed near & beyond the saturation point.\nStake pool URIs will provide an additional means for small pools to acquire delegation and maintain stability, supporting diversity and possibly fault-tolerance in the Cardano network through a more even distribution of stake.\nInterfaces that connect delegators with pools beyond the highly contested top choices of the in-wallet ranking algorithms are important to avoid saturation and maintain decentralization.\nLarger pools and collectives can also use these URIs to link to, and spread delegation between, a family of pools they own to avoid any one of their pools becoming saturated.\nPool links allow for interfaces to initiate delegation transactions without requiring any code modifications to the wallets themselves.\nURIs for weighted stake pool lists provide alternatives to using a JSON file to implement delegation portfolios in a way that may better suit certain platforms, applications, or social contexts.\nThe core implementation should follow the BIP-21 standard (with bitcoin: replaced with web+cardano:)\nbitcoin:\nweb+cardano:\nExamples:\n<a href=\"web+cardano:Ae2tdPwUPEZ76BjmWDTS7poTekAvNqBjgfthF92pSLSDVpRVnLP7meaFhVd\">Donate</a> <a href=\"web+cardano://stake?c94e6fe1123bf111b77b57994bcd836af8ba2b3aa72cfcefbec2d3d4\">Stake with us</a> <a href=\"web+cardano://stake?POOL1=3.14159&POOL2=2.71828\">Split between our 2 related pools</a> <a href=\"web+cardano://stake?COSD\">Choose our least saturated pool</a> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=consensus2023\">Claim $HOSKY</a>\n<a href=\"web+cardano:Ae2tdPwUPEZ76BjmWDTS7poTekAvNqBjgfthF92pSLSDVpRVnLP7meaFhVd\">Donate</a> <a href=\"web+cardano://stake?c94e6fe1123bf111b77b57994bcd836af8ba2b3aa72cfcefbec2d3d4\">Stake with us</a> <a href=\"web+cardano://stake?POOL1=3.14159&POOL2=2.71828\">Split between our 2 related pools</a> <a href=\"web+cardano://stake?COSD\">Choose our least saturated pool</a> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=consensus2023\">Claim $HOSKY</a>\nThe protocol term (e.g. //stake) is called the authority as defined in Wikipedia Uniform Resource Identifier Syntax.\n//stake\ncardano: is chosen over ada: because other projects that implement this standard tend to take the project name over the currency name (this makes sense if we consider this protocol as a generic way for interacting with the blockchain through wallets and dApps - as opposed to a simple payment system).\ncardano:\nada:\nDepending on the protocol registration method (see Rationale), browsers generally enforce a web+ or ext+ prefix for non-whitelisted protocols (note: bitcoin: was whitelisted; see registerProtocolHandler Permitted schemes). The prefix ext+ is recommended for extensions, but not mandatory (see protocol_handlers).\nweb+\next+\nbitcoin:\next+\nThis top-level definition is mainly to allow switching to a particular protocol for each separately defined authority, with a payment link being the default:\nauthority\nWhen authority is unspecified, it is a payment URI (with an address and an optional amount parameter;\nauthority\nWhen authority is explicit (containing // followed by the authority keyword), it is defined in the //stake case below or in a separate CIP for that protocol.\nauthority\n//\n//stake\ncardanouri = \"web+cardano:\" (paymentref | authorityref) authorityref = (stakepoolref | otherref) otherref = \"//\" authority query\ncardanouri = \"web+cardano:\" (paymentref | authorityref) authorityref = (stakepoolref | otherref) otherref = \"//\" authority query\nFor grammar reference, see:\nWikipedia Augmented Backus Naur form\nRFC 2234: Augmented BNF for Syntax Specifications: ABNF\nUnicode in ABNF\npaymentref = cardanoaddress [ \"?\" amountparam ] cardanoaddress = *(base58 | bech32) amountparam = \"amount=\" *digit [ \".\" *digit ]\npaymentref = cardanoaddress [ \"?\" amountparam ] cardanoaddress = *(base58 | bech32) amountparam = \"amount=\" *digit [ \".\" *digit ]\nThe amount parameter must follow the same rules described in BIP-21, namely, it must be specified in decimal ADA, without commas and using the period (.) decimal separator.\nstakepoolref = \"//stake\" query query = ( \"?\" stakepoolpair) *( \"&\" stakepoolpair) stakepoolpair = stakepool [ \"=\" proportion] stakepool = poolhexid | poolticker poolhexid = 56HEXDIG poolticker = 3*5UNICODE proportion = *digit [ \".\" *digit ]\nstakepoolref = \"//stake\" query query = ( \"?\" stakepoolpair) *( \"&\" stakepoolpair) stakepoolpair = stakepool [ \"=\" proportion] stakepool = poolhexid | poolticker poolhexid = 56HEXDIG poolticker = 3*5UNICODE proportion = *digit [ \".\" *digit ]\nFor brevity, essential in many Internet contexts, poolticker must be supported here in addition to the unambiguous poolhexid.\npoolticker\npoolhexid\nproportion\nIf only one stake pool is specified, any proportion is meaningless and ignored.\nIf all stake pools have a numerical proportion, each component of the resulting stake distribution will have the same ratio as the provided proportion to the sum of all the propotions.\nproportion\nAny missing proportion is assigned a precise value of 1.\nproportion\n1\nIf a stake pool is listed multiple times, the URI is rejected as invalid.\nWhen there is more than one pool registered with any of the specified poolticker parameters (whether for pool groups which have the same ticker for all pools, or for separate pools using the same ticker), the choice to which pool(s) to finally delegate is left to the user through the wallet UI.\npoolticker\nThe wallet UI should always confirm the exact delegation choice even when it is unambiguous from the URI. When the user has multiple wallets, the wallet UI must select which wallet(s) the user will be delegating from.\nIf, during a wallet or other application's development process, it is still only able to support single pool links, these parameters in the URI query string should (by preference of the wallet UI designers) either be ignored or generate a warning message, to avoid leading the user to believe they are implementing a currently unsupported but perhaps popularly referenced multi-pool delegation list:\nany value for the first URI query argument;\nany URI query argument beyond the first.\nAn ABNF grammar should be specified and explained similarly for each CIP that defines a new Cardano URI authority by explicitly defining the terms authority and query as for the \"Stake pool\" case above.\nauthority\nquery\nFor payment links, we cannot prompt the user to send the funds right away as they may not be fully aware of the URI they clicked or were redirected to. Instead, it may be better to simply pre-populate fields in a transaction. For either payment or staking links, we should be wary of people who disguise links as actually opening up a phishing website that LOOKS like that corresponding part of the wallet UI. If wallets create stake pool links, the actual ada or lovelace balance should not be used literally as the proportion figure, to avoid revealing the identity of the wallet owner who is creating the portfolio (e.g. the proportions could be scaled to normalise the largest to 1).\nFor payment links, we cannot prompt the user to send the funds right away as they may not be fully aware of the URI they clicked or were redirected to. Instead, it may be better to simply pre-populate fields in a transaction.\nFor either payment or staking links, we should be wary of people who disguise links as actually opening up a phishing website that LOOKS like that corresponding part of the wallet UI.\nIf wallets create stake pool links, the actual ada or lovelace balance should not be used literally as the proportion figure, to avoid revealing the identity of the wallet owner who is creating the portfolio (e.g. the proportions could be scaled to normalise the largest to 1).\nproportion\n1\nAn alternative solution to the original problem described above is to use standard URL links in combination with a routing backend system. The routing system is used to redirect to the app's URI. The advantage of this scheme is that it allows to provide a fallback mechanism to handle the case when no application implementing the protocol is installed (for instance, by redirecting to the App Store or Google Play). This is the approach behind iOS Universal Links and Android App Links. In general, it provides a better user experience but requires a centralized system which makes it unsuitable for as a multi-app standard.\nFor background, see\nAndroid Developer Docs Add intent filters for incoming links\nApple Developer Docs Defining a custom URL scheme for your app\nReact Native Linking\nBIP-21 is limited to only features Bitcoin supports. A similar feature for Ethereum would, for example, also support gas as an extra parameter. BIP-21 is easily extensible but we have to take precaution to avoid different wallets having different implementations of these features as they become available on Cardano. To get an idea of some extra features that could be added, consider this (still under discussion) proposal for Ethereum: EIP-681\nURIs facilitate the \"social element\" of delegated staking and pool promotion through a socially familiar, easily accessible, and less centralised convention for sharing stake pool references and potential delegation portfolios without having to construct or host a JSON file.\nThe processing of a JSON file delivered by a web server will depend highly on a user's platform and might not even be seen by the wallet application at all. With a properly associated web+cardano: protocol, developers and users have another option available in case JSON files are not delivered properly to the wallet application.\nweb+cardano:\nFor a CIP based on this principle, see CIP-0017.\nThere exist one or more wallets supporting Payment URIs. Yoroi Begin Wallet\nYoroi\nBegin Wallet\nThere exist one or more wallets supporting Stake Pool URIs. TBD\nTBD\nThere exist other CIPs or drafts defining additional URI protocols. CIP-0099\nCIP-0099\nThere exist one or more wallets supporting additional URI protocols. Yoroi (CIP-0099) Begin Wallet (CIP-0099) VESPR (CIP-0099)\nYoroi (CIP-0099)\nBegin Wallet (CIP-0099)\nVESPR (CIP-0099)\nEncourage wallet and dApp developers to support all currently defined URI protocols, keeping in mind these are each likely to be considered separately:\nPayment URIs\nStake Pool URIs\nall other URI schemes defined in separate CIPs\nEducation and advocacy about these standards should be done by:\nDevelopers of applications and standards requiring new URI schemes\nCardano sponsoring companies\nCommunity advocates\nCIP editors\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0014 | User-Facing Asset Fingerprint\n\nThis specification defines a user-facing asset fingerprint as a bech32-encoded blake2b-160 digest of the concatenation of the policy id and the asset name.\nThe Mary era of Cardano introduces the support for native assets. On the blockchain, native assets are uniquely identified by both their so-called policy id and asset name. Neither the policy id nor the asset name are intended to be human-readable data.\nOn the one hand, the policy id is a hash digest of either a monetary script or a Plutus script. On the other hand, the asset name is an arbitrary bytestring of up to 32 bytes (which does not necessarily decode to a valid UTF-8 sequence). In addition, it is possible for an asset to have an empty asset name, or, for assets to have identical asset names under different policies.\nBecause assets are manipulated in several user-facing features on desktop and via hardware applications, it is useful to come up with a short(er) and human-readable identifier for assets that user can recognize and refer to when talking about assets. We call such an identifier an asset fingerprint.\nWe define the asset fingerprint in pseudo-code as:\nassetFingerprint := encodeBech32 ( datapart = hash ( algorithm = 'blake2b' , digest-length = 20 , message = policyId | assetName ) , humanReadablePart = 'asset' )\nassetFingerprint := encodeBech32 ( datapart = hash ( algorithm = 'blake2b' , digest-length = 20 , message = policyId | assetName ) , humanReadablePart = 'asset' )\nwhere | designates the concatenation of two byte strings. The digest-length is given in bytes (so, 160 bits).\n|\ndigest-length\ncip14-js\n{-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE QuasiQuotes #-} {-# LANGUAGE TypeApplications #-}\n{-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE QuasiQuotes #-} {-# LANGUAGE TypeApplications #-}\n-- package: base >= 4.0.0 import Prelude import Data.Function ( (&) ) -- package: bech32 >= 1.0.2 import qualified Codec.Binary.Bech32 as Bech32 -- package: bech32-th >= 1.0.2 import Codec.Binary.Bech32.TH ( humanReadablePart ) -- package: bytestring >= 0.10.0.0 import Data.ByteString ( ByteString ) -- package: cryptonite >= 0.22 import Crypto.Hash ( hash ) import Crypto.Hash.Algorithms ( Blake2b_160 ) -- package: memory >= 0.14 import Data.ByteArray ( convert ) -- package: text >= 1.0.0.0 import Data.Text ( Text )\n-- package: base >= 4.0.0 import Prelude import Data.Function ( (&) ) -- package: bech32 >= 1.0.2 import qualified Codec.Binary.Bech32 as Bech32 -- package: bech32-th >= 1.0.2 import Codec.Binary.Bech32.TH ( humanReadablePart ) -- package: bytestring >= 0.10.0.0 import Data.ByteString ( ByteString ) -- package: cryptonite >= 0.22 import Crypto.Hash ( hash ) import Crypto.Hash.Algorithms ( Blake2b_160 ) -- package: memory >= 0.14 import Data.ByteArray ( convert ) -- package: text >= 1.0.0.0 import Data.Text ( Text )\nnewtype PolicyId = PolicyId ByteString newtype AssetName = AssetName ByteString newtype AssetFingerprint = AssetFingerprint Text mkAssetFingerprint :: PolicyId -> AssetName -> AssetFingerprint mkAssetFingerprint (PolicyId policyId) (AssetName assetName) = (policyId <> assetName) & convert . hash @_ @Blake2b_160 & Bech32.encodeLenient hrp . Bech32.dataPartFromBytes & AssetFingerprint where hrp = [humanReadablePart|asset|]\nnewtype PolicyId = PolicyId ByteString newtype AssetName = AssetName ByteString newtype AssetFingerprint = AssetFingerprint Text mkAssetFingerprint :: PolicyId -> AssetName -> AssetFingerprint mkAssetFingerprint (PolicyId policyId) (AssetName assetName) = (policyId <> assetName) & convert . hash @_ @Blake2b_160 & Bech32.encodeLenient hrp . Bech32.dataPartFromBytes & AssetFingerprint where hrp = [humanReadablePart|asset|]\n:information_source: policy_id and asset_name are hereby base16-encoded; their raw, decoded, versions should be used when computing the fingerprint.\npolicy_id\nasset_name\n- policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_name: \"\" asset_fingerprint: asset1rjklcrnsdzqp65wjgrg55sy9723kw09mlgvlc3 - policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc37e asset_name: \"\" asset_fingerprint: asset1nl0puwxmhas8fawxp8nx4e2q3wekg969n2auw3 - policy_id: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209 asset_name: \"\" asset_fingerprint: asset1uyuxku60yqe57nusqzjx38aan3f2wq6s93f6ea - policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_name: 504154415445 asset_fingerprint: asset13n25uv0yaf5kus35fm2k86cqy60z58d9xmde92 - policy_id: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209 asset_name: 504154415445 asset_fingerprint: asset1hv4p5tv2a837mzqrst04d0dcptdjmluqvdx9k3 - policy_id: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209 asset_name: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_fingerprint: asset1aqrdypg669jgazruv5ah07nuyqe0wxjhe2el6f - policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_name: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209 asset_fingerprint: asset17jd78wukhtrnmjh3fngzasxm8rck0l2r4hhyyt - policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_name: 0000000000000000000000000000000000000000000000000000000000000000 asset_fingerprint: asset1pkpwyknlvul7az0xx8czhl60pyel45rpje4z8w\n- policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_name: \"\" asset_fingerprint: asset1rjklcrnsdzqp65wjgrg55sy9723kw09mlgvlc3 - policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc37e asset_name: \"\" asset_fingerprint: asset1nl0puwxmhas8fawxp8nx4e2q3wekg969n2auw3 - policy_id: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209 asset_name: \"\" asset_fingerprint: asset1uyuxku60yqe57nusqzjx38aan3f2wq6s93f6ea - policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_name: 504154415445 asset_fingerprint: asset13n25uv0yaf5kus35fm2k86cqy60z58d9xmde92 - policy_id: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209 asset_name: 504154415445 asset_fingerprint: asset1hv4p5tv2a837mzqrst04d0dcptdjmluqvdx9k3 - policy_id: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209 asset_name: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_fingerprint: asset1aqrdypg669jgazruv5ah07nuyqe0wxjhe2el6f - policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_name: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209 asset_fingerprint: asset17jd78wukhtrnmjh3fngzasxm8rck0l2r4hhyyt - policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373 asset_name: 0000000000000000000000000000000000000000000000000000000000000000 asset_fingerprint: asset1pkpwyknlvul7az0xx8czhl60pyel45rpje4z8w\nThe asset fingerprint needs to be somewhat unique (although collisions are plausible, see next section) and refer to a particular asset. It must therefore include both the policy id and the asset name.\nThe asset fingerprint needs to be somewhat unique (although collisions are plausible, see next section) and refer to a particular asset. It must therefore include both the policy id and the asset name.\nUsing a hash gives us asset id of a same deterministic length which is short enough to display reasonably well on small screens.\nUsing a hash gives us asset id of a same deterministic length which is short enough to display reasonably well on small screens.\nWe use bech32 as a user-facing encoding since it is both user-friendly and quite common within the Cardano eco-system (e.g. addresses, pool ids, keys).\nWe use bech32 as a user-facing encoding since it is both user-friendly and quite common within the Cardano eco-system (e.g. addresses, pool ids, keys).\nWith a 160-bit digest, an attacker needs at least 2 80 operations to find a collision. Although 2 80 operations is relatively low (it remains expansive but doable for an attacker), it is considered safe within the context of an asset fingerprint as a mean of user verification within a particular wallet. An attacker may obtain advantage if users can be persuaded that a certain asset is in reality another (which implies to find a collision, and make both assets at the reach of the user).\nWith a 160-bit digest, an attacker needs at least 2 80 operations to find a collision. Although 2 80 operations is relatively low (it remains expansive but doable for an attacker), it is considered safe within the context of an asset fingerprint as a mean of user verification within a particular wallet. An attacker may obtain advantage if users can be persuaded that a certain asset is in reality another (which implies to find a collision, and make both assets at the reach of the user).\nWe recommend however that in addition to the asset fingerprint, applications also show whenever possible a visual checksum calculated from the policy id and the asset name as specified in CIP-YET-TO-COME. Such generated images, which are designed to be unique and easy to distinguish, in combination with a readable asset fingerprint gives strong verification means to end users.\nWe recommend however that in addition to the asset fingerprint, applications also show whenever possible a visual checksum calculated from the policy id and the asset name as specified in CIP-YET-TO-COME. Such generated images, which are designed to be unique and easy to distinguish, in combination with a readable asset fingerprint gives strong verification means to end users.\nAsset fingerprints as described have been universally adopted in: wallets, blockchain explorers, query layers, token minting utilities, NFT specifications, and CLI tools.\nReference implementations available in both Javascript and Haskell.\nPublic presentation with confirmed interest in adopting this standard in advance of Mary ledger era.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0015 | Registration Transaction Metadata Format\n\nThis CIP details a transaction metadata format, used by Cardano's Project Catalyst for voter registrations.\nProject Catalyst uses a sidechain for it's voting system. One of the desirable properties of this sidechain is that even if its' safety is compromised, it doesn't cause a loss of funds on Cardano mainnet. To achieve this, instead of using Cardano wallets' recovery phrase on the sidechain, we introduce the \"voting key\".\nHowever, since Catalyst uses stake-based voting, a user needs to associate their mainnet Ada to their voting key. This can be achieved through a voter registration transaction.\nWe therefore need a voter registration transaction that serves three purposes:\nRegisters a voting key to be included in the sidechain Associates Mainnet ada to this voting key Declare an address to receive Catalyst voting rewards\nRegisters a voting key to be included in the sidechain\nAssociates Mainnet ada to this voting key\nDeclare an address to receive Catalyst voting rewards\nA voting key is simply an ED25519 key. How this key is created is up to the wallet, although it is not recommended that the wallet derives this key deterministicly from a mnemonic used on Cardano.\nA Catalyst registration transaction is a regular Cardano transaction with a specific transaction metadata associated with it.\nNotably, there should be four entries inside the metadata map:\nVoting registration example:\n61284: { // voting_key - CBOR byte array 1: \"0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663\", // stake_pub - CBOR byte array 2: \"0xad4b948699193634a39dd56f779a2951a24779ad52aa7916f6912b8ec4702cee\", // reward_address - CBOR byte array 3: \"0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee47b60edc7772855324c85033c638364214cbfc6627889f81c4\", // nonce 4: 5479467 }\n61284: { // voting_key - CBOR byte array 1: \"0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663\", // stake_pub - CBOR byte array 2: \"0xad4b948699193634a39dd56f779a2951a24779ad52aa7916f6912b8ec4702cee\", // reward_address - CBOR byte array 3: \"0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee47b60edc7772855324c85033c638364214cbfc6627889f81c4\", // nonce 4: 5479467 }\nThe entries under keys 1, 2, 3, and 4 represent the Catalyst voting key, the staking key on the Cardano network, the address to receive rewards, and a nonce, respectively. A registration with these metadata will be considered valid if the following conditions hold:\nThe nonce is an unsigned integer (of CBOR major type 0) that should be monotonically rising across all transactions with the same staking key. The advised way to construct a nonce is to use the current slot number. This is a simple way to keep the nonce increasing without having to access the previous transaction data.\nThe advised way to construct a nonce is to use the current slot number.\nThis is a simple way to keep the nonce increasing without having to access the previous transaction data.\nThe reward address is a Shelley address discriminated for the same network this transaction is submitted to.\nTo produce the signature field, the CBOR representation of a map containing a single entry with key 61284 and the registration metadata map in the format above is formed, designated here as sign_data. This data is signed with the staking key as follows: first, the blake2b-256 hash of sign_data is obtained. This hash is then signed using the Ed25519 signature algorithm. The signature metadata entry is added to the transaction under key 61285 as a CBOR map with a single entry that consists of the integer key 1 and signature as obtained above as the byte array value.\n61284\nsign_data\nsign_data\nSignature example:\n61285: { // signature - ED25119 signature CBOR byte array 1: \"0x8b508822ac89bacb1f9c3a3ef0dc62fd72a0bd3849e2381b17272b68a8f52ea8240dcc855f2264db29a8512bfcd522ab69b982cb011e5f43d0154e72f505f007\" }\n61285: { // signature - ED25119 signature CBOR byte array 1: \"0x8b508822ac89bacb1f9c3a3ef0dc62fd72a0bd3849e2381b17272b68a8f52ea8240dcc855f2264db29a8512bfcd522ab69b982cb011e5f43d0154e72f505f007\" }\nThis CIP is not to be versioned using a traditional scheme, rather if any large technical changes require a new proposal to replace this one. Small changes can be made if they are completely backwards compatible.\nCatalyst Fund 3:\nadded the reward_address inside the key_registration field.\nreward_address\nkey_registration\nCatalyst Fund 4:\nadded the nonce field to prevent replay attacks;\nnonce\nchanged the signature algorithm from one that signed sign_data directly to signing the Blake2b hash of sign_data to accommodate memory-constrained hardware wallet devices.\nsign_data\nsign_data\nIt was planned that since Fund 4, registration_signature and the staking_pub_key entry inside the key_registration field will be deprecated. This has been deferred to a future revision of the protocol.\nregistration_signature\nstaking_pub_key\nkey_registration\nThe described metadata format allows for the association of a voting key with a stake credential on a Cardano network.\nCardano uses the UTxO model so to completely associate a wallet's balance with a voting key (i.e. including enterprise addresses), we would need to associate every payment key to a voting key individually. Although there are attempts at this (see CIP-0008), the resulting data structure is a little excessive for on-chain metadata (which we want to keep small).\nGiven the above, we choose to only associate staking keys with voting keys. Since most Cardano wallets only use base addresses for Shelley wallet types, in most cases this should perfectly match the user's wallet.\nA future change of the Catalyst system may make use of a time-lock script to commit ADA on the mainnet for the duration of a voting period. The voter registration metadata in this method will not need an association with the staking key. Therefore, the staking_pub_key map entry and the registration_signature payload with key 61285 will no longer be required.\nstaking_pub_key\nregistration_signature\n61285\nThis metadata format is implemented by at least 3 wallets Deadalus https://daedaluswallet.io/ Eternl https://eternl.io/, Flint https://flint-wallet.com/, Typhon https://typhonwallet.io/, Yoroi https://yoroi-wallet.com/\nDeadalus https://daedaluswallet.io/\nEternl https://eternl.io/,\nFlint https://flint-wallet.com/,\nTyphon https://typhonwallet.io/,\nYoroi https://yoroi-wallet.com/\nThis metadata format is used by Catalyst for at least 3 funds This format has been used up to and included Catalyst fund 10\nThis format has been used up to and included Catalyst fund 10\nAuthor(s) to provide a schema cddl file See the schema file\nSee the schema file\nAuthor(s) to provide a test vectors file See test vector file\nSee test vector file\nAuthor(s) to provide a npm package to support the creation of this metadata format catalyst-registration-js\ncatalyst-registration-js\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0016 | Cryptographic Key Serialisation Formats\n\nThis CIP defines serialisation formats for the following types of cryptographic keys across the Cardano eco-system:\nRegular Ed25519 keys\nRegular Ed25519 keys\nBIP32-Ed25519 extended keys (Ed25519 extended keys with BIP32-style derivation)\nBIP32-Ed25519 extended keys (Ed25519 extended keys with BIP32-style derivation)\nThroughout the Cardano eco-system, different projects have used different serialisation formats for cryptographic keys.\nFor example, for BIP32-Ed25519 extended signing keys, the cardano-crypto implementation supports a 128-byte binary serialization format, while jcli and cardano-addresses supports a 96-byte binary serialization format.\ncardano-crypto\njcli\ncardano-addresses\nAnother example would be cardano-cli which supports a custom JSON format, referred to as \"text envelope\", (which can be used for serialising keys) that isn't supported by other projects in the eco-system.\ncardano-cli\nThis has introduced compatibility problems for both users and developers:\nUsers cannot easily utilize their keys across different tools and software in the Cardano eco-system as they may be serialized in different ways.\nUsers cannot easily utilize their keys across different tools and software in the Cardano eco-system as they may be serialized in different ways.\nDevelopers wanting to support the different serialisation formats may need to write potentially error-prone (de)serialisation and conversion operations.\nDevelopers wanting to support the different serialisation formats may need to write potentially error-prone (de)serialisation and conversion operations.\nTherefore, this CIP aims to define standard cryptographic key serialisation formats to be used by projects throughout the Cardano eco-system.\nFor the verification (public) key binary format, we simply use the raw 32-byte Ed25519 public key data.\nThis structure should be Bech32 encoded, using one of the appropriate *_vk prefixes defined in CIP-0005.\n*_vk\nFor extended verification (public) keys, we define the following 64-byte binary format:\n+-----------------------+-----------------------+ | Public Key (32 bytes) | Chain Code (32 bytes) | +-----------------------+-----------------------+\n+-----------------------+-----------------------+ | Public Key (32 bytes) | Chain Code (32 bytes) | +-----------------------+-----------------------+\nThat is, a 32-byte Ed25519 public key followed by a 32-byte chain code.\nThis structure should be Bech32 encoded, using one of the appropriate *_xvk prefixes defined in CIP-0005.\n*_xvk\nFor the signing (private) key binary format, we simply use the raw 32-byte Ed25519 private key data.\nThis structure should be Bech32 encoded, using one of the appropriate *_sk prefixes defined in CIP-0005.\n*_sk\nFor extended signing (private) keys, we define the following 96-byte binary format:\n+---------------------------------+-----------------------+ | Extended Private Key (64 bytes) | Chain Code (32 bytes) | +---------------------------------+-----------------------+\n+---------------------------------+-----------------------+ | Extended Private Key (64 bytes) | Chain Code (32 bytes) | +---------------------------------+-----------------------+\nThat is, a 64-byte Ed25519 extended private key followed by a 32-byte chain code.\nThis structure should be Bech32 encoded, using one of the appropriate *_xsk prefixes defined in CIP-0005.\n*_xsk\nAs mentioned in the Abstract, the original cardano-crypto implementation defined a 128-byte binary serialization format for BIP32-Ed25519 extended signing keys:\ncardano-crypto\n+---------------------------------+-----------------------+-----------------------+ | Extended Private Key (64 bytes) | Public Key (32 bytes) | Chain Code (32 bytes) | +---------------------------------+-----------------------+-----------------------+\n+---------------------------------+-----------------------+-----------------------+ | Extended Private Key (64 bytes) | Public Key (32 bytes) | Chain Code (32 bytes) | +---------------------------------+-----------------------+-----------------------+\nHowever, as it turns out, keeping around the 32-byte Ed25519 public key is redundant as it can easily be derived from the Ed25519 private key (the first 32 bytes of the 64-byte extended private key).\nTherefore, because other projects such as jcli and cardano-addresses already utilize the more compact 96-byte format, we opt to define that as the standard.\njcli\ncardano-addresses\nConfirm support by applications and tools from different developers: jcli cardano-signer cardano-serialization-lib\njcli\ncardano-signer\ncardano-serialization-lib\nN/A\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0017 | Cardano Delegation Portfolio\n\nThis document details a common format for sharing Cardano delegation portfolio across various tools and wallets.\nStakeholders have indicated the desire to split their stake in various sizes and delegate to n pools from a single wallet/mnemonic. Albeit there are no monetary incentive for users to do this, the desire to drive decentralisation is sufficiently prevalent to justify it. Furthermore, stakeholders want to introduce a certain social element to this activity by sharing their delegation portfolio with other stakeholders. This specification should help to standardize the representation of portfolios across tools for more interoperability.\nWe'll use JSON as a data format for it is commonly used and supported across many programming languages, and is also relatively readable on itself. Portfolios should abide by the JSON schema given in appendix.\nAt minima, a portfolio should cover a list of delegation choices (pools and weights) and have a human-readable name for easier identification.\nFor each pool, we demand a weight which can capture a certain stake proportion within the portfolio. The value is an integer, and relative to other weights in the portfolio. For example, a portfolio with two pools and respective weights of 1 and 2 means that we expect users to assign twice more stake to the second pool than the first. Fundamentally, this means that for every 3 Ada, 1 Ada should go to the first pool, and 2 Ada should go to the second. Note that this is equivalent to having weights of 10/20 or 14 / 28. Weights are ultimately interpreted as fractions.\nweight\n1\n2\n10\n20\n14\n28\nPortfolios which treat all stake pools equally should use the same weight (e.g. 1) for each pool.\n1\n{ \"name\": \"Metal 🤘\" , \"description\": \"Pools supporting Metal music across the world.\" , \"pools\": [ { \"id\": \"d59123f4dce7c62fa74bd37a759c7ba665dbabeb28f08b4e5d4802ca\" , \"name\": \"Dark Tranquility\" , \"ticker\": \"DARK\" , \"weight\": 42 } , { \"id\": \"5f3833027fe8c8d63bc5e75960d9a22df52e41bdf62af5b689663c50\" , \"ticker\": \"NITRO\" , \"weight\": 14 } , { \"id\": \"a16abb03d87b86f30bb743aad2e2504b126286fe744d3d2f6a0b4aec\" , \"name\": \"Loudness\" , \"ticker\": \"LOUD\" , \"weight\": 37 } , { \"id\": \"9f9bdee3e053e3102815b778db5ef8d55393f7ae83b36f906f4c3a47\" , \"weight\": 25 } ] }\n{ \"name\": \"Metal 🤘\" , \"description\": \"Pools supporting Metal music across the world.\" , \"pools\": [ { \"id\": \"d59123f4dce7c62fa74bd37a759c7ba665dbabeb28f08b4e5d4802ca\" , \"name\": \"Dark Tranquility\" , \"ticker\": \"DARK\" , \"weight\": 42 } , { \"id\": \"5f3833027fe8c8d63bc5e75960d9a22df52e41bdf62af5b689663c50\" , \"ticker\": \"NITRO\" , \"weight\": 14 } , { \"id\": \"a16abb03d87b86f30bb743aad2e2504b126286fe744d3d2f6a0b4aec\" , \"name\": \"Loudness\" , \"ticker\": \"LOUD\" , \"weight\": 37 } , { \"id\": \"9f9bdee3e053e3102815b778db5ef8d55393f7ae83b36f906f4c3a47\" , \"weight\": 25 } ] }\nJSON is widely used, widely supported and quite lightweight. Makes for a reasonable choice of data format. Using JSON schema for validation is quite common when dealing with JSON and it's usually sufficiently precise to enable good interoperability. The portfolio should only capture information that are not subject to radical change. That is, stake pools parameters like pledge or fees are excluded since they can be changed fairly easily using on-chain certificate updates. The JSON schema doesn't enforce any additionalProperties: false for neither the top-level object definition nor each stake pool objects. This allows for open extension of the objects with custom fields at the discretion of applications implementing this standard. The semantic of well-known properties specified in this document is however fixed. Since the portfolio format isn't immediately user-facing, we favor base16 over bech32 for the pool id's encoding for there's better support and tooling for the former.\nJSON is widely used, widely supported and quite lightweight. Makes for a reasonable choice of data format.\nJSON is widely used, widely supported and quite lightweight. Makes for a reasonable choice of data format.\nUsing JSON schema for validation is quite common when dealing with JSON and it's usually sufficiently precise to enable good interoperability.\nUsing JSON schema for validation is quite common when dealing with JSON and it's usually sufficiently precise to enable good interoperability.\nThe portfolio should only capture information that are not subject to radical change. That is, stake pools parameters like pledge or fees are excluded since they can be changed fairly easily using on-chain certificate updates.\nThe portfolio should only capture information that are not subject to radical change. That is, stake pools parameters like pledge or fees are excluded since they can be changed fairly easily using on-chain certificate updates.\nThe JSON schema doesn't enforce any additionalProperties: false for neither the top-level object definition nor each stake pool objects. This allows for open extension of the objects with custom fields at the discretion of applications implementing this standard. The semantic of well-known properties specified in this document is however fixed.\nThe JSON schema doesn't enforce any additionalProperties: false for neither the top-level object definition nor each stake pool objects. This allows for open extension of the objects with custom fields at the discretion of applications implementing this standard. The semantic of well-known properties specified in this document is however fixed.\nadditionalProperties: false\nSince the portfolio format isn't immediately user-facing, we favor base16 over bech32 for the pool id's encoding for there's better support and tooling for the former.\nSince the portfolio format isn't immediately user-facing, we favor base16 over bech32 for the pool id's encoding for there's better support and tooling for the former.\nThe format used by Adafolio share a lot of similarities with the proposed format in this CIP. In order to power its frontend user interface, Adafolio contains however several fields which we consider too volatile and unnecessary to the definition of a portfolio. This doesn't preclude the format used by Adafolio as a valid portfolio format (see also point (4). in the rationale above).\nThe only point of incompatibility regards the pool_id field (in Adafolio) vs the id field (in this proposal) which we deem more consistent with regards to other field.\npool_id\nid\nAt least one pair of applications (wallets, explorers or other tools) together support the following: generation of the specified portfolio file format interpretation and use of the specified portfolio file format\ngeneration of the specified portfolio file format\ninterpretation and use of the specified portfolio file format\nProvide a reference implementation and/or parsing library to read and/or write files in this schema.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0018 | Multi-Stake-Keys Wallets\n\nThis document describes how to evolve sequential wallets from Cardano to support multiple stake keys. This proposal is an extension of CIP-1852 and CIP-0011.\nCardano wallets originally approached stake delegation by considering a single stake key per wallet. While this was beneficial in terms of ease of implementation and simplicity of reasoning, this is unsuitable for many users with large stakes. Indeed, the inability to split out the stake into multiple pools often leads to over-saturation of existing pools in case of delegation. The only workaround so far requires users to split their funds into multiple accounts and manage them independently. This can be quite cumbersome for a sufficiently large number of accounts.\nEven for smaller actors, it can be interesting to delegate to multiple pools to limit risks. Pools may underperform or simply change their parameters from a day to another (for which wallets still do not warn users about). Delegating to more pools can reduce the impact of pool failure (for one or more epochs) or unattended changes in pool fees. It may as well be a matter of choice if users do want to delegate to several independent entities for various other reasons.\nAnother concern regards privacy leaks coming with the existing wallet scheme. Since the same stake key is associated with every single address of the wallet, it creates a kind of watermark that allows for tracing all funds belonging to the same wallet very easily (it suffices to look at the stake part of addresses). By allowing the wallet to hold multiple stake keys, rotating through them when creating changes does make the traceability a bit harder. One could imagine using a hundred stake keys delegated to the same pool.\nThe restriction from CIP-0011 regarding the derivation index reserved for stake key is rendered obsolete by this specification. That is, one is allowed to derive indexes beyond the first one (index = 0) to effectively administrate multi-stake keys accounts. The creation of new stake keys is however tightly coupled to the registration of associated stake keys to allow wallet software to automatically discover stake keys on-chain. In this design, stake key indexes form at all time a contiguous sequence with no gap.\nWe introduce the concept of UTxO stake key pointer to reliably keep track of stake keys on the blockchain. The gist is to require that every key registration and/or deregistration consume and create a special UTxO which in itself is pointing to the next available stake key of the wallet. Such pointer allows piggybacking on the existing UTxO structure to cope with concurrency issues and rollbacks that are inherent to a distributed system such as Cardano. Plus, this mechanism only demands a low overhead for wallet software and may be recognized as a special spending pattern by hardware devices.\nIn more details, we require that beyond the first stake key (index = 0), all registrations must satisfy the following rules:\nStake keys must be derived sequentially, from 0 and onwards. Every stake key registration must be accompanied by a matching delegation certificate. Every registration transaction must create a special output of exactly minUTxOValue Ada such that its address is an enterprise address with a single payment part using the stake key hash of the next available stake key of the wallet to be registered after processing the registration transaction. Unless no key beyond the first one is registered, every registration transaction must consume the special UTxO stake key pointer corresponding to the previous key registration (resp. de-registration) for that wallet.\nStake keys must be derived sequentially, from 0 and onwards.\nEvery stake key registration must be accompanied by a matching delegation certificate.\nEvery registration transaction must create a special output of exactly minUTxOValue Ada such that its address is an enterprise address with a single payment part using the stake key hash of the next available stake key of the wallet to be registered after processing the registration transaction.\nminUTxOValue\nUnless no key beyond the first one is registered, every registration transaction must consume the special UTxO stake key pointer corresponding to the previous key registration (resp. de-registration) for that wallet.\nNote The minUTxOValue is fixed by the protocol. It is defined as part of the Shelley genesis and can be updated via on-chain protocol updates. At the moment, this equals 1 Ada on the mainnet.\nminUTxOValue\nFor example, a wallet that has already registered stake keys 0, 1 and 2 have a UTxO for which the payment part is a hash of the stake key at index=3. If the wallet wants to register two new stake keys at index 3 and 4, it'll do so in a single transaction, by consuming the pointer UTxO and by creating a new one for which the payment part would be a hash of the stake key at index=5.\nNote that this only requires two signatures from stake keys at indexes 3 and 4.\nKey de-registrations work symmetrically and require that:\nStake keys are de-registered sequentially, from the highest and downwards. Unless the first key of the wallet is being de-registered, every de-registration transaction must create a special output of exactly minUtxOValue Ada such that its address is an enterprise address with a single payment part using the stake key hash of the next stake key of the wallet after processing the de-registration transaction. Every de-registration must consume the special UTxO stake key pointer corresponding to the previous key de-registration (resp. registration) for that wallet.\nStake keys are de-registered sequentially, from the highest and downwards.\nUnless the first key of the wallet is being de-registered, every de-registration transaction must create a special output of exactly minUtxOValue Ada such that its address is an enterprise address with a single payment part using the stake key hash of the next stake key of the wallet after processing the de-registration transaction.\nminUtxOValue\nEvery de-registration must consume the special UTxO stake key pointer corresponding to the previous key de-registration (resp. registration) for that wallet.\nAs stated in the introduction, this proposal is built on top of CIP-0011 such that backward compatibility is preserved when a single key is used. In fact, The management of the first stake key at index 0 remains unchanged and does not require any pointer. This preserves backward compatibility with the existing design for a single stake key wallet and offers a design that can be implemented on top, retro-actively.\nNevertheless, we do assume that existing wallets following CIP-0011 are already fully capable of discovering addresses using stake keys not belonging to the wallet. Some may even report them as mangled.\nNote An address is said mangled when it has a stake part, and the stake part isn't recognized as belonging to its associated wallet. That is, the payment part and the stake part appear to come from two different sources. This could be the case if the address has been purposely constructed in such a way (because the stake rights and funds are managed by separate entities), or because the stake part refers to a key hash which is no longer known of the wallet (because the associated key registration was rolled back).\nAs a result, this extension would not incapacitate existing wallets since the payment ownership is left untouched. However, wallets not supporting the extension may display addresses delegated to keys beyond the first one as mangled and may also fail to report rewards correctly across multiple keys.\nWe deem this to be an acceptable and fairly minor consequence but encourage existing software to raise awareness about this behaviour.\nCarrying an extra UTxO pointer makes it possible to not worry (too much) about concurrency issues and problems coming with either, multiple instances of the wallet (like many users do between a mobile and desktop wallet) or the usual rollbacks which may otherwise create gaps in the indexes. By forcing all registration (resp. deregistration) transactions to be chained together, we also enforce that any rollbacks do maintain consistency of the index state: if any intermediate transaction is rolled back, then transactions they depend on are also rolled back.\nCarrying an extra UTxO pointer makes it possible to not worry (too much) about concurrency issues and problems coming with either, multiple instances of the wallet (like many users do between a mobile and desktop wallet) or the usual rollbacks which may otherwise create gaps in the indexes. By forcing all registration (resp. deregistration) transactions to be chained together, we also enforce that any rollbacks do maintain consistency of the index state: if any intermediate transaction is rolled back, then transactions they depend on are also rolled back.\nThe first registration induces an extra cost for the end-user for the wallet needs to create a new UTxO with a minimum value. That UTxO is however passed from registration to registration afterwards without any extra cost. It can also be fully refunded upon de-registering the last stake key. So in practice, it works very much like a key deposit.\nThe first registration induces an extra cost for the end-user for the wallet needs to create a new UTxO with a minimum value. That UTxO is however passed from registration to registration afterwards without any extra cost. It can also be fully refunded upon de-registering the last stake key. So in practice, it works very much like a key deposit.\nWe do not allow mixing up key registration and key deregistration as part of the same transaction for it makes the calculation of the pointer trickier for wallet processing transactions. A single transaction either move the pointer up or down.\nWe do not allow mixing up key registration and key deregistration as part of the same transaction for it makes the calculation of the pointer trickier for wallet processing transactions. A single transaction either move the pointer up or down.\nThere's in principle nothing preventing someone from sending money to the special key-registration tracking address. Wallets should however only keep track of UTxOs created as part of transactions that register stake keys (and have therefore been authorized by the wallet itself). Applications are however encouraged to collect any funds sent to them in an ad-hoc manner on such keys.\nThere's in principle nothing preventing someone from sending money to the special key-registration tracking address. Wallets should however only keep track of UTxOs created as part of transactions that register stake keys (and have therefore been authorized by the wallet itself). Applications are however encouraged to collect any funds sent to them in an ad-hoc manner on such keys.\nThere exists one or more reference implementations with appropriate testing illustrating the viability of this approach and specification.\nUpdate this proposal to account for the Conway Ledger era, which brings new types of certificates for registering stake keys.\nUpdate this proposal to account for the Conway Ledger era, which brings new types of certificates for registering stake keys.\nDevelop the proposed Reference Implementation as suggested when this CIP was originally published (see Discussion link in header for history).\nDevelop the proposed Reference Implementation as suggested when this CIP was originally published (see Discussion link in header for history).\nContact wallet and dApp representatives in the community to develop and maintain interest in their support for this specification.\nContact wallet and dApp representatives in the community to develop and maintain interest in their support for this specification.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0019 | Cardano Addresses\n\nThis specification describes the structure of addresses in Cardano, covering both addresses introduced in the Shelley era and the legacy format from the Byron era.\nDocument design choices for posterity. Most applications interacting with the Cardano blockchain will likely not have any need for this level of details, however, some might. This CIP is meant to capture this knowledge.\nIn Cardano, an address is a sequence of bytes that conforms to a particular format, which we describe below.\nHowever, users will typically come into contact with addresses only after these addresses have been encoded into sequences of human-readable characters. In Cardano, the Bech32 and Base58 encodings are used to encode addresses, as opposed to standard hexadecimal notation (Base16, example 0x8A7B). These encoded sequence of characters have to be distinguished from the byte sequences that they encode, but lay users will (and should) perceive the encoded form as \"the\" address.\n0x8A7B\nBy convention, Shelley and stake addresses are encoded using Bech32, with the exception that Cardano does not impose a length limit on the sequence of characters. The human-readable prefixes are defined in CIP-0005; the most common prefix is addr, representing an address on mainnet. Bech32 is the preferred encoding, as its built-in error detection may protect users against accidental misspellings or truncations.\naddr\nAgain by convention, Byron addresses are encoded in Base58.\nHistorically, Byron addresses were introduced before the design of Bech32, which solves various issues of the Base58 encoding format (see Bech32's motivation for more detail). Byron addresses were however kept as Base58 to easily distinguish them from new addresses introduced in Shelley, massively making use of Bech32 for encoding small binary objects.\nCave: In principle, it is possible for a Shelley address to be encoded in Base58 and a Byron address to be encoded in Bech32 (without length limit). However, implementations are encouraged to reject addresses that were encoded against convention, as this helps with the goal that lay users only encounter a single, canonical version of every address.\nExamples of different addresses encoded in different eras:\n37btjrVyb4KDXBNC4haBVPCrro8AQPHwvCMp3RFhhSVWwfFmZ6wwzSK6JK1hY6wHNmtrpTf1kdbva8TCneM2YsiXT7mrzT21EacHnPpz5YyUdj64na\naddr1vpu5vlrf4xkxv2qpwngf6cjhtw542ayty80v8dyr49rf5eg0yu80w\nstake1vpu5vlrf4xkxv2qpwngf6cjhtw542ayty80v8dyr49rf5egfu2p0u\nIn Cardano, the sequence of bytes (after decoding with Bech32 or Base58) that represents an address comprises two parts, a one-byte header and a payload of several bytes. Depending on the header, the interpretation and length of the payload varies.\nIn the header-byte, bits [7;4] indicate the type of addresses being used; we'll call these four bits the header type. The remaining four bits [3;0] are either unused or refer to what we'll call the network tag. There are currently 11 types of addresses in Cardano which we'll divide into three categories: Shelley addresses, stake addresses, and Byron addresses.\n1 byte variable length <------> <-------------------> ┌────────┬─────────────────────┐ │ header │ payload │ └────────┴─────────────────────┘ 🔎 ╎ 7 6 5 4 3 2 1 0 ╎ ┌─┬─┬─┬─┬─┬─┬─┬─┐ ╰╌╌╌╌╌╌╌╌ │t│t│t│t│n│n│n│n│ └─┴─┴─┴─┴─┴─┴─┴─┘\n1 byte variable length <------> <-------------------> ┌────────┬─────────────────────┐ │ header │ payload │ └────────┴─────────────────────┘ 🔎 ╎ 7 6 5 4 3 2 1 0 ╎ ┌─┬─┬─┬─┬─┬─┬─┬─┐ ╰╌╌╌╌╌╌╌╌ │t│t│t│t│n│n│n│n│ └─┴─┴─┴─┴─┴─┴─┴─┘\nSee also the more detailed ABNF grammar in annex.\nExcept for Byron addresses (type 8 = 1000), the second half of the header (bits [3;0]) refers to the network tag which can have the following values and semantics. Other values of the network tag are currently reserved for future network types. In the case of Byron addresses, bits [3;0] have a completely separate definition detailed in the section below.\n1000\n. . . . n n n n\n....0000\n....0001\nThere are currently 8 types of Shelley addresses summarized in the table below:\nt t t t . . . .\n0000....\nPaymentKeyHash\nStakeKeyHash\n0001....\nScriptHash\nStakeKeyHash\n0010....\nPaymentKeyHash\nScriptHash\n0011....\nScriptHash\nScriptHash\n0100....\nPaymentKeyHash\nPointer\n0101....\nScriptHash\nPointer\n0110....\nPaymentKeyHash\n0111....\nScriptHash\nPaymentKeyHash and StakeKeyHash refer to blake2b-224 hash digests of Ed25519 verification keys. How keys are obtained is out of the scope of this specification. Interested readers may look at CIP-1852 for more details.\nPaymentKeyHash and StakeKeyHash refer to blake2b-224 hash digests of Ed25519 verification keys. How keys are obtained is out of the scope of this specification. Interested readers may look at CIP-1852 for more details.\nPaymentKeyHash\nStakeKeyHash\nblake2b-224\nScriptHash refer to blake2b-224 hash digests of serialized monetary scripts. How scripts are constructed and serialized is out of the scope of this specification.\nScriptHash refer to blake2b-224 hash digests of serialized monetary scripts. How scripts are constructed and serialized is out of the scope of this specification.\nScriptHash\nblake2b-224\nPointer is detailed in the section below.\nPointer is detailed in the section below.\nPointer\nFundamentally, the first part of a Shelley address indicates the ownership of the funds associated with the address. We call it the payment part. Whoever owns the payment parts owns any funds at the address. As a matter of fact, in order to spend from an address, one must provide a witness attesting that the address can be spent. In the case of a PaymentKeyHash, it means providing a signature of the transaction body made with the signing key corresponding to the hashed public key (as well as the public key itself for verification). For monetary scripts, it means being able to provide the source script and meet the necessary conditions to validate the script.\nPaymentKeyHash\nThe second part of a Shelley address indicates the owner of the stake rights associated with the address. We call it the delegation part. Whoever owns the delegation parts owns the stake rights of any funds associated with the address. In most scenarios, the payment part and the delegation part are owned by the same party. Yet it is possible to construct addresses where both parts are owned and managed by separate entities. We call such addresses mangled addresses or hybrid addresses.\nSome addresses (types 6 and 7) carry no delegation part whatsoever. Their associated stake can't be delegated. They can be used by parties who want to prove that they are not delegating funds which is typically the case for custodial businesses managing funds on the behalf of other stakeholders. Delegation parts can also be defined in terms of on-chain pointers.\nNote From the Conway ledger era, new pointer addresses cannot be added to Mainnet.\nIn an address, a chain pointer refers to a point of the chain containing a stake key registration certificate. A point is identified by 3 coordinates:\nAn absolute slot number\nA transaction index (within that slot)\nA (delegation) certificate index (within that transaction)\nThese coordinates form a concise way of referring to a stake key (typically half the size of a stake key hash). They are serialized as three variable-length positive numbers following the ABNF grammar here below:\nPOINTER = VARIABLE-LENGTH-UINT ; slot number | VARIABLE-LENGTH-UINT ; transaction index | VARIABLE-LENGTH-UINT ; certificate index VARIABLE-LENGTH-UINT = (%b1 | UINT7 | VARIABLE-LENGTH-UINT) / (%b0 | UINT7) UINT7 = 7BIT\nPOINTER = VARIABLE-LENGTH-UINT ; slot number | VARIABLE-LENGTH-UINT ; transaction index | VARIABLE-LENGTH-UINT ; certificate index VARIABLE-LENGTH-UINT = (%b1 | UINT7 | VARIABLE-LENGTH-UINT) / (%b0 | UINT7) UINT7 = 7BIT\nLike Shelley addresses, stake addresses (also known as reward addresses) start with a single header byte identifying their type and the network, followed by 28 bytes of payload identifying either a stake key hash or a script hash.\nt t t t . . . .\n1110....\nStakeKeyHash\n1111....\nScriptHash\nStakeKeyHash refers to blake2b-224 hash digests of Ed25519 verification keys. How keys are obtained is out of the scope of this specification. Interested readers may look at CIP-1852 for more details.\nStakeKeyHash refers to blake2b-224 hash digests of Ed25519 verification keys. How keys are obtained is out of the scope of this specification. Interested readers may look at CIP-1852 for more details.\nStakeKeyHash\nblake2b-224\nScriptHash refers to blake2b-224 hash digests of serialized monetary scripts. How scripts are constructed and serialized is out of the scope of this specification.\nScriptHash refers to blake2b-224 hash digests of serialized monetary scripts. How scripts are constructed and serialized is out of the scope of this specification.\nScriptHash\nblake2b-224\nBefore diving in, please acknowledge that a lot of the supported capabilities of Byron addresses have remained largely unused. The initial design showed important trade-offs and rendered it unpractical to sustain the long-term goals of the network. A new format was created when introducing Shelley and Byron addresses were kept only for backward compatibility. Byron addresses are also sometimes called bootstrap addresses.\nLike many other objects on the Cardano blockchain yet unlike Shelley addresses, Byron addresses are CBOR-encoded binary objects. Conveniently enough, the first 4 bits of their first byte are always equal to 1000.... which allows us to land back on our feet w.r.t to the address type. Their internal structure is however vastly different and a bit unusual.\n1000....\n┌────────┬──────────────┬────────┐ │ root │ attributes │ type │ └────────┴──────────────┴────────┘ ╎ ╎ ╎ ╎ ╎ ╰╌╌ Standard ╎ ╎ ╰╌╌ Redeem ╎ ╎ ╎ ╰╌╌ Derivation Path ╎ ╰╌╌ Network Tag ╎ ╎ ┌────────┬─────────────────┬──────────────┐ ╰╌╌╌╌ double-hash ( │ type │ spending data │ attributes │ ) └────────┴─────────────────┴──────────────┘ ╎ ╰╌╌ Verification Key ╰╌╌ Redemption Key\n┌────────┬──────────────┬────────┐ │ root │ attributes │ type │ └────────┴──────────────┴────────┘ ╎ ╎ ╎ ╎ ╎ ╰╌╌ Standard ╎ ╎ ╰╌╌ Redeem ╎ ╎ ╎ ╰╌╌ Derivation Path ╎ ╰╌╌ Network Tag ╎ ╎ ┌────────┬─────────────────┬──────────────┐ ╰╌╌╌╌ double-hash ( │ type │ spending data │ attributes │ ) └────────┴─────────────────┴──────────────┘ ╎ ╰╌╌ Verification Key ╰╌╌ Redemption Key\nThe address root uniquely identifies the address and is a double-hash digest (SHA3-256, and then Blake2b-224) of the address type, spending data, and attributes.\nroot\nThen comes the address attributes which are both optional. The network tag is present only on test networks and contains an identifier that is used for network discrimination. The derivation path (detailed below) was used by legacy so-called random wallets in the early days of Cardano and its usage was abandoned with the introduction of Yoroi and so-called Icarus addresses.\nFinally, the address type allows for distinguishing different sub-types of Byron addresses. Redeem addresses are used inside the Byron genesis configuration and were given to early investors who helped to fund the project.\nA full and more detailed CDDL specification of Byron addresses is given in the annex to the CIP.\nHistorically, Cardano wallets have been storing information about the wallet structure directly within the address. This information comes in the form of two derivation indexes (in the sense of child key derivation as defined in BIP-0032) which we call derivation path. To protect the wallet's anonymity, the derivation path is stored encrypted using a ChaCha20/Poly1305 authenticated cipher.\nAll test vectors below use the following payment key, stake key, script and pointer:\naddr_vk1w0l2sr2zgfm26ztc6nl9xy8ghsk5sh6ldwemlpmp9xylzy4dtf7st80zhd\naddr_vk1w0l2sr2zgfm26ztc6nl9xy8ghsk5sh6ldwemlpmp9xylzy4dtf7st80zhd\nstake_vk1px4j0r2fk7ux5p23shz8f3y5y2qam7s954rgf3lg5merqcj6aetsft99wu\nstake_vk1px4j0r2fk7ux5p23shz8f3y5y2qam7s954rgf3lg5merqcj6aetsft99wu\nscript1cda3khwqv60360rp5m7akt50m6ttapacs8rqhn5w342z7r35m37\nscript1cda3khwqv60360rp5m7akt50m6ttapacs8rqhn5w342z7r35m37\n(2498243, 27, 3)\n(2498243, 27, 3)\nmainnet: type-00: addr1qx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer3n0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgse35a3x type-01: addr1z8phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gten0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgs9yc0hh type-02: addr1yx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzerkr0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shs2z78ve type-03: addr1x8phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gt7r0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shskhj42g type-04: addr1gx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer5pnz75xxcrzqf96k type-05: addr128phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtupnz75xxcrtw79hu type-06: addr1vx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzers66hrl8 type-07: addr1w8phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcyjy7wx type-14: stake1uyehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gh6ffgw type-15: stake178phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcccycj5 testnet: type-00: addr_test1qz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer3n0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgs68faae type-01: addr_test1zrphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gten0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgsxj90mg type-02: addr_test1yz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzerkr0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shsf5r8qx type-03: addr_test1xrphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gt7r0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shs4p04xh type-04: addr_test1gz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer5pnz75xxcrdw5vky type-05: addr_test12rphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtupnz75xxcryqrvmw type-06: addr_test1vz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzerspjrlsz type-07: addr_test1wrphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcl6szpr type-14: stake_test1uqehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gssrtvn type-15: stake_test17rphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcljw6kf\nmainnet: type-00: addr1qx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer3n0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgse35a3x type-01: addr1z8phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gten0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgs9yc0hh type-02: addr1yx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzerkr0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shs2z78ve type-03: addr1x8phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gt7r0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shskhj42g type-04: addr1gx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer5pnz75xxcrzqf96k type-05: addr128phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtupnz75xxcrtw79hu type-06: addr1vx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzers66hrl8 type-07: addr1w8phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcyjy7wx type-14: stake1uyehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gh6ffgw type-15: stake178phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcccycj5 testnet: type-00: addr_test1qz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer3n0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgs68faae type-01: addr_test1zrphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gten0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgsxj90mg type-02: addr_test1yz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzerkr0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shsf5r8qx type-03: addr_test1xrphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gt7r0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shs4p04xh type-04: addr_test1gz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer5pnz75xxcrdw5vky type-05: addr_test12rphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtupnz75xxcryqrvmw type-06: addr_test1vz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzerspjrlsz type-07: addr_test1wrphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcl6szpr type-14: stake_test1uqehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gssrtvn type-15: stake_test17rphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcljw6kf\nAs stated in Motivation this CIP is provided for informational purposes regarding a single deliberate design. Further rationale and motivation for this design are available in the Design Specification for Delegation and Incentives in Cardano - Section 3.2 :: Addresses & Credentials .\nIntersectMBO/cardano-addresses (Byron & Shelley)\nIntersectMBO/cardano-addresses (Byron & Shelley)\nIntersectMBO/cardano-ledger-specs (Byron)\nIntersectMBO/cardano-ledger-specs (Byron)\nIntersectMBO/cardano-ledger-specs (Shelley)\nIntersectMBO/cardano-ledger-specs (Shelley)\nConfirmation by consensus, with no reported dispute since publication, that this document fully descibes how Cardano addresses are universally implemented.\nPublish this documentation for confirmation that it accurately describes conventionals of universal use.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0020 | Transaction message/comment metadata\n\nThis describes a basic JSON schema to add messages/comments/memos as transaction metadata by using the metadatum label 674: allowing informational, commerical, or any other text to be included in a transaction on the Cardano blockchain.\nWe have the utilities on the cardano blockchain now since the introduction of the \"allegra-era\". A simple consens about adding messages, comments or memos to transactions is still missing. So the CIP authors came together to form a first implementation of this. It is straight and simple, additional keys and content can be added later. The IOG main wallet Daedalus can now also directly show attached metadata information in the transaction details view. This CIP is the missing link to bring it together.\nDaedalus\nSome of the current Tools/Sites/Explorers that have implemented it already:\nCNTools\nJorManager\nStakePoolOperator Scripts\nCardanoscan.io\nAdaStat.net\nEternl Wallet\nCardanoWall\nNami Wallet\nCNFT\nCardano Explorer\nSundaeSwap\nMinswap\nMuesliSwap\nDripDropz.io\nTyphon Wallet\nLedger Live\nThe specification for the individual strings follow the general design specification for JSON metadata, which is already implemented and in operation on the cardano blockchain. The used metadatum label is \"674\":, this number was choosen because it is the T9 encoding of the string \"msg\". The message content has the key \"msg\": and consists of an array of individual message-strings. The number of theses message-strings must be at least one for a single message, more for multiple messages/lines. Each of theses individual message-strings array entries must be at most 64 bytes when UTF-8 encoded.\n\"674\":\n\"msg\":\n{ \"674\": { \"msg\": [ \"message-string 1\" //Optional: ,\"message-string 2\",\"message-string 3\" ... ] } }\n{ \"674\": { \"msg\": [ \"message-string 1\" //Optional: ,\"message-string 2\",\"message-string 3\" ... ] } }\n{ \"674\": { \"msg\": [ \"This is a comment for the transaction xyz, thank you very much!\" ] } }\n{ \"674\": { \"msg\": [ \"This is a comment for the transaction xyz, thank you very much!\" ] } }\n{ \"674\": { \"msg\": [ \"Invoice-No: 1234567890\", \"Customer-No: 555-1234\", \"P.S.: i will shop again at your store :-)\" ] } }\n{ \"674\": { \"msg\": [ \"Invoice-No: 1234567890\", \"Customer-No: 555-1234\", \"P.S.: i will shop again at your store :-)\" ] } }\nLedger Live is offering a memo field\nDaedalus shows the metadata text (could be improved if CIP is implemented):\nCardanoscan.io, Adastat.net and other tools implemented it already, to show messages along transactions:\neternl.io has added it with a message field on the sending-page, and shows it also on the transactions-page:\nStakePool Operator Scripts: It works on the commandline like any other script of the collection by just adding the \"msg: ...\" parameter to a transaction. This automatically generates the needed metadata.json structure and attaches it to the transaction itself.\nCNTools:\nThis design is simple, so many tools on the cardano blockchain can implement it easily. The array type was choosen to have consistency, no need to switch between a string or an array format, or testing against a string or array format. Updates in the future are possible, like adding a versioning key \"ver\":, adding a key \"utxo\": to provide specific data for every tx-out#idx in the transaction, adding the \"enc\": key like for encrypted messages, making subarrays in the message-strings, etc. But for now, we need a common agreement to provide general messages/comments/memos with this CIP. The starting design war choosen as simple as possible to keep the additional transaction fees as low as possible.\n\"ver\":\n\"utxo\":\n\"enc\":\nWould be a good idea to hide the message/comment/note behind a \"show unmoderated content\" button/drop-down. Like the Metadata display on the Cardano Explorer. Also, it should be displayed as plain-text non-clickable. To enhance security further, URLs could be automatically deleted or hidden from such comments, to not welcome bad actors with phishing attempts. Another solution to start with would be to really limit the character space for display in Wallets, like limiting it to a-zA-z0-9 and a handful of special chars like +-_#()[]: without a . \"/\\ chars, so a domain or html code would not work. Last points are worth for discussions of course, because it would also filter out unicode.\na-zA-z0-9\n+-_#()[]:\n.<>\"/\\\nIt is up to the wallet-/display-/receiver-implementor to parse and check the provided metadata. As for the current state, its not possible to have the same label \"674\" more than once in a cardano transaction. So a check about that can be ignored at the moment. This CIP provides the correct implementation format, the parsing should search for the \"674\" metadata label and the \"msg\" key underneath it. There should also be a check, that the provided data within that \"msg\" key is an array. All other implementations like a missing \"msg\" key, or a single string instead of an array, should be marked by the display-implementor as \"invalid\". Additional keys within the \"674\" label should not affect the parsing of the \"msg\" key. As written above, we will likely see more entries here in the future like a \"version\" key for example, so additional keys should not harm the parsing of the \"msg\" key.\nA transaction message should be considered valid if the following apply:\nLabel = 674.\nhas property \"msg\".\nmsg property contains an array of strings, even for a single-line message.\nEach line has a maximum length of 64 characters.\nIf there are additional properties, they don't invalidate the message. They can just be ignored.\nIf any of the above is not met, ignore the metadata as a transaction message. Can still be displayed as general metadata to the transaction.\nOptional to consider for the implementer:\nFor message creation both single-line and multi-line input should be considered valid. The wallet/tool isn't required to support multi-line input.\nMessage display in explorers/wallets should however preferably support multi-line messages even if it only supports single-line on creation. Not a requirement but should at least indicate that there are more data if only the first line is displayed. Maybe a link to explorer etc in the case it's not possible to solve in UI in a good way.\nThere exist a variety of wallet-based, dApp, and CLI implementations of this standard, developed by a a wide variety of providers, and is in regular use.\nAs per the first two Discussion links:\nThe format in this CIP has been the ground base for supporting transaction messages / comments / memos.\nThe format and its interpretation have been considered and implemented by both creator/sender implementations and wallet/receiver/display implementations.\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0021 | Transaction requirements for interoperability with hardware wallets\n\nThis CIP describes all the restrictions applicable to Cardano transactions which need to be signed by hardware wallets.\nDue to certain limitations of hardware (abbrev. HW) wallets, especially very small memory and a limited set of data types supported by Ledger, HW wallets are not able to process all valid transactions which are supported by Cardano nodes.\nThe limitations also result in an inability of HW wallets to see the whole transaction at once. Transaction data are streamed into HW wallets in small chunks and they compute a rolling hash of the transaction body which is signed at the end. Consequently, a HW wallet only provides witness signatures, and the transaction body which was signed has to be reconstructed by the client. We thus need a common transaction serialization format which will allow no ambiguity. In addition, the format must define ordering of map keys in such a way that it s possible to check for duplicate keys by HW wallets.\nSeveral of the restrictions also stem from security or UX concerns, e.g. the forbidden combination of pool registration certificates and withdrawals in a single transaction (see reasoning below).\nTo ensure interoperability, SW wallets and other tools working with HW wallets should only use transactions which conform to the following rules.\nCertain transaction elements, described as allowed in this document, might not be supported by some HW wallets. Support also depends on HW wallet firmware or app versions. If transaction signing fails for a transaction which is built according to this specification, make sure to check the documentation of the HW wallet you are using.\nNote: It might take some time for recent Cardano ledger spec changes to be implemented for HW wallets. Thus it might happen that further restrictions might apply on top of the restrictions mentioned in this CIP until the changes are implemented on HW wallets.\nTransactions must be serialized in line with suggestions from Section 3.9 of CBOR specification RFC. In particular:\nIntegers must be as small as possible.\nThe expression of lengths in major types 2 through 5 must be as short as possible.\nThe keys in every map must be sorted from lowest value to highest.\nIndefinite-length items must be made into definite-length items.\nSee the RFC for details.\nConway introduced optional 258 tags in certain items that are considered sets semantically but encoded as arrays in CBOR. HW wallets will support this optional encoding, but it must be consistent across the transaction: either there are no tags 258 in sets, or there are such tags everywhere (as described in the CDDL specification).\nThe following transaction body entries must not be included:\n6 : update\n6 : update\n20 : proposal procedures\n20 : proposal procedures\nHW wallets support at most int64 for signed integers and uint64 for unsigned integers i.e. larger integers are not supported overall. Additionally, any integer value must fit in the appropriate type.\nint64\nuint64\nThe number of the following transaction elements individually must not exceed UINT16_MAX, i.e. 65535:\nUINT16_MAX\ninputs in transaction body\noutputs in transaction body\nasset groups (policy IDs) in an output or in the mint field\ntokens (asset names) in an asset group\ncertificates in transaction body\npool owners in a pool registration certificate\npool relays in a pool registration certificate\nwithdrawals in transaction body\ncollateral inputs in transaction body\nrequired signers in transaction body\nreference inputs in transaction body\nthe total number of witnesses\nFor voting procedures, it is only allowed to include a single voter with a single voting procedure.\nUnless mentioned otherwise in this CIP, optional empty lists and maps must not be included as part of the transaction body or its elements.\nSince Conway, the CDDL specification is stricter, so many arrays are now treated as non-empty sets, and some maps are required to be non-empty. HW wallets enforce this in many cases.\nA new, \"post Alonzo\", output format has been introduced in the Babbage ledger era which uses a map instead of an array to store the output data. For now, both the \"legacy\" (array) and \"post Alonzo\" (map) output formats are supported by HW wallets but we encourage everyone to migrate to the \"post Alonzo\" format as support for the \"legacy\" output format might be removed in the future. Both formats can be mixed within a single transaction, both in outputs and in the collateral return output.\nOutputs containing no multi-asset tokens must be serialized as a simple tuple, i.e. [address, coin, ?datum_hash] instead of [address, [coin, {}], ?datum_hash].\n[address, coin, ?datum_hash]\n[address, [coin, {}], ?datum_hash]\nIf the data of datum_option is included in an output, it must not be empty. script_ref (reference script) must also not be empty if it is included in an output.\ndata\ndatum_option\nscript_ref\nSince multiassets (policy_id and asset_name) are represented as maps, both need to be sorted in accordance with the specified canonical CBOR format. Also, an output or the mint field must not contain duplicate policy_ids and a policy must not contain duplicate asset_names.\npolicy_id\nasset_name\npolicy_id\nasset_name\nCertificates of the following types are not supported and must not be included:\ngenesis_key_delegation\ngenesis_key_delegation\nmove_instantaneous_rewards_cert\nmove_instantaneous_rewards_cert\nstake_vote_deleg_cert\nstake_vote_deleg_cert\nstake_reg_deleg_cert\nstake_reg_deleg_cert\nvote_reg_deleg_cert\nvote_reg_deleg_cert\nstake_vote_reg_deleg_cert\nstake_vote_reg_deleg_cert\nIf a transaction contains a pool registration certificate, then it must not contain:\nany other certificate;\nany withdrawal;\nmint entry;\nany output containing datum, datum hash or reference script;\nscript data hash;\nany collateral input;\nany required signer;\ncollateral return output;\ntotal collateral;\nreference inputs;\nvoting procedures;\ntreasury value;\ndonation value.\nIt is allowed to arbitrarily combine other supported certificate types.\nSince withdrawals are represented as a map of reward accounts, withdrawals also need to be sorted in accordance with the specified canonical CBOR format. A transaction must not contain duplicate withdrawals.\nHW wallets do not serialize auxiliary data because of their complex structure. They only include the given auxiliary data hash in the transaction body. The only exception is Catalyst voting registration because it requires a signature computed by the HW wallet.\nIn this exceptional case, auxiliary data must be encoded in their \"tuple\" format:\n[ transaction_metadata: { * transaction_metadatum_label => transaction_metadatum }, auxiliary_scripts: [ * native_script ]]\n[ transaction_metadata: { * transaction_metadatum_label => transaction_metadatum }, auxiliary_scripts: [ * native_script ]]\nThe auxiliary_scripts must be an array of length 0.\nauxiliary_scripts\nAs HW wallets don't return the whole serialized transaction, a common CBOR serialization is needed so that software wallets and other tools interacting with HW wallets are be able to deterministically reproduce the transaction body built and signed by the HW wallet.\nThe specified canonical CBOR format is consistent with how certain other data are serialized (e.g. Plutus script data in Alonzo) and allows the use of standard CBOR libraries out of the box.\nGenerally, HW wallets require that any key hash credential (and withdrawal address too) is given by the derivation path of the key (otherwise the user will not be aware that the key belongs to his wallet). This does not apply to Plutus transactions where HW wallets instead aim for maximum flexibility at the cost of users being potentially misled. (It is very hard to foresee how Plutus script authors would use various transaction elements and any restriction applied by HW wallets might break a use case which is otherwise perfectly sound and safe.)\nWhen signing a transaction, Ledger and Trezor use a transaction signing mode that describes upfront what the intent is (the software wallet is responsible for choosing an appropriate mode). The transaction is then validated according to the mode. There are, in principle, four options:\nStake pool registration transaction. Stake pool registration certificates are signed on their own, the transaction should contain nothing that is not necessary.\nOrdinary transaction. Credentials must be given as key paths.\nMultisig transaction. Credentials must be script hashes; only multisig keys are allowed.\nPlutus transaction. The only mode that allows elements related to running Plutus scripts (script data hash etc.). No extra restrictions on transaction elements or their combinations. The drawback is that more is shown to the user (e.g. witnesses are not hidden as in ordinary transactions). Please only use this mode if no other mode is sufficient.\nThis brief description does not aim to capture the full complexity of signing modes; always verify that transactions you aim to construct are supported by other tools you will rely on (hardware wallets, software wallets, command-line tools like cardano-hw-cli etc.).\ncardano-hw-cli\nAllowing duplicate policy_ids (or asset_names) might lead to inconsistencies between what is displayed to the user and how nodes and other tools might interpret the duplicate keys, i.e. all policies (or asset names) would be shown to the user, but nodes and other tools might eventually interpret only a single one of them.\npolicy_id\nasset_name\nCombining withdrawals and pool registration certificates isn't allowed because both are signed by staking keys by pool owners. If it was allowed to have both in a transaction then the witness provided by a pool owner might inadvertently serve as a witness for a withdrawal for the owner's account.\nSimilarly to multiassets, allowing duplicate withdrawals might lead to inconsistencies between what is displayed to the user and how nodes and other tools might interpret the duplicate keys.\nThe specified auxiliary data format was chosen in order to be compatible with other Cardano tools, which mostly use this serialization format.\nA DRep witness can serve for both certificates and votes at the same time. Unlike with stake keys (where combining pool registration with e.g. withdrawals is forbidden), no restriction is imposed on the combination of certificates and votes. We think that votes and DRep certificates are rare and substantially distinguished parts of a transaction, signed by DRep keys which are likely to only be used by users with deep enough understanding (and, unlike stake keys, are always visible when providing witnesses). A single vote or a DRep certificate is unlikely to have a major effect (esp. not on the loss of funds). If submitting unintended votes turns out to be a problem, it is likely better to solve it on the level of Cardano blockchain ledger by providing a mechanism allowing for replacing or cancelling votes.\nConfirmation (by default if no ongoing incompatibilities) since Alonzo ledger era that this interoperability between software and hardware wallets has been generally achieved.\nTools exist which can be used to validate or transform transactions into a HW wallet compatible format if possible: cardano-hw-interop-library cardano-hw-cli (which uses the interop library)\ncardano-hw-interop-library\ncardano-hw-interop-library\ncardano-hw-cli (which uses the interop library)\ncardano-hw-cli\nThe following list of features with missing support on particular hardware devices is subject to occasional changes. Some features might be added, but some could also be removed (e.g. if they take too much space needed for other features).\nEverything described here as allowed should (eventually) work on these devices.\nMissing features:\nsigning operational certificates\nderivation of native script hashes\nstake pool registration and retirement\ndisplay of certain details of Byron addresses (though addresses themselves are supported)\nMissing features:\nderivation of stake pool cold keys\nsigning operational certificates\nsigning pool registration certificates as operator (only as owner is allowed)\nderivation of DRep and constitutional committee keys\nDRep certificates (registration, retirement, update)\nconstitutional committee certificates\nvoting procedures\ntreasury and donation elements of transactions\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0022 | Pool operator verification\n\nThis proposal describes a method allowing a stakepool operator to provide credentials to verify that they are the rightful manager for their stakepool.\nMany websites such pooltool.io, adapools.org, and others need to allow pool operators special access to modify the way their pool appears on the website. SPOCRA and other organizations also have a need to allow voting on proposals and ensure that each vote cast is from a valid pool operator. Today, these sites and organizations all use different techniques for validating pool operators.\npooltool.io - Validates operators by receiving 1 ada spent from the pool's registered rewards account\nadapools.org - Validates operators by requesting that the operator include a special generated value in their extended pool metadata json file.\nThis proposal is to simplify and streamline a single approach that all can reference in order to verify that a pool operator is who they say they are.\nIn order to achieve the goals of this CIP, the pool operator needs to provide some credential or credentials to the validating party which cannot be spoofed. The VRF pool keys and VRF signature algorithm implemented in libsodium are chosen to build and provide this credential/signature. This signature can then be validated and the operator verified without ever exposing any of the pool's private key information. This technique is very similar to verifying that a block produced by another pool is valid. The only difference is that instead of validating the slot seed for a given pool, we're validating a pre-determined message hash.\nStakepool Operator (SPO) sends their pool_id and public pool.vrf.vkey to the Validating Server (VS) VS validates that the vrf hash in the pool's registration certificate on the blockchain matches the blake2b hash of the sent vkey. Note: The VS should use the latest registration certificate on the chain for matching as the VRF is a \"hot\" key and can be changed at any time by the pool operator. A single point-in-time verification is sufficient to properly identify the pool operator. The VS sends a challenge request to the SPO which is the domain of the VS and a random 64-byte nonce. The SPO creates a blake2b hash of \"cip-0022{domain}{random_nonce}\" and then signs this with their private VRF key. The SPO sends this to VS as the challenge response within a 5-minute window to the VS The VS validates the signed challenge response\nStakepool Operator (SPO) sends their pool_id and public pool.vrf.vkey to the Validating Server (VS)\nVS validates that the vrf hash in the pool's registration certificate on the blockchain matches the blake2b hash of the sent vkey. Note: The VS should use the latest registration certificate on the chain for matching as the VRF is a \"hot\" key and can be changed at any time by the pool operator. A single point-in-time verification is sufficient to properly identify the pool operator.\nThe VS sends a challenge request to the SPO which is the domain of the VS and a random 64-byte nonce.\nThe SPO creates a blake2b hash of \"cip-0022{domain}{random_nonce}\" and then signs this with their private VRF key.\nThe SPO sends this to VS as the challenge response within a 5-minute window to the VS\nThe VS validates the signed challenge response\n// Server side, create inputs for a challenge. Store this and only allow responses // within 5 minutes to be successful. val random = SecureRandom() val domain = \"pooltool.io\" val nonce = ByteArray(64) random.nextBytes(nonce) println(\"domain: $domain, nonce: ${nonce.toHexString()}\")\n// Server side, create inputs for a challenge. Store this and only allow responses // within 5 minutes to be successful. val random = SecureRandom() val domain = \"pooltool.io\" val nonce = ByteArray(64) random.nextBytes(nonce) println(\"domain: $domain, nonce: ${nonce.toHexString()}\")\n// Node operational VRF-Verification-Key: pool.vrf.vkey //{ // \"type\": \"VrfVerificationKey_PraosVRF\", // \"description\": \"VRF Verification Key\", // \"cborHex\": \"5820e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381\" //} // // Node operational VRF-Signing-Key: pool.vrf.skey //{ // \"type\": \"VrfSigningKey_PraosVRF\", // \"description\": \"VRF Signing Key\", // \"cborHex\": \"5840adb9c97bec60189aa90d01d113e3ef405f03477d82a94f81da926c90cd46a374e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381\" //} // We assume the pool operator has access to the pool's vrf secret key val skeyCbor = \"5840adb9c97bec60189aa90d01d113e3ef405f03477d82a94f81da926c90cd46a374e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381\".hexToByteArray() val vrfSkey = (CborReader.createFromByteArray(skeyCbor).readDataItem() as CborByteString).byteArrayValue() val vkeyCbor = \"5820e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381\".hexToByteArray() val vrfVkey = (CborReader.createFromByteArray(vkeyCbor).readDataItem() as CborByteString).byteArrayValue() // Client side, construct and sign the challenge val challengeSeed = \"cip-0022${domain}\".toByteArray() + nonce val challenge = SodiumLibrary.cryptoBlake2bHash(challengeSeed, null) println(\"challenge: ${challenge.toHexString()}\") val signature = SodiumLibrary.cryptoVrfProve(vrfSkey, challenge) println(\"signature: ${signature.toHexString()}\")\n// Node operational VRF-Verification-Key: pool.vrf.vkey //{ // \"type\": \"VrfVerificationKey_PraosVRF\", // \"description\": \"VRF Verification Key\", // \"cborHex\": \"5820e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381\" //} // // Node operational VRF-Signing-Key: pool.vrf.skey //{ // \"type\": \"VrfSigningKey_PraosVRF\", // \"description\": \"VRF Signing Key\", // \"cborHex\": \"5840adb9c97bec60189aa90d01d113e3ef405f03477d82a94f81da926c90cd46a374e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381\" //} // We assume the pool operator has access to the pool's vrf secret key val skeyCbor = \"5840adb9c97bec60189aa90d01d113e3ef405f03477d82a94f81da926c90cd46a374e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381\".hexToByteArray() val vrfSkey = (CborReader.createFromByteArray(skeyCbor).readDataItem() as CborByteString).byteArrayValue() val vkeyCbor = \"5820e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381\".hexToByteArray() val vrfVkey = (CborReader.createFromByteArray(vkeyCbor).readDataItem() as CborByteString).byteArrayValue() // Client side, construct and sign the challenge val challengeSeed = \"cip-0022${domain}\".toByteArray() + nonce val challenge = SodiumLibrary.cryptoBlake2bHash(challengeSeed, null) println(\"challenge: ${challenge.toHexString()}\") val signature = SodiumLibrary.cryptoVrfProve(vrfSkey, challenge) println(\"signature: ${signature.toHexString()}\")\n// Server side, verify the message based on only knowing the pool_id, public vkey, signature, and constructing // the challenge ourselves the same way the client should have. val challengeSeed = \"cip-0022${domain}\".toByteArray() + nonce val challenge = SodiumLibrary.cryptoBlake2bHash(challengeSeed, null) // Get the vkeyHash for a pool from the \"query pool-params\" cardano-cli command // This comes from the pool's registration certificate on the chain. val vkeyHash = \"f58bf0111f8e9b233c2dcbb72b5ad400330cf260c6fb556eb30cefd387e5364c\".hexToByteArray() // Verify that the vkey from the latest minted block on the blockchain (or the client supplied if they // haven't yet minted a block) is the same as the one on-chain in the pool's registration certificate val vkeyHashVerify = SodiumLibrary.cryptoBlake2bHash(vrfVkey, null) assertThat(vkeyHash).isEqualTo(vkeyHashVerify) // Verify that the signature matches val verification = SodiumLibrary.cryptoVrfVerify(vrfVkey, signature, challenge) println(\"verification: ${verification.toHexString()}\") println(\"Verification SUCCESS!\")\n// Server side, verify the message based on only knowing the pool_id, public vkey, signature, and constructing // the challenge ourselves the same way the client should have. val challengeSeed = \"cip-0022${domain}\".toByteArray() + nonce val challenge = SodiumLibrary.cryptoBlake2bHash(challengeSeed, null) // Get the vkeyHash for a pool from the \"query pool-params\" cardano-cli command // This comes from the pool's registration certificate on the chain. val vkeyHash = \"f58bf0111f8e9b233c2dcbb72b5ad400330cf260c6fb556eb30cefd387e5364c\".hexToByteArray() // Verify that the vkey from the latest minted block on the blockchain (or the client supplied if they // haven't yet minted a block) is the same as the one on-chain in the pool's registration certificate val vkeyHashVerify = SodiumLibrary.cryptoBlake2bHash(vrfVkey, null) assertThat(vkeyHash).isEqualTo(vkeyHashVerify) // Verify that the signature matches val verification = SodiumLibrary.cryptoVrfVerify(vrfVkey, signature, challenge) println(\"verification: ${verification.toHexString()}\") println(\"Verification SUCCESS!\")\nvrfSkey: adb9c97bec60189aa90d01d113e3ef405f03477d82a94f81da926c90cd46a374e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381 vrfVkey: e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381 domain: pooltool.io, nonce: c936ab102a86442c7120f75fa903b41d9f6f984a9373a6fa0b7b8cb020530318bdec84512468681c7d8454edf3a0e0bf21f59c401028030a8fb58117edc8b03c challenge: 6977c480a3acb4c838ba95bb84d1f4db1c2591ea6ebe5805ed0394f706c23b05 signature: a3c9624aa14f6f0fba3d47d3f9a13bb55f0790eacd7bad9a89ce89fecb9e7eb8ca0d19aea8b6a7be39ae3e8b9768211b4d8aa789e82c1e150826fe15a0b0323f08e18635deb94c49d7f4421750d44903 signatureHash: 9ca4c7e63ba976dfbe06c7a0e6ec4aec5a5ef04b721ffc505222606dfc3d01572ddce3b55ac5c9470f061f137dafe31669794ea48118d1682d888efbe0cb4d1a verification: 9ca4c7e63ba976dfbe06c7a0e6ec4aec5a5ef04b721ffc505222606dfc3d01572ddce3b55ac5c9470f061f137dafe31669794ea48118d1682d888efbe0cb4d1a Verification SUCCESS!\nvrfSkey: adb9c97bec60189aa90d01d113e3ef405f03477d82a94f81da926c90cd46a374e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381 vrfVkey: e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381 domain: pooltool.io, nonce: c936ab102a86442c7120f75fa903b41d9f6f984a9373a6fa0b7b8cb020530318bdec84512468681c7d8454edf3a0e0bf21f59c401028030a8fb58117edc8b03c challenge: 6977c480a3acb4c838ba95bb84d1f4db1c2591ea6ebe5805ed0394f706c23b05 signature: a3c9624aa14f6f0fba3d47d3f9a13bb55f0790eacd7bad9a89ce89fecb9e7eb8ca0d19aea8b6a7be39ae3e8b9768211b4d8aa789e82c1e150826fe15a0b0323f08e18635deb94c49d7f4421750d44903 signatureHash: 9ca4c7e63ba976dfbe06c7a0e6ec4aec5a5ef04b721ffc505222606dfc3d01572ddce3b55ac5c9470f061f137dafe31669794ea48118d1682d888efbe0cb4d1a verification: 9ca4c7e63ba976dfbe06c7a0e6ec4aec5a5ef04b721ffc505222606dfc3d01572ddce3b55ac5c9470f061f137dafe31669794ea48118d1682d888efbe0cb4d1a Verification SUCCESS!\nImplementing this simplifies and commonizes the process for verifying that a pool operator is who they say they are in 3rd party systems. Having a common way of verify pool operators also allows simple integration into pool management tools.\nThere is also some overlap with CIP-0006 and the rawdata-sign command although it specifies generating a new key instead of utilizing the pool's existing vrf.skey to sign like this proposal.\nrawdata-sign\nvrf.skey\nTools that have implemented, or are implementing, this proposal: CNCLI JorManager StakePoolOperator Scripts CNTools\nCNCLI\nJorManager\nStakePoolOperator Scripts\nCNTools\nConsensus between providers of the most popular tools and CLIs for stake pool operators that this approch is viable and desirable.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0023 | Fair Min Fees\n\nThis proposes to create a more fair marketplace for stakepools by reducing the minimum fixed pool fee and adding a minimum variable pool fee.\nThe current minimum fixed pool fee places a large and unfair burden on delegators to pools with smaller amounts of stake. This incentivizes people to delegate to pools with higher stake causing centralization and creating an unequal playing field for stakepool operators.\nReducing the minimum fixed pool fee and adding a minimum variable pool fee reduces the imbalance between stakepools with less or more stake to a more reasonable range that allows for more fair competition between stakepools and more fair rewards for delegators to stakepools with less stake.\nThis creates a more fair marketplace for all stakepool operators and increases decentralization, which is a goal of Cardano.\n| Name of the Parameter | New Parameter (Y/N) | Deleted Parameter (Y/N) | Proposed Value | Summary Rationale for Change | |----------------------- |-------------------- |------------------------ |--------------- | ---------------------------- | | minPoolCost | N | N | 50000000 | See Rationale section. | |----------------------- |-------------------- |------------------------ |--------------- | ---------------------------- | | minPoolRate | Y | N | .015 | See Rationale section. | |----------------------- |-------------------- |------------------------ |--------------- | ---------------------------- |\n| Name of the Parameter | New Parameter (Y/N) | Deleted Parameter (Y/N) | Proposed Value | Summary Rationale for Change | |----------------------- |-------------------- |------------------------ |--------------- | ---------------------------- | | minPoolCost | N | N | 50000000 | See Rationale section. | |----------------------- |-------------------- |------------------------ |--------------- | ---------------------------- | | minPoolRate | Y | N | .015 | See Rationale section. | |----------------------- |-------------------- |------------------------ |--------------- | ---------------------------- |\nThere are 2 ways to handle the existing pool certificates with minimum variable pool fee less than the proposed value:\nThese pool certificates would be treated as if they had the new mimimum variable pool fee. These pool certificates would be invaalid and pool operators would be required to create new certificates with valid values. If option 1 is possible, it would provide a better user experience for both stakepool operators and delegators and would cause less disruption to the network.\nThese pool certificates would be treated as if they had the new mimimum variable pool fee.\nThese pool certificates would be invaalid and pool operators would be required to create new certificates with valid values. If option 1 is possible, it would provide a better user experience for both stakepool operators and delegators and would cause less disruption to the network.\nThe PHP code in minfees.php in the pull request allows exploration of the effects of choosing different values for the minimum fixed and variable fees. Running minfees without any arguments gives the usage message as below.\nphp minfees.php Usage: php minfees.php <min_fixed_fee> <min_rate_fee> <pledge> min_fixed_fee: Minimum fixed fee in ADA. An integer greater than or equal to 0. min_rate_fee: Minimum rate fee in decimal. Example: 1.5% = .015 A real number greater than or equal to 0. pledge: Optional pledge amount in ADA. Defaults to 100000 A real number greater than or equal to 0.\nphp minfees.php Usage: php minfees.php <min_fixed_fee> <min_rate_fee> <pledge> min_fixed_fee: Minimum fixed fee in ADA. An integer greater than or equal to 0. min_rate_fee: Minimum rate fee in decimal. Example: 1.5% = .015 A real number greater than or equal to 0. pledge: Optional pledge amount in ADA. Defaults to 100000 A real number greater than or equal to 0.\nRunning minfees with the proposed values gives the following comparison of current and proposed pool operator and staker results at various pool stake levels.\nphp minfees.php 50 .015 100000 Reserve: 13b Total stake: 32b Tx fees: 0 Rewards available in epoch: 31.2m Pool saturation: 64m Pledge: 100k Staker Delegation: 100k Current Fixed Fee: 340 Current Rate: 0% New Fixed Fee: 50 New Rate: 1.5% +---------- Current ----------+ +-------- Proposed ---------+ Pool Total Pool Staker Staker Current Pool Staker Staker New Stake Rewards Cur Fee Cur Fee Cur Rew Fee % New Fee New Fee New Rew Fee % 2m 1501 340 17 58.1 22.7% 71.8 3.6 71.5 4.8% 5m 3752 340 6.8 68.2 9.1% 105.5 2.1 72.9 2.8% 10m 7503 340 3.4 71.6 4.5% 161.8 1.6 73.4 2.2% 20m 15007 340 1.7 73.3 2.3% 274.4 1.4 73.7 1.8% 30m 22511 340 1.1 73.9 1.5% 386.9 1.3 73.7 1.7% 64m 48023 340 0.5 74.5 0.7% 769.6 1.2 73.8 1.6%\nphp minfees.php 50 .015 100000 Reserve: 13b Total stake: 32b Tx fees: 0 Rewards available in epoch: 31.2m Pool saturation: 64m Pledge: 100k Staker Delegation: 100k Current Fixed Fee: 340 Current Rate: 0% New Fixed Fee: 50 New Rate: 1.5% +---------- Current ----------+ +-------- Proposed ---------+ Pool Total Pool Staker Staker Current Pool Staker Staker New Stake Rewards Cur Fee Cur Fee Cur Rew Fee % New Fee New Fee New Rew Fee % 2m 1501 340 17 58.1 22.7% 71.8 3.6 71.5 4.8% 5m 3752 340 6.8 68.2 9.1% 105.5 2.1 72.9 2.8% 10m 7503 340 3.4 71.6 4.5% 161.8 1.6 73.4 2.2% 20m 15007 340 1.7 73.3 2.3% 274.4 1.4 73.7 1.8% 30m 22511 340 1.1 73.9 1.5% 386.9 1.3 73.7 1.7% 64m 48023 340 0.5 74.5 0.7% 769.6 1.2 73.8 1.6%\nDefinitions: Pool Stake - Total stake delegated to pool. Total Rewards - Total rewards generated by the pool in one epoch. Pool Cur Fee - The total amount of fees taken by the pool with current parameters. Staker Cur Fee - The amount of fees paid by a staker who delegates 100k ADA with currarameters. Staker Cur Rew - The amount of rewards received by a staker who delegates 100k ADA with currarameters. Current Fee - The percentage of rewards taken by the pool as fees with currarameters. Pool New Fee - The total amount of fees taken by the pool with proposed parameters. Staker New Fee - The amount of fees paid by a staker who delegates 100k ADA with proposed parameters. Staker New Rew - The amount of rewards received by a staker who delegates 100k ADA with proposed parameters. New Fee - The percentage of rewards taken by the pool as fees with proposed parameters. Note: All amounts other than s are in ADA.\nThe table above shows that currently a delegator staking 100k ADA to a stakepool with 2m ADA total delegation to the pool is paying an exorbitant 22.7 in fees while the same delegator staking with a fully saturated pool would only pay 0.7 in fees. This is a substantial and unfair advantage that large pools have in the stakepool marketplace. This is a strong incentive to centralize stake to fewer larger pools which reduces the resiliency of the network.\nThe proposed minimum fees bring this imbalance into a more reasonable range of 1.6 to 4.8 . It is much more likely that a small stakepool with other advantages or selling points would be able to convince a delegator to accept about 2 less ADA in rewards per epoch for their 100k delegation than about 17 ADA as in the current case. This is particularly true as the price of ADA increases. At current price of 0.90USD,adelegatorstaking100kADAisgivingupover0.90 USD, a delegator staking 100k ADA is giving up over 0.90USD,adelegatorstaking100kADAisgivingupover1000 USD per year by delegating to a small pool! This does not even include the amount lost by comounding rewards being staked over the year.\n16.5 ADA/epoch * 73 epochs/year = 1204.5 ADA/year 1204.5 ADA/year * 0.90USD/ADA=0.90 USD/ADA = 0.90USD/ADA=1084.05 USD/year\nWith proposed parameters the same delegator would only be giving up about $150 USD per year to support a small pool.\n2.3 ADA/epoch * 73 epochs/year = 167.9 ADA/year 167.9 ADA/year * 0.90USD/ADA=0.90 USD/ADA = 0.90USD/ADA=151.11 USD/year\nThe calculations below show that given the price increase in ADA compared to when the protocol parameters were first set, we can maintain viable funding for stakepool operators with the proposed parameter changes.\nAnnual pool operator funding given initial parameters: 340 ADA/epoch * 0.08USD/ADA=0.08 USD/ADA = 0.08USD/ADA=27.20 USD/epoch 27.20USD/epoch 73epochs/year=27.20 USD/epoch * 73 epochs/year = 27.20USD/epoch 73epochs/year=1985.60 USD/year\nAnnual pool operator funding given proposed parameters for stakepool with 2 million ADA delegation: 71.8 ADA/epoch * 0.90USD/ADA=0.90 USD/ADA = 0.90USD/ADA=64.62 USD/epoch 64.62USD/epoch 73epochs/year=64.62 USD/epoch * 73 epochs/year = 64.62USD/epoch 73epochs/year=4717.26 USD/year\nAnnual pool operator funding given proposed parameters for fully saturated stakepool: 769.6 ADA/epoch * 0.90USD/ADA=0.90 USD/ADA = 0.90USD/ADA=692.64 USD/epoch 692.64USD/epoch 73epochs/year=692.64 USD/epoch * 73 epochs/year = 692.64USD/epoch 73epochs/year=50,562.72 USD/year\nIn summary, the proposed parameter changes would create a more fair marketplace for stakepools, provide more fair rewards for delegators to smaller pools and would lower incentives for centralization providing a more resilient network.\nSee the minfees.php code to test different potential values of the parameters.\nThe new parameter minPoolRate is implemented in the protocol and enacted through a hard fork.\nminPoolRate\nThe minimum value of the existing parameter minPoolCost is adjusted in the protocol parameters (not requiring a hard fork).\nminPoolCost\nCode must be written in the cardano-node to check the provided variable rate when creating pool certificates to make sure it meets the new requirements. If backward compatibility option 1 is chosen it would require some new code in the cardano-node to treat the below minimum rate as the minimum rate.\nIf backward compatibility option 1 is chosen it would require some new code in the cardano-node to treat the below minimum rate as the minimum rate.\nAgreement by the Ledger team as defined in CIP-0084 under Expectations for ledger CIPs including \"expert opinion\" on changes to rewards & incentives.\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0024 | Non-Centralizing Rankings\n\nModify the current Daedalus ranking system by removing the centralizing Nash Equilibrium goal of the ranking methodology in order to give more fair rankings and improve the viability of the stake pool operator community and the network overall. To do this we need to remove the stated goal of having k fully saturated pools and all other pools having no stake other than owner pledge, which goes against the Cardano goal of decentralization.\nThere are two main reasons for changing the current ranking methodology:\nAllow for more than k successful stake pools. Provide better decentralization away from a very few stake pool operators creating many pools.\nAllow for more than k successful stake pools.\nAllow for more than k successful stake pools.\nProvide better decentralization away from a very few stake pool operators creating many pools.\nProvide better decentralization away from a very few stake pool operators creating many pools.\nThis is a modification of the ranking methodology defined in section 5.6 Non-Myopic Utility of Shelley Ledger: Delegation/Incentives Design Spec. (SL-D1 v.1.20, 2020/07/06) as follows:\nRemove the following statement from section 5.6:\nRemove the following statement from section 5.6:\n\"The idea is to first rank all pools by desirability , to then assume that the k most desirable pools will eventually be saturated, whereas all other pools will lose all their members, then to finally base all reward calculations on these assumptions.\"\nRemove the following statement from section 5.6.1:\nRemove the following statement from section 5.6.1:\n\"We predict that pools with rank k will eventually be saturated, whereas pools with rank\nk will lose all members and only consist of the owner(s).\"\nAdd the following to section 5.6.1:\nAdd the following to section 5.6.1:\nFor all pools with proposed_pool_stake greater than saturation_warning_stake add k to their rank. Where: proposed_pool_stake = pool_live_stake + proposed_user_stake saturation_warning_stake = (total_stake / k) * saturation_warning_level saturation_warning_level is a real number greater than 0 representing the percent of saturation which is undesirable. A proposed value for saturation_warning_level is 0.95 meaning 95 saturated.\nFor example, if a pool has non-myopic desirability rank of 3, pool_live_stake of 207m ADA, proposed_user_stake of 100k ADA with total_stake of 31.7b ADA, k = 150 and saturation_warning_level = 0.95, we would calculate: 207m + 100k (31.7b / 150) * 0.95 and see that 207.1m 200.8m is true so we would change the pool rank to 153 (3 + k) and all pools previously ranked 4 through 153 would move up 1 rank.\nRemove secion 5.6.2. Remove section 5.6.3. Remove section 5.6.4. Add to secion 5.6.5.\nRemove secion 5.6.2.\nRemove secion 5.6.2.\nRemove section 5.6.3.\nRemove section 5.6.3.\nRemove section 5.6.4.\nRemove section 5.6.4.\nAdd to secion 5.6.5.\nAdd to secion 5.6.5.\nFor example, apparent performance, desirability and ranking can be made non-myopic for ranking purposes as follows:\ndnm[n] := average(d[1]...d[n],and[n + 1]...and[i]) if n i average(d[1]...d[n]) if n = i (dnm[n - 1] * h) + (d[n] * (1 - h)) otherwise.\nwhere: n = epoch number beginning at n = 1 in the first epoch that the pool is eligible for potential rewards. dnm[n] = the non-myopic desirability of the pool in the nth epoch. d[n] = the desirability in the nth epoch unaware of historical desirability. and[n] = the average desirability of the network as a whole in the nth epoch unaware of historical desirability. h = historical influence factor, which is any real number between 0 and 1 exlusive. i = integer(1 / h) which is the initial number of epochs during which we use the average desirability\nAs an example, setting h to 0.1 would mean that the initial number of epochs for using the averaging functions (i) would be 10. If a pool has been eligible to receive rewards (n) for 3 epochs then we use the average of the pool's desirability for those 3 epochs and the overall network desirability for the prior 7 epochs. After the 10th epoch we would use 90 of the previous epoch's non-myopic historical desirability and 10 of the current epoch's desirability to arrive at the new non-myopic desirability.\nThis gives a more reasonable ranking for newer pools that do not have enough historical data to provide fair rankings.\nUsing this non-centralizing ranking methodology gives a more fair ranking of stake pools based on performance, pledge and saturation which will encourage delegators to choose better pools. It will also bring the rankings more in line with the general Cardano principle of increasing decentralization.\nOne or more wallet software implements this new ranking approach.\nAuthor has offered to produce an implementation of this change as a cardano-wallet / Daedalus pull request if shown where the current desirability equation is implemented in the code.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0025 | Media Token Metadata Standard\n\nThis proposal defines an Media Token Metadata Standard for Native Tokens.\nTokens on Cardano are a part of the ledger. Unlike on Ethereum, where metadata can be attached to a token through a smart contract, this isn't possible on Cardano because tokens are native and Cardano uses a UTxO ledger, which makes it hard to directly attach metadata to a token. So the link to the metadata needs to be established differently.\nCardano has the ability to send metadata in a transaction, allowing the creation of a link between a token and the metadata. To make the link unique, the metadata should be appended to the same transaction, where the token forge happens:\nGiven a token in a EUTXOma ledger, we can ask where did this token come from? Since tokens are always created in specific forging operations, we can always trace them back through their transaction graph to their origin.\nSection 4 in the paper UTXOma:UTXO with Multi-Asset Support\nThat being said, we have unique metadata link to a token and can always prove that with 100 certainty. No one else can manipulate the link except if the policy allows it to (update mechanism).\nThis is the registered transaction_metadatum_label value\ntransaction_metadatum_label\nThe structure allows for multiple token mints, also with different policies, in a single transaction.\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array>, \"mediaType\": image/<mime_sub_type>, \"description\": <string | array>, \"files\": [{ \"name\": <string>, \"mediaType\": <mime_type>, \"src\": <uri | array>, <other_properties> }], <other properties> } }, \"version\": <version_id> } }\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array>, \"mediaType\": image/<mime_sub_type>, \"description\": <string | array>, \"files\": [{ \"name\": <string>, \"mediaType\": <mime_type>, \"src\": <uri | array>, <other_properties> }], <other properties> } }, \"version\": <version_id> } }\nVersion 1 Version 2\nIn version 1 the asset_name must be utf-8 encoded and in text format for the key in the metadata map. In version 2 the the raw bytes of the asset_name are used.\nIn version 1 the asset_name must be utf-8 encoded and in text format for the key in the metadata map. In version 2 the the raw bytes of the asset_name are used.\n1\nasset_name\nutf-8\n2\nasset_name\nIn version 1 the policy_id must be in text format for the key in the metadata map. In version 2 the the raw bytes of the policy_id are used.\nIn version 1 the policy_id must be in text format for the key in the metadata map. In version 2 the the raw bytes of the policy_id are used.\n1\npolicy_id\n2\npolicy_id\nThe name property is marked as required.\nThe name property is marked as required.\nname\nThe image property is required and must be a valid Uniform Resource Identifier (URI) pointing to a resource with mime type image/*. Note that this resource is used as thumbnail or the actual link if the token is an image (ideally = 1MB). If the string representing the resource location is 64 characters, an array may be used in place of a simple JSON string type, which viewers will automatically concatenate to create a single URI. Please note that if distributed storage systems like IPFS or Arweave are used it is required to use a URI containing the respective scheme (e.g., ipfs:// or ar://) and not merely the content identifier (CID) as token viewers may not be able to locate the file. Valid identifiers would include: \"https://cardano.org/favicon-32x32.png\" \"ipfs://QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\" [\"ipfs://\", \"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"] Invalid identifiers would include: \"cardano.org/favicon-32x32.png\" \"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\" [\"Qm\", \"bQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"] If an inline base64-encoded image will be used, the data must be prepended with a valid data: mime_type ;base64 prefix as specified by the data URL scheme standard to indicate the image is an inline data object See the OpenSea \"IPFS and Arweave URIs\" section in their reference guide for more helpful information on the topic.\nThe image property is required and must be a valid Uniform Resource Identifier (URI) pointing to a resource with mime type image/*. Note that this resource is used as thumbnail or the actual link if the token is an image (ideally = 1MB). If the string representing the resource location is 64 characters, an array may be used in place of a simple JSON string type, which viewers will automatically concatenate to create a single URI.\nimage\nimage/*\nPlease note that if distributed storage systems like IPFS or Arweave are used it is required to use a URI containing the respective scheme (e.g., ipfs:// or ar://) and not merely the content identifier (CID) as token viewers may not be able to locate the file. Valid identifiers would include: \"https://cardano.org/favicon-32x32.png\" \"ipfs://QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\" [\"ipfs://\", \"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"] Invalid identifiers would include: \"cardano.org/favicon-32x32.png\" \"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\" [\"Qm\", \"bQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"]\nipfs://\nar://\nValid identifiers would include: \"https://cardano.org/favicon-32x32.png\" \"ipfs://QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\" [\"ipfs://\", \"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"]\n\"https://cardano.org/favicon-32x32.png\"\n\"https://cardano.org/favicon-32x32.png\"\n\"ipfs://QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"\n\"ipfs://QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"\n[\"ipfs://\", \"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"]\n[\"ipfs://\", \"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"]\nInvalid identifiers would include: \"cardano.org/favicon-32x32.png\" \"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\" [\"Qm\", \"bQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"]\n\"cardano.org/favicon-32x32.png\"\n\"cardano.org/favicon-32x32.png\"\n\"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"\n\"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"\n[\"Qm\", \"bQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"]\n[\"Qm\", \"bQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp\"]\nIf an inline base64-encoded image will be used, the data must be prepended with a valid data: mime_type ;base64 prefix as specified by the data URL scheme standard to indicate the image is an inline data object\ndata:<mime_type>;base64\nSee the OpenSea \"IPFS and Arweave URIs\" section in their reference guide for more helpful information on the topic.\nThe description property is optional.\nThe description property is optional.\ndescription\nThe mediaType and files properties are optional. mediaType is however required inside files. The src property inside files is an URI pointing to a resource of this mime type. If the mime type is image/*, mediaType points to the same image, like the on in the image property, but in an higher resolution.\nThe mediaType and files properties are optional. mediaType is however required inside files. The src property inside files is an URI pointing to a resource of this mime type. If the mime type is image/*, mediaType points to the same image, like the on in the image property, but in an higher resolution.\nmediaType\nfiles\nmediaType\nfiles\nsrc\nfiles\nimage/*\nmediaType\nimage\nThe version property is also optional. If not specified the version is 1. It will become mandatory in future versions if needed.\nThe version property is also optional. If not specified the version is 1. It will become mandatory in future versions if needed.\nversion\n1\nThis structure really just defines the basis. New properties and standards can be defined later on for varies uses cases. That's why there is an \"other properties\" tag.\nThis structure really just defines the basis. New properties and standards can be defined later on for varies uses cases. That's why there is an \"other properties\" tag.\nThe retrieval of the metadata should be the same for all however.\nThe retrieval of the metadata should be the same for all however.\nOptional fields allow to save space in the blockchain. Consequently the minimal structure for a single token is:\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array> } } } }\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array> } } } }\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array> } }, \"version\": 2 } }\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array> } }, \"version\": 2 } }\nMime types: RFC6838: Media Type Specifications and Registration Procedures\nCIP about reserved labels: CIP-0010: Transaction Metadata Label Registry\nEIP-721\nURI/URL standards: RFC3986, RFC2397\nAs mentioned above this metadata structure allows to have either one token or multiple tokens with also different policies in a single mint transaction. A third party tool can then fetch the token metadata seamlessly. It doesn't matter if the metadata includes just one token or multiple. The procedure for the third party is always the same:\nFind the latest mint transaction with the label 721 in the metadata of the specific token that mints a positive amount of the token Lookup the 721 key Lookup the Policy Id of the token Lookup the Asset name of the token You end up with the correct metadata for the token\nFind the latest mint transaction with the label 721 in the metadata of the specific token that mints a positive amount of the token\nLookup the 721 key\nLookup the Policy Id of the token\nLookup the Asset name of the token\nYou end up with the correct metadata for the token\nUsing the latest mint transaction with the label 721 as valid metadata for a token allows to update the metadata link of this token. As soon as a new mint transaction is occurring including metadata with the label 721 and a positive amount of the token, the metadata link is considered updated and the new metadata should be used. This is only possible if the policy allows to mint or burn the same token again.\nSince modern token policies or ledger rules should generally make burning of tokens permissionless, the metadata update is restricted to minting (as in positive amounts) transaction and excludes burning transactions explicitly.\nTo keep token metadata compatible with changes coming up in the future, we use the version property. A future version will introduce schema.org.\nversion\nSupport of this NFT definition in a commercially significant number and variety of NFT-related services and wallets.\nEvolution of this document and standard beyond its early adoption and use cases (up through the point when alternative NFT standards have emerged).\nPromulgation of this standard among NFT creators, minting services, token analytic / query services, and wallets.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0026 | Cardano Off-Chain Metadata\n\nWe introduce a standard for off-chain metadata that can map opaque on-chain identifiers to metadata suitable for human consumption. This will support user-interface components, and allow resolving trust issues in a moderately decentralized way.\nOn the blockchain, we tend to refer to things by hashes or other opaque identifiers. But these are not very human friendly:\nIn the case of hashes, we often want to know the preimage of the hash, such as The script corresponding to an output locked by a script hash The public key corresponding to a public key hash\nThe script corresponding to an output locked by a script hash\nThe public key corresponding to a public key hash\nWe want other metadata as appropriate, such as A human-friendly name The name of the creator, their website, an icon, etc. etc.\nA human-friendly name\nThe name of the creator, their website, an icon, etc. etc.\nWe would like such metadata to be integrated into the UI of our applications For example, if I ve accepted a particular name for a currency, I d like to see that name everywhere in the UI instead of the hash\nFor example, if I ve accepted a particular name for a currency, I d like to see that name everywhere in the UI instead of the hash\nWe want the security model of such metadata to be sound For example, we don t want users to be phished by misleading metadata\nFor example, we don t want users to be phished by misleading metadata\nWe think there is a case for a metadata distribution system that would fill these needs in a consistent fashion. This would be very useful for Plutus, multi-asset support, and perhaps even some of the existing Cardano infrastructure. Moreover, since much of the metadata which we want to store is not determined by the blockchain, we propose a system that is independent of the blockchain, and relies on non-blockchain trust mechanisms.\nThe Rationale section provides additional justifications for the design decisions in this document.\nMany pieces of information on the Cardano blockchain are stored as hashes, and only revealed at later stages when required for validation. This is the case for example of scripts (Plutus or phase-1), datums, and public keys. It is likely that (some) users will want to know the preimages of those hashes in a somewhat reliable way and, before they are revealed on-chain.\nUnlike some (un)popular opinions suggest, a blockchain is a poor choice for a content database. Blockchains are intrinsically ledgers and they are good at recording events or hashes. Yet, there are several elements for which hashes aren't providing a great user experience and to which we would rather attach some human-readable metadata (like names, websites, contact details etc...). This is the case for stake pool for instance for which SMASH already provides a solution. This is also the case for monetary policies and scripts. In both cases, having the ability to attach extra metadata to some hash with a way to ensure the authenticity of the data is useful.\nThis specification covers some parts of a bigger system likely involving multiple components. What part is being implemented and by who is considered out of the scope of this specification. We however envision a setup in which users have access to a client application (a.k.a the wallet), which itself is able to connect to some remote server. We assume the server to also offer a user-interface (either via a graphical user-interface or a application programming interface) for accepting content.\nNote The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"NOT RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.\nThere are several places in this document where we need an arbitrary hash function. We will henceforth refer to this simply as hash . The hash function MUST be Blake2b-256 (unless explicitly said otherwise). The hash of a string is the hash of the bytes of the string according to its encoding.\nMetadata consists of a mandatory metadata subject, and a number of metadata properties which describes that subject. Each property consists of a mapping from property names to property values, sequence numbers and signatures.\nMetadata subjects, property names, and property values must all be represented as UTF-8 encoded strings. In addition, property values must parse as valid JSON.\nThere is no particular interpretation attached to a metadata subject: it can be anything (see however the special-case of phase-1 monetary policy below). We anticipate however that the primary use-case for it will be something that appears on the blockchain, like the hash of a script.\nWe will refer to a whole metadata as a metadata object and to a particular property assignment for a particular metadata subject as a metadata entry. We will say that a metadata object is well-formed when it validates according to the JSON-schema specification given in annex. To be valid, a metadata object MUST be (at least) well-formed.\n{ \"subject\": \"a5408d0db0d942fd80374\", \"contact\": { \"value\": \"Cid Kramer\", \"sequenceNumber\": 0, \"signatures\": [ { \"signature\": \"79a4601\", \"publicKey\": \"bc77d04\" } ] } }\n{ \"subject\": \"a5408d0db0d942fd80374\", \"contact\": { \"value\": \"Cid Kramer\", \"sequenceNumber\": 0, \"signatures\": [ { \"signature\": \"79a4601\", \"publicKey\": \"bc77d04\" } ] } }\nMetadata entries MUST have a sequenceNumber associated with them. This is a monotonically increasing integer which is used to ensure that clients and servers do not revert to earlier versions of an entry. Upon receiving new metadata objects, servers SHOULD verify the sequence number for each entry already known for that subject and reject submissions with a lower sequence number.\nsequenceNumber\nMetadata entries MAY have attestations signatures associated with them, in the form of an array of objects. Attestation signatures are indeed annotated. An annotated signature for a message is an object with of a publicKey, and a signature of a specified message (see below) by the corresponding private key.\nsignatures\npublicKey\nsignature\nnote\nWhen we say signature in the rest of this document we mean annotated signature .\nAn attestation signature for an entry is a signature of the entry message:\nhash( hash(CBOR(subject)) + hash(CBOR(name)) + hash(CBOR(value)) + hash(CBOR(sequenceNumber)) )\nhash( hash(CBOR(subject)) + hash(CBOR(name)) + hash(CBOR(value)) + hash(CBOR(sequenceNumber)) )\nwhere + designates the concatenation of two bytestrings and CBOR designates a function which encodes its input into binary according to RFC-8949. That is, JSON strings are encoded as major type 3, JSON integers as major types 0 or 1, JSON floats as major type 7, JSON arrays as major type 4, JSON objects as major type 5, JSON booleans as major type 7 and JSON null as major type 7; according to the specification.\n+\nCBOR\nFor example, the attestation message for the example entry above is:\nhash( hash(0x75613534303864306462306439343266643830333734) + // text \"a5408d0db0d942fd80374\" hash(0x67636F6E74616374) + // text \"contact\" hash(0x6A436964204B72616D6572) + // text \"Cid Kramer\" hash(0x00) // uint 0 ) // cd731afcc904c521e0c6b3cc0b560b8157ee29c3e41cd15f8dc8984edf600029\nhash( hash(0x75613534303864306462306439343266643830333734) + // text \"a5408d0db0d942fd80374\" hash(0x67636F6E74616374) + // text \"contact\" hash(0x6A436964204B72616D6572) + // text \"Cid Kramer\" hash(0x00) // uint 0 ) // cd731afcc904c521e0c6b3cc0b560b8157ee29c3e41cd15f8dc8984edf600029\nThe publicKey and the signature MUST be base16-encoded and stored as JSON strings. All signatures must be verifiable through the Ed25519 digital signature scheme and public keys must therefore be 32-byte long Ed25519 public keys.\npublicKey\nsignature\nThe following properties are considered well-known, and the JSON in their values MUST have the given structure and semantic interpretation. New properties can be added to this list by amending this CIP. The role of well-known properties is to facilitate integration between applications implementing this CIP. Nevertheless, registries are encouraged to not restrict properties to only this limited set but, registries (or metadata servers) MUST verify the well-formedness of those properties when present in a metadata object.\n{ \"type\": \"object\", \"description\": \"A hashing algorithm identifier and a base16-enocoded bytestring, such that the bytestring is the preimage of the metadata subject under that hash function.\", \"requiredProperties\": [ \"alg\", \"msg\" ], \"properties\": { \"alg\": { \"type\": \"string\", \"description\": \"A hashing algorithm identifier. The length of the digest is given by the subject.\", \"enum\": [ \"sha1\", \"sha\", \"sha3\", \"blake2b\", \"blake2s\", \"keccak\", \"md5\" ] }, \"msg\": { \"type\": \"string\", \"description\": \"The actual preimage.\", \"encoding\": \"base16\" } } }\n{ \"type\": \"object\", \"description\": \"A hashing algorithm identifier and a base16-enocoded bytestring, such that the bytestring is the preimage of the metadata subject under that hash function.\", \"requiredProperties\": [ \"alg\", \"msg\" ], \"properties\": { \"alg\": { \"type\": \"string\", \"description\": \"A hashing algorithm identifier. The length of the digest is given by the subject.\", \"enum\": [ \"sha1\", \"sha\", \"sha3\", \"blake2b\", \"blake2s\", \"keccak\", \"md5\" ] }, \"msg\": { \"type\": \"string\", \"description\": \"The actual preimage.\", \"encoding\": \"base16\" } } }\n{ \"type\": \"string\", \"description\": \"A human-readable name for the metadata subject, suitable for use in an interface or in running text.\", \"maxLength\": 50, \"minLength\": 1 }\n{ \"type\": \"string\", \"description\": \"A human-readable name for the metadata subject, suitable for use in an interface or in running text.\", \"maxLength\": 50, \"minLength\": 1 }\n{ \"type\": \"string\", \"description\": \"A longer description of the metadata subject, suitable for use when inspecting the metadata subject itself.\", \"maxLength\": 500 }\n{ \"type\": \"string\", \"description\": \"A longer description of the metadata subject, suitable for use when inspecting the metadata subject itself.\", \"maxLength\": 500 }\n{ \"type\": \"string\", \"description\": \"A short identifier for the metadata subject, suitable to show in listings or tiles.\", \"maxLength\": 9, \"minLength\": 2 }\n{ \"type\": \"string\", \"description\": \"A short identifier for the metadata subject, suitable to show in listings or tiles.\", \"maxLength\": 9, \"minLength\": 2 }\n{ \"type\": \"integer\", \"description\": \"When the metadata subject refers to a monetary policy, refers to the number of decimals of the currency.\", \"minimum\": 0, \"maximum\": 19 },\n{ \"type\": \"integer\", \"description\": \"When the metadata subject refers to a monetary policy, refers to the number of decimals of the currency.\", \"minimum\": 0, \"maximum\": 19 },\n{ \"type\": \"string\", \"description\": \"A universal resource identifier pointing to additional information about the metadata subject.\", \"format\": \"uri\", \"maxLength\": 250 }\n{ \"type\": \"string\", \"description\": \"A universal resource identifier pointing to additional information about the metadata subject.\", \"format\": \"uri\", \"maxLength\": 250 }\n{ \"type\": \"string\", \"description\": \"An `image/png` object which is 64KB in size at most.\", \"encoding\": \"base64\", \"maxLength\": 87400 }\n{ \"type\": \"string\", \"description\": \"An `image/png` object which is 64KB in size at most.\", \"encoding\": \"base64\", \"maxLength\": 87400 }\nApplications that want to display token metadata MUST verify signatures of metadata entries against a set of trusted keys for certain subjects. We will call such applications \"clients\". Conceptually we expect clients to maintain a mapping of many subjects to many verification keys. In case where a metadata entry contains no signatures, when none of the provided signatures was produced by a known key for the corresponding subject or when none of the provided signatures verifies: the metadata entry MUST be considered invalid and not be presented to end-users.\nNote that:\nIn this scenario, a single valid signature is sufficient to consider a metadata entry valid but there can be many signatures (invalid or valid). So long as one is valid, the entry is considered verified.\nIn this scenario, a single valid signature is sufficient to consider a metadata entry valid but there can be many signatures (invalid or valid). So long as one is valid, the entry is considered verified.\nThe verification is done per entry. That is, a metadata object may contain both verified and unverified entries. Plus, entries under the same subject may be verified by different keys.\nThe verification is done per entry. That is, a metadata object may contain both verified and unverified entries. Plus, entries under the same subject may be verified by different keys.\nThe way by which the trusted keys are registered into clients is unspecified although we already consider the following, non-overlapping, complementary, options:\nClients MAY explicitly prompt the consumer / end-user about whether to accept a certain entry. While this is unpractical for some cases (e.g. token metadata), it may be relevant for some. Clients MAY come with a set of pre-configured well-known trusted keys chosen at the discretion of the application editor. End-users SHOULD have the ability to add/remove keys from their trusted set. This allows end-users to introduce trusted keys they know before they end up in the pre-configured set (which likely follow the application release cycle). In this context, keys are advertised by the signing authority by some means, for instance, on social media or on another form of public key registry (e.g. keybase.io) Mappings of subjects to keys MAY be recorded on-chain, using transaction metadata and an appropriate label registered on CIP-0010. In some scenarios, the context within which the transaction is signed may be enough to reliably trust the legitimacy of a mapping. For example, in the case of a monetary policy, one could imagine registering trusted keys in the same transaction minting tokens. Because the transaction is inserted in the ledger, it must have been signed by the token issuer and therefore, the specified keys are without doubt acknowledged by the token issuer. As a result, clients having access to on-chain data can automatically discover new mappings from observing the chain.\nClients MAY explicitly prompt the consumer / end-user about whether to accept a certain entry. While this is unpractical for some cases (e.g. token metadata), it may be relevant for some.\nClients MAY explicitly prompt the consumer / end-user about whether to accept a certain entry. While this is unpractical for some cases (e.g. token metadata), it may be relevant for some.\nClients MAY come with a set of pre-configured well-known trusted keys chosen at the discretion of the application editor.\nClients MAY come with a set of pre-configured well-known trusted keys chosen at the discretion of the application editor.\nEnd-users SHOULD have the ability to add/remove keys from their trusted set. This allows end-users to introduce trusted keys they know before they end up in the pre-configured set (which likely follow the application release cycle). In this context, keys are advertised by the signing authority by some means, for instance, on social media or on another form of public key registry (e.g. keybase.io)\nEnd-users SHOULD have the ability to add/remove keys from their trusted set. This allows end-users to introduce trusted keys they know before they end up in the pre-configured set (which likely follow the application release cycle). In this context, keys are advertised by the signing authority by some means, for instance, on social media or on another form of public key registry (e.g. keybase.io)\nMappings of subjects to keys MAY be recorded on-chain, using transaction metadata and an appropriate label registered on CIP-0010. In some scenarios, the context within which the transaction is signed may be enough to reliably trust the legitimacy of a mapping. For example, in the case of a monetary policy, one could imagine registering trusted keys in the same transaction minting tokens. Because the transaction is inserted in the ledger, it must have been signed by the token issuer and therefore, the specified keys are without doubt acknowledged by the token issuer. As a result, clients having access to on-chain data can automatically discover new mappings from observing the chain.\nMappings of subjects to keys MAY be recorded on-chain, using transaction metadata and an appropriate label registered on CIP-0010. In some scenarios, the context within which the transaction is signed may be enough to reliably trust the legitimacy of a mapping. For example, in the case of a monetary policy, one could imagine registering trusted keys in the same transaction minting tokens. Because the transaction is inserted in the ledger, it must have been signed by the token issuer and therefore, the specified keys are without doubt acknowledged by the token issuer. As a result, clients having access to on-chain data can automatically discover new mappings from observing the chain.\nnote\nThis approach is now considered superseded by the more general approach. While it has some nice properties, new applications should consider sticking to the general case even for phase-1 monetary policies. Servers who seek backward-compatibility should implement both.\nWe consider the case of phase-1 monetary policies introduced during the Allegra era of Cardano. Such policies are identified by a simple (native) script which is validated by the ledger during the so-called phase-1 validations. Such scripts are made of key hashes, combined via a set of basic first-order logic primitives (ALL, ANY-OF, N-OF...) such that, it is possible to statically verify whether a set of signatures would validate the script.\nThese scripts therefore make relatively good verification mechanisms for metadata associated with phase-1 monetary policies. Hence, we introduce the following well-known property:\n{ \"type\": \"string\", \"description\": \"A CBOR-serialized phase-1 monetary policy, used as a pre-image to produce a policyId.\", \"encoding\": \"base16\", \"minLength\": 56, \"maxLength\": 120 }\n{ \"type\": \"string\", \"description\": \"A CBOR-serialized phase-1 monetary policy, used as a pre-image to produce a policyId.\", \"encoding\": \"base16\", \"minLength\": 56, \"maxLength\": 120 }\nMetadata objects that contain an extra top-level property policy MUST therefore abide by the following rules:\npolicy\nTheir subject MUST be an assetId, encoded in base16; where the assetId is the concatenation of a policyId (28 bytes) and an assetName (up to 32 bytes). The policy MUST therefore re-hash (through blake2b-224) into the first 28 bytes of the metadata's subject (the policy id). Every metadata entry MUST have a set of signatures such that, the monetary script given in policy can be validated using the provided signatures, irrespective of the time constraints. Said differently, the public keys from the annotated signature must re-hash into key hashes present in the policy AND each key must verify its associated signature AND the provided signatures must be sufficient to validate the monetary script according to the semantic given by the cardano ledger without considering the time constraints.\nTheir subject MUST be an assetId, encoded in base16; where the assetId is the concatenation of a policyId (28 bytes) and an assetName (up to 32 bytes).\nassetId\nassetId\npolicyId\nassetName\nThe policy MUST therefore re-hash (through blake2b-224) into the first 28 bytes of the metadata's subject (the policy id).\npolicy\nEvery metadata entry MUST have a set of signatures such that, the monetary script given in policy can be validated using the provided signatures, irrespective of the time constraints. Said differently, the public keys from the annotated signature must re-hash into key hashes present in the policy AND each key must verify its associated signature AND the provided signatures must be sufficient to validate the monetary script according to the semantic given by the cardano ledger without considering the time constraints.\nFor example, consider the following phase-1 monetary policy, represented in JSON:\n{ \"type\": \"all\", \"scripts\": [ { \"keyHash\": \"2B0C33E73D2A70733EDC971D19E2CAFBADA1692DB2D35E7DC9453DF2\", \"type\": \"sig\" }, { \"keyHash\": \"E2CAFBADA1692DB2D35E7DC9453DF22B0C33E73D2A70733EDC971D19\", \"type\": \"sig\" } ] }\n{ \"type\": \"all\", \"scripts\": [ { \"keyHash\": \"2B0C33E73D2A70733EDC971D19E2CAFBADA1692DB2D35E7DC9453DF2\", \"type\": \"sig\" }, { \"keyHash\": \"E2CAFBADA1692DB2D35E7DC9453DF22B0C33E73D2A70733EDC971D19\", \"type\": \"sig\" } ] }\nTo validate such a policy, each entry would require 2 signatures:\nfrom 2B0C33E73D2A70733EDC971D19E2CAFBADA1692DB2D35E7DC9453DF2\n2B0C33E73D2A70733EDC971D19E2CAFBADA1692DB2D35E7DC9453DF2\nand from E2CAFBADA1692DB2D35E7DC9453DF22B0C33E73D2A70733EDC971D19\nE2CAFBADA1692DB2D35E7DC9453DF22B0C33E73D2A70733EDC971D19\nThe policy MAY also contain some additional time constraints (VALID-AFTER, VALID-BEFORE) specifying a certain slot number. For the sake of verifying policies, these should be ignored and consider valid.\nThe following section gives some recommendations to application developers willing to implement a metadata server / registry. Following these recommendations will facilitate interoperability between applications and also, provide some good security foundation for the server. In this context, what we refer to as a metadata server is a web server that exposes the functionality of a simple key-value store, where the keys are metadata subjects and property names, and the values are their property values.\nThe metadata server SHOULD implement the following HTTP methods:\nGET /metadata/{subject}/property/{property name}\nGET /metadata/{subject}/property/{property name}\nThis SHOULD return the property values for the given property name (if any) associated with the subject. This is returned as a array of JSON objects whose key is the property name and return the complete JSON entries for that subject+name (including value, sequenceNumber and signatures).\nvalue\nsequenceNumber\nsignatures\nThe metadata server SHOULD set the Content-Length header to allow clients to decide if they wish to download sizeable metadata.\nGET /metadata/{subject}/properties\nGET /metadata/{subject}/properties\nThis SHOULD return all the property names which are available for that subject (if any). These are returned as a JSON list of strings.\nGET /metadata/{subject}\nGET /metadata/{subject}\nThis SHOULD return a array of the full metadata objects associated with this subject, including the subject and all properties associated with it.\nPOST /metadata/query REQUEST BODY : a JSON object with the keys: “subjects” : A list of subjects, encoded as strings in the same way as the queries above. “properties” : An optional list of property names, encoded as strings in the same way as the query above.\nPOST /metadata/query REQUEST BODY : a JSON object with the keys: “subjects” : A list of subjects, encoded as strings in the same way as the queries above. “properties” : An optional list of property names, encoded as strings in the same way as the query above.\nThis endpoint provides a way to batch queries, making several requests of the server with only one HTTP request.\nIf only subjects is supplied, this query SHOULD return a list of subjects with all their properties. The response format will be as similar as possible to the GET /metadata/{subject} request, but nested inside a list.\n“subjects”\nGET /metadata/{subject}\nIf subjects and properties are supplied, the query will return a list of subjects, with their properties narrowed down to only those specified by properties .\n“subjects”\n“properties”\n“properties”\nsuggestion\nMetadata servers MAY provide other methods of querying metadata, such as:\nSearching for all mappings whose name property value is a particular string\nSearching for all metadata items which are signed by a particular cryptographic key, or uploaded by a particular user\nThe metadata server needs some way to add and modify metadata entries. The method for doing so is largely up to the implementor, but recommend to abide by the following rules:\nThe server MUST only accept updates for metadata entries that have a higher sequence number than the previous entry. The server MUST always verify well-formedness of metadata entries before accepting them. The server MUST always cryptographically verify metadata entries' signatures before accepting them. The server MAY reject entries with no signatures. The server MAY support the POST, PUT, and DELETE verbs to modify metadata entries.\nThe server MUST only accept updates for metadata entries that have a higher sequence number than the previous entry.\nThe server MUST always verify well-formedness of metadata entries before accepting them.\nThe server MUST always cryptographically verify metadata entries' signatures before accepting them.\nThe server MAY reject entries with no signatures.\nThe server MAY support the POST, PUT, and DELETE verbs to modify metadata entries.\nIf the server supports modifications to metadata entries, it SHOULD provide some form of authentication which controls who can modify them. Servers MUST NOT use the attestation signatures on metadata entries as part of authentication (with the exception of phase-1 monetary policy). Attestation signatures are per-entry, and are orthogonal to determining who controls the metadata for a subject.\nSimple systems may want to use little more than cryptographic signatures, but more sophisticated systems could have registered user accounts and control access in that way.\nThe metadata server MAY provide a mechanism auditing changes to metadata, for example by storing the update history of an entry and allowing users to query it (via the API or otherwise).\nAllowing unrestricted updates to non-verifiable metadata would allow malicious users to squat or take over another user s metadata subjects. This is why we recommend that server implementations have some kind of authentication system to mitigate this.\nOne approach is to have a system of ownership of metadata subjects. This notion of ownership is vague, although in some cases there are obvious choices (e.g. for a multisig minting policy, the obvious owners are the signatories who can authorize minting). It is up to the server to pick a policy for how to decide on owners, and enforce security; or indeed to take a different approach entirely.\nSee the earlier Authentication section for a description of a possible approach to managing ownership.\nDepending on the authentication mechanism they use, servers may also need to worry about replay attacks, where an attacker submits a (correctly signed) old record to revert a legitimate change. However, these can be unconditionally prevented by correctly implementing sequence numbers as described earlier, which prevents old entries from being accepted.\nSimilar to the recommendations for metadata servers, this section gives some recommendations to application developers willing to implement a metadata client. Following these recommendations will facilitate interoperability between applications and also, provide some good security foundation for the clients. A metadata client refers to the component that communicates with a metadata server and maintains the user s trusted metadata mapping. This may be implemented as part of a larger system, or may be an independent component (which could be shared between multiple other systems, e.g. a wallet frontend and a blockchain explorer).\nThe metadata client MUST allow the metadata server (or servers) that it references to be configured by the end-user.\nThe metadata client MUST maintain a local mapping of trusted keys and metadata entries.\nThe metadata client MUST only accept updates with a higher sequence number than the entry it currently has.\nThe metadata client MUST always verify well-formedness or metadata entries before accepting them.\nThe metadata client MUST always verify any signatures on metadata entries before accepting them.\nThe metadata client SHOULD allow browsing of its trusted keys.\nThe metadata client SHOULD allow modifications of its trusted keys by the end-user.\nThe metadata client SHOULD use a Trust On First Use (TOFU) strategy for updating this store. When an update is requested for a metadata entry, the client should query the server, but not trust the resulting metadata entry unless the user agrees to use it (either by explicit consent, or implicitly via the set of trusted keys). If the entry is trusted, it should be copied to the local store.\nThe metadata client MAY be configurable to limit the amount downloaded.\nThe metadata client MAY accept user updates for metadata entries.\nThe design space for a metadata server is quite large. For example, any of the following examples could work, or other combinations of these features:\nA single centralized server maintained by the Cardano Foundation, and updated by emailing the administrator (no user-facing front end website)\nAn open-source federation of servers with key-based authentication.\nA commercial server with an account-based system, a web-frontend, and a payments system for storage.\nThis design aims to be agnostic about the details of the implementation, by specifying only a very simple interface between the client, server, and system-component users (wallet, etc.). This allows:\nProgressive enhancement of servers over time. Initially servers may provide a very bare-bones implementation, but this can be improved later.\nCompetition between multiple implementations, including from third parties.\nFor much of the metadata we are concerned with there is no right answer. The metadata server is thus playing a key trusted role - even if that trust is partial because users can rely on attestation signatures. We therefore believe that it is critical that we allow users to choose which metadata server (or servers) they refer to.\nAn analogy is with DNS nameservers: these are a trusted piece of infrastructure for any particular user, but users have a choice of which nameservers to use, and can use multiple.\nThis also makes it possible for these servers to be a true piece of community infrastructure, rather than something wedded to a major player (although we hope that the Cardano Foundation and IOHK will produce a competitive offering).\nA key distinction is between metadata that is verifiably correct and that which is non-verifiable.\nThe key example of verifiable metadata is hashe pre-images. Where the metadata subject is a hash, the preimage of that hash is verifiable metadata, since we can simply hash it and check that it matches. This covers several cases that we are interested in:\nMapping script hashes to the script\nMapping public key hashes to the public key\nHowever, most metadata is non-verifiable:\nHuman-readable names for scripts are not verifiable: there is no right answer to what the name of a script is.\nMany scripts do not have an obvious owner (certainly this is true for Plutus scripts), but even for those which do (e.g. phase-1 monetary scripts) this does not mean that the owner is trusted! See the Security section below for more discussion on trust.\nAny associations with authors, websites, icons, etc are similarly non-verifiable as there is no basis for establishing trust.\nMost of the security considerations relate to non-verifiable metadata. Verifiable metadata can generally always be accepted as is (provided that it is verified). Our threat model is that non-verifiable metadata may always have been provided by an attacker.\nAccepting non-verifiable metadata blindly can lead to attacks. For example, a malicious server or user might attempt to name their currency Ada . If we blindly accept this and overwrite the existing mapping for Ada , this would lead to easy phishing attacks. The approach we take is heavily inspired by petname systems, GPG keyservers, and local address books. The user always has a local mapping which is trusted, and then adding to or updating that mapping requires explicit user consent, unless we can prove that this is trustworthy (i.e. the metadata is verifiable).\nHow can users decide whether to trust an update? This is where the attestation signatures come into play. If a user trusts the entity which signs the metadata record, that may be sufficient for them to accept it as a legitimate update.\nClients could also be tricked into downloading large amounts of metadata that the user does not want. For this reason clients should expose some kind of configurable download limiting, and we suggest that the server set the Content-Length header to support this. However, this problem is no worse than that faced by the average web browser, so we do not think it will be a problem in practice.\nClients also need to worry about replay attacks, where they are sent old (correctly signed) records in an attempt to roll back a legitimate update. The easiest way to avoid this is to correctly implement sequence numbers, in which case old updates will be rejected.\nThis design needs to store a fair amount of data in a shared location. We might wonder whether we should use the blockchain for this: we could store metadata updates in transaction metadata.\nHowever, storing this information on-chain does not actually help us:\nThe trust model for metadata is different than that of the ledger and transactions. The only trust we have (and can expect to have) in the metadata is that it is signed by a particular key, regardless of the purpose or nature of the data. E.g. when posting a script, there is no explicit association between the script and the signing key other than the owner of the key choosing to post it\nThe metadata is precisely that: metadata. While it is about the chain, it does not directly affect ledger state transitions and therefore we should not require it to be associated with a specific transaction.\nBesides, on-chain storage comes with considerable downsides:\nHigher cost to users for modifications and storage\nIncreases in the UTXO size\nAwkwardness of querying the data\nSize limits on transaction metadata\nFor this reason, we think that a traditional database is a much better fit. However, it would be perfectly possible for someone to produce an implementation that was backed by the chain if they believed that that could be competitive.\nMetadata may potentially be sizable. For example, preimages of hashes can in principle be any size!\nServers will want some way to manage this to avoid abuse. However, this is a typical problem faced by web services and can be solved in the usual ways: size limits per account, charging for storage, etc.\nSee Special Case: Phase-1 Monetary Policies which covers existing implementations.\nDocument commercial or community implementations of any of the use cases described above. cardano-token-registry\ncardano-token-registry\nProvide a reference implementation: Off-chain metadata tools\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0027 | CNFT Community Royalties Standard\n\nThis proposed standard will allow for uniform royalties' distributions across the secondary market space. It is easy to implement using metadata only, and does not require a smart contract. However, it is scalable to allow for the usage of a downstream smart contract, as needed by the asset creator.\nThere is a significant interest within the Cardano Community for an implementation of royalties distribution when a Cardano Asset is resold on the secondary market. It has become a common theme to see and hear statements that the only thing stopping artists from adopting Cardano, is that they are waiting for an implementation of royalties.\nAt the present time, smart contracts do not create a simple mechanism to implement royalties. By developing a community standard, we can resolve the immediate need for royalties, and create a path forward for a potential future iteration of smart contracts.\nA new tag of 777 is proposed for this implementation. The community guidelines have been agreed as follows:\nA brand new unused policy for implementation is required. The royalties tags are to be written to an unnamed token, using the policy to be used for the intended Cardano Assets. Only the first minted set of instructions will be honored. Any future updates or rewrites will be ignored. This prevents a Cardano Asset maker from changing the royalties at a future date. Within this created asset will be the metadata for royalties distributions. It will use a tag of 777, and then have two tags to identify the percentage of future sales requested as a royalty, and the payment address to forward those royalties to. Those tags will be \"rate\" and \"addr\" respectively. The \"rate\" key tag can be any floating point value from 0.0 to 1.0, to represent between 0 and 100 percent. For example, a 12.5 percent royalty would be represented with \"rate\": \"0.125\". Previous version 1.0 of this proposal used pct instead of rate. Marketplaces to continue to honor legacy pct tag. The \"addr\" key tag can be a string value, or an array. It is to include a single payment address. By allowing for an array, the payment address can exceed the per line 64 character limitation. This payment address could be part of a smart contract, which should allow for greater flexibility of royalties distributions, controlled by the asset creator. The royalty token must be minted prior to creating any assets off the policy. All markets will be instructed to look only for the first minted asset on a policy, which would need to be the unnamed 777 token.\nA brand new unused policy for implementation is required.\nThe royalties tags are to be written to an unnamed token, using the policy to be used for the intended Cardano Assets.\nOnly the first minted set of instructions will be honored. Any future updates or rewrites will be ignored. This prevents a Cardano Asset maker from changing the royalties at a future date.\nWithin this created asset will be the metadata for royalties distributions. It will use a tag of 777, and then have two tags to identify the percentage of future sales requested as a royalty, and the payment address to forward those royalties to. Those tags will be \"rate\" and \"addr\" respectively.\nThe \"rate\" key tag can be any floating point value from 0.0 to 1.0, to represent between 0 and 100 percent. For example, a 12.5 percent royalty would be represented with \"rate\": \"0.125\". Previous version 1.0 of this proposal used pct instead of rate. Marketplaces to continue to honor legacy pct tag.\nThe \"addr\" key tag can be a string value, or an array. It is to include a single payment address. By allowing for an array, the payment address can exceed the per line 64 character limitation. This payment address could be part of a smart contract, which should allow for greater flexibility of royalties distributions, controlled by the asset creator.\nThe royalty token must be minted prior to creating any assets off the policy. All markets will be instructed to look only for the first minted asset on a policy, which would need to be the unnamed 777 token.\n{ \"777\": { \"rate\": \"0.2\", \"addr\": \"addr1v9nevxg9wunfck0gt7hpxuy0elnqygglme3u6l3nn5q5gnq5dc9un\" } }\n{ \"777\": { \"rate\": \"0.2\", \"addr\": [ \"addr1q8g3dv6ptkgsafh7k5muggrvfde2szzmc2mqkcxpxn7c63l9znc9e3xa82h\", \"pf39scc37tcu9ggy0l89gy2f9r2lf7husfvu8wh\" ] } }\nCreate policy for planned assets. Mint no name token with community standard royalties metadata. Burn no name token to free up UTxO (recommended, but not required). Mint planned assets using this same policy.\nCreate policy for planned assets.\nMint no name token with community standard royalties metadata.\nBurn no name token to free up UTxO (recommended, but not required).\nMint planned assets using this same policy.\nBy creating a new tag for the distinct purpose of royalties distributions, Cardano Asset makers, and Marketplaces can uniformly apply royalties to assets with predictable results.\nBy creating the instructions on a single, no name token, all marketplaces will know the correct location of the royalties asset, without having to further locate it.\nBy enforcing the requirement of honoring only the first mint, cardano asset buyers and owners can predict the future resale value of the assets in their possession.\nThe solution is scalable to any desired royalty percentage. It is easy to work with this new standard, and does not require an in depth understanding of smart contracts.\nSupport of royalty distribution according to this standard by multiple significant NFT related platforms.\nImplementation in libraries supporting NFT minting, including: Mesh (Minting Royalty Token)\nMesh (Minting Royalty Token)\nIncorporate input from many Cardano NFT related entities, including: Artano BuffyBot CNFT.io Digital Syndicate Fencemaker MADinArt NFT-Maker.io Hydrun Tokhun\nArtano\nBuffyBot\nCNFT.io\nDigital Syndicate\nFencemaker\nMADinArt\nNFT-Maker.io\nHydrun\nTokhun\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0028 | Protocol Parameters (Alonzo Era)\n\nThis CIP extends CIP-0009 to include the new protocol parameters that have been introduced for Alonzo, specifically those relating to the costing of Plutus scripts. It describes the initial settings for those parameters.\nWe need to document the chain of changes to the protocol parameters. This document describes precisely the changes that have been made from CIP-0009, allowing the differences to be determined. It thus supplements rather than replaces CIP-0009.\nThe new updatable protocol parameter values for Alonzo are given below (in JSON format). Any of these parameters may be changed by submitting a parameter update proposal to the chain, and without triggering a \"hard fork\". Note that these parameters are given using the names used in the genesis file. Be aware that some parameters are shown differently using cardano-cli query protocol-parameters -- this has been raised as an issue with the development team.\ncardano-cli query protocol-parameters\n{ \"lovelacePerUTxOWord\": 34482, \"executionPrices\": { \"prSteps\": { \"numerator\" : 721, \"denominator\" : 10000000 }, \"prMem\": { \"numerator\" : 577, \"denominator\" : 10000 } }, \"maxTxExUnits\": { \"exUnitsMem\": 10000000, \"exUnitsSteps\": 10000000000 }, \"maxBlockExUnits\": { \"exUnitsMem\": 50000000, \"exUnitsSteps\": 40000000000 }, \"maxValueSize\": 5000, \"collateralPercentage\": 150, \"maxCollateralInputs\": 3, \"costModels\": { \"PlutusV1\": { \"sha2_256-memory-arguments\": 4, \"equalsString-cpu-arguments-constant\": 1000, \"cekDelayCost-exBudgetMemory\": 100, \"lessThanEqualsByteString-cpu-arguments-intercept\": 103599, \"divideInteger-memory-arguments-minimum\": 1, \"appendByteString-cpu-arguments-slope\": 621, \"blake2b-cpu-arguments-slope\": 29175, \"iData-cpu-arguments\": 150000, \"encodeUtf8-cpu-arguments-slope\": 1000, \"unBData-cpu-arguments\": 150000, \"multiplyInteger-cpu-arguments-intercept\": 61516, \"cekConstCost-exBudgetMemory\": 100, \"nullList-cpu-arguments\": 150000, \"equalsString-cpu-arguments-intercept\": 150000, \"trace-cpu-arguments\": 150000, \"mkNilData-memory-arguments\": 32, \"lengthOfByteString-cpu-arguments\": 150000, \"cekBuiltinCost-exBudgetCPU\": 29773, \"bData-cpu-arguments\": 150000, \"subtractInteger-cpu-arguments-slope\": 0, \"unIData-cpu-arguments\": 150000, \"consByteString-memory-arguments-intercept\": 0, \"divideInteger-memory-arguments-slope\": 1, \"divideInteger-cpu-arguments-model-arguments-slope\": 118, \"listData-cpu-arguments\": 150000, \"headList-cpu-arguments\": 150000, \"chooseData-memory-arguments\": 32, \"equalsInteger-cpu-arguments-intercept\": 136542, \"sha3_256-cpu-arguments-slope\": 82363, \"sliceByteString-cpu-arguments-slope\": 5000, \"unMapData-cpu-arguments\": 150000, \"lessThanInteger-cpu-arguments-intercept\": 179690, \"mkCons-cpu-arguments\": 150000, \"appendString-memory-arguments-intercept\": 0, \"modInteger-cpu-arguments-model-arguments-slope\": 118, \"ifThenElse-cpu-arguments\": 1, \"mkNilPairData-cpu-arguments\": 150000, \"lessThanEqualsInteger-cpu-arguments-intercept\": 145276, \"addInteger-memory-arguments-slope\": 1, \"chooseList-memory-arguments\": 32, \"constrData-memory-arguments\": 32, \"decodeUtf8-cpu-arguments-intercept\": 150000, \"equalsData-memory-arguments\": 1, \"subtractInteger-memory-arguments-slope\": 1, \"appendByteString-memory-arguments-intercept\": 0, \"lengthOfByteString-memory-arguments\": 4, \"headList-memory-arguments\": 32, \"listData-memory-arguments\": 32, \"consByteString-cpu-arguments-intercept\": 150000, \"unIData-memory-arguments\": 32, \"remainderInteger-memory-arguments-minimum\": 1, \"bData-memory-arguments\": 32, \"lessThanByteString-cpu-arguments-slope\": 248, \"encodeUtf8-memory-arguments-intercept\": 0, \"cekStartupCost-exBudgetCPU\": 100, \"multiplyInteger-memory-arguments-intercept\": 0, \"unListData-memory-arguments\": 32, \"remainderInteger-cpu-arguments-model-arguments-slope\": 118, \"cekVarCost-exBudgetCPU\": 29773, \"remainderInteger-memory-arguments-slope\": 1, \"cekForceCost-exBudgetCPU\": 29773, \"sha2_256-cpu-arguments-slope\": 29175, \"equalsInteger-memory-arguments\": 1, \"indexByteString-memory-arguments\": 1, \"addInteger-memory-arguments-intercept\": 1, \"chooseUnit-cpu-arguments\": 150000, \"sndPair-cpu-arguments\": 150000, \"cekLamCost-exBudgetCPU\": 29773, \"fstPair-cpu-arguments\": 150000, \"quotientInteger-memory-arguments-minimum\": 1, \"decodeUtf8-cpu-arguments-slope\": 1000, \"lessThanInteger-memory-arguments\": 1, \"lessThanEqualsInteger-cpu-arguments-slope\": 1366, \"fstPair-memory-arguments\": 32, \"modInteger-memory-arguments-intercept\": 0, \"unConstrData-cpu-arguments\": 150000, \"lessThanEqualsInteger-memory-arguments\": 1, \"chooseUnit-memory-arguments\": 32, \"sndPair-memory-arguments\": 32, \"addInteger-cpu-arguments-intercept\": 197209, \"decodeUtf8-memory-arguments-slope\": 8, \"equalsData-cpu-arguments-intercept\": 150000, \"mapData-cpu-arguments\": 150000, \"mkPairData-cpu-arguments\": 150000, \"quotientInteger-cpu-arguments-constant\": 148000, \"consByteString-memory-arguments-slope\": 1, \"cekVarCost-exBudgetMemory\": 100, \"indexByteString-cpu-arguments\": 150000, \"unListData-cpu-arguments\": 150000, \"equalsInteger-cpu-arguments-slope\": 1326, \"cekStartupCost-exBudgetMemory\": 100, \"subtractInteger-cpu-arguments-intercept\": 197209, \"divideInteger-cpu-arguments-model-arguments-intercept\": 425507, \"divideInteger-memory-arguments-intercept\": 0, \"cekForceCost-exBudgetMemory\": 100, \"blake2b-cpu-arguments-intercept\": 2477736, \"remainderInteger-cpu-arguments-constant\": 148000, \"tailList-cpu-arguments\": 150000, \"encodeUtf8-cpu-arguments-intercept\": 150000, \"equalsString-cpu-arguments-slope\": 1000, \"lessThanByteString-memory-arguments\": 1, \"multiplyInteger-cpu-arguments-slope\": 11218, \"appendByteString-cpu-arguments-intercept\": 396231, \"lessThanEqualsByteString-cpu-arguments-slope\": 248, \"modInteger-memory-arguments-slope\": 1, \"addInteger-cpu-arguments-slope\": 0, \"equalsData-cpu-arguments-slope\": 10000, \"decodeUtf8-memory-arguments-intercept\": 0, \"chooseList-cpu-arguments\": 150000, \"constrData-cpu-arguments\": 150000, \"equalsByteString-memory-arguments\": 1, \"cekApplyCost-exBudgetCPU\": 29773, \"quotientInteger-memory-arguments-slope\": 1, \"verifySignature-cpu-arguments-intercept\": 3345831, \"unMapData-memory-arguments\": 32, \"mkCons-memory-arguments\": 32, \"sliceByteString-memory-arguments-slope\": 1, \"sha3_256-memory-arguments\": 4, \"ifThenElse-memory-arguments\": 1, \"mkNilPairData-memory-arguments\": 32, \"equalsByteString-cpu-arguments-slope\": 247, \"appendString-cpu-arguments-intercept\": 150000, \"quotientInteger-cpu-arguments-model-arguments-slope\": 118, \"cekApplyCost-exBudgetMemory\": 100, \"equalsString-memory-arguments\": 1, \"multiplyInteger-memory-arguments-slope\": 1, \"cekBuiltinCost-exBudgetMemory\": 100, \"remainderInteger-memory-arguments-intercept\": 0, \"sha2_256-cpu-arguments-intercept\": 2477736, \"remainderInteger-cpu-arguments-model-arguments-intercept\": 425507, \"lessThanEqualsByteString-memory-arguments\": 1, \"tailList-memory-arguments\": 32, \"mkNilData-cpu-arguments\": 150000, \"chooseData-cpu-arguments\": 150000, \"unBData-memory-arguments\": 32, \"blake2b-memory-arguments\": 4, \"iData-memory-arguments\": 32, \"nullList-memory-arguments\": 32, \"cekDelayCost-exBudgetCPU\": 29773, \"subtractInteger-memory-arguments-intercept\": 1, \"lessThanByteString-cpu-arguments-intercept\": 103599, \"consByteString-cpu-arguments-slope\": 1000, \"appendByteString-memory-arguments-slope\": 1, \"trace-memory-arguments\": 32, \"divideInteger-cpu-arguments-constant\": 148000, \"cekConstCost-exBudgetCPU\": 29773, \"encodeUtf8-memory-arguments-slope\": 8, \"quotientInteger-cpu-arguments-model-arguments-intercept\": 425507, \"mapData-memory-arguments\": 32, \"appendString-cpu-arguments-slope\": 1000, \"modInteger-cpu-arguments-constant\": 148000, \"verifySignature-cpu-arguments-slope\": 1, \"unConstrData-memory-arguments\": 32, \"quotientInteger-memory-arguments-intercept\": 0, \"equalsByteString-cpu-arguments-constant\": 150000, \"sliceByteString-memory-arguments-intercept\": 0, \"mkPairData-memory-arguments\": 32, \"equalsByteString-cpu-arguments-intercept\": 112536, \"appendString-memory-arguments-slope\": 1, \"lessThanInteger-cpu-arguments-slope\": 497, \"modInteger-cpu-arguments-model-arguments-intercept\": 425507, \"modInteger-memory-arguments-minimum\": 1, \"sha3_256-cpu-arguments-intercept\": 0, \"verifySignature-memory-arguments\": 1, \"cekLamCost-exBudgetMemory\": 100, \"sliceByteString-cpu-arguments-intercept\": 150000 } } }\n{ \"lovelacePerUTxOWord\": 34482, \"executionPrices\": { \"prSteps\": { \"numerator\" : 721, \"denominator\" : 10000000 }, \"prMem\": { \"numerator\" : 577, \"denominator\" : 10000 } }, \"maxTxExUnits\": { \"exUnitsMem\": 10000000, \"exUnitsSteps\": 10000000000 }, \"maxBlockExUnits\": { \"exUnitsMem\": 50000000, \"exUnitsSteps\": 40000000000 }, \"maxValueSize\": 5000, \"collateralPercentage\": 150, \"maxCollateralInputs\": 3, \"costModels\": { \"PlutusV1\": { \"sha2_256-memory-arguments\": 4, \"equalsString-cpu-arguments-constant\": 1000, \"cekDelayCost-exBudgetMemory\": 100, \"lessThanEqualsByteString-cpu-arguments-intercept\": 103599, \"divideInteger-memory-arguments-minimum\": 1, \"appendByteString-cpu-arguments-slope\": 621, \"blake2b-cpu-arguments-slope\": 29175, \"iData-cpu-arguments\": 150000, \"encodeUtf8-cpu-arguments-slope\": 1000, \"unBData-cpu-arguments\": 150000, \"multiplyInteger-cpu-arguments-intercept\": 61516, \"cekConstCost-exBudgetMemory\": 100, \"nullList-cpu-arguments\": 150000, \"equalsString-cpu-arguments-intercept\": 150000, \"trace-cpu-arguments\": 150000, \"mkNilData-memory-arguments\": 32, \"lengthOfByteString-cpu-arguments\": 150000, \"cekBuiltinCost-exBudgetCPU\": 29773, \"bData-cpu-arguments\": 150000, \"subtractInteger-cpu-arguments-slope\": 0, \"unIData-cpu-arguments\": 150000, \"consByteString-memory-arguments-intercept\": 0, \"divideInteger-memory-arguments-slope\": 1, \"divideInteger-cpu-arguments-model-arguments-slope\": 118, \"listData-cpu-arguments\": 150000, \"headList-cpu-arguments\": 150000, \"chooseData-memory-arguments\": 32, \"equalsInteger-cpu-arguments-intercept\": 136542, \"sha3_256-cpu-arguments-slope\": 82363, \"sliceByteString-cpu-arguments-slope\": 5000, \"unMapData-cpu-arguments\": 150000, \"lessThanInteger-cpu-arguments-intercept\": 179690, \"mkCons-cpu-arguments\": 150000, \"appendString-memory-arguments-intercept\": 0, \"modInteger-cpu-arguments-model-arguments-slope\": 118, \"ifThenElse-cpu-arguments\": 1, \"mkNilPairData-cpu-arguments\": 150000, \"lessThanEqualsInteger-cpu-arguments-intercept\": 145276, \"addInteger-memory-arguments-slope\": 1, \"chooseList-memory-arguments\": 32, \"constrData-memory-arguments\": 32, \"decodeUtf8-cpu-arguments-intercept\": 150000, \"equalsData-memory-arguments\": 1, \"subtractInteger-memory-arguments-slope\": 1, \"appendByteString-memory-arguments-intercept\": 0, \"lengthOfByteString-memory-arguments\": 4, \"headList-memory-arguments\": 32, \"listData-memory-arguments\": 32, \"consByteString-cpu-arguments-intercept\": 150000, \"unIData-memory-arguments\": 32, \"remainderInteger-memory-arguments-minimum\": 1, \"bData-memory-arguments\": 32, \"lessThanByteString-cpu-arguments-slope\": 248, \"encodeUtf8-memory-arguments-intercept\": 0, \"cekStartupCost-exBudgetCPU\": 100, \"multiplyInteger-memory-arguments-intercept\": 0, \"unListData-memory-arguments\": 32, \"remainderInteger-cpu-arguments-model-arguments-slope\": 118, \"cekVarCost-exBudgetCPU\": 29773, \"remainderInteger-memory-arguments-slope\": 1, \"cekForceCost-exBudgetCPU\": 29773, \"sha2_256-cpu-arguments-slope\": 29175, \"equalsInteger-memory-arguments\": 1, \"indexByteString-memory-arguments\": 1, \"addInteger-memory-arguments-intercept\": 1, \"chooseUnit-cpu-arguments\": 150000, \"sndPair-cpu-arguments\": 150000, \"cekLamCost-exBudgetCPU\": 29773, \"fstPair-cpu-arguments\": 150000, \"quotientInteger-memory-arguments-minimum\": 1, \"decodeUtf8-cpu-arguments-slope\": 1000, \"lessThanInteger-memory-arguments\": 1, \"lessThanEqualsInteger-cpu-arguments-slope\": 1366, \"fstPair-memory-arguments\": 32, \"modInteger-memory-arguments-intercept\": 0, \"unConstrData-cpu-arguments\": 150000, \"lessThanEqualsInteger-memory-arguments\": 1, \"chooseUnit-memory-arguments\": 32, \"sndPair-memory-arguments\": 32, \"addInteger-cpu-arguments-intercept\": 197209, \"decodeUtf8-memory-arguments-slope\": 8, \"equalsData-cpu-arguments-intercept\": 150000, \"mapData-cpu-arguments\": 150000, \"mkPairData-cpu-arguments\": 150000, \"quotientInteger-cpu-arguments-constant\": 148000, \"consByteString-memory-arguments-slope\": 1, \"cekVarCost-exBudgetMemory\": 100, \"indexByteString-cpu-arguments\": 150000, \"unListData-cpu-arguments\": 150000, \"equalsInteger-cpu-arguments-slope\": 1326, \"cekStartupCost-exBudgetMemory\": 100, \"subtractInteger-cpu-arguments-intercept\": 197209, \"divideInteger-cpu-arguments-model-arguments-intercept\": 425507, \"divideInteger-memory-arguments-intercept\": 0, \"cekForceCost-exBudgetMemory\": 100, \"blake2b-cpu-arguments-intercept\": 2477736, \"remainderInteger-cpu-arguments-constant\": 148000, \"tailList-cpu-arguments\": 150000, \"encodeUtf8-cpu-arguments-intercept\": 150000, \"equalsString-cpu-arguments-slope\": 1000, \"lessThanByteString-memory-arguments\": 1, \"multiplyInteger-cpu-arguments-slope\": 11218, \"appendByteString-cpu-arguments-intercept\": 396231, \"lessThanEqualsByteString-cpu-arguments-slope\": 248, \"modInteger-memory-arguments-slope\": 1, \"addInteger-cpu-arguments-slope\": 0, \"equalsData-cpu-arguments-slope\": 10000, \"decodeUtf8-memory-arguments-intercept\": 0, \"chooseList-cpu-arguments\": 150000, \"constrData-cpu-arguments\": 150000, \"equalsByteString-memory-arguments\": 1, \"cekApplyCost-exBudgetCPU\": 29773, \"quotientInteger-memory-arguments-slope\": 1, \"verifySignature-cpu-arguments-intercept\": 3345831, \"unMapData-memory-arguments\": 32, \"mkCons-memory-arguments\": 32, \"sliceByteString-memory-arguments-slope\": 1, \"sha3_256-memory-arguments\": 4, \"ifThenElse-memory-arguments\": 1, \"mkNilPairData-memory-arguments\": 32, \"equalsByteString-cpu-arguments-slope\": 247, \"appendString-cpu-arguments-intercept\": 150000, \"quotientInteger-cpu-arguments-model-arguments-slope\": 118, \"cekApplyCost-exBudgetMemory\": 100, \"equalsString-memory-arguments\": 1, \"multiplyInteger-memory-arguments-slope\": 1, \"cekBuiltinCost-exBudgetMemory\": 100, \"remainderInteger-memory-arguments-intercept\": 0, \"sha2_256-cpu-arguments-intercept\": 2477736, \"remainderInteger-cpu-arguments-model-arguments-intercept\": 425507, \"lessThanEqualsByteString-memory-arguments\": 1, \"tailList-memory-arguments\": 32, \"mkNilData-cpu-arguments\": 150000, \"chooseData-cpu-arguments\": 150000, \"unBData-memory-arguments\": 32, \"blake2b-memory-arguments\": 4, \"iData-memory-arguments\": 32, \"nullList-memory-arguments\": 32, \"cekDelayCost-exBudgetCPU\": 29773, \"subtractInteger-memory-arguments-intercept\": 1, \"lessThanByteString-cpu-arguments-intercept\": 103599, \"consByteString-cpu-arguments-slope\": 1000, \"appendByteString-memory-arguments-slope\": 1, \"trace-memory-arguments\": 32, \"divideInteger-cpu-arguments-constant\": 148000, \"cekConstCost-exBudgetCPU\": 29773, \"encodeUtf8-memory-arguments-slope\": 8, \"quotientInteger-cpu-arguments-model-arguments-intercept\": 425507, \"mapData-memory-arguments\": 32, \"appendString-cpu-arguments-slope\": 1000, \"modInteger-cpu-arguments-constant\": 148000, \"verifySignature-cpu-arguments-slope\": 1, \"unConstrData-memory-arguments\": 32, \"quotientInteger-memory-arguments-intercept\": 0, \"equalsByteString-cpu-arguments-constant\": 150000, \"sliceByteString-memory-arguments-intercept\": 0, \"mkPairData-memory-arguments\": 32, \"equalsByteString-cpu-arguments-intercept\": 112536, \"appendString-memory-arguments-slope\": 1, \"lessThanInteger-cpu-arguments-slope\": 497, \"modInteger-cpu-arguments-model-arguments-intercept\": 425507, \"modInteger-memory-arguments-minimum\": 1, \"sha3_256-cpu-arguments-intercept\": 0, \"verifySignature-memory-arguments\": 1, \"cekLamCost-exBudgetMemory\": 100, \"sliceByteString-cpu-arguments-intercept\": 150000 } } }\nThe meaning of the fields is:\n{ \"prSteps\": { \"numerator\" : 721, \"denominator\" : 10000000}, \"prMem\": { \"numerator\" : 577, \"denominator\" : 10000 } }\n{ \"exUnitsMem\": 10000000, \"exUnitsSteps\": 10000000000 }\n{ \"exUnitsMem\": 50000000, \"exUnitsSteps\": 40000000000 }\n{ \"PlutusV1\": { ... } }\nEach version of the Plutus interpreter may use different cost model parameters and settings. Although the parameters are updatable, they are likely to be changed only when introducing new Plutus interpreter versions at a \"hard fork\". For simplicity, the details of the parameter settings is omitted here.\nminUTxOValue is no longer used. It is replaced by lovelacePerUTxOWord.\nminUTxOValue\nlovelacePerUTxOWord\nThere are no changes to the non-updatable protocol parameters.\nThe majority of the parameters are needed to enable the use of Plutus scripts on-chain. They relate to the fees calculations for transactions that include Plutus scripts.\nexecutionPrices are specified in fractions of lovelace per Plutus CPU execution step or memory unit. These have been set to be consistent with the cost for a full transaction.\nexecutionPrices\nlovelacePerUTxOWord replaces minUTxOValue. Rather than determining a fixed minimum deposit, the new value scales each word that is used. The value is set to give a very similar result for an ada-only UTxO entry (previously, 1,000,000 lovelace; now 999,978 lovelace, since each ada-only UTxO entry is 29 words).\nlovelacePerUTxOWord\nminUTxOValue\ncollateralPercentagehas been chosen to be higher than the transaction fee. Collateral should only be used to pay fees if a user has deliberately submitted a transaction that is known to fail. Setting the percentage high acts to discourage the submission of rogue transactions, which maliciously consume chain resources.\ncollateralPercentage\nmaxCollateralInputs has been set to allow the option of multiple inputs to be used to pay collateral, if needed (e.g. so that multiple instance of a transaction can be submitted without sharing a single collateral, that might restrict concurrency or cause script failure if the collateral was not available).\nmaxCollateralInputs\nmaxValueSize has been set based on benchmarking.\nmaxValueSize\ncostModels has been set for PlutusV1 based on benchmarking inputs. Each Plutus Core primitive has associated costs.\ncostModels\nPlutusV1\nThe Alonzo ledger era is activated.\nDocumented parameters have been in operational use by Cardano Node and Ledger as of the Alonzo ledger era.\nAlonzo ledger era parameters are deemed correct by working groups at IOG.\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0029 | Phase-1 Monetary Scripts Serialization Formats\n\nThis specification describes how to serialize Phase-1 monetary scripts (a.k.a. \"native scripts\") to various formats (JSON, CBOR) to facilitate inter-operability between applications.\nWhile the existence of scripts is well-known, and have an unambiguous on-chain representation. There's no agreed upon format for off-chain or higher-level interfaces for which a binary string is a poor fit. This CIP regroups both the on-chain binary format and other, more verbose, formats like JSON.\nThis specification covers at present two serialization formats: JSON and CBOR. The CBOR matches exactly the on-chain representation and is extracted from the cardano-ledger-specs source code and put here as a convenient place to lookup while the source can change location over time.\nThe CBOR serialization format is given as a CDDL specification in annexe. When a hash of the phase-1 monetary script is needed, it usually refers to a Blake2b-192 digest of the corresponding serialized script byte-string prefixed with a null byte: \\x00.\n\\x00\nThe JSON format is given as a JSON schema in annexe. It is preferred in user interfaces such as command-lines or APIs, where some level of human inspection may be required.\nScripts may contain unbounded integers! Implementation parsing them, either from CBOR or JSON shall be prepared to handle possible big integers ( = 2 64).\n- json: { \"type\": \"sig\" , \"keyHash\": \"00000000000000000000000000000000000000000000000000000000\" } cbor: \"8200581c00000000000000000000000000000000000000000000000000000000\" - json: { \"type\": \"all\" , \"scripts\": [ { \"type\": \"sig\" , \"keyHash\": \"00000000000000000000000000000000000000000000000000000000\" } , { \"type\": \"any\" , \"scripts\": [ { \"type\": \"after\" , \"slot\": 42 } , { \"type\": \"sig\" , \"keyHash\": \"00000000000000000000000000000000000000000000000000000001\" } ] } ] } cbor: \"8201828200581c000000000000000000000000000000000000000000000000000000008202828204182a8200581c00000000000000000000000000000000000000000000000000000001\" - json: { \"type\": \"before\" , \"slot\": 42 } cbor: \"8205182a\" - json: { \"type\": \"atLeast\" , \"required\": 2 , \"scripts\": [ { \"type\": \"sig\", \"keyHash\": \"00000000000000000000000000000000000000000000000000000000\" } , { \"type\": \"sig\", \"keyHash\": \"00000000000000000000000000000000000000000000000000000001\" } , { \"type\": \"sig\", \"keyHash\": \"00000000000000000000000000000000000000000000000000000002\" } ] } cbor: \"00830302838200581c000000000000000000000000000000000000000000000000000000008200581c000000000000000000000000000000000000000000000000000000018200581c00000000000000000000000000000000000000000000000000000002\"\n- json: { \"type\": \"sig\" , \"keyHash\": \"00000000000000000000000000000000000000000000000000000000\" } cbor: \"8200581c00000000000000000000000000000000000000000000000000000000\" - json: { \"type\": \"all\" , \"scripts\": [ { \"type\": \"sig\" , \"keyHash\": \"00000000000000000000000000000000000000000000000000000000\" } , { \"type\": \"any\" , \"scripts\": [ { \"type\": \"after\" , \"slot\": 42 } , { \"type\": \"sig\" , \"keyHash\": \"00000000000000000000000000000000000000000000000000000001\" } ] } ] } cbor: \"8201828200581c000000000000000000000000000000000000000000000000000000008202828204182a8200581c00000000000000000000000000000000000000000000000000000001\" - json: { \"type\": \"before\" , \"slot\": 42 } cbor: \"8205182a\" - json: { \"type\": \"atLeast\" , \"required\": 2 , \"scripts\": [ { \"type\": \"sig\", \"keyHash\": \"00000000000000000000000000000000000000000000000000000000\" } , { \"type\": \"sig\", \"keyHash\": \"00000000000000000000000000000000000000000000000000000001\" } , { \"type\": \"sig\", \"keyHash\": \"00000000000000000000000000000000000000000000000000000002\" } ] } cbor: \"00830302838200581c000000000000000000000000000000000000000000000000000000008200581c000000000000000000000000000000000000000000000000000000018200581c00000000000000000000000000000000000000000000000000000002\"\nThe preimage for computing script hashes is prefixed with \\x00 to distinguish them from phase-2 monetary scripts (a.k.a Plutus Script) which are then prefixed with \\x01. This is merely a discriminator tag.\nThe preimage for computing script hashes is prefixed with \\x00 to distinguish them from phase-2 monetary scripts (a.k.a Plutus Script) which are then prefixed with \\x01. This is merely a discriminator tag.\n\\x00\n\\x01\nThe current JSON format is based off the cardano-cli's format which has been used widely for minting tokens and is likely the most widely accepted format at the moment.\nThe current JSON format is based off the cardano-cli's format which has been used widely for minting tokens and is likely the most widely accepted format at the moment.\nThere exist official software releases supporting this serialization format: cardano-cli cardano-api\ncardano-cli\ncardano-api\nIncorporating this serialization format into Cardano software libraries and command line tools.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0030 | Cardano dApp-Wallet Web Bridge\n\nThis documents describes a webpage-based communication bridge allowing webpages (i.e. dApps) to interface with Cardano wallets. This is done via injected javascript code into webpages. This specification defines the manner that such code is to be accessed by the webpage/dApp, as well as defining the API for dApps to communicate with the user's wallet. This document currently concerns the Shelley-Mary era but will have a second version once Plutus is supported. This specification is intended to cover similar use cases as web3 for Ethereum or EIP-0012 for Ergo. The design of this spec was based on the latter.\nIn order to facilitate future dApp development, we will need a way for dApps to communicate with the user's wallet. While Cardano does not yet support smart contracts, there are still various use cases for this, such as NFT management. This will also lay the groundwork for an updated version of the spec once the Alonzo hardfork is released which can extend it to allow for Plutus support.\nA string representing an address in either bech32 format, or hex-encoded bytes. All return types containing Address must return the hex-encoded bytes format, but must accept either format for inputs.\nAddress\nA hex-encoded string of the corresponding bytes.\ncbor<T>\nA hex-encoded string representing CBOR corresponding to T defined via CDDL either inside of the Shelley Multi-asset binary spec or, if not present there, from the CIP-0008 signing spec. This representation was chosen when possible as it is consistent across the Cardano ecosystem and widely used by other tools, such as cardano-serialization-lib, which has support to encode every type in the binary spec as CBOR bytes.\nT\ntype DataSignature = {| signature: cbor<COSE_Sign1>, key: cbor<COSE_Key>, |};\ntype DataSignature = {| signature: cbor<COSE_Sign1>, key: cbor<COSE_Key>, |};\nIf we have CBOR specified by the following CDDL referencing the Shelley-MA CDDL:\ntransaction_unspent_output = [ input: transaction_input, output: transaction_output, ]\ntransaction_unspent_output = [ input: transaction_input, output: transaction_output, ]\nthen we define\ntype TransactionUnspentOutput = cbor<transaction_unspent_output>\ntype TransactionUnspentOutput = cbor<transaction_unspent_output>\nThis allows us to use the output for constructing new transactions using it as an output as the transaction_output in the Shelley Multi-asset CDDL does not contain enough information on its own to spend it.\ntransaction_output\ntype Paginate = {| page: number, limit: number, |};\ntype Paginate = {| page: number, limit: number, |};\nUsed to specify optional pagination for some API calls. Limits results to {limit} each page, and uses a 0-indexing {page} to refer to which of those pages of {limit} items each. dApps should be aware that if a wallet is modified between paginated calls that this will change the pagination, e.g. some results skipped or showing up multiple times but otherwise the wallet must respect the pagination order.\nAn extension is an object with a single field \"cip\" that describes a CIP number extending the API (as a plain integer, without padding). For example:\n\"cip\"\n{ \"cip\": 30 }\n{ \"cip\": 30 }\nAPIErrorCode { InvalidRequest: -1, InternalError: -2, Refused: -3, AccountChange: -4, } APIError { code: APIErrorCode, info: string }\nAPIErrorCode { InvalidRequest: -1, InternalError: -2, Refused: -3, AccountChange: -4, } APIError { code: APIErrorCode, info: string }\nInvalidRequest - Inputs do not conform to this spec or are otherwise invalid.\nInternalError - An error occurred during execution of this API call.\nRefused - The request was refused due to lack of access - e.g. wallet disconnects.\nAccountChange - The account has changed. The dApp should call wallet.enable() to reestablish connection to the new account. The wallet should not ask for confirmation as the user was the one who initiated the account change in the first place.\nwallet.enable()\nDataSignErrorCode { ProofGeneration: 1, AddressNotPK: 2, UserDeclined: 3, } type DataSignError = { code: DataSignErrorCode, info: String }\nDataSignErrorCode { ProofGeneration: 1, AddressNotPK: 2, UserDeclined: 3, } type DataSignError = { code: DataSignErrorCode, info: String }\nProofGeneration - Wallet could not sign the data (e.g. does not have the secret key associated with the address)\nAddressNotPK - Address was not a P2PK address and thus had no SK associated with it.\nUserDeclined - User declined to sign the data\ntype PaginateError = {| maxSize: number, |};\ntype PaginateError = {| maxSize: number, |};\n{maxSize} is the maximum size for pagination and if the dApp tries to request pages outside of this boundary this error is thrown.\nTxSendErrorCode = { Refused: 1, Failure: 2, } type TxSendError = { code: TxSendErrorCode, info: String }\nTxSendErrorCode = { Refused: 1, Failure: 2, } type TxSendError = { code: TxSendErrorCode, info: String }\nRefused - Wallet refuses to send the tx (could be rate limiting)\nFailure - Wallet could not send the tx\nTxSignErrorCode = { ProofGeneration: 1, UserDeclined: 2, } type TxSignError = { code: TxSignErrorCode, info: String }\nTxSignErrorCode = { ProofGeneration: 1, UserDeclined: 2, } type TxSignError = { code: TxSignErrorCode, info: String }\nProofGeneration - User has accepted the transaction sign, but the wallet was unable to sign the transaction (e.g. not having some of the private keys)\nUserDeclined - User declined to sign the transaction\nIn order to initiate communication from webpages to a user's Cardano wallet, the wallet must provide the following javascript API to the webpage. A shared, namespaced cardano object must be injected into the page if it did not exist already. Each wallet implementing this standard must then create a field in this object with a name unique to each wallet containing a wallet object with the following methods. The API is split into two stages to maintain the user's privacy, as the user will have to consent to cardano.{walletName}.enable() in order for the dApp to read any information pertaining to the user's wallet with {walletName} corresponding to the wallet's namespaced name of its choice.\ncardano\nwallet\ncardano.{walletName}.enable()\n{walletName}\ncardano.{walletName}.enable({ extensions: Extension[] } = {}): Promise<API>\nErrors: APIError\nAPIError\nThis is the entrypoint to start communication with the user's wallet. The wallet should request the user's permission to connect the web page to the user's wallet, and if permission has been granted, the full API will be returned to the dApp to use. The wallet can choose to maintain a whitelist to not necessarily ask the user's permission every time access is requested, but this behavior is up to the wallet and should be transparent to web pages using this API. If a wallet is already connected this function should not request access a second time, and instead just return the API object.\nAPI\nUpon start, dApp can explicitly request a list of additional functionalities they expect as a list of CIP numbers capturing those extensions. This is used as an extensibility mechanism to document what functionalities can be provided by the wallet interface. CIP-0030 provides a set of base interfaces that every wallet must support. Then, new functionalities are introduced via additional CIPs and may be all or partially supported by wallets.\nDApps are expected to use this endpoint to perform an initial handshake and ensure that the wallet supports all their required functionalities. Note that it's possible for two extensions to be mutually incompatible (because they provide two conflicting features). While we may try to avoid this as much as possible while designing CIPs, it is also the responsability of wallet providers to assess whether they can support a given combination of extensions, or not. Hence wallets aren't expected to fail should they not recognize or not support a particular combination of extensions. Instead, they should decide what they enable and reflect their choice in the response to api.getExtensions() in the Full API. As a result, dApps may fail and inform their users or may use a different, less-efficient, strategy to cope with a lack of functionality.\napi.getExtensions()\nIt is at the extension author's discretion if they wish to separate their endpoints from the base API via namespacing. Although, it is highly recommend that authors do namespace all of their extensions. If namespaced, endpoints must be preceded by .cipXXXX. from the API object, without any leading zeros.\n.cipXXXX.\nAPI\nFor example; CIP-0123's endpoints should be accessed by:\napi.cip123.endpoint1() api.cip123.endpoint2()\napi.cip123.endpoint1() api.cip123.endpoint2()\nAuthors should be careful when omitting namespacing. Omission should only be considered when creating endpoints to override those defined in this specification or other extensions. Even so when overriding; the new functionality should not prevent dApps from accessing past functionality thus overriding must ensure backwards compatibility.\nAny namespace omission needs to be fully justified via the proposal's Rationale section, with explanation to why it is necessary. Any potential backwards compatibility considerations should be noted to give wallets and dApps a clear unambiguous direction.\nExtensions that are draft, in development, or prototyped should not use extension naming nor should they use official namspacing until assigned a CIP number. Draft extension authors are free to test their implementation endpoints by using the Experimental API. Once a CIP number is assigned implementors should move functionality out of the experimental API.\nYes. Extensions may have other extensions as pre-requisite. Some newer extensions may also invalidate functionality introduced by earlier extensions. There's no particular rule or constraints in that regards. Extensions are specified as CIP, and will define what it entails to enable them.\nYes. They all are CIPs.\nYes. Extensions may introduce new endpoints or error codes, and modify existing ones. Although, it is recommended that endpoints are namespaced. Extensions may even change the rules outlined in this very proposal. The idea being that wallet providers should start off implementing this CIP, and then walk their way to implementing their chosen extensions.\nNo. It's up to wallet providers to decide which extensions they ought to support.\ncardano.{walletName}.isEnabled(): Promise<bool>\nErrors: APIError\nAPIError\nReturns true if the dApp is already connected to the user's wallet, or if requesting access would return true without user confirmation (e.g. the dApp is whitelisted), and false otherwise. If this function returns true, then any subsequent calls to wallet.enable() during the current session should succeed and return the API object.\nwallet.enable()\nAPI\ncardano.{walletName}.apiVersion: String\nThe version number of the API that the wallet supports. Set to 1.\n1\ncardano.{walletName}.supportedExtensions: Extension[]\nA list of extensions supported by the wallet. Extensions may be requested by dApps on initialization. Some extensions may be mutually conflicting and this list does not thereby reflect what extensions will be enabled by the wallet. Yet it informs on what extensions are known and can be requested by dApps if needed.\ncardano.{walletName}.name: String\nA name for the wallet which can be used inside of the dApp for the purpose of asking the user which wallet they would like to connect with.\ncardano.{walletName}.icon: String\nA URI image (e.g. data URI base64 or other) for img src for the wallet which can be used inside of the dApp for the purpose of asking the user which wallet they would like to connect with.\nUpon successful connection via cardano.{walletName}.enable(), a javascript object we will refer to as API (type) / api (instance) is returned to the dApp with the following methods. All read-only methods (all but the signing functionality) should not require any user interaction as the user has already consented to the dApp reading information about the wallet's state when they agreed to cardano.{walletName}.enable(). The remaining methods api.signTx() and api.signData() must request the user's consent in an informative way for each and every API call in order to maintain security.\ncardano.{walletName}.enable()\nAPI\napi\ncardano.{walletName}.enable()\napi.signTx()\napi.signData()\nThe API chosen here is for the minimum API necessary for dApp - Wallet interactions without convenience functions that don't strictly need the wallet's state to work. The API here is for now also only designed for Shelley's Mary hardfork and thus has NFT support. When Alonzo is released with Plutus support this API will have to be extended.\napi.getExtensions(): Promise<Extension[]>\nErrors: APIError\nAPIError\nRetrieves the list of extensions enabled by the wallet. This may be influenced by the set of extensions requested in the initial enable request.\nenable\napi.getNetworkId(): Promise<number>\nErrors: APIError\nAPIError\nReturns the network id of the currently connected account. 0 is testnet and 1 is mainnet but other networks can possibly be returned by wallets. Those other network ID values are not governed by this document. This result will stay the same unless the connected account has changed.\napi.getUtxos(amount: cbor<value> = undefined, paginate: Paginate = undefined): Promise<TransactionUnspentOutput[] | null>\nErrors: APIError, PaginateError\nAPIError\nPaginateError\nIf amount is undefined, this shall return a list of all UTXOs (unspent transaction outputs) controlled by the wallet. If amount is not undefined, this request shall be limited to just the UTXOs that are required to reach the combined ADA/multiasset value target specified in amount, and if this cannot be attained, null shall be returned. The results can be further paginated by paginate if it is not undefined.\namount\nundefined\namount\nundefined\namount\nnull\npaginate\nundefined\napi.getCollateral(params: { amount: cbor<Coin> }): Promise<TransactionUnspentOutput[] | null>\nErrors: APIError\nAPIError\nThe function takes a required object with parameters. With a single required parameter for now: amount. (NOTE: some wallets may be ignoring the amount parameter, in which case it might be possible to call the function without it, but this behavior is not recommended!). Reasons why the amount parameter is required:\namount\namount\nDapps must be motivated to understand what they are doing with the collateral, in case they decide to handle it manually. Depending on the specific wallet implementation, requesting more collateral than necessarily might worsen the user experience with that dapp, requiring the wallet to make explicit wallet reorganisation when it is not necessary and can be avoided. If dapps don't understand how much collateral they actually need to make their transactions work - they are placing more user funds than necessary in risk.\nDapps must be motivated to understand what they are doing with the collateral, in case they decide to handle it manually.\nDepending on the specific wallet implementation, requesting more collateral than necessarily might worsen the user experience with that dapp, requiring the wallet to make explicit wallet reorganisation when it is not necessary and can be avoided.\nIf dapps don't understand how much collateral they actually need to make their transactions work - they are placing more user funds than necessary in risk.\nSo requiring the amount parameter would be a by-spec behavior for a wallet. Not requiring it is possible, but not specified, so dapps should not rely on that and the behavior is not recommended.\namount\nThis shall return a list of one or more UTXOs (unspent transaction outputs) controlled by the wallet that are required to reach AT LEAST the combined ADA value target specified in amount AND the best suitable to be used as collateral inputs for transactions with plutus script inputs (pure ADA-only utxos). If this cannot be attained, an error message with an explanation of the blocking problem shall be returned. NOTE: wallets are free to return utxos that add up to a greater total ADA value than requested in the amount parameter, but wallets must never return any result where utxos would sum up to a smaller total ADA value, instead in a case like that an error message must be returned.\namount\namount\nThe main point is to allow the wallet to encapsulate all the logic required to handle, maintain, and create (possibly on-demand) the UTXOs suitable for collateral inputs. For example, whenever attempting to create a plutus-input transaction the dapp might encounter a case when the set of all user UTXOs don't have any pure entries at all, which are required for the collateral, in which case the dapp itself is forced to try and handle the creation of the suitable entries by itself. If a wallet implements this function it allows the dapp to not care whether the suitable utxos exist among all utxos, or whether they have been stored in a separate address chain (see https://github.com/cardano-foundation/CIPs/pull/104), or whether they have to be created at the moment on-demand - the wallet guarantees that the dapp will receive enough utxos to cover the requested amount, or get an error in case it is technically impossible to get collateral in the wallet (e.g. user does not have enough ADA at all).\nThe amount parameter is required, specified as a string (BigNumber) or a number, and the maximum allowed value must be agreed to be something like 5 ADA. Not limiting the maximum possible value might force the wallet to attempt to purify an unreasonable amount of ADA just because the dapp is doing something weird. Since by protocol the required collateral amount is always a percentage of the transaction fee, it seems that the 5 ADA limit should be enough for the foreseeable future.\namount\nstring\nnumber\napi.getBalance(): Promise<cbor<value>>\nErrors: APIError\nAPIError\nReturns the total balance available of the wallet. This is the same as summing the results of api.getUtxos(), but it is both useful to dApps and likely already maintained by the implementing wallet in a more efficient manner so it has been included in the API as well.\napi.getUtxos()\napi.getUsedAddresses(paginate: Paginate = undefined): Promise<Address[]>\nErrors: APIError\nAPIError\nReturns a list of all used (included in some on-chain transaction) addresses controlled by the wallet. The results can be further paginated by paginate if it is not undefined.\npaginate\nundefined\napi.getUnusedAddresses(): Promise<Address[]>\nErrors: APIError\nAPIError\nReturns a list of unused addresses controlled by the wallet.\napi.getChangeAddress(): Promise<Address>\nErrors: APIError\nAPIError\nReturns an address owned by the wallet that should be used as a change address to return leftover assets during transaction creation back to the connected wallet. This can be used as a generic receive address as well.\napi.getRewardAddresses(): Promise<Address[]>\nErrors: APIError\nAPIError\nReturns the reward addresses owned by the wallet. This can return multiple addresses e.g. CIP-0018.\napi.signTx(tx: cbor<transaction>, partialSign: bool = false): Promise<cbor<transaction_witness_set>>\nErrors: APIError, TxSignError\nAPIError\nTxSignError\nRequests that a user sign the unsigned portions of the supplied transaction. The wallet should ask the user for permission, and if given, try to sign the supplied body and return a signed transaction. If partialSign is true, the wallet only tries to sign what it can. If partialSign is false and the wallet could not sign the entire transaction, TxSignError shall be returned with the ProofGeneration code. Likewise if the user declined in either case it shall return the UserDeclined code. Only the portions of the witness set that were signed as a result of this call are returned to encourage dApps to verify the contents returned by this endpoint while building the final transaction.\npartialSign\npartialSign\nTxSignError\nProofGeneration\nUserDeclined\napi.signData(addr: Address, payload: Bytes): Promise<DataSignature>\nErrors: APIError, DataSignError\nAPIError\nDataSignError\nThis endpoint utilizes the CIP-0008 signing spec for standardization/safety reasons. It allows the dApp to request the user to sign a payload conforming to said spec. The user's consent should be requested and the message to sign shown to the user. The payment key from addr will be used for base, enterprise and pointer addresses to determine the EdDSA25519 key used. The staking key will be used for reward addresses. This key will be used to sign the COSE_Sign1's Sig_structure with the following headers set:\naddr\nCOSE_Sign1\nSig_structure\nalg (1) - must be set to EdDSA (-8)\nalg\nEdDSA\nkid (4) - Optional, if present must be set to the same value as in the COSE_key specified below. It is recommended to be set to the same value as in the \"address\" header.\nkid\nCOSE_key\n\"address\"\n\"address\" - must be set to the raw binary bytes of the address as per the binary spec, without the CBOR binary wrapper tag\n\"address\"\nThe payload is not hashed and no external_aad is used.\nexternal_aad\nIf the payment key for addr is not a P2Pk address then DataSignError will be returned with code AddressNotPK. ProofGeneration shall be returned if the wallet cannot generate a signature (i.e. the wallet does not own the requested payment private key), and UserDeclined will be returned if the user refuses the request. The return shall be a DataSignature with signature set to the hex-encoded CBOR bytes of the COSE_Sign1 object specified above and key shall be the hex-encoded CBOR bytes of a COSE_Key structure with the following headers set:\naddr\nDataSignError\nAddressNotPK\nProofGeneration\nUserDeclined\nDataSignature\nsignature\nCOSE_Sign1\nkey\nCOSE_Key\nkty (1) - must be set to OKP (1)\nkty\nOKP\nkid (2) - Optional, if present must be set to the same value as in the COSE_Sign1 specified above.\nkid\nCOSE_Sign1\nalg (3) - must be set to EdDSA (-8)\nalg\nEdDSA\ncrv (-1) - must be set to Ed25519 (6)\ncrv\nEd25519\nx (-2) - must be set to the public key bytes of the key used to sign the Sig_structure\nx\nSig_structure\napi.submitTx(tx: cbor<transaction>): Promise<hash32>\nErrors: APIError, TxSendError\nAPIError\nTxSendError\nAs wallets should already have this ability, we allow dApps to request that a transaction be sent through it. If the wallet accepts the transaction and tries to send it, it shall return the transaction id for the dApp to track. The wallet is free to return the TxSendError with code Refused if they do not wish to send it, or Failure if there was an error in sending it (e.g. preliminary checks failed on signatures).\nTxSendError\nRefused\nFailure\nMultiple experimental namespaces are used:\nunder api (ex: api.experimental.myFunctionality).\napi\napi.experimental.myFunctionality\nunder cardano.{walletName} (ex: window.cardano.{walletName}.experimental.myFunctionality)\ncardano.{walletName}\nwindow.cardano.{walletName}.experimental.myFunctionality\nThe benefits of this are:\nWallets can add non-standardized features while still following the CIP30 structure dApp developers can use these functions explicitly knowing they are experimental (not stable or standardized) New features can be added to CIP30 as experimental features and only moved to non-experimental once multiple wallets implement it It provides a clear path to updating the CIP version number (when functions move from experimental - stable)\nWallets can add non-standardized features while still following the CIP30 structure\ndApp developers can use these functions explicitly knowing they are experimental (not stable or standardized)\nNew features can be added to CIP30 as experimental features and only moved to non-experimental once multiple wallets implement it\nIt provides a clear path to updating the CIP version number (when functions move from experimental - stable)\nSee justification and explanations provided with each API endpoint.\nExtensions provide an extensibility mechanism and a way to negotiate (possibly conflicting) functionality between a DApp and a wallet provider. There's rules enforced as for what extensions a wallet decide to support or enable. The current mechanism only gives a way for wallets to communicate their choice back to a DApp.\nWe use object as extensions for now to leave room for adding fields in the future without breaking all existing interfaces. At this point in time however, objects are expected to be singleton.\nExtensions can be seen as a smart versioning scheme. Except that, instead of being a monotonically increasing sequence of numbers, they are multi-dimensional feature set that can be toggled on and off at will. This is a versioning \" -la-carte\" which is useful in a context where:\nThere are multiple concurrent standardization efforts on different fronts to accommodate a rapidly evolving ecosystem; Not everyone agrees and has desired to support every existing standard; There's a need from an API consumer standpoint to clearly identify what features are supported by providers.\nThere are multiple concurrent standardization efforts on different fronts to accommodate a rapidly evolving ecosystem;\nNot everyone agrees and has desired to support every existing standard;\nThere's a need from an API consumer standpoint to clearly identify what features are supported by providers.\nBy encouraging the explicit namespacing of each extension we aim to improve the usability of extensions for dApps. By allowing special cases where namespacing can be dropped we maintain good flexibility in extension design.\nThe interface is implemented and supported by various wallet providers. See also: cardano-caniuse.\nThe interface is used by DApps to interact with wallet providers. Few examples: https://www.jpg.store/ https://app.minswap.org/ https://muesliswap.com/ https://exchange.sundaeswap.finance/ https://app.indigoprotocol.io/\nhttps://www.jpg.store/\nhttps://app.minswap.org/\nhttps://muesliswap.com/\nhttps://exchange.sundaeswap.finance/\nhttps://app.indigoprotocol.io/\nProvide some reference implementation of wallet providers Berry-Pool/nami-wallet Emurgo/yoroi-wallet\nProvide some reference implementation of wallet providers\nBerry-Pool/nami-wallet\nEmurgo/yoroi-wallet\nProvide some reference implementation of the dapp connector cardano-foundation/connect-with-wallet\nProvide some reference implementation of the dapp connector\ncardano-foundation/connect-with-wallet\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0031 | Reference inputs\n\nWe introduce a new kind of input, a reference input, which allows looking at an output without spending it. This will facilitate access to information stored on the blockchain without the churn associated with spending and recreating UTXOs.\nDatums in transaction outputs provide a way to store and access information on the blockchain. However, they are quite constrained in a number of ways. Most notably, in order to access the information which is contained in them, you have to spend the output that the datum is attached to.\nThis has a number of undesirable features:\nEven if the output is recreated after being spent, it is still a new output. Any other user who wishes to look at the data cannot spend the old output (which is gone), but must rather spend the new output (which they will not know about until the next block). In practice this throttles some applications to one \"operation\" per block. Looking at the information in the datum requires spending the output, which means that care must be taken over the distribution of any funds in the output (possibly requiring scripts), and spending conditions must be met. This is overly stringent, inconvenient, and expensive.\nEven if the output is recreated after being spent, it is still a new output. Any other user who wishes to look at the data cannot spend the old output (which is gone), but must rather spend the new output (which they will not know about until the next block). In practice this throttles some applications to one \"operation\" per block.\nLooking at the information in the datum requires spending the output, which means that care must be taken over the distribution of any funds in the output (possibly requiring scripts), and spending conditions must be met. This is overly stringent, inconvenient, and expensive.\nWe would like to have a mechanism for accessing the information in datums that avoids these issues.\nHere are some cases where we expect this to be helpful.\nInspecting the state (datum, or locked value) of an on-chain application without having to consume the output, e.g. checking the current state of a stablecoin state machine. On-chain data providers that store data in outputs, to be referenced by other scripts. (With inline datums) Creating a single UTXO with data to be used in multiple subsequent transactions, but only paying the cost for submitting it once.\nInspecting the state (datum, or locked value) of an on-chain application without having to consume the output, e.g. checking the current state of a stablecoin state machine.\nOn-chain data providers that store data in outputs, to be referenced by other scripts.\n(With inline datums) Creating a single UTXO with data to be used in multiple subsequent transactions, but only paying the cost for submitting it once.\nThis proposal was designed in tandem with two further proposals which make use of reference scripts, CIP-32 (Inline datums) and CIP-33 (Reference scripts). A full assessment of the worth of this proposal should take into account the fact that it is an enabler for these other proposals.\nWe introduce a new kind of transaction input, a reference input. Transaction authors specify inputs as either normal (spending) inputs or reference inputs.\nA reference input is a transaction input, which is linked to a particular transaction output as normal, except that it references the output rather than spending it. That is:\nA referenced output must exist in the UTXO set.\nAny value on a referenced output is not considered when balancing the transaction.\nThe spending conditions on referenced outputs are not checked, nor are the witnesses required to be present. i.e. validators are not required to pass (nor are the scripts themselves or redeemers required to be present at all), and signatures are not required for pubkey outputs.\ni.e. validators are not required to pass (nor are the scripts themselves or redeemers required to be present at all), and signatures are not required for pubkey outputs.\nReferenced outputs are not removed from the UTXO set if the transaction validates.\nReference inputs are visible to scripts.\nFor clarity, the following two behaviours which are present today are unchanged by this proposal:\nTransactions must spend at least one output.1 Spending an output does require the spending conditions to be checked.2\nTransactions must spend at least one output.1\nSpending an output does require the spending conditions to be checked.2\nScripts are passed information about transactions via the script context. The script context therefore needs to be augmented to contain information about reference inputs.\nChanging the script context will require a new Plutus language version in the ledger to support the new interface. The change in the new interface is: a new field is added to the structure which contains the list of reference inputs.\nThe interface for old versions of the language will not be changed. Scripts with old versions cannot be spent in transactions that include reference inputs, attempting to do so will be a phase 1 transaction validation failure.\nToday, transactions can contain extra datums that are not required for validation. Currently the extra datums must all be pre-images of datum hashes that appear in outputs. We change this so that the extra datums can also be pre-images of datum hashes that appear in reference inputs.3\nNote that the existing mechanism includes the hash of the extra datum structure in the transaction body, so that additional datums cannot be stripped by an attacker without voiding transaction signatures.\nThe CDDL for transaction bodies will change to the following to reflect the new field.\ntransaction_body = { 0 : set<transaction_input> ; inputs ... , ? 16 : set<transaction_input> ; reference inputs }\ntransaction_body = { 0 : set<transaction_input> ; inputs ... , ? 16 : set<transaction_input> ; reference inputs }\nThe key idea of this proposal is to use UTXOs to carry information. But UTXOs are currently a bad fit for distributing information. Because of locality, we have to include outputs that we use in the transaction, and the only way we have of doing that is to spend them - and a spent output cannot then be referenced by anything else. To put it another way: outputs are resource-like, but information is not resource-like.\nThe solution is to add a way to inspect (\"reference\") outputs without spending them. This allows outputs to play double duty as resource containers (for the value they carry) and information containers (for the data they carry).\nWe have a number of requirements that we need to fulfil.\nDeterminism It must be possible to predict the execution of scripts precisely, given the transaction.\nIt must be possible to predict the execution of scripts precisely, given the transaction.\nLocality All data involved in transaction validation should be included in the transaction or the outputs which it spends (or references).\nAll data involved in transaction validation should be included in the transaction or the outputs which it spends (or references).\nNon-interference As far as possible, transactions should not interfere with others. The key exception is when transactions consume resources that other transactions want (usually by consuming UTXO entries).\nAs far as possible, transactions should not interfere with others. The key exception is when transactions consume resources that other transactions want (usually by consuming UTXO entries).\nReplay protection The system should not be attackable (e.g. allow unexpected data reads) by replaying old traffic.\nThe system should not be attackable (e.g. allow unexpected data reads) by replaying old traffic.\nStorage control and garbage-collection incentives The amount of storage required by the system should have controls that prevent it from overloading nodes, and ideally should have incentives to shrink the amount of storage that is used over time.\nThe amount of storage required by the system should have controls that prevent it from overloading nodes, and ideally should have incentives to shrink the amount of storage that is used over time.\nOptimized storage The system should be amenable to optimized storage solutions.\nThe system should be amenable to optimized storage solutions.\nData transfer into scripts Scripts must have a way to observe the data.\nScripts must have a way to observe the data.\nTool and library support It should not be too difficult for tools and libraries to work with the new system.\nIt should not be too difficult for tools and libraries to work with the new system.\nHydra The system should be usable without compromising the needs of Hydra, such as parallel processing of independent parts of the transaction graph.\nThe system should be usable without compromising the needs of Hydra, such as parallel processing of independent parts of the transaction graph.\nReference inputs satisfy these requirements, mostly by inheriting them from the corresponding properties of UTXOs.\nReferencing an output still requires the output to be presented as part of the transaction and be unspent, so determinism is preserved.\nReferenced outputs appear as transaction inputs, so locality is preserved.\nOutputs can be referenced multiple times, so we have non-interference even under heavy usage of referencing. Conflicts can occur only if the output is actually spent.\nReplay protection is inherited because a transaction must always spend at least one UTXO, and UTXOs can only be spent once.\nStorage control is inherited from control of the UTXO set, via the min UTXO value incentives. Data can be \"retired\" by spending the input (rather than referencing it), reclaiming the min UTXO value.\nOptimized storage is inherited directly from work on improving UTXO storage.\nData transfer into scripts works automatically since scripts can see transaction inputs.\nTools and libraries will only have to deal with a new kind of transaction input, and unless they care about scripts they can likely ignore them entirely.\nHydra is able to work with reference inputs (see below).\nThere are various approaches to augmenting Cardano with the ability to store and retrieve data more easily. All of these require some kind of resource-like system for the storage that is used for data. The main argument in favour of the reference input approach is that it reuses our existing resource-like system: UTXOs.\nAs we've seen above, this allows us to satisfy most of our requirements for free. Any other approach would need new solutions to these problems.\nAs a brief example, suppose we wanted to instead implement data storage as an on-chain, hash-indexed data store (a proposal we considered).\nThen we would need to answer a lot of questions:\nHow is the data accessed and retrieved? Are these stateful operations? Does this make transaction ordering more significant, threatening determinism?\nHow is the storage usage controlled? Do people pay for storing data? Do they pay for a fixed period, or perpetually? How is data retired?\nHow is the storage going to be implemented? How will this affect node memory usage?\nHow are scripts going to access the data in the store?\nHow will tools interact with and visualize the store and changes to it?\nScripts definitely need to see reference inputs. But we have at least two options for how we represent this in the script context: put the reference inputs in their own field; or include them with the other inputs, but tag them appropriately.\nKeeping them separate seems wise given the potential for confusing reference inputs for normal inputs. That would be quite a dangerous programming error, as it might lead a script to believe that e.g. an output had been spent when in fact it had only been referenced.\nWe also have the question of what to do about old scripts. We can't really present the information about reference inputs to them in a faithful way: representing them as spending inputs would be wildly misleading, and there is nowhere else to put them. We could omit the information entirely, but this is dangerous in a different way. Omitting information may lead scripts to make assumptions about the transaction that are untrue; for this reason we prefer not to silently omit information as a general principle. That leaves us only one option: reject transactions where we would have to present information about reference inputs to old scripts.\nCurrently this proposal has somewhat limited utility, because datums in outputs are just hashes, and the spending party is required to find and provide the preimage themselves. Therefore the CIP-32 proposal for inline datums is complementary, as it allow UTXOs to carry the data itself rather than a hash.\nIn the mean time (or if CIP-32 is not implemented), we still want users to be able to provide the datums corresponding to datum hashes in reference outputs, so that at least it is possible to reference the data, albeit clumsily. The easiest solution here is to reuse the existing mechanism for providing pre-images of datum hashes which appear in outputs.\nProviding datum hash pre-images remains optional, since there are reasons to use reference inputs even without looking at the datum, e.g. to look at the value in an output.\nThe motivation of this proposal mainly requires looking at the datums of outputs. But reference inputs allow us to do more: they let us look at the value locked in an output (i.e. how much Ada or other tokens it contains).\nThis is actually a very important feature. Since anyone can lock an output with any address, addresses are not that useful for identifying particular outputs on chain, and instead we usually rely on looking for particular tokens in the value locked by the output. Hence, if a script is interested in referring to the data attached to a particular output, it will likely want to look at the value that is locked in the output.\nFor example, an oracle provider would need to distinguish the outputs that they create (with good data) from outputs created by adversaries (with bad data). They can do this with a token, so long as scripts can then see the token!\nWe want reference inputs to be usable inside Hydra heads. This raises a worry: since Hydra processes independent parts of the transaction graph in parallel, what happens if it accepts one transaction that references an output, and one that spends the output, but then the first transaction gets put into a checkpoint before the second?\nFortunately, Hydra already has to deal with double-spend conflicts of this kind (although in the naive protocol this results in a decommit). This proposal simply introduces a new kind of conflict, a reference-spend conflict. Reference-spend conflicts should be handled by the same mechanism that is used to handle double-spend conflicts.\nOne thing that a user might want to do is to control who can reference an output. For example, an oracle provider might want to only allow a transaction to reference a particular output if the transaction also pays them some money.\nReference inputs alone do not provide any way to do this. Another mechanism would be required, but there is no consensus on what the design should be, so it is currently out of scope for this proposal. A brief summary of a few options and reasons why they are not obvious choices is included below.\nA key issue is that the choice to control referencing must lie with the creator of the output, not the spender. Therefore we must include some kind of change to outputs so that the creator can record their requirements.\nA \"check input\" is like a reference input except that the spending conditions are checked. That is, it acts as proof that you could spend the input, but does not in fact spend it.\nSince check inputs cause validator scripts to be run, it seems like they could allow us to control referencing. There are two wrinkles:\nThe same script would be used for both referencing and spending, overloading the meaning of the validator script. This is still usable, however, since the redeemer could be used to indicate which action is being taken.\nWe would need a flag on outputs to say \"this output cannot be referenced, but only checked\". Exactly what this should look like is an open question, perhaps it should be generic enough to control all the possible ways in which an output might be used (of which there would be three).\n\"Referencing conditions\" would mean adding a new field to outputs to indicate under what conditions the output may be referenced. This could potentially be an entire additional address, since the conditions might be any of the normal spending conditions (public key or script witnessing).\nHowever, this would make outputs substantially bigger and more complicated.\nReference inputs are very similar to Ergo's \"data inputs\". We chose to name them differently since \"data\" is already a widely used term with risk for confusion. We might also want to introduce other \"verb\" inputs in future.\nFully implemented in Cardano as of the Vasil protocol upgrade.\nPasses all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency and usability.\nThis CIP is licensed under CC-BY-4.0.\nThis restriction already exists, and is important. It seems unnecessary, since transactions must always pay fees and fees must come from somewhere, but fees could in principle be paid via reward withdrawals, so the requirement to spend a UTXO is relevant. That is, this proposal does not change outputs or the spending of outputs, it instead adds a new way of referring to outputs. Pre-images of datum hashes that appear in spent inputs are of course mandatory.\nThis restriction already exists, and is important. It seems unnecessary, since transactions must always pay fees and fees must come from somewhere, but fees could in principle be paid via reward withdrawals, so the requirement to spend a UTXO is relevant.\nThis restriction already exists, and is important. It seems unnecessary, since transactions must always pay fees and fees must come from somewhere, but fees could in principle be paid via reward withdrawals, so the requirement to spend a UTXO is relevant.\nThat is, this proposal does not change outputs or the spending of outputs, it instead adds a new way of referring to outputs.\nThat is, this proposal does not change outputs or the spending of outputs, it instead adds a new way of referring to outputs.\nPre-images of datum hashes that appear in spent inputs are of course mandatory.\nPre-images of datum hashes that appear in spent inputs are of course mandatory.\n2023 Cardano Foundation\n\n---\n\nCIP-0032 | Inline datums\n\nWe propose to allow datums themselves to be attached to outputs instead of datum hashes. This will allow much simpler communication of datum values between users.\nConceptually, datums are pieces of data that are attached to outputs. However, in practice datums are implemented by attaching hashes of datums to outputs, and requiring that the spending transaction provides the actual datum.\nThis is quite inconvenient for users. Datums tend to represent the result of computation done by the party who creates the output, and as such there is almost no chance that the spending party will know the datum without communicating with the creating party. That means that either the datum must be communicated between parties off-chain, or communicated on-chain by including it in the witness map of the transaction that creates the output (\"extra datums\"). This is also inconvenient for the spending party, who must watch the whole chain to spot it.\nIt would be much more convenient to just put the datum itself in an output, which is what we propose.\nWe expect that, provided we are able to bring the cost low enough, a large proportion of dapp developers will make use of this feature, as it will simplify their systems substantially.\nTransaction outputs are changed so that the datum field can contain either a hash or a datum (an \"inline datum\").\nThe min UTXO value for an output with an inline datum depends on the size of the datum, following the coinsPerUTxOWord protocol parameter.\ncoinsPerUTxOWord\nWhen an output with an inline datum is spent, the spending transaction does not need to provide the datum itself.\nScripts are passed information about transactions via the script context. The script context therefore needs to be augmented to contain information about inline datums.\nChanging the script context will require a new Plutus language version in the ledger to support the new interface.\nThere are two changes in the new version of the interface:\nThe datum field on transaction outputs can either be a hash or the actual datum.\nThe datum field on transaction inputs can either be a hash or the actual datum.\nThe interface for old versions of the language will not be changed. Scripts with old versions cannot be spent in transactions that include inline datums, attempting to do so will be a phase 1 transaction validation failure.\nThe CDDL for transaction outputs will change as follows to reflect the new alternative.\ntransaction_output = [ address , amount : value , ? datum : $hash32 / plutus_data ]\ntransaction_output = [ address , amount : value , ? datum : $hash32 / plutus_data ]\nTODO: should there be a dedicated production for datum-hash-or-datum? Does it need to be tagged?\nThe key idea of this proposal is simply to restore the conceptually straightforward situation where datums are attached to outputs. Historically, this was the way that the EUTXO model was designed, and switching to datum hashes on outputs was done to avoid bloating UTXO entries, which at that time (pre-multiasset) were constant-size (see 1 page 7).\nNow that we have variable-sized UTXO entries and the accounting to support them, we can restore inline datums.\nSince inline datums change very little about the model apart from where data is stored, we don't need to worry about violating any of the other requirements of the ledger, but we do need to worry about the effect on the size of the UTXO set.\nThis proposal gives users a way to put much larger amounts of data into the UTXO set. Won t this lead to much worse UTXO set bloat?\nThe answer is that we already have a mechanism to discourage this, namely the minimum UTXO value. If inline datums turns out to drive significantly increased space usage, then we may need to increase coinsPerUTxOWord in order to keep the UTXO size down. That will be costly and inconvenient for users, but will still allow them to use inline datums where they are most useful and the cost is bearable. Furthermore, we hope that we will in fact be able to reduce coinsPerUTxOWord when the upcoming work on moving the UTXO mostly to on-disk storage is complete.\ncoinsPerUTxOWord\ncoinsPerUTxOWord\nAnother guard rail would be to enforce upper limits on the size of inline datums. At the extreme, we could bound them to the size of a hash, which would guarantee no more space usage than today. However, this is much worse for users, since it introduces a sharp discontinuity where an inline datum is entirely acceptable, until it crosses the size threshold at which point it is unacceptable, and there is no way to avoid this. Generally we prefer to avoid such discontinuities in favour of gradually increasing costs.\nIn practice, what is implemented here may depend on whether the UTXO-on-disk work is completed by the time that this proposal is implemented.\nWe could deprecate the other methods of specifying datums (datum hashes or datum hashes+extra datums). However, the other approaches also have some advantages.\nTransmission costs: creator pays versus consumer pays Inline datums: creator pays Datum hashes: consumer pays Datum hashes+extra datums: both pay\nInline datums: creator pays\nDatum hashes: consumer pays\nDatum hashes+extra datums: both pay\nMin UTXO value costs Inline datums: depends on data size Datum hashes: fixed cost Datum hashes+extra datums: fixed cost\nInline datums: depends on data size\nDatum hashes: fixed cost\nDatum hashes+extra datums: fixed cost\nPrivacy Inline datums: datum is immediately public Datum hashes: datum is not public until consumed Datum hashes+extra datums: datum is immediately public, but only to chain-followers\nInline datums: datum is immediately public\nDatum hashes: datum is not public until consumed\nDatum hashes+extra datums: datum is immediately public, but only to chain-followers\nCommunication of datums Inline datums: easy on-chain communication Datum hashes: off-chain communication necessary Datum hashes+extra datums: complicated on-chain communication\nInline datums: easy on-chain communication\nDatum hashes: off-chain communication necessary\nDatum hashes+extra datums: complicated on-chain communication\nAny one of these factors could be important to particular use cases, so it is good to retain the other options.\nIn principle we do not need to let scripts see whether a datum is inline or not. We could pretend that inline datums are non-inline and insert them into the datum witness map.\nThe underlying question is: do we want scripts to be able to make assertions about whether datums are inline or not? There are reasons to want to do this: not using inline datums causes communication issues for users, and so it is quite reasonable that an application developer may want to be able to enforce their use.\nFurthermore, as a general principle we try to keep the script context as faithful to real transactions as possible. Even if the use for the information is not immediately obvious, we try to err on the side of providing it and letting users decide.\nHence we do include information about inline datums in the script context.\nThere are a couple of options for how to change the representation of the script context to include the new information, and whether to make a backwards compatibility effort for old language versions.\nFor the new script context:\nMatch the ledger representation as much as possible: change the fields on inputs and outputs to be either a datum hash or a datum. Try to only have one way to look up datums: put inline datums in the datum witness map and insert their hashes into the corresponding inputs and outputs; optionally add a boolean to inputs and outputs to indicate whether the datum was originally inline.\nMatch the ledger representation as much as possible: change the fields on inputs and outputs to be either a datum hash or a datum.\nTry to only have one way to look up datums: put inline datums in the datum witness map and insert their hashes into the corresponding inputs and outputs; optionally add a boolean to inputs and outputs to indicate whether the datum was originally inline.\nFor backwards compatibility:\nDon't try to represent inline datums for scripts using old language versions: old scripts simply can't be run in transactions that use inline datums (since we can't represent the information). Rewrite inline datums as non-inline datums: put inline datums in the datum witness map and insert their hashes into the corresponding inputs and outputs.\nDon't try to represent inline datums for scripts using old language versions: old scripts simply can't be run in transactions that use inline datums (since we can't represent the information).\nRewrite inline datums as non-inline datums: put inline datums in the datum witness map and insert their hashes into the corresponding inputs and outputs.\nFor the new script context, option 1 has the significant advantage of matching the ledger representation of transactions. This makes it easier to implement, and also avoids conceptual overhead for users who would have to distinguish the two ways of representing transactions. While the conceptual distance here may be small, if we let it grow over time then it may become quite confusing.\nWe then have the choice of what to do about backwards compatibility. Option 2 would work, but is more complicated for the ledger and is inconsistent in representation with our choice for the new script context (inline datums are sometimes represented faithfully, and sometimes put in the witness map). Option 1 is simple, but doesn't allow old scripts to work with inline datums. This would not be so bad if it just meant that old scripts could not be spent in transactions that include inline datums, but it also introduces a new way to make an unspendable output. If a user creates an output with an old script and an inline datum, then any transaction spending that output will include an inline datum, which we would not allow.\nUnfortunately, we cannot prevent users from creating such outputs in general, since script addresses do not include the script language, so the ledger cannot tell whether the inline datum is permissible or not. Client code typically will be able to do this, since it will usually know the script.\nThe mitigating factor is that we expect this to be uncommon in practice, particularly since we expect that most users will move to the new version relatively quickly, since we expect support for inline datums to be desirable, and released alongside other desirable features.\nHence we choose both option 1s and do not provide backwards compatibility for old language versions.\nFully implemented in Cardano as of the Vasil protocol upgrade.\nPasses all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency and usability.\nThis CIP is licensed under CC-BY-4.0.\nChakravarty, Manuel M. T. et al., \"The extended UTXO model\"\nChakravarty, Manuel M. T. et al., \"The extended UTXO model\"\nChakravarty, Manuel M. T. et al., \"The extended UTXO model\"\n2023 Cardano Foundation\n\n---\n\nCIP-0033 | Reference scripts\n\nWe propose to allow scripts (\"reference scripts\") to be attached to outputs, and to allow reference scripts to be used to satisfy script requirements during validation, rather than requiring the spending transaction to do so. This will allow transactions using common scripts to be much smaller.\nScript sizes pose a significant problem. This manifests itself in two ways:\nEvery time a script is used, the transaction which caused the usage must supply the whole script as part of the transaction. This bloats the chain, and passes on the cost of that bloat to users in the form of transaction size fees. Transaction size limits are problematic for users. Even if individual scripts do not hit the limits, a transaction which uses multiple scripts has a proportionally greater risk of hitting the limit.\nEvery time a script is used, the transaction which caused the usage must supply the whole script as part of the transaction. This bloats the chain, and passes on the cost of that bloat to users in the form of transaction size fees.\nTransaction size limits are problematic for users. Even if individual scripts do not hit the limits, a transaction which uses multiple scripts has a proportionally greater risk of hitting the limit.\nWe would like to alleviate these problems.\nThe key idea is to use reference inputs and modified outputs which carry actual scripts (\"reference scripts\"), and allow such reference scripts to satisfy the script witnessing requirement for a transaction. This means that the transaction which uses the script will not need to provide it at all, so long as it referenced an output which contained the script.\nWe extend transaction outputs with a new optional field, which contains a script (a \"reference script\").\nThe min UTXO value for an output with an additional script field depends on the size of the script, following the coinsPerUTxOWord protocol parameter.\ncoinsPerUTxOWord\nWhen we are validating a transaction and we look for the script corresponding to a script hash, in addition to the scripts provided in the transaction witnesses, we also consider any reference scripts from the outputs referred to by the inputs of the transaction.\nScripts are passed information about transactions via the script context. We propose to augment the script context to include some information about reference scripts.\nChanging the script context will require a new Plutus language version in the ledger to support the new interface. The change is: a new optional field is added to outputs and inputs to represent reference scripts. Reference scripts are represented by their hash in the script context.\nThe interface for old versions of the language will not be changed. Scripts with old versions cannot be spent in transactions that include reference scripts, attempting to do so will be a phase 1 transaction validation failure.\nThe CDDL for transaction outputs will change as follows to reflect the new field.\ntransaction_output = [ address , amount : value , ? datum : $hash32 , ? ref_script : plutus_script ]\ntransaction_output = [ address , amount : value , ? datum : $hash32 , ? ref_script : plutus_script ]\nTODO: can we use a more generic type that allows any script in a forwards-compatible way?\nThe key idea of this proposal is stop sending frequently-used scripts to the chain every time they are used, but rather make them available in a persistent way on-chain.\nThe implementation approach follows in the wake of CIP-31 (Reference inputs) and CIP-32 (Inline datums). The former considers how to do data sharing on chain, and concludes that referencing UTXOs is a good solution. The latter shows how we can safely store substantial data in UTXOs by taking advantage of existing mechanisms for size control.\nIt is therefore natural to use the same approach for scripts: put them in UTXOs, and reference them using reference inputs.\nThere are a few possible alternatives for where to store reference scripts in outputs.\nIn principle, we could add an \"inline scripts\" extension that allowed scripts themselves to be used in the address field instead of script hashes. We could then use such scripts as reference scripts.\nHowever, this approach suffers from a major confusion about the functional role of the script. You would only be able to provide a reference script that also controlled the spending of the output. This is clearly not what you want: the reference script could be anything, perhaps a script only designed for use in quite specific circumstances; whereas in many cases the user will likely want to retain control over the output with a simple public key.\nWith inline datums, we could put reference scripts in the datum field of outputs.\nThis approach has two problems. First, there is a representation confusion: we would need some way to know that a particular datum contained a reference script. We could do this implicitly, but it would be better to have an explicit marker.\nSecondly, this prevents having an output which is locked by a script that needs a datum and has a reference script in it. While this is a more unusual situation, it's not out of the question. For example, a group of users might want to use a Plutus-based multisig script to control the UTXO with a reference script in it.\nA new field is the simplest solution: it avoids these problems because the new field clearly has one specific purpose, and we do not overload the meanings of the other fields.\nThis proposal gives people a clear incentive to put large amounts (i.e. kilobytes) of data in outputs as reference scripts.\nThis is essentially the same problem which is faced in CIP-32, and we can take the same stance. We don't want to bloat the UTXO set unnecessarily, but we already have mechanisms for limiting that (in the form of the min UTXO value), and these should work transparently for reference scripts as they will for inline datums.\nWe don't strictly need to change the script context. We could simply omit any information about reference scripts carried by outputs. This would mean that we don't need to change the interface.\nWe don't have obvious use cases for the information about reference scripts, but the community may come up with use cases, and our general policy is to try and include as much information about the transaction as we can, unless there is a good reason not to.\nWe also have the question of what to do about old scripts. We can't really present the information about reference scripts to them in a faithful way, there is nowhere to put the information. We could omit the information entirely, but this is dangerous in a different way. Omitting information may lead scripts to make assumptions about the transaction that are untrue; for this reason we prefer not to silently omit information as a general principle. That leaves us only one option: reject transactions where we would have to present information about reference scripts to old scripts.\nFully implemented in Cardano as of the Vasil protocol upgrade.\nPasses all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency and usability.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0034 | Chain ID Registry\n\nCurrently Cardano has two easily-usable networks: \"mainnet\" and \"testnet\". However, in the future, we expect more networks to exist and so we need some way to refer to these networks to be able to write better multi-network applications and systems.\nCardano currently has three ways to refer to a network:\nThe \"network ID\" included in every address and also optionally present in the transaction body. This only stores 16 possibilities (4 bits)\nThe \"network magic\" used for Byron addresses and in the handshake with other nodes in the network layer. This is a random 32-bit number\nThe genesis block hash (a 28-byte number)\nBlockchains can have multiple deployments of the same codebase. For example:\nTest networks where the base asset has no value (so devs can test at no cost) Test networks where the protocol is simplified for ease of testing (ex: Cardano but with a Proof of Authority consensus for stable block production in testing) Test networks for new features Plutus Application Backend (PAB) testnet Shelley incentivized testnet Forks that diverge in feature set (the many forks of Bitcoin and Ethereum)\nTest networks where the base asset has no value (so devs can test at no cost)\nTest networks where the protocol is simplified for ease of testing (ex: Cardano but with a Proof of Authority consensus for stable block production in testing)\nTest networks for new features Plutus Application Backend (PAB) testnet Shelley incentivized testnet\nPlutus Application Backend (PAB) testnet Shelley incentivized testnet\nPlutus Application Backend (PAB) testnet\nShelley incentivized testnet\nForks that diverge in feature set (the many forks of Bitcoin and Ethereum)\ndApps may be deployed on specific testnets that match their criteria and wallets need to know about these networks to know how to behave.\nAdditionally, having a standardized registry for networks allows easy integration into the broader crypto ecosystem via standards like CAIP-2\nWe create a machine-readable registry of networks\nAll entries in this registry should have the following entries:\nUser-friendly name\nNetwork ID\nNetwork magic\nGenesis hash\nWhen representing these networks in a human-readable string, the following format shall be used:\ncip34:NetworkId-NetworkMagic\ncip34:NetworkId-NetworkMagic\nWe pick this format for the following reason:\nThe network ID is too small to be used by itself. You can see from chainlist that 16 possibilities is too few\nThe genesis hash is too long and user-unfriendly to be used.\nThere are at least two (from different providers) wallets, libraries, CLI packages, or other tools which use this standard for network identification.\nDevelop and publish reference implementation: CIP34-JS\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0035 | Changes to Plutus Core\n\nThis CIP specifies a process for proposing changes to Plutus Core, its builtins, and its interface to the Cardano ledger. It gives a taxonomy of typical changes, and explains how these changes may be made, which in some cases requires a CIP. It introduces the 'Plutus' CIP category for tracking these.\nThe Plutus Core language, its builtins, and its interface to the ledger are all likely to evolve significantly over time. There are many reasons for this:\nWe may be able to increase performance, improve safety, or reduce script sizes by changing the language.\nWe may be able to improve performance by providing builtin versions of expensive functions.\nWe may need to provide builtin versions of sensitive functions (e.g. cryptography) in order to ensure access to high-quality implementations.\nWe may find bugs in the implementation that need to be fixed.\nWe may need to change the interface to the ledger in order to represent changes in transaction formats.\nWe may wish to remove elements which have been deprecated due to the addition of improved versions.\nand more\nThis CIP gives a taxonomy of changes, explains how such changes might be implemented in Cardano, and prescribes processes for proposing such changes.\nThis CIP assumes general familiarity with Plutus Core and the Cardano ledger, but we give some brief background here.\nPlutus Core is a script language used in the Cardano ledger. For the purposes of this document, Plutus Core consists of various language constructs, and also builtin types and functions.\nPlutus Core has a number of builtin types, such as integers, and builtin functions, such as integer addition. Builtin functions provide access to functionality that would be difficult or expensive to implement using the basic constructs of Plutus Core, which is otherwise little more than the untyped lambda calculus. Builtin functions can operate only over builtin types or arbitrary Plutus Core terms treated opaquely. Builtin types come with a size metric which is used by costing functions. For example, the size metric for integers returns the bit-size of the integer.\nThe performance of Plutus Core scripts has two components: how expensive the script actually is to run (real performance) and how expensive we say it is to run in the ledger (model performance).\nModel performance is calculated by costing evaluation in abstract resource units (\"exunits\") of CPU and memory. Individual steps of evaluation are costed, and builtin functions must also come with a costing function that provides costing information for them. The costing function for a builtin function is a mathematical function which takes the sizes of the arguments (as computed by their size metrics), and returns an estimate of the budget that will be used to perform that function. For example, the costing function for addition says that the CPU and memory costs are both proportional to the maximum of the sizes of the input integers (with some appropriate constants).\nDetermining costing functions is done empirically, by running the function in question against a large number of inputs and choosing a costing function that fits the data well.\nPlutus Core has a language version (LV). This is the version of the Plutus Core programming language itself, and it controls e.g. which constructs are available in the language. Changing any of the features which are guarded by the language version requires a new language version to be supported by the chain. Note that changing the set of builtin types or functions does not require a new language version; any individual Plutus Core language version is compatible with any set of builtin types and functions.\nDepending on the type of change, a major or minor bump to the language version may be required. The following table shows typical examples.\nSince we always need a new Plutus Core langauge version for any change to the language, in the rest of this document we will focus on the introduction of a new langauge version as the proxy for changes to the language.\nThe Cardano ledger recognizes various kinds of scripts identified by language. This language tag is the only way that the ledger has to distinguish between different types of script. Hence if we require different behaviour, we need a new language. We refer to these languages as ledger languages (LLs).\nPart of the specification of a language in the ledger is how scripts of that language are run, what arguments they are passed, how those arguments are structured, etc. We refer to this as the ledger-script interface.\nBecause we want to occasionally change e.g. the ledger-script interface for Plutus Core scripts, this means we need several ledger languages which all run scripts written in Plutus Core.2 All Plutus Core ledger languages which are used in the ledger must be supported forever, in order to be able to validate the history of the chain.\nLedger languages also have an associated subset of the protocol parameters. These parameters provide the ability to control some aspects of evaluation without a software update. The most notable example is a set of parameters which parameterize the costing of program execution. Hence, a different Plutus Core ledger language can have a different set of costing protocol parameters.\nWe can change the behaviour of ledger languages in a backwards compatible way with new protocol versions. This ensures that the new behaviour only becomes available at a particular time in the history of the chain.\nOverall the combination of ledger language and protocol version controls:\nThe protocol parameters which are available\nThe ledger-script interface\nThe Plutus Core language versions that are available\nThe set of builtin types and values that are available\nThis document considers the following types of change:\nThe Plutus Core language Adding a new Plutus Core language version The Plutus Core builtin functions and types Adding a new builtin function or type Removing a builtin function or type Changing the behaviour of a builtin function or type The ledger-script interface Changing the interface between the ledger and the interpreter Protocol parameters Improving model performance (i.e. changing the costing parameters so that scripts use less budget) Regressing model performance (i.e. changing the costing parameters so that scripts use more budget) Adding costing parameters Removing costing parameters Performance of the Plutus Core interpreter Improving real performance Regressing real performance\nThe Plutus Core language Adding a new Plutus Core language version\nAdding a new Plutus Core language version\nAdding a new Plutus Core language version\nThe Plutus Core builtin functions and types Adding a new builtin function or type Removing a builtin function or type Changing the behaviour of a builtin function or type\nAdding a new builtin function or type Removing a builtin function or type Changing the behaviour of a builtin function or type\nAdding a new builtin function or type\nRemoving a builtin function or type\nChanging the behaviour of a builtin function or type\nThe ledger-script interface Changing the interface between the ledger and the interpreter\nChanging the interface between the ledger and the interpreter\nChanging the interface between the ledger and the interpreter\nProtocol parameters Improving model performance (i.e. changing the costing parameters so that scripts use less budget) Regressing model performance (i.e. changing the costing parameters so that scripts use more budget) Adding costing parameters Removing costing parameters\nImproving model performance (i.e. changing the costing parameters so that scripts use less budget) Regressing model performance (i.e. changing the costing parameters so that scripts use more budget) Adding costing parameters Removing costing parameters\nImproving model performance (i.e. changing the costing parameters so that scripts use less budget)\nRegressing model performance (i.e. changing the costing parameters so that scripts use more budget)\nAdding costing parameters\nRemoving costing parameters\nPerformance of the Plutus Core interpreter Improving real performance Regressing real performance\nImproving real performance Regressing real performance\nImproving real performance\nRegressing real performance\nChanges to Plutus Core can be released onto Cardano in four ways, with ascending levels of difficulty:\nA protocol parameter change (PP), taking effect as soon as the new parameters are accepted (in a new epoch). A software update (SU) to the node, taking effect when nodes upgrade. A hard fork (HF) (accompanied by a software update), requiring a software update for the new protocol version, and taking effect after the hard fork. A new Plutus Core ledger language (LL), introduced in a hard fork, and taking effect for scripts that use the new language, but not for those that use the old language.\nA protocol parameter change (PP), taking effect as soon as the new parameters are accepted (in a new epoch).\nA software update (SU) to the node, taking effect when nodes upgrade.\nA hard fork (HF) (accompanied by a software update), requiring a software update for the new protocol version, and taking effect after the hard fork.\nA new Plutus Core ledger language (LL), introduced in a hard fork, and taking effect for scripts that use the new language, but not for those that use the old language.\nIntuitively, these correspond to how compatible the change is.\nA backwards- and forwards-compatible change can be deployed with a software update, as nobody can perceive the difference.\nA backwards-compatible (but not forwards-compatible) change must be deployed in a hard fork, since it makes more blocks acceptable than before.\nA backwards-incompatible change requires a new Plutus Core ledger language, so that the ledger can distinguish them, and maintain the old behaviour for old scripts.\nThe following table lists, for each type of change in \"Types of change\", what kind of release it requires.\nThis CIP deals with the types of change listed in \"Types of change\". That list aims to cover the most typical changes to Plutus Core, but it is not exhaustive. CIPs which do not propose changes in the list but whose authors believe they significantly affect Plutus Core should nonetheless be assigned to the Plutus category.\nAdditionally, there is significant overlap with the Ledger category around the ledger-script interface and the protocol parameters. CIPs which change these parts of Cardano should generally use the Plutus category and not the Ledger category, although the Editors may ask the Ledger reviewers to comment.\nThe following table gives the current set of reviewers for Plutus CIPs.\nThis proposal requires that some of the changes listed in \"Types of change\" (specified below) should:\nBe proposed in a CIP. Go through additional process in addition to the usual CIP process.\nBe proposed in a CIP.\nGo through additional process in addition to the usual CIP process.\nThe additional process mostly takes the form of additional information that should be present in the CIP before it moves to particular stages.\nThe requirement to propose a change via a CIP is, as all CIPs are, advisory. In exceptional circumstances or where swift action is required, we expect that changes may still be made without following this process. In such circumstances, a retrospective CIP SHOULD be made after the fact to record the changes and the rationale for them.\nChanges that require a CIP do not have to each be in an individual CIP, they can be included in batches or in other CIPs. So, for example, a single CIP could propose multiple new builtin functions, or a CIP proposing a change to the ledger could also propose a change to the ledger-script interface.\nAll changes that require a CIP SHOULD adhere to the following generic process.\nIn order to move to Proposed status:\nThe Specification MUST include: The type of change that is being proposed. For changes to Plutus Core itself, a formal specification of the changes which is precise enough to update the Plutus Core specification from.\nThe type of change that is being proposed.\nFor changes to Plutus Core itself, a formal specification of the changes which is precise enough to update the Plutus Core specification from.\nThe Acceptance Criteria MUST include: The external implementations are available. The plutus repository is updated with the specification of the proposal. The plutus repository is updated with an implementation of the proposal.\nThe external implementations are available.\nThe plutus repository is updated with the specification of the proposal.\nplutus\nThe plutus repository is updated with an implementation of the proposal.\nplutus\nThe Implementation Plan MUST include: The type of release that the change requires.\nThe type of release that the change requires.\nProposals for additions to the set of Plutus Core builtins SHOULD be proposed in a CIP and SHOULD adhere to the following additional process.\nIn order to move to Proposed status:\nThe Specification MUST include: Names and types/kinds for the new functions or types. A source for the implementation (e.g. a library which can be linked against); or a generic description of the functionality which is implementable in any programming language. For new types A description of how constants of this type will be serialized and deserialized. A precise description of the measure used for the size of a value of that type. For new builtin functions: a costing function for the builtin function.\nNames and types/kinds for the new functions or types.\nA source for the implementation (e.g. a library which can be linked against); or a generic description of the functionality which is implementable in any programming language.\nFor new types A description of how constants of this type will be serialized and deserialized. A precise description of the measure used for the size of a value of that type.\nA description of how constants of this type will be serialized and deserialized.\nA precise description of the measure used for the size of a value of that type.\nFor new builtin functions: a costing function for the builtin function.\nThe Rationale MUST include: If an external implementation is provided: an argument that it satisfies the following non-exhaustive list of criteria: It is trustworthy It always terminates It (or its Haskell bindings) never throw any exceptions Its behaviour is predictable (e.g. does not have worst-case behaviour with much worse performance) Discussion of how any measures and costing functions were determined.\nIf an external implementation is provided: an argument that it satisfies the following non-exhaustive list of criteria: It is trustworthy It always terminates It (or its Haskell bindings) never throw any exceptions Its behaviour is predictable (e.g. does not have worst-case behaviour with much worse performance)\nIt is trustworthy\nIt always terminates\nIt (or its Haskell bindings) never throw any exceptions\nIts behaviour is predictable (e.g. does not have worst-case behaviour with much worse performance)\nDiscussion of how any measures and costing functions were determined.\nThe Acceptance Criteria MUST include: The ledger is updated to include new protocol parameters to control costing of the new builtins.\nThe ledger is updated to include new protocol parameters to control costing of the new builtins.\nThe Rationale of a CIP should always be a clear argument for why the CIP should be adopted. In this case we recommend including:\nAn argument for the utility of the new builtins.\nExamples of real-world use cases where the new additions would be useful.\nIf feasible, a comparison with an implementation using the existing features, and an argument why the builtin is preferable (e.g. better performance).\nProtocol parameter updates that affect Plutus Core should be proposed in the Ledger category and following its processes. The only additional process required is review by the Plutus reviewers.\nThis CIP does not require any process for proposing performance changes.\nA \"bug fix\" is a change to behaviour where:\nThe implemented behaviour does not match the specification; or\nThe specified behaviour is clearly wrong (in the judgement of relevant experts)\nIn this case the fix may be submitted directly to the plutus repository and is NOT required to go through the CIP process. It must still be released as appropriate. For example, if a bug fix changes behaviour, it will have to wait for a new Plutus Core ledger language.\nplutus\nProposals for other additions, removals, or changes to behaviour of any part of Plutus Core or its builtins SHOULD be proposed in a CIP.\nNot being able to make removals or changes to behaviour without a LL is quite painful. For example, it means that we cannot just fix bugs in the semantics: we must remain bug-for-bug compatible with any given LL.\nIt is tempting to think that if we can show that a particular behaviour has never been used in the history of the chain, then changing it is backwards-compatible, since it won t change the validation of any of the actual chain. However, this is sadly untrue.\nThe behaviour could be triggered (potentially deliberately) in the interval between the update proposal being accepted and it being implemented. This is extremely dangerous and could lead to an un-managed hard fork. The behaviour could be triggered in a script which has not yet been executed on the chain, but whose hash is used to lock an output. This could lead to that output being unexpectedly un-spendable, or some other change in behaviour. Moreover, since we only have the hash of the script, we have no way of telling whether this is the case.\nThe behaviour could be triggered (potentially deliberately) in the interval between the update proposal being accepted and it being implemented. This is extremely dangerous and could lead to an un-managed hard fork.\nThe behaviour could be triggered in a script which has not yet been executed on the chain, but whose hash is used to lock an output. This could lead to that output being unexpectedly un-spendable, or some other change in behaviour. Moreover, since we only have the hash of the script, we have no way of telling whether this is the case.\nSo even if a behaviour is obscure, we cannot just change it.\nChanging the binary format in a backwards compatible way may mean that binary scripts which previously would have been invalid might now deserialize correctly into a program.\nThere is a worry here: scripts which fail execution (a phase 2 validation failure) actually get posted on the chain as failures. We must be careful not to turn any such failures into successes, otherwise we could break history.\nHowever, we do not need to worry in this case, since checking that a script deserializes properly is a phase 1 validation check, so no scripts will be posted as failures due to failing to deserialize, so we cannot break any such postings by making deserialization more lenient.\nPerformance changes must be carefully managed in order to avoid the possibility of an accidental hard fork.\nConsider an example of improving performance. The interpreter gets 50 faster (real performance), which is released as a software update. Now, we want to lower the cost model parameters (model performance) so that users will be charged fewer resources for their scripts.\nHowever, the parameter change means that scripts which previously would have exceeded the transaction/block limit become acceptable. The parameter change is not itself a hard fork, because all the nodes accept the new parameters by consensus. But if the model performance tracks real performance well, then nodes which have not adopted the software update may have issues with the real performance of these newly allowed scripts! In the worst case, they might suffer resource exhaustion, preventing them from following the new chain, which is tantamount to a hard fork.\nThis is a relatively unlikely scenario, since it requires a situation where nodes are close enough to their resource limits that an (effective) regression in real performance of scripts can push them over the edge. Nonetheless, the cautious approach is to not perform such parameter updates until we are sure that all nodes are using a version with the required software update, i.e. after the protocol version has been bumped in a hard fork.\nConversely, if (for some reason) we needed to regress the real performance of the interpreter, we should only do this after all the nodes have accepted a regression in model performance (increasing the cost model parameters).\nBuiltin functions in Plutus Core are implemented via Haskell functions. Often these implementations come from somewhere else, e.g. a cryptography library written in C.\nIt is vitally important that these libraries are trustworthy. The Plutus Core package (and hence its dependencies) are linked into the Cardano node. A buffer overrun vulnerability in the implementation of a builtin function could therefore become an attack on a node.\nWe expect additions to builtins to be particularly common, and to have lots of interest from the community.\nHowever, the process of adding new builtins is not totally straightforward, due in particular to the need to find a good implementation and to cost it. Surfacing these difficulties quickly is a key goal of this process.\nFinally, builtins are a comparatively structured extension point for the language. In comparison, proposals for changes to Plutus Core itself are likely to be much more heterogeneous.\nLedger languages incur a large maintenance cost. Each one must continue to work, perfectly, in perpetuity. Furthermore, they may need their own, independent set of cost model protocol parameters, etc.\nSo it is very desirable to keep the number of ledger languages down. The simplest way to do this is to batch changes, and only release a new ledger language occasionally.\nThis CIP requires the acceptance of the Plutus team, which it has in virtue of its authorship.\nNo implementation is required.\nThis CIP is licensed under CC-BY-4.0.\nSee \"Are backwards-compatible binary format changes really safe?\". The Plutus Core family of ledger languages are sometimes referred to as the Plutus Core ledger language versions, and named as such (\"PlutusV1\", \"PlutusV2\" etc.) although they actually entirely distinct languages from the perspective of the ledger. In this document we will use the more precise language and refer to them just as distinct ledger languages. The binary format change is backwards compatible unless it breaches the limit of how many builtin functions or types can be encoded, in which case that must be changed, forcing a new LL. See \"Why do performance changes require extra steps?\". See \"Why do performance changes require extra steps?\".\nSee \"Are backwards-compatible binary format changes really safe?\".\nSee \"Are backwards-compatible binary format changes really safe?\".\nThe Plutus Core family of ledger languages are sometimes referred to as the Plutus Core ledger language versions, and named as such (\"PlutusV1\", \"PlutusV2\" etc.) although they actually entirely distinct languages from the perspective of the ledger. In this document we will use the more precise language and refer to them just as distinct ledger languages.\nThe Plutus Core family of ledger languages are sometimes referred to as the Plutus Core ledger language versions, and named as such (\"PlutusV1\", \"PlutusV2\" etc.) although they actually entirely distinct languages from the perspective of the ledger. In this document we will use the more precise language and refer to them just as distinct ledger languages.\nThe binary format change is backwards compatible unless it breaches the limit of how many builtin functions or types can be encoded, in which case that must be changed, forcing a new LL.\nThe binary format change is backwards compatible unless it breaches the limit of how many builtin functions or types can be encoded, in which case that must be changed, forcing a new LL.\nSee \"Why do performance changes require extra steps?\".\nSee \"Why do performance changes require extra steps?\".\nSee \"Why do performance changes require extra steps?\".\nSee \"Why do performance changes require extra steps?\".\n2023 Cardano Foundation\n\n---\n\nCIP-0036 | Catalyst Registration Transaction Metadata Format (Updated)\n\nCardano uses a sidechain for its treasury system. One needs to \"register\" to participate on this sidechain by submitting a registration transaction on the mainnet chain. This CIP details the registration transaction format. This is a revised version of the original CIP-15 | Registration Transaction Metadata Format.\nCardano uses a sidechain for its treasury system (\"Catalyst\") and for other voting purposes. One of the desirable properties of this sidechain is that even if its safety is compromised, it doesn't cause loss of funds on the main Cardano chain. To achieve this, instead of using your wallet's recovery phrase on the sidechain, we need to use a brand new \"voting key\".\nHowever, since 1 ADA = 1 vote, a user needs to associate their mainnet ADA to their new voting key. This can be achieved through a registration transaction.\nIn addition, to encourage participation by a broader range of ADA holders, it should be possible to delegate one's rights to vote to (possibly multiple) representatives and/or expert voters. Such delegations will still be able to receive Catalyst rewards.\nWe therefore need a registration transaction that serves three purposes:\nRegisters a \"voting key\" to be included in the sidechain and/or delegates to existing \"voting key\"s Associates mainnet ADA to this voting key(s) Declares a payment address to receive Catalyst rewards\nRegisters a \"voting key\" to be included in the sidechain and/or delegates to existing \"voting key\"s\nAssociates mainnet ADA to this voting key(s)\nDeclares a payment address to receive Catalyst rewards\nNote: This schema does not attempt to differentiate delegations from direct registrations, as the two options have exactly the same format. It also does not distinguish between delegations that are made as \"private\" arrangements (proxy votes) from those that are made by delegating to representatives who promote themselves publicly. Distinguishing these possibilities is left to upper layers or future revisions of this standard, if required. In this document, we will use the term 'delegations' to refer to all these possibilities.\nA registration transaction is a regular Cardano transaction with a specific transaction metadata associated with it.\nNotably, there should be five entries inside the metadata map:\nA non-empty array of delegations, as described below;\nA stake address for the network that this transaction is submitted to (to point to the Ada that is being delegated);\nA Shelley payment address (see CIP-0019) discriminated for the same network this transaction is submitted to, to receive rewards.\nA nonce that identifies that most recent delegation\nA non-negative integer that indicates the purpose of the vote. This is an optional field to allow for compatibility with CIP-15. For now, we define 0 as the value to use for Catalyst, and leave others for future use. A new registration should not invalidate a previous one with a different voting purpose value.\nA delegation assigns (a portion of) the ADA controlled by one or more UTxOs on mainnet to the voting key in the sidechain as voting power. The UTxOs can be identified via the stake address at some designated point in time.\nEach delegation therefore contains:\na voting key: simply an ED25519 public key. This is the spending credential in the sidechain that will receive voting power from this delegation. For direct voting it's necessary to have the corresponding private key to cast votes in the sidechain. How this key is created is up to the wallet.\nthe weight that is associated with this key: this is a 4-byte unsigned integer (CBOR major type 0, The weight may range from 0 to 2 32-1) that represents the relative weight of this delegation over the total weight of all delegations in the same registration transaction.\nThe terms \"(CIP-36) voting keys\" and \"(CIP-36) vote keys\" should be used interchangeably to indicate the keys described in this specification. But it should be made clear that implementations should favour the term \"(CIP-36) vote key\" and that the association of both terms aims to reduce the possibility of confusion.\nThe term governance should not be associated with these keys nor should these keys be associated with other vote or voting keys used in the ecosystem. When discussing these keys in a wider context they should be specified by such terminology as \"CIP-36 vote keys\" or \"CIP-36 style vote keys\".\nTo avoid linking voting keys directly with Cardano spending keys, the voting key derivation path must start with a specific segment:\nm / 1694' / 1815' / account' / chain / address_index\nm / 1694' / 1815' / account' / chain / address_index\nWe recommend that implementors only use address_index=0 to avoid the need for voting key discovery.\naddress_index=0\nSupporting tooling should clearly define and differentiate this as a unique key type, describing such keys as \"CIP-36 vote keys\". When utilizing Bech32 encoding the appropriate cvote_sk and cvote_vk prefixes should be used as described in CIP-05.\ncvote_sk\ncvote_vk\nExamples of acceptable keyTypes for supporting tools:\nkeyType\nkeyType\nCIP36VoteSigningKey_ed25519\nCIP36VoteExtendedSigningKey_ed25519\nCIP36VoteVerificationKey_ed25519\nCIP36VoteExtendedVerificationKey_ed25519\nFor hardware implementations:\nkeyType\nCIP36VoteVerificationKey_ed25519\nCIP36VoteHWSigningFile_ed25519\nThe intention with this design is that if projects beyond Catalyst implement this specification they are able to add to themselves keyType Descriptions.\nkeyType\n61284: { // delegations - CBOR byte array 1: [[\"0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663\", 1], [\"0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee\", 3]], // stake_pub - CBOR byte array 2: \"0xad4b948699193634a39dd56f779a2951a24779ad52aa7916f6912b8ec4702cee\", // payment_address - CBOR byte array 3: \"0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee47b60edc7772855324c85033c638364214cbfc6627889f81c4\", // nonce 4: 5479467 // voting_purpose: 0 = Catalyst 5: 0 }\n61284: { // delegations - CBOR byte array 1: [[\"0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663\", 1], [\"0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee\", 3]], // stake_pub - CBOR byte array 2: \"0xad4b948699193634a39dd56f779a2951a24779ad52aa7916f6912b8ec4702cee\", // payment_address - CBOR byte array 3: \"0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee47b60edc7772855324c85033c638364214cbfc6627889f81c4\", // nonce 4: 5479467 // voting_purpose: 0 = Catalyst 5: 0 }\nThe entries under keys 1, 2, 3, 4 and 5 represent the Catalyst delegation array, the staking credential on the Cardano network, the payment address to receive rewards, a nonce, and a voting purpose, respectively. A registration with these metadata will be considered valid if the following conditions hold:\nThe nonce is an unsigned integer (of CBOR major type 0) that should be monotonically rising across all transactions with the same staking key. The advised way to construct a nonce is to use the current slot number. This is a simple way to keep the nonce increasing without having to access the previous transaction data.\nThe payment address is a Shelley payment address discriminated for the same network this transaction is submitted to.\nThe delegation array is not empty\nThe weights in the delegation array are not all zero\nDelegation to the voting key 0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663 will have relative weight 1 and delegation to the voting key 0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee relative weight 3 (for a total weight of 4). Such a registration will assign 1/4 and 3/4 of the value in ADA to those keys respectively, with any remainder assigned to the second key.\n0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663\n0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee\nThe registration witness depends on the type of stake credential used. To produce the witness field in case of a staking public key, the CBOR representation of a map containing a single entry with key 61284 and the registration metadata map in the format above is formed, designated here as sign_data. This data is signed with the staking key as follows: first, the blake2b-256 hash of sign_data is obtained. This hash is then signed using the Ed25519 signature algorithm. The witness metadata entry is added to the transaction under key 61285 as a CBOR map with a single entry that consists of the integer key 1 and signature as obtained above as the byte array value.\nsign_data\nsign_data\n61285: { // witness - ED25119 signature CBOR byte array 1: \"0x8b508822ac89bacb1f9c3a3ef0dc62fd72a0bd3849e2381b17272b68a8f52ea8240dcc855f2264db29a8512bfcd522ab69b982cb011e5f43d0154e72f505f007\" }\n61285: { // witness - ED25119 signature CBOR byte array 1: \"0x8b508822ac89bacb1f9c3a3ef0dc62fd72a0bd3849e2381b17272b68a8f52ea8240dcc855f2264db29a8512bfcd522ab69b982cb011e5f43d0154e72f505f007\" }\nThis deregistration format is currently only specified for Catalyst (vote_purpose=0), other voting chain purposes may handle a deregistration in a different way. There was a discussion before if an empty delegation array could also be used to fulfil a deregistration. This idea was cancelled, because it would currently require additional resources in the Hardware-Wallets state machine to do additional checks about an empty array. So the decision was made to leave the registration part untouched and only add the deregistration via the unused key 61286. Wallets/Tools are not forced to support this deregistration method.\nDefinition:\nA deregistration removes all the voting power (associated stake amount) for the provided stake credential from the delegated vote-public-keys.\nA deregistration resets the state of the stake credential on the voting chain like they were never registered before.\nA deregistration transaction is a regular Cardano transaction with a specific transaction metadata associated with it.\nNotably, there should be three entries inside the metadata map (key 61286):\nThe public key of the stake signing key\nA nonce that identifies that most recent deregistration.\nA non-negative integer that indicates the purpose of the vote. For now, we define 0 as the value to use for Catalyst, and leave others for future use.\nBe aware, the deregistration metadata key is 61286, and not 61284 like it is used for a registration! The registraton metadata format and specification is independent from the deregistration one, and may not be supported by all wallets/tools.\n{ 61286: { // stake_pub - CBOR byte array 1: \"0x57758911253f6b31df2a87c10eb08a2c9b8450768cb8dd0d378d93f7c2e220f0\", // nonce 2: 74412400, // voting_purpose: 0 = Catalyst 3: 0 }, 61285: { // witness - ED25119 signature CBOR byte array 1: \"0xadb7c90955c348e432545276798478f02ee7c2be61fd44d22f9de22131d9bcf0b23eb413766b74b9e7ba740e71266467a5d35363411346972db9e7b710b00603\" } }\n{ 61286: { // stake_pub - CBOR byte array 1: \"0x57758911253f6b31df2a87c10eb08a2c9b8450768cb8dd0d378d93f7c2e220f0\", // nonce 2: 74412400, // voting_purpose: 0 = Catalyst 3: 0 }, 61285: { // witness - ED25119 signature CBOR byte array 1: \"0xadb7c90955c348e432545276798478f02ee7c2be61fd44d22f9de22131d9bcf0b23eb413766b74b9e7ba740e71266467a5d35363411346972db9e7b710b00603\" } }\nCBOR-Hex: A219EF66A301582057758911253F6B31DF2A87C10EB08A2C9B8450768CB8DD0D378D93F7C2E220F0021A046F7170030019EF65A1015840ADB7C90955C348E432545276798478F02EE7C2BE61FD44D22F9DE22131D9BCF0B23EB413766B74B9E7BA740E71266467A5D35363411346972DB9E7B710B00603\nA219EF66A301582057758911253F6B31DF2A87C10EB08A2C9B8450768CB8DD0D378D93F7C2E220F0021A046F7170030019EF65A1015840ADB7C90955C348E432545276798478F02EE7C2BE61FD44D22F9DE22131D9BCF0B23EB413766B74B9E7BA740E71266467A5D35363411346972DB9E7B710B00603\nThe entries under keys 1, 2 and 3 represent the staking credential on the Cardano network, a nonce, and a voting purpose, respectively. A deregistration with these metadata will be considered valid if the following conditions hold:\nThe stake credentials is a stake public-key byte array (of CBOR major type 2)\nThe nonce is an unsigned integer (of CBOR major type 0) that should be monotonically rising across all transactions with the same staking key. The advised way to construct a nonce is to use the current slot number. This is a simple way to keep the nonce increasing without having to access the previous transaction data.\nThe voting_purpose is an unsigned integer (of CBOR major type 0)\nTo produce the witness field in case of a staking public key, the CBOR representation of a map containing a single entry with key 61286 and the deregistration metadata map in the format above is formed, designated here as sign_data. This data is signed with the staking key as follows: first, the blake2b-256 hash of sign_data is obtained. This hash is then signed using the Ed25519 signature algorithm. The witness metadata entry is added to the transaction under key 61285 as a CBOR map with a single entry that consists of the integer key 1 and signature as obtained above as the byte array value.\nsign_data\nsign_data\nSee the schema file.\nSee test vector file.\nThis method has been used since Fund 2. For future fund iterations, a new method making use of time-lock scripts may be introduced as described below.\nRecall: Cardano uses the UTXO model so to completely associate a wallet's balance with a voting key (i.e. including enterprise addresses), we would need to associate every payment key to a voting key individually. Although there are attempts at this (see [CIP-0008]), the resulting data structure is a little excessive for on-chain metadata (which we want to keep small)\nGiven the above, we choose to associate staking credentials with voting keys. At the moment, the only supported staking credential is a staking key. Since most Cardano wallets only use base addresses for Shelley wallet types, in most cases this should perfectly match the user's wallet.\nThe voting power that is associated with each delegated voting key is derived from the user's total voting power as follows.\nThe total weight is calculated as a sum of all the weights; The user's total voting power is calculated as a whole number of ADA (rounded down); The voting power associated with each voting key in the delegation array is calculated as the weighted fraction of the total voting power (rounded down); Any remaining voting power is assigned to the last voting key in the delegation array.\nThe total weight is calculated as a sum of all the weights;\nThe user's total voting power is calculated as a whole number of ADA (rounded down);\nThe voting power associated with each voting key in the delegation array is calculated as the weighted fraction of the total voting power (rounded down);\nAny remaining voting power is assigned to the last voting key in the delegation array.\nThis ensures that the voter's total voting power is never accidentally reduced through poor choices of weights, and that all voting powers are exact ADA.\nA future change of the Catalyst system may make use of a time-lock script to commit ADA on the mainnet for the duration of a voting period. The voter registration metadata in this method will not need an association with a staking credential. Therefore, the staking_credential map entry and the registration_witness payload with key 61285 will no longer be required.\nstaking_credential\nregistration_witness\nFund 3 added the reward_address inside the key_registration field.\nreward_address\nkey_registration\nFund 4:\nadded the nonce field to prevent replay attacks;\nnonce\nchanged the signature algorithm from one that signed sign_data directly to signing the Blake2b hash of sign_data to accommodate memory-constrained hardware wallet devices.\nsign_data\nsign_data\nIt was planned that since Fund 4, registration_signature and the staking_pub_key entry inside the key_registration field will be deprecated. This has been deferred to a future revision of the protocol.\nregistration_signature\nstaking_pub_key\nkey_registration\nFund 8:\nrenamed the voting_key field to delegations and add support for splitting voting power across multiple vote keys.\nvoting_key\ndelegations\nadded the voting_purpose field to limit the scope of the delegations.\nvoting_purpose\nrename the staking_pub_key field to stake_credential and registration_signature to registration_witness to allow for future credentials additions.\nstaking_pub_key\nstake_credential\nregistration_signature\nregistration_witness\nFund 10:\nReplaced the reward_address field with payment_address field, keeping it at index 3. Stipulating that payment_address must be a Shelley payment address, otherwise voting reward payments will not be received. Note: up to Catalyst's Fund 9, voting rewards were paid via MIR transfer to a stake address provided within the reward_address field. From Fund 10 onwards, a regular payment address must be provided in the payment_address field to receive voting rewards. This allows Catalyst to avoid MIR transfers and instead pay voting rewards via regular transactions.\nreward_address\npayment_address\npayment_address\nNote: up to Catalyst's Fund 9, voting rewards were paid via MIR transfer to a stake address provided within the reward_address field. From Fund 10 onwards, a regular payment address must be provided in the payment_address field to receive voting rewards. This allows Catalyst to avoid MIR transfers and instead pay voting rewards via regular transactions.\nreward_address\npayment_address\nFund 11:\nadded the deregistration metadata format.\nderegistration\nHave this registration format supported in Catalyst infrastructure for two Catalyst funds.\nHave this registration format supported by three wallets/tools. cardano-signer\ncardano-signer\nAuthors to provide test vector file. See the schema file.\nSee the schema file.\nAuthors to provide CDDL schema file. See test vector file.\nSee test vector file.\nAuthors to test this format for security and compatibility with existing Catalyst infrastructure.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0037 | Dynamic Saturation Based on Pledge\n\nThe pledge should be used to calculate the saturation point of a pool: setting a maximum delegation proportional to pledge.\nCurrently Cardano has been plagued with an ever increasing amount of single entity Stake Pool Operators (SPO) creating multiple pools. The pools that are known to be operated by single entity SPOs account for just 18.72 of the total stake and 50 of the total stake can be attributed to at least 23 single entities (as of 3rd Dec 2021).\nThe vision of Cardano is that it everyone should be able to have the opportunity to be able to run a stake pool regardless of their financial capabilities and this is even more important for developing countries.\nThe issue we're currently facing is that many SPOs have been able to exploit a loophole that has allowed them to use their influence to create many sub-pools without any restrictions. As the size of their pools grow, instead of honouring the saturation limits already in place, they are bypassing them by creating more sub-pools. From a technical point, it is theoretically possible for a single entity to run enough stake pools to control more then 50 of the total stake.\nWhile unlikely, the system does not take into account external influences (eg. political) that could harm the system. Shelly has been active for less then 2 years and we are already seeing the risks on a small scale. As real world adoption continues, there could be situations in the next decade that could jeopardise the decentralisation of Cardano. One example could be that a political party could run a ISPO (Initial Stake Pool Offering) on a mass scale. Seeing as Cardano has the potential to be used for voting (even at a political level), the integrity of Cardano could be questioned if political parties controlled stake pools with large shares of total stake.\nThe pledge used in a pool was meant to show how serious a SPO is and how much \"skin in the game\" they had. The idea was that if they have more pledge, they have more to lose if something goes wrong. This seems to have lost it's meaning as SPOs that already have a dominating position have created many sub-pools. Some have split up their pledge evenly, some have next to no pledge and have gained high amounts of stake through influential means. This has not only reduced the security of Cardano, but has also lost the meaning of having pledge in the pool.\nFor example, a SPO might have a pledge of 30,000 ADA across 10 pools, while another SPO might have 1,000,000 pledge but only has 1 pool with a small amount of active stake. Seeing as the SPO with 1M pledge has more overall, they have more to lose and should be trusted more. Since there is no technical advantage to having a high pledge, the meaning and purpose of a pledge is redundant.\nTo make the pledge a meaningful metric that is fair to all SPOs and aligns with the core values of Cardano, the pledge should be used to calculate the saturation point of a pool. This will mean that SPOs, no matter how many pools they operate, will have a maximum saturation based on their total pledge. For example, if a pool operates a single pool and wanted to open up another pool with the same amount of stake, they will have to assign the equal amount of pledge against that new pool. If a pool wishes to up their saturation point, they will need to assign a higher amount of pledge.\nThis proposal has had active discussions for over 6 months and is now waiting for a review from IOG to provide feedback on the feasibility and soundness of the approach.\nThis CIP should be considered a medium priority as it directly impacts the health and growth of the Cardano ecosystem. Large Cardano pools have had several years to take advantage of the lack of oversight and have control of a large portion of mining operations. This continued lack of restrictions will further damage the trust and reliability of the framework. Timelines around the implementation of the CIP will depend on urgency, however its implementation should be trivial as there are no new parameters required or risks involved. Further this CIP from a high-level aspect only requires an update to the existing algorithm. From an external context, the CIP will require trivial updates as it should be self-contained in cardano-node.\nThis CIP should be considered a medium priority as it directly impacts the health and growth of the Cardano ecosystem. Large Cardano pools have had several years to take advantage of the lack of oversight and have control of a large portion of mining operations. This continued lack of restrictions will further damage the trust and reliability of the framework.\nThis CIP should be considered a medium priority as it directly impacts the health and growth of the Cardano ecosystem. Large Cardano pools have had several years to take advantage of the lack of oversight and have control of a large portion of mining operations. This continued lack of restrictions will further damage the trust and reliability of the framework.\nTimelines around the implementation of the CIP will depend on urgency, however its implementation should be trivial as there are no new parameters required or risks involved. Further this CIP from a high-level aspect only requires an update to the existing algorithm. From an external context, the CIP will require trivial updates as it should be self-contained in cardano-node.\nTimelines around the implementation of the CIP will depend on urgency, however its implementation should be trivial as there are no new parameters required or risks involved. Further this CIP from a high-level aspect only requires an update to the existing algorithm. From an external context, the CIP will require trivial updates as it should be self-contained in cardano-node.\nFor this to be able to work, there firstly needs to be an upper limit and a lower limit. The K parameter can still be used as the upper saturation limit of a single pool. As in, if a SPO has enough pledge assigned to a single pool, that pool will be able to run at the maximum saturation point of K. The lower limit is in place to safe guard small SPOs and allow them to grow.\nAn example of how Dynamic Saturation would be calculated:\n500,000 ADA Pledged = Saturation point at 100 of K\n250,000 ADA Pledged = Saturation point at 50 of K\n125,000 ADA Pledged = Saturation point at 25 of K\n62,500 ADA Pledged = Saturation point at 12.5 of K\nTo not penalise small pools, there should be a lower limit saturation point, such as 10 of K. Based on this, a pool with a pledge of 50,000 has the same saturation point as a pool with 25,000 pledge, both being 10 of K. This will allow smaller pools to still have some growth potential.\nThe only way a pool can receive more stake across all their pools without impacting their rewards is to increase their total pledge.\nExample with SPO of 1,000,000 Pledge with current k implementation:\nExample with SPO of 1,000,000 Pledge with Dynamic Saturation:\nAs we can see above with the current implementation of K, a pool owner can split the pool and double their delegators active stake (or however many times they split).\nThe Dynamic Saturation method caps the pools so that the amount of total stake across all pools is the same no matter how much they split up their pledge and the only benefit will be the extra fee collected (340 ADA) per pool.\nThis will mean that the saturation metric will have a direct corelation to an SPOs total pledge.\nAfter some discussions among the community and some help from https://github.com/cffls, the below code example has been proposed for how the dynamic saturation could work.\nlet lovelace = 1000000; function calc_sat(pledge){ k = 500; e = 0.2; l = 125; total_supply = 33719282563 * lovelace; orig_sat = total_supply / k; new_sat = orig_sat * Math.max(e, min(1 / k, pledge / orig_sat * l)); final_sat = max(new_sat, orig_sat); console.log(`pledge: ${pledge / lovelace}, sat: ${final_sat / lovelace}`); } function max(val1, val2){ if(val1 < val2){ return val1; } return val2; } function min(val1, val2){ if(val1 > val2){ return val1; } return val2; } calc_sat(50000 * lovelace); calc_sat(100000 * lovelace); calc_sat(150000 * lovelace); calc_sat(250000 * lovelace); calc_sat(500000 * lovelace); calc_sat(750000 * lovelace); calc_sat(1000000 * lovelace); calc_sat(2000000 * lovelace);\nlet lovelace = 1000000; function calc_sat(pledge){ k = 500; e = 0.2; l = 125; total_supply = 33719282563 * lovelace; orig_sat = total_supply / k; new_sat = orig_sat * Math.max(e, min(1 / k, pledge / orig_sat * l)); final_sat = max(new_sat, orig_sat); console.log(`pledge: ${pledge / lovelace}, sat: ${final_sat / lovelace}`); } function max(val1, val2){ if(val1 < val2){ return val1; } return val2; } function min(val1, val2){ if(val1 > val2){ return val1; } return val2; } calc_sat(50000 * lovelace); calc_sat(100000 * lovelace); calc_sat(150000 * lovelace); calc_sat(250000 * lovelace); calc_sat(500000 * lovelace); calc_sat(750000 * lovelace); calc_sat(1000000 * lovelace); calc_sat(2000000 * lovelace);\nResults:\n[Log] pledge: 50000, sat: 13487713.0252 [Log] pledge: 100000, sat: 13487713.0252 [Log] pledge: 150000, sat: 18750000 [Log] pledge: 250000, sat: 31250000 [Log] pledge: 500000, sat: 62500000 [Log] pledge: 750000, sat: 67438565.126 [Log] pledge: 1000000, sat: 67438565.126 [Log] pledge: 2000000, sat: 67438565.126\n[Log] pledge: 50000, sat: 13487713.0252 [Log] pledge: 100000, sat: 13487713.0252 [Log] pledge: 150000, sat: 18750000 [Log] pledge: 250000, sat: 31250000 [Log] pledge: 500000, sat: 62500000 [Log] pledge: 750000, sat: 67438565.126 [Log] pledge: 1000000, sat: 67438565.126 [Log] pledge: 2000000, sat: 67438565.126\nSince a single entity SPO only has a certain amount of ADA they can pledge, they will eventually hit their saturation point no matter how many pools they create. The only way they can add more delegators is to increase their pledge. Once they run out of pledge and reach their saturation point, the delegators will have no choice but to move to another SPO and increase decentralisation.\nIn the above example, the base pledge of 500,000 ADA should be set as a parameter that can be adjusted in the future. E.g, if it is found that it is too low or too high to gain 100 saturation, it can be adjusted in the same way k can be adjusted.\nOne of the questions raised by the community was, will the lower limit stop the growth of small pools if it is set at a level where they can't reach the expected annual 5 return on ADA. This case could be handled a few ways, but the main aim would be to keep it at a sustainable amount for small pools.\nThe value is a percentage of k, such as 10 . This percentage could increase as needed, such as to 15 of k. The value could be calculated based on the average of active stake compared to active pools. E.g, active stake = 23837 M / 3000 = saturation point of 7.94 M ADA\nThe value is a percentage of k, such as 10 . This percentage could increase as needed, such as to 15 of k.\nThe value could be calculated based on the average of active stake compared to active pools. E.g, active stake = 23837 M / 3000 = saturation point of 7.94 M ADA\nThe new relationship between pledge and saturation defined here is implemented in the Ledger and enacted through a hard-fork.\nAgreement by the Ledger team as defined in CIP-0084 under Expectations for ledger CIPs including \"expert opinion\" on changes to rewards & incentives.\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0040 | Collateral Output\n\nThis document describes adding a new output type to transactions called Collateral Outputs.\nAs of Alonzo, transactions that call Plutus smart contracts are required to put up collateral to cover the potential cost of smart contract execution failure. Inputs used as collateral have the following properties:\nCannot contain any tokens (only ADA) Cannot be a script address Must be a UTXO input Must be at least some percentage of the fee in the tx (concrete percentage decided by a protocol parameter) Can be the same UTXO entry as used in non-collateral tx input Is consumed entirely (no change) if the contract execution fails during phase 2 validation Is not consumed if phase phase 2 validation succeeds\nCannot contain any tokens (only ADA)\nCannot be a script address\nMust be a UTXO input\nMust be at least some percentage of the fee in the tx (concrete percentage decided by a protocol parameter)\nCan be the same UTXO entry as used in non-collateral tx input\nIs consumed entirely (no change) if the contract execution fails during phase 2 validation\nIs not consumed if phase phase 2 validation succeeds\nAdditionally, there cannot be more than maxColInputs (protocol parameter) inputs and the inputs have to cover a percentage of the fee defined by collateralPercent (protocol parameter)\nHowever,\nRestriction #1 is problematic because hardcore dApp users rarely have UTXO entries that do not contain any tokens. To combat this, wallets have created a special wallet-dependent \"collateral\" UTXO to reserve for usage of collateral for dApps which is not a great UX.\nRestriction #6 is problematic because wallets want to protect users from signing transactions with large collateral as they cannot verify whether or not the transaction will fail when submitted (especially true for hardware wallets)\nIf phrase-2 verification fails, we can send outputs to a special output marked as the collateral output.\nThere are two ways to create collateral outputs\nAdd collateral outputs as a new field inside the transaction. This change is similar to how collateral inputs were created a new field Change the definition of outputs as TxOut = Addr Value DataHash? Source? where source (optional for backwards compatibility) is an enum 0 = regular output, 1 = collateral output.\nAdd collateral outputs as a new field inside the transaction. This change is similar to how collateral inputs were created a new field\nChange the definition of outputs as TxOut = Addr Value DataHash? Source? where source (optional for backwards compatibility) is an enum 0 = regular output, 1 = collateral output.\nTxOut = Addr × Value × DataHash? × Source?\n0 = regular output, 1 = collateral output\nOption #1 provides the best backwards compatibility because we don't expect phase-2 validation to be a common occurrence and so wallets that (due to not being updated) never check collateral outputs will still in the overwhelming majority of cases return the correct result.\nAdditionally, this requires updating the collateral requirement.\nIf no collateral output is specified (and therefore no tokens are in the collateral input), then we keep the old definition\nubalance (collateral txb ◁ utxo) ≥ quot (txfee txb * (collateralPercent pp)) 100\nubalance (collateral txb ◁ utxo) ≥ quot (txfee txb * (collateralPercent pp)) 100\nHowever, if collateral output is specified, then\nEach collateral output needs to satisfy the same minimum ADA requirement as regular outputs Collateral output needs to be balanced according to sum(collateral_input) = sum(collateral_output) + collateral_consumed Where collateral_consumed is equal to the old formula (quot (txfee txb * (collateralPercent pp)) 100). Note that when collateral is consumed, any certificate, etc. in the transaction is ignored so they have no impact on the change calculation.\nEach collateral output needs to satisfy the same minimum ADA requirement as regular outputs\nCollateral output needs to be balanced according to sum(collateral_input) = sum(collateral_output) + collateral_consumed Where collateral_consumed is equal to the old formula (quot (txfee txb * (collateralPercent pp)) 100). Note that when collateral is consumed, any certificate, etc. in the transaction is ignored so they have no impact on the change calculation.\nsum(collateral_input) = sum(collateral_output) + collateral_consumed\ncollateral_consumed\nquot (txfee txb * (collateralPercent pp)) 100\nSome use-cases like hardware wallets, who do not have access to the content of the collateral inputs, cannot easily check if the collateral is balanced. Similar to how we specify an explicit fee as part of the transaction body to tackle this problem, the transaction body also needs a new field that explicitly specified how much collateral will be consumed in the case of phase-2 validation failure.\nFully implemented in Cardano as of the Vasil protocol upgrade.\nPasses all Ledger team requirements for desirability and feasibility.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0042 | New Plutus Builtin serialiseData\n\nThis document describes the addition of a new Plutus builtin for serialising BuiltinData to BuiltinByteString.\nBuiltinData\nBuiltinByteString\nAs part of developing on-chain script validators for the Hydra Head protocol, we stumble across a peculiar need for on-chain scripts: we need to verify and compare digests obtained from hashing elements of the script's surrounding transaction.\nIn this particular context, those elements are transaction outputs (a.k.a. TxOut). While Plutus already provides built-in for hashing data-structure (e.g. sha2_256 :: BuiltinByteString - BuiltinByteString), it does not provide generic ways of serialising some data type to BuiltinByteString.\nTxOut\nsha2_256 :: BuiltinByteString -> BuiltinByteString\nBuiltinByteString\nIn an attempt to pursue our work, we have implemented an on-chain library (plutus-cbor) for encoding data-types as structured CBOR / RFC 8949 in a relatively efficient way (although still quadratic, it is as efficient as it can be with Plutus' available built-ins) and measured the memory and CPU cost of encoding TxOut in a script validator on-chain.\nTxOut\nThe above graph shows the memory and CPU costs relative against a baseline, of encoding a TxOut using plutus-cbor in function of the number of assets present in that TxOut. The costs on the y-axis are relative to the maximum execution budgets (as per mainnet's parameters, December 2021) allowed for a single script execution. As can be seen, this is of linear complexity, i.e. O(n) in terms of the number of assets. These results can be reproduced using the encoding-cost executable in our repository.\nTxOut\nplutus-cbor\nTxOut\nNote that we have also calculated similar costs for ada-only TxOut, in function of the number of TxOut which is about twice as worse but of similar linear shape.\nTxOut\nTxOut\nWe we can see on the graph, the cost is manageable for a small number of assets (or equivalently, a small number of outputs) but rapidly becomes limiting. Ideally, we would prefer the transaction size to be the limiting factor when it comes to the number of outputs we can handle in a single validation.\nBesides, in our discussions with the Marlowe team, we also discovered that they shared a similar problem when it came to serialising merkleized ASTs.\nUnderneath it all, it seems that it would be beneficial to have a new built-in at our disposal to serialise any Plutus BuiltinData to BuiltinByteString such that validators could leverage more optimized implementations and bytestring builders via built-ins than what's available on-chain, hopefully reducing the overall memory and CPU costs.\nBuiltinData\nBuiltinByteString\nWe define a new Plutus built-in function with the following type signature:\nserialiseData :: BuiltinData -> BuiltinByteString\nserialiseData :: BuiltinData -> BuiltinByteString\nBehind the scene, we expect this function to use a well-known encoding format to ease construction of such serialisation off-chain (in particular, for non-Haskell off-chain contract codes). A natural choice of binary data format in this case is CBOR which is:\nEfficient; Relatively simple; Use pervasively across the Cardano ecosystem\nEfficient;\nRelatively simple;\nUse pervasively across the Cardano ecosystem\nFurthermore, the Plutus' ecosystem already provides a quite opinionated implementation of a CBOR encoder for built-in Data. For the sake of documenting it as part of this proposal, we provide here-below the CDDL specification of that existing implementation:\nData\nplutus_data = constr<plutus_data> / { * plutus_data => plutus_data } / [ * plutus_data ] / big_int / bounded_bytes constr<a> = #6.121([]) / #6.122([a]) / #6.123([a, a]) / #6.124([a, a, a]) / #6.125([a, a, a, a]) / #6.126([a, a, a, a, a]) / #6.127([a, a, a, a, a, a]) ; similarly for tag range: #6.1280 .. #6.1400 inclusive / #6.102([uint, [* a]]) big_int = int / big_uint / big_nint big_uint = #6.2(bounded_bytes) big_nint = #6.3(bounded_bytes) bounded_bytes = bytes .size (0..64)\nplutus_data = constr<plutus_data> / { * plutus_data => plutus_data } / [ * plutus_data ] / big_int / bounded_bytes constr<a> = #6.121([]) / #6.122([a]) / #6.123([a, a]) / #6.124([a, a, a]) / #6.125([a, a, a, a]) / #6.126([a, a, a, a, a]) / #6.127([a, a, a, a, a, a]) ; similarly for tag range: #6.1280 .. #6.1400 inclusive / #6.102([uint, [* a]]) big_int = int / big_uint / big_nint big_uint = #6.2(bounded_bytes) big_nint = #6.3(bounded_bytes) bounded_bytes = bytes .size (0..64)\nNOTE: The CDDL specification is extracted from the wider alonzo_cddl specification of the Cardano ledger.\nThe Data type is a recursive data-type, so costing it properly is a little tricky. The Plutus source code defines an instance of ExMemoryUsage for Data with the following interesting note:\nData\nExMemoryUsage\nData\nThis accounts for the number of nodes in a Data object, and also the sizes of the contents of the nodes. This is not ideal, but it seems to be the best we can do. At present this only comes into play for 'equalsData', which is implemented using the derived implementation of '==' [...].\nData\nWe propose to re-use this instance to define a cost model linear in the size of data defined by this instance. What remains is to find a proper coefficient and offset for that linear model. To do so, we can benchmark the execution costs of encoding arbitrarily generated Data of various sizes, and retro-fit the cost into a linear model (provided that the results are still attesting for that type of model).\nData\nBenchmarking and costing serialiseData was done in this PR according to this strategy. As the benchmark is not very uniform, because some cases of Data \"structures\" differ in CPU time taken to process, the linear model is used as an upper bound and thus conservatively overestimating actual costs.\nserialiseData\nData\nEasy to implement as it reuses existing code of the Plutus codebase;\nEasy to implement as it reuses existing code of the Plutus codebase;\nSuch built-in is generic enough to also cover a wider set of use-cases, while nicely fitting ours;\nSuch built-in is generic enough to also cover a wider set of use-cases, while nicely fitting ours;\nFavoring manipulation of structured Data is an appealing alternative to many ByteString manipulation use-cases;\nFavoring manipulation of structured Data is an appealing alternative to many ByteString manipulation use-cases;\nData\nByteString\nCBOR as encoding is a well-known and widely used standard in Cardano, existing tools can be used;\nCBOR as encoding is a well-known and widely used standard in Cardano, existing tools can be used;\nThe hypothesis on the cost model here is that serialisation cost would be proportional to the ExMemoryUsage for Data; which means, given the current implementation, proportional to the number and total memory usage of nodes in the Data tree-like structure.\nThe hypothesis on the cost model here is that serialisation cost would be proportional to the ExMemoryUsage for Data; which means, given the current implementation, proportional to the number and total memory usage of nodes in the Data tree-like structure.\nExMemoryUsage\nData\nData\nBenchmarking the costs of serialising TxOut values between plutus-cbor and cborg confirms cborg and the existing encodeData's implementation in Plutus as a great candidate for implementing the built-in: Results can be reproduced with the plutus-cbor benchmark.\nBenchmarking the costs of serialising TxOut values between plutus-cbor and cborg confirms cborg and the existing encodeData's implementation in Plutus as a great candidate for implementing the built-in:\nTxOut\nResults can be reproduced with the plutus-cbor benchmark.\nWe have identified that the cost mainly stems from concatenating bytestrings; so possibly, an alternative to this proposal could be a better way to concatenate (or to cost) bytestrings (Builders in Plutus?)\nWe have identified that the cost mainly stems from concatenating bytestrings; so possibly, an alternative to this proposal could be a better way to concatenate (or to cost) bytestrings (Builders in Plutus?)\nIf costing for BuiltinData is unsatisfactory, maybe we want have only well-known input types, e.g. TxIn, TxOut, Value and so on.. WellKnown t = t - BuiltinByteString\nIf costing for BuiltinData is unsatisfactory, maybe we want have only well-known input types, e.g. TxIn, TxOut, Value and so on.. WellKnown t = t - BuiltinByteString\nBuiltinData\nTxIn\nTxOut\nValue\nWellKnown t => t -> BuiltinByteString\nAdditional built-in: so can be added to PlutusV1 and PlutusV2 without breaking any existing script validators. A hard-fork is however required as it would makes more blocks validate.\nRelease it as a backward-compatible change within the Vasil protocol upgrade\nUsing the existing sizing metric for Data, determine a costing function (using existing tooling / benchmarks? TBD)\nData\nThe Plutus team updates plutus to add the built-in to PlutusV1 and PlutusV2 and uses a suitable cost function\nThe binary format of Data is documented and embraced as an interface within plutus. (see cardano-ledger's CDDL specification)\nData\nplutus\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0045 | Decentralized WebRTC dApp-Wallet Communication\n\nWe want to introduce a decentralized communication method between dApps and wallets based on WebTorrent trackers and WebRTC. This CIP also contains a proof of concept implementation injecting the wallet rpc methods into the dApps global window object similar to CIP-0030.\nIn a decentralized ecosystem a communication between wallet-apps and dApps is still challanging. The inter-app communication on mobile devices does not directly allow remote procedure calls and is mostly restricted to Universal Links (iOS) or Deeplinks (Android). State-of-the-art solutions like WalletConnect tackle these problems using WebRTC communication which also works across devices, but requires a central signaling server to estalblish a WebRTC connection. In this CIP we want to introduce an architecture which uses WebTorrent trackers for the peer discovery to remove the need of this central component.\nsequenceDiagram dApp-->>+Signaling Server: Who am I? Signaling Server-->>dApp: You are <ip:port> Wallet-->>+Signaling Server: Who am I? Signaling Server-->>Wallet: You are <ip:port> dApp->>Wallet: messages Wallet->>dApp: messages\nsequenceDiagram dApp-->>+Signaling Server: Who am I? Signaling Server-->>dApp: You are <ip:port> Wallet-->>+Signaling Server: Who am I? Signaling Server-->>Wallet: You are <ip:port> dApp->>Wallet: messages Wallet->>dApp: messages\nThe data will be send peer to peer via a WebRTC data channel once the ip discovery has been finished. E.g. WalletConnect expects/provides a relay server URL to initialize the connection. While this method allows dApps to communitcate peer-to-peer with wallets it also leads to a possible SPOF.\nflowchart LR subgraph dApp subgraph .torrent server end end subgraph Wallet[Wallet App] end dApp-->|.torrent| TrackerA[Tracker 1] dApp-->|.torrent| TrackerB[Tracker 2] dApp-->|.torrent| TrackerC[...] dApp-->|.torrent| TrackerD[Tracker n] dApp-->|.torrent| TrackerA[Tracker 1] dApp-->|.torrent| TrackerB[Tracker 2] dApp-->|.torrent| TrackerC[...] dApp-->|.torrent| TrackerD[Tracker n] dApp-->|Share public key| Wallet\nflowchart LR subgraph dApp subgraph .torrent server end end subgraph Wallet[Wallet App] end dApp-->|.torrent| TrackerA[Tracker 1] dApp-->|.torrent| TrackerB[Tracker 2] dApp-->|.torrent| TrackerC[...] dApp-->|.torrent| TrackerD[Tracker n] dApp-->|.torrent| TrackerA[Tracker 1] dApp-->|.torrent| TrackerB[Tracker 2] dApp-->|.torrent| TrackerC[...] dApp-->|.torrent| TrackerD[Tracker n] dApp-->|Share public key| Wallet\nDeep links, Universal Links, or even the clipboard could be utilized to share the identifier (public key) on the same device (in cases of a wallet based on web technology like Ionic). For sharing the identifier across different devices, QR codes would come into play. This method could be applied, for example, between a wallet mobile app and a dapp running on a PC, or vice versa. The wallet application would then initiate a query to a list of trackers using this distinct identifier in order to establish the WebRTC connection. After this process is completed, the data is transmitted peer-to-peer following the WebRTC standard, for instance, to invoke RPC calls.\nflowchart LR Wallet-->|queries| TrackerA[Tracker 1] Wallet-->|queries| TrackerB[Tracker 2] Wallet-->|queries| TrackerC[...] Wallet-->|queries| TrackerD[Tracker n] Wallet-->|queries| TrackerA[Tracker 1] Wallet-->|queries| TrackerB[Tracker 2] Wallet-->|queries| TrackerC[...] Wallet-->|queries| TrackerD[Tracker n] subgraph dApp subgraph .torrent server end end Wallet<--Establish WebRTC data channel\\n for peer to peer communication-->dApp\nflowchart LR Wallet-->|queries| TrackerA[Tracker 1] Wallet-->|queries| TrackerB[Tracker 2] Wallet-->|queries| TrackerC[...] Wallet-->|queries| TrackerD[Tracker n] Wallet-->|queries| TrackerA[Tracker 1] Wallet-->|queries| TrackerB[Tracker 2] Wallet-->|queries| TrackerC[...] Wallet-->|queries| TrackerD[Tracker n] subgraph dApp subgraph .torrent server end end Wallet<--Establish WebRTC data channel\\n for peer to peer communication-->dApp\nThe keys (public key and corresponding 64-byte secret key) are generated using a function that implements Ed25519. This function requires a (random) seed, which can also be stored and re-used to ensure that whenever a client employs a dApp or a Wallet, the same key pair is generated consistently, even if the browser or mobile app is restarted. The public key will be used as an identifier for the torrent-based peer discovery. When this identifier is shared through methods like QR codes or links, it needs to be compliant to the following Cardano Uri Scheme (CIP-0013):\nweb+cardano://connect/v1?identifier=<public_key>\nweb+cardano://connect/v1?identifier=<public_key>\nThe idea of using WebTorrent trackers instead of signaling servers for peer discovery was already mentioned in Aug 2018 by Chris McCormick: \"I've also been tinkering with WebTorrent. [...] Working with this technology made me realise something the other day: it's now possible to host back-end services, or \"servers\" inside browser tabs. [...] So anyway, I've made this weird thing to enable developers to build \"backend\" services which run in browser tabs\"\nMcCormick's idea has been developed and open sourced as a library called bugout (MIT).\nFor this proof of concept we wrote two small pieces of software:\nA html page aka the dApp\nAn ionic react app (to target mutliple devices) aka the wallet app\nThe whole code is provided within this demo implementation that also contains a step-by-step guide.\nThe dApp consists of a standard HTML5 template including the following lines of code:\n<script src=\"https://chr15m.github.io/bugout/bugout.min.js\"></script> <script> var bugout = new Bugout({ seed: localStorage[\"poc-server-seed\"], announce: [ 'udp://tracker.opentrackr.org:1337/announce', 'udp://open.tracker.cl:1337/announce', 'udp://opentracker.i2p.rocks:6969/announce', 'https://opentracker.i2p.rocks:443/announce', 'wss://tracker.files.fm:7073/announce', 'wss://spacetradersapi-chatbox.herokuapp.com:443/announce', 'ws://tracker.files.fm:7072/announce' ] }); localStorage[\"poc-server-seed\"] = bugout.seed; var connected = false; bugout.on(\"connections\", function (clients) { if (clients == 0 && connected == false) { connected = true; console.log(\"[info]: server ready\"); console.log(`[info]: share this address with your wallet app -> ${bugout.address()}`); } console.log(`[info]: ${clients} clients connected`); }); bugout.register(\"api\", function (address, args, callback) { const api = { version: args.api.version, address: address } for (method of args.api.methods) { api[method] = () => new Promise((resolve, reject) => { bugout.rpc(address, method, {}, (result) => resolve(result)); }); } window.cardano = window.cardano || {}; window.cardano[args.api.name] = api; console.log(`[info]: injected api of ${args.api.name} into window.cardano`); }); </script>\n<script src=\"https://chr15m.github.io/bugout/bugout.min.js\"></script> <script> var bugout = new Bugout({ seed: localStorage[\"poc-server-seed\"], announce: [ 'udp://tracker.opentrackr.org:1337/announce', 'udp://open.tracker.cl:1337/announce', 'udp://opentracker.i2p.rocks:6969/announce', 'https://opentracker.i2p.rocks:443/announce', 'wss://tracker.files.fm:7073/announce', 'wss://spacetradersapi-chatbox.herokuapp.com:443/announce', 'ws://tracker.files.fm:7072/announce' ] }); localStorage[\"poc-server-seed\"] = bugout.seed; var connected = false; bugout.on(\"connections\", function (clients) { if (clients == 0 && connected == false) { connected = true; console.log(\"[info]: server ready\"); console.log(`[info]: share this address with your wallet app -> ${bugout.address()}`); } console.log(`[info]: ${clients} clients connected`); }); bugout.register(\"api\", function (address, args, callback) { const api = { version: args.api.version, address: address } for (method of args.api.methods) { api[method] = () => new Promise((resolve, reject) => { bugout.rpc(address, method, {}, (result) => resolve(result)); }); } window.cardano = window.cardano || {}; window.cardano[args.api.name] = api; console.log(`[info]: injected api of ${args.api.name} into window.cardano`); }); </script>\nThe wallet app is a standard ionic react app built by the ionic cli:\nionic start WalletApp blank --type=react cd WalletApp npm i bugout\nionic start WalletApp blank --type=react cd WalletApp npm i bugout\nThe following lines of code were added to the index.tsx file:\nconst bugout = new Bugout( \"<HASH provided by the dAPP>\", { announce: [ 'udp://tracker.opentrackr.org:1337/announce', 'udp://open.tracker.cl:1337/announce', 'udp://opentracker.i2p.rocks:6969/announce', 'https://opentracker.i2p.rocks:443/announce', 'wss://tracker.files.fm:7073/announce', 'wss://spacetradersapi-chatbox.herokuapp.com:443/announce', 'ws://tracker.files.fm:7072/announce' ] }); bugout.on(\"server\", function() { console.log(\"[info]: connected to server\") bugout.rpc(\"<HASH provided by the dAPP>\", \"api\", {\"api\": { version: \"1.0.3\", name: 'boostwallet', methods: [\"getRewardAddresses\"] }}); }); bugout.register(\"getRewardAddresses\", (address:string, args:any, callback:Function) => { callback([\"e1820506cb0ce54ae755b2512b6cf31856d7265e8792cb86afc94e0872\"]); });\nconst bugout = new Bugout( \"<HASH provided by the dAPP>\", { announce: [ 'udp://tracker.opentrackr.org:1337/announce', 'udp://open.tracker.cl:1337/announce', 'udp://opentracker.i2p.rocks:6969/announce', 'https://opentracker.i2p.rocks:443/announce', 'wss://tracker.files.fm:7073/announce', 'wss://spacetradersapi-chatbox.herokuapp.com:443/announce', 'ws://tracker.files.fm:7072/announce' ] }); bugout.on(\"server\", function() { console.log(\"[info]: connected to server\") bugout.rpc(\"<HASH provided by the dAPP>\", \"api\", {\"api\": { version: \"1.0.3\", name: 'boostwallet', methods: [\"getRewardAddresses\"] }}); }); bugout.register(\"getRewardAddresses\", (address:string, args:any, callback:Function) => { callback([\"e1820506cb0ce54ae755b2512b6cf31856d7265e8792cb86afc94e0872\"]); });\nThis example has a few restrictions:\nbugout is currently not compatible with Webpack 5, so polyfills are not automatically included and a react-scripts eject is needed to add them to the webpack.config.js file bugout does not directly provide type declarations. There are some declarations within a PR, but they need to be adjusted (a few parameters are not mandatory) and added to a bugout.d.ts file.\nbugout is currently not compatible with Webpack 5, so polyfills are not automatically included and a react-scripts eject is needed to add them to the webpack.config.js file\nbugout is currently not compatible with Webpack 5, so polyfills are not automatically included and a react-scripts eject is needed to add them to the webpack.config.js file\nbugout does not directly provide type declarations. There are some declarations within a PR, but they need to be adjusted (a few parameters are not mandatory) and added to a bugout.d.ts file.\nbugout does not directly provide type declarations. There are some declarations within a PR, but they need to be adjusted (a few parameters are not mandatory) and added to a bugout.d.ts file.\nsequenceDiagram dApp-->>Wallet: Share address using Deeplink /Universal Link / QR Wallet-->>Tracker List: Accept connection and start quering using the address Wallet-->>dApp: Once connected register RPC functions and share the API dApp-->>dApp: Inject the API to window.cardano[walletName] dApp-->>Wallet: Call RPC functions (e.g. getRewardAddresses) Wallet-->>Wallet: If needed bring app to foregrund (e.g. request signing) Wallet-->>dApp: Send response\nsequenceDiagram dApp-->>Wallet: Share address using Deeplink /Universal Link / QR Wallet-->>Tracker List: Accept connection and start quering using the address Wallet-->>dApp: Once connected register RPC functions and share the API dApp-->>dApp: Inject the API to window.cardano[walletName] dApp-->>Wallet: Call RPC functions (e.g. getRewardAddresses) Wallet-->>Wallet: If needed bring app to foregrund (e.g. request signing) Wallet-->>dApp: Send response\nWe decided to spawn the server within the dApp to force the user to manually scan a QR code (using a wallet app) or accept an \"Open with WalletAppName \" ui dialog (in case of Universal Links or Deeplinks). This prevents the user from connecting the wallet to an unwanted dApp. Additionally we need to add a few security checks to prevent a misusage of this method.\n<WalletAppName>\nThe wallet app needs to verifiy the origin (address) of the RPC call\ndApps should ask the user for permission to inject the wallet names into the window.cardano object to prevent XSS attack (Maybe using a graphical representation of the wallet app address e.g. blockies)\nThe purpose of this CIP mainly consists of two parts. It addresses the current lack of dApp mobile support, but at the same time provides an even more decentralized alternative to state-of-the-art communication methods. To achieve this goal we have introduced a WebTorrent and WebRTC based architecture. To demonstrate a viable implementation, we have implemented a proof of concept which also shows how a rpc method injection like CIP-0030 might look like.\nA library should be build to make it easy from dAPP and wallet side to implement the proposed communication method\nThe library target should be browser to avoid the need of manual polyfills\nMobile testing on different devices and operating systems needs to be done with a special focus to the wallet app running in background mode\nPotential security issues and attack vectors need to be discussed in detail We discussed potential security issues in the CIP discussion and we implement an identicon solutin, but it is obviously an never ending task\nWe discussed potential security issues in the CIP discussion and we implement an identicon solutin, but it is obviously an never ending task\nWe discussed potential security issues in the CIP discussion and we implement an identicon solutin, but it is obviously an never ending task\nA full reference implementation is needed to test if the entire user flow and at the same time provide this as a how-to for developers Has been implemented here https://github.com/fabianbormann/cip-0045-demo-implementation\nHas been implemented here https://github.com/fabianbormann/cip-0045-demo-implementation\nHas been implemented here https://github.com/fabianbormann/cip-0045-demo-implementation\nFork/Extend bugout to add webpack 5 and typescript support\nPovide a general intermediate cardano-connect typescript library to provide A check for mobile/desktop environment Depending on the environment provide interfaces for CIP-0030 / and / or CIP-? Add a full implementation of the server/client side code above to define a communication standard similar to CIP-0030 (getRewardAddresses, signData, signTx, ...)\nA check for mobile/desktop environment Depending on the environment provide interfaces for CIP-0030 / and / or CIP-? Add a full implementation of the server/client side code above to define a communication standard similar to CIP-0030 (getRewardAddresses, signData, signTx, ...)\nA check for mobile/desktop environment\nDepending on the environment provide interfaces for CIP-0030 / and / or CIP-?\nAdd a full implementation of the server/client side code above to define a communication standard similar to CIP-0030 (getRewardAddresses, signData, signTx, ...)\nStart discussions about security gaps within the proposed method with various developers and also look for research papers\nCheck if the wallet app also reacts to rpc calls in background mode on Android\nCheck if the wallet app also reacts to rpc calls in background mode on iOS\nImplement the library within an example dApp: Implemented in Cardano Ballot for the summit voting 2023 Implemented by (SundeaSwap)[https://www.youtube.com/watch?v=mRpXIh-DyYM] Implemented into the cardano-connect-with-wallet core and react library Implemented in walkinwallet\nImplemented in Cardano Ballot for the summit voting 2023 Implemented by (SundeaSwap)[https://www.youtube.com/watch?v=mRpXIh-DyYM] Implemented into the cardano-connect-with-wallet core and react library Implemented in walkinwallet\nImplemented in Cardano Ballot for the summit voting 2023\nImplemented by (SundeaSwap)[https://www.youtube.com/watch?v=mRpXIh-DyYM]\nImplemented into the cardano-connect-with-wallet core and react library\nImplemented in walkinwallet\nThe re-implementation of bugout that matches the expectations below is now available as meerkat\nThe re-implementation of bugout that matches the expectations below is now available as meerkat\nA general cardano-connect typescript library with 100 CIP-30 support has been provided\nA general cardano-connect typescript library with 100 CIP-30 support has been provided\nThe copy & paste demo implementation is ready to use\nThe copy & paste demo implementation is ready to use\nCardano Foundation's connect-with-wallet component does include the dApp part of CIP-45 (via feature flag), so that dApp developers don't need to write a single line of code if they rely on this component\nCardano Foundation's connect-with-wallet component does include the dApp part of CIP-45 (via feature flag), so that dApp developers don't need to write a single line of code if they rely on this component\nThe wording of the CIP-45 has been changed. Many thanks to @jehrhardt for his valuable explanation and suggestions\nThe wording of the CIP-45 has been changed. Many thanks to @jehrhardt for his valuable explanation and suggestions\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0049 | ECDSA and Schnorr signatures in Plutus Core\n\nSupport ECDSA and Schnorr signatures over the SECP256k1 curve in Plutus Core; specifically, allow validation of such signatures as builtins. These builtins work over BuiltinByteStrings.\nBuiltinByteString\nSignature schemes based on the SECP256k1 curve are common in the blockchain industry; a notable user of these is Bitcoin. Supporting signature schemes which are used in other parts of the industry provides an interoperability benefit: we can verify signatures produced by other systems as they are today, without requiring other people to produce signatures specifically for us. This not only provides us with improved interoperability with systems based on Bitcoin, but also compatibility with other interoperability systems, such as Wanchain and Renbridge, which use SECP256k1 signatures for verification. Lastly, if we can verify Schnorr signatures, we can also verify Schnorr-compatible multi or threshold signatures, such as MuSig2 or Frost.\nTwo new builtin functions would be provided:\nA verification function for ECDSA signatures using the SECP256k1 curve; and\nA verification function for Schnorr signatures using the SECP256k1 curve.\nThese would be based on secp256k1, a reference implementation of both kinds of signature scheme in C. This implementation would be called from Haskell using direct bindings to C. These bindings would be defined in cardano-base, using its existing DSIGN interface, with new builtins in Plutus Core on the basis of the DSIGN interface for both schemes.\nsecp256k1\ncardano-base\nDSIGN\nDSIGN\nThe builtins would be costed as follows: ECDSA signature verification has constant cost, as the message, verification key and signature are all fixed-width; Schnorr signature verification is instead linear in the message length, as this can be arbitrary, but as the length of the verification key and signature are constant, the costing will be constant in both.\nMore specifically, Plutus would gain the following primitive operations:\nverifyEcdsaSecp256k1Signature :: BuiltinByteString - BuiltinByteString - BuiltinByteString - BuiltinBool, for verifying 32-byte message hashes signed using the ECDSA signature scheme on the SECP256k1 curve; and\nverifyEcdsaSecp256k1Signature :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString -> BuiltinBool\nverifySchnorrSecp256k1Signature :: BuiltinByteString - BuiltinByteString - BuiltinByteString - BuiltinBool, for verifying arbitrary binary messages signed using the Schnorr signature scheme on the SECP256k1 curve.\nverifySchnorrSecp256k1Signature :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString -> BuiltinBool\nBoth functions take parameters of a specific part of the signature scheme, even though they are all encoded as BuiltinByteStrings. In order, for both functions, these are:\nBuiltinByteString\nA verification key; An input to verify (either the message itself, or a hash); A signature.\nA verification key;\nAn input to verify (either the message itself, or a hash);\nA signature.\nThe two different schemes handle deserialization internally: specifically, there is a distinction made between 'external' representations, which are expected as arguments, and 'internal' representations, used only by the implementations themselves. This creates different expecations for each argument for both of these schemes; we describe these below.\nFor the ECDSA signature scheme, the requirements are as follows. Note that these differ from the serialization used by Bitcoin, as the serialisation of signatures uses DER-encoding, which result in variable size signatures up to 72 bytes (instead of the 64 byte encoding we describe in this document).\nThe verification key must correspond to the (x, y) coordinates of a point on the SECP256k1 curve, where x, y are unsigned integers in big-endian form.\nThe verification key must correspond to a result produced by secp256k1_ec_pubkey_serialize, when given a length argument of 33, and the SECP256K1_EC_COMPRESSED flag. This implies all of the following: The verification key is 33 bytes long. The first byte corresponds to the parity of the y coordinate; this is 0x02 if y is even, and 0x03 otherwise. The remaining 32 bytes are the bytes of the x coordinate.\nsecp256k1_ec_pubkey_serialize\nSECP256K1_EC_COMPRESSED\nThe verification key is 33 bytes long.\nThe first byte corresponds to the parity of the y coordinate; this is 0x02 if y is even, and 0x03 otherwise.\n0x02\n0x03\nThe remaining 32 bytes are the bytes of the x coordinate.\nThe input to verify must be a 32-byte hash of the message to be checked. We assume that the caller of verifyEcdsaSecp256k1Signature receives the message and hashes it, rather than accepting a hash directly: doing so can be dangerous. Typically, the hashing function used would be SHA256; however, this is not required, as only the length is checked.\nverifyEcdsaSecp256k1Signature\nThe signature must correspond to two unsigned integers in big-endian form; henceforth r and s.\nThe signature must correspond to a result produced by secp256k1_ecdsa_serialize_compact. This implies all of the following: The signature is 64 bytes long. The first 32 bytes are the bytes of r. The last 32 bytes are the bytes of s. r 32 bytes s 32 bytes --------- signature ----------\nsecp256k1_ecdsa_serialize_compact\nThe signature is 64 bytes long.\nThe first 32 bytes are the bytes of r.\nThe last 32 bytes are the bytes of s.\n┏━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┓ ┃ r <32 bytes> │ s <32 bytes> ┃ ┗━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┛ <--------- signature ---------->\n┏━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┓ ┃ r <32 bytes> │ s <32 bytes> ┃ ┗━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┛ <--------- signature ---------->\nFor the Schnorr signature scheme, we have the following requirements, as described in the requirements for BIP-340:\nThe verification key must correspond to the (x, y) coordinates of a point on the SECP256k1 curve, where x, y are unsigned integers in big-endian form.\nThe verification key must correspond to a result produced by secp256k1_xonly_pubkey_serialize. This implies all of the following: The verification key is 32 bytes long. The bytes of the signature correspond to the x coordinate.\nsecp256k1_xonly_pubkey_serialize\nThe verification key is 32 bytes long.\nThe bytes of the signature correspond to the x coordinate.\nThe input to verify is the message to be checked; this can be of any length, and can contain any bytes in any position.\nThe signature must correspond to a point R on the SECP256k1 curve, and an unsigned integer s in big-endian form.\nThe signature must follow the BIP-340 standard for encoding. This implies all of the following: The signature is 64 bytes long. The first 32 bytes are the bytes of the x coordinate of R, as a big-endian unsigned integer. The last 32 bytes are the bytes of s. R 32 bytes s 32 bytes --------- signature ----------\nThe signature is 64 bytes long.\nThe first 32 bytes are the bytes of the x coordinate of R, as a big-endian unsigned integer.\nThe last 32 bytes are the bytes of s.\ns\n┏━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┓ ┃ R <32 bytes> │ s <32 bytes> ┃ ┗━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┛ <--------- signature ---------->\n┏━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┓ ┃ R <32 bytes> │ s <32 bytes> ┃ ┗━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┛ <--------- signature ---------->\nThe builtin operations will error with a descriptive message if given inputs that don't correspond to the constraints above, return False if the signature fails to verify the input given the key, and True otherwise.\nFalse\nTrue\nWe consider the implementation trustworthy: secp256k1 is the reference implementation for both signature schemes, and is already being used in production by Bitcoin. Specifically, ECDSA signatures over the SECP256k1 curve were used by Bitcoin before Taproot, while Schnorr signatures over the same curve have been used since Taproot.\nsecp256k1\nAn alternative approach could be to provide low-level primitives, which would allow any signature scheme (not just the ones under consideration here) to be implemented by whoever needs them. While this approach is certainly more flexible, it has two significant drawbacks:\nIt requires 'rolling your own crypto', rather than re-using existing implementations. This has been shown historically to be a bad idea; furthermore, if existing implementations have undergone review and audit, any such re-implementations would give us the same assurances as those that have been reviewed and audited.\nIt would be significantly costlier, as the computation would happen in Plutus Core. Given the significant on-chain size restrictions, this would likely be too costly for general use: many such schemes rely on large precomputed tables, for example, which are totally unviable on-chain.\nIt may be possible that some set of primitive can avoid both of these issues (for example, the suggestions in this CIP); in the meantime, providing direct support for commonly-used schemes such as these is worthwhile.\nAt the Plutus Core level, implementing this proposal induces no backwards-incompatibility: the proposed new primitives do not break any existing functionality or affect any other builtins. Likewise, at levels above Plutus Core (such as PlutusTx), no existing functionality should be affected.\nPlutusTx\nOn-chain, this requires a hard fork.\nInclude tests of functionality with implementation.\nSatisfaction of CIP-0035 requirements (Additions to the Plutus Core Builtins) including costing.\nInclusion of SECP in Plutus core (as of Valentine hard fork).\nProvide an implementation: by MLabs, merged into Plutus.\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0050 | Pledge Leverage-Based Staking Rewards\n\nImproving decentralization is absolutely necessary for the long term health and growth of the Cardano ecosystem. The current reward formula has resulted in a stable but stagnant level of decentralization. With the benefit of hindsight over the last year the intent of (a0,k)(a0, k)(a0,k) has not resulted in the desired decentralization outcome. This CIP provides the justification, methods, metrics, and implementation for an improvement program to increase decentralization of the Cardano network.\nThe proposed reward equation retains the function of kkk for diminishing rewards based on stake and enforces diminishing rewards based on pledge leverage, LLL. The proposed equation enforces a set of principles to ensure stakeholders of dramatically different size can all achieve the same maximum yield. The yield ceiling feature prevents the formation of two classes of stakeholders and removes some of the benefits of centralization. The economic motivations of the largest stakeholders will be aligned with decentralization, reward diversification, fault tolerance, and ensuring the sybil protection of the entire community.\nWarning Conflict of Interest Declaration\nThe author is employed full time as a Research Assistant Professor in Nuclear Engineering at the University of Tennessee. The author earns no revenue from the Cardano ecosystem, does not operate a stake pool, is not seeking Cardano Foundation stakepool delegation support, is not seeking Catalyst funding support, and owns 25k ADA delegated to the BALNC stake pool ($drliesenfelt).\nImproving decentralization is absolutely necessary for the long term health and growth of the Cardano ecosystem. The current reward formula has resulted in a stable but stagnant level of decentralization. The motivation is to provide the justification, methods, metrics, and implementation schedule for an improvement program to increase decentralization of the Cardano network.\nKKK and a0a0a0 are input parameters to the reward formulation designed to promote decentralization. The original intention for parameter kkk was for it to set a soft-cap on a pool s size and thereby encourage the system to converge toward kkk pools of equal size delivering equal return per unit of stake for all delegates. In the ideal world this would mean kkk pools that are roughly equally saturated producing a roughly equal proportion of the blocks [1]. An underlying assumption was that one entity would run only one pool and design discussions about these parameters have described running multiple pools as a form of Sybil attack [2].\nHowever, the input parameters have not achieved these goals. Currently there are single entities that run 5,10,20 or even 50 separate pools. It is proposed that the average resulting decentralization should be measured based on the stake held by entire entities/groups, rather than a count of individual pools. K-effective is hereby used to measure the average resulting decentralization and is computed using Equation 1. The Nakamoto Coefficient is approximately half of k-effective rounded up to the nearest integer. K-effective provides a higher resolution quantification of network decentralization compared to the Nakamoto Coefficient.\nThe Cardano network currently produces ~21,600 blocks per epoch with ~2400 groups producing between 0 and ~3600 blocks per group. If averaged, approximately 40 equal sized groups would each be creating ~527 blocks per epoch. The historical decentralization of the network shown in Figure 1 has improved from 30.0 in epoch 245 to between 35.0 and 43.0 after epoch 260. This effective decentralization or k-effective is not even close to the 500 figure targeted by the current k=500 parameter. A partial example of the table used to compute k-effective is shown in Figure 2.\n(1) https://iohk.io/en/blog/posts/2020/11/05/parameters-and-decentralization-the-way-ahead/\n(2) https://iohk.io/en/blog/posts/2018/10/29/preventing-sybil-attacks/\nRef(1)Ref (1)Ref(1)\nFigure 1. Historical k-effective from epoch 245 to present.\nFigure 2. K-effective table.\nTo cite Aggelos Kiayias, Chief Scientist of IOG:\nCentral to the mechanism s behavior are two parameters: kkk and a0a0a0. The k-parameter caps the rewards of pools to 1/k of the total available. The a0a0a0 parameter creates a benefit for pledging more stake into a single pool; adding X amount of pledge to a pool increases its rewards additively by up to a0a0a0*X. This is not to the detriment of other pools; any rewards left unclaimed due to insufficient pledging will be returned to the Cardano s reserves and allocated in the future. [3]\nPaired with the assessment of stake pools performed by the delegates, this mechanism provides the right set of constraints for the system to converge to a configuration of k equal size pools with the maximum amount of pledge possible. [3]\nThe analysis of the current reward formula in [4] equated 1 pool to 1 entity. In the real world 1 entity can choose to delegate to another entity, operate one pool, or operate many pools. This oversight in the original analysis contributed to the proliferation of multipools in defiance of kkk parameter increases.\n(3) https://iohk.io/en/blog/posts/2020/11/13/the-general-perspective-on-staking-in-cardano/\n(4) https://arxiv.org/ftp/arxiv/papers/1807/1807.11218.pdf\nFrom 4.1 Our RSS construction of Reward Sharing Schemes for Stake Pools [5] the current rewards equation is:\nRef(2)Ref(2)Ref(2)\nwhere:\nA natural choice is =1/k = 1/k =1/k, where kkk is the target number of pools, and k, k, k, are fixed parameters.\nThe following are current protocol parameters:\nk=500k = 500k=500\n=a0=0.3 = a0 = 0.3 =a0=0.3\nThe a0a0a0 parameter represents the fraction of the rewards (R/(1+a0))(R/(1+a0))(R/(1+a0)) which are not paid out unless all of the stake is pledged. An a0a0a0 of 0.3 ensures that 1.0 - 1.0/(1.0+0.3) = 23 of the total rewards R will be withheld from low pledge fraction pools and returned to the reserve. The effect of this formula is that increased pledge results in retaining more of the available rewards R. However, this benefit is not linear, rather it is drastically biased towards the saturation limit. The =min , \\sigma = min\\\\{ , \\\\} =min , term enforces a reward limit based on kkk. Visualizing the resulting field of outcomes at various pledge amounts from 0.00 to 100.0 is necessary. The red dotted line Max Reward represents the maximum available yield available at the stated network size.\nThe intent of parameters (a0,k)(a0, k)(a0,k) has not been realized. The graph of k-effective shows that increasing kkk from 150 to 500 did not result in a proportional increase to decentralization. The k parameter is currently 500/35=14.29500 / 35 = 14.29500/35=14.29 times larger than the effective decentralization k-effective.\nAnother important determinant of the ability for small pools to compete with larger pools is the mandatory minimum fee (minFee parameter) which is currently 340 . This minimum fee is a higher percentage of the total rewards for a small pool compared to a larger pool. It means that delegator yields will not exceed 95 of a saturated pool delegator yields until the pool has at least 15.0 saturation. This is a significant barrier to entry for small pools.\nBillions of ADA is currently staked in pools with nearly 0 pledge and extremely high leverage. Also, a billion ADA is currently pledged in nearly saturated private pools closed to community delegation. There are very few public pools accepting community delegation with pledge amounts between 5M and 60M and the vast majority of public pools have less than 1M pledge. The following bubble chart shows the distribution of stake as a function of group leverage on a log(Stake) vs log(Leverage) scale. The current pledge incentive mechanism only becomes relevant in a small segment of this chart below a leverage of 10 and above a pledge amount of 10M . The Single Pool Operator Alliance (SPA) is a collective of ~2200 individual pools and pool operators with a collective stake of 5B at an average leverage factor of only 22.\nIn the original design, parameter a0a0a0 represented the influence the operator s pledge had on the desirability of the pool. In other words, more pledge should mean the pool would be more desirable for stake delegation. However the current reward formula has not produced this effect. See Figure 2. With increasing pledge as a proportion of total stake there is little noticeable effect on rewards until very high pledge percentages. These very high pledge percentages are not attainable except by extremely large stakeholders. Furthermore having such a high pledge percentage would defeat the purpose of community staking since the pool would already be saturated when the maximum pledge benefit is earned.\nThe reality of the past 18 months is that pool operators have split pledge across multiple pools because it is more profitable to earn fees (minFee + margin ) than it is to benefit from increasing their pledge to a single pool. The small increase in yield for pools with less than 10M pledge is also much less than the random statistical uncertainty of rewards per epoch. The pledge incentive is currently a statistically unnoticed benefit used only by large private stakeholders. The current reward equation has sacrificed fair egalitarian rewards for an incentive that is not providing Sybil protection as intended.\nThe SundaeSwap Initial Stake Offering (ISO) proved that community delegator stake can be very mobile. More than a couple billion in stake centralized into ISO groups. After realizing the popularity of their ISO resulted in a centralization of the network SundaeSwap launched a Reverse ISO for the benefit of single pools. The RISO temporarily reversed the centralization trend as a billion ADA was delegated to small pools. The Cardano network reached a decentralization factor of 43.22 in epoch 321. After the RISO these decentralization gains reversed because the underlying incentives of the reward formula have not changed. This proves that the community is engaged and their stake is mobile, especially for yield and gains.\nAn intentional design of the current reward equation is to purposefully allow large stakeholders pledging to private pools to earn the maximum possible yields while excluding community delegation. The vast majority of pools with pledge 10 stake saturation have an asymptotic yield currently approaching a maximum yield of 4.25 . A high pledge fraction in a private pool can earn up to the maximum available 5.5 yield, 30 more yield than low pledge community pools.\nThe R/(1+a0)R/(1+a0)R/(1+a0) term guarantees that small pools will not earn the same fraction of the reserve as large pledged pools. Currently, approximately 1.25B is earning full rewards in 19 pools. If left unchanged over time custodial wealth management companies will be able to offer materially higher yields than individuals. Wave financial is an example of a company currently providing this business model to clients. Eternl/ccvalut is introducing multi-signature features to allow many individuals to collectively pledge to pools. Over time this difference results in two different classes of stakeholders and erodes the decentralized self-custodial appeal of Cardano.\nForcing the k-parameter to be radically different from the effective decentralization, k-effective, of the network has resulted in unintended consequences. When k was increased large groups created new pools to retain delegators. If the k-parameter is increased without updating the rewards formula, more large stakeholders will be able to earn full yields by pledging to private pools excluding community delegation.\nLarge differences between k-parameter and the k-effective of the network represents a stress on the current state of the network. More pools under the control of a smaller number of groups does not improve decentralization and in fact takes more time and resources to propagate the blockchain to more relays and nodes. A k-effective of 100.0 with an adjustable k-parameter informed by and leading k-effective is reasonable long term goal. If the k-effective becomes 100.0, it is likely that the Nakamoto Coefficient will be 50.\nPrinciples matter. -CH\nEverybody in the community should be treated fairly from tiny starfish delegators (1-2k ) to massive blue whales ( 100M ) and exchanges. Everybody in the community should have the opportunity to on average earn the same yield and there should not be two classes of stakeholders. There should be a very clear cause and effect relationship between the input parameters and the resulting K-effective decentralization. The equation must require, not incentivize, pool operators to pledge to support community delegation and sybil protection. The decentralization result will be quantified in terms of group decentralization of block production. Implementation should be smooth, easy, clear, and beneficial for all stakeholders and operators. A new reward equation should be computationally simple and elegant.\nEverybody in the community should be treated fairly from tiny starfish delegators (1-2k ) to massive blue whales ( 100M ) and exchanges.\nEverybody in the community should have the opportunity to on average earn the same yield and there should not be two classes of stakeholders.\nThere should be a very clear cause and effect relationship between the input parameters and the resulting K-effective decentralization.\nThe equation must require, not incentivize, pool operators to pledge to support community delegation and sybil protection.\nThe decentralization result will be quantified in terms of group decentralization of block production.\nImplementation should be smooth, easy, clear, and beneficial for all stakeholders and operators.\nA new reward equation should be computationally simple and elegant.\nk, k-parameter\nThe desired number of stake pools for decentralization, which inversely sets a \"soft cap\" on the stake pools saturation limit (size). The pool size is determined by total circulating supply divided by k number of pools. Pool size is encouraged to converge toward k for optimal staking rewards, in theory.\nk-effective\nThe average resulting decentralization or \"effective decentralization\" taking into account groups or entities of stake pools. One group could be running multiple stake pools, which lowers the realistic, effective decentralization.\nNakamoto Coefficient, Minimum Attack Vector (MAV)\nThe minimum number of entities in a given subsystem required to get to 51 of the total capacity, or more simply, how many stake pool groups it would take to gain 51 control of the blockchain. This is a simple yet decisive decentralization measurement. It is not limited to pools, but any attack vector. However, for the scope of this paper it is assumed to be pools only. The Nakamoto Coefficient is approximately half of k-effective rounded up to the nearest integer.\nSybil Attack\nAn attack where the attacker pretends to be an honest pool or pools to gather stake and power in the network. The attacker can create an infinite number of pools in his effort to maximize his own power in the network.\nPledge\nThe amount of stake pledged by a stake pool in ADA. This combats a sybil attack with an economic cost of a \"down-payment\" or stake pool pledge when creating a new stake pool.\nPledge Leverage, L\nThe ratio between an entity s pool stake and pledge defined by the parameter L. A leverage-based stake pool saturation is based on pledge rather than a fixed size driven by k-parameter. An entity having high leverage means its total stake is much higher than its pledge.\nVRF Score\nVerifiable Random Function (VRF) score is essentially the lowest random number generated by the stake pools that determines what pool won the slot leader battle to mint a block. The lowest random number VFR score generated by a pool wins the block.\nPrivate vs Public Stake Pools\nCardano stake pool operators can be either private, using only their own self-bonded stake, or public, by accepting delegation. A private stake pool retains 100 of the pool s profits but must also provide 100 of the pool s staking power. Most stake pool explorers will not give the pool visibility as a staking option because they are already fully saturated. Private stake pool operators optimize rewards by pledging the whole stake cap for their pool. If you have enough ADA to meet the saturation point alone, your rewards will be 100 of the potential earned when adjusted for the a0 parameter. Public stake pools that do not have full pledge must make up their stake from public delegation.\nThe proposed reward retains the function of kkk for limiting rewards based on stake and introduces parameter LLL for enforcing reward limits based on pledge leverage. The equation equally balances both reward parameters. The pledge leverage parameter LLL is intended to range from 10,000.0 down to 1.0. An LLL value of 100.0 would require pools to pledge 1.0 of stake and an LLL of 1.0 would require all pools to be 100.0 pledged.\nThe new equation is computationally simple and purposefully does not use logarithms, exponents, or geometric curves. Instead of an incentive based tradeoff between egalitarian rewards and a perceived Sybil resilience the new equation enforces both egalitarian rewards and pledge-based Sybil resilience. A simple flat egalitarian yield ceiling with pledge leverage enforcement for Sybil defense has a profound psychological effect: Stakeholders know there is no way to game the system for yield, either individually or collectively with governance, and pledge is absolutely mandatory. Without any engineered bias Cardano decentralization would converge to the diversity of the underlying community, services, and stakeholder distribution. If this proposal is eventually adopted changes in community diversity, not changes in a formula or parameters, would change decentralization.\nThe LLL parameter will establish a maximum pledge leverage before limiting rewards, similar to the k parameter for pool size. Pledge leverage establishes a different saturation point for each pool based on its pledge. The LLL parameter enforces the principle that growing pledge is absolutely required to support growing delegation. This change will directly align the LLL parameter for protecting the network from Sybil behavior. The pledge leverage factor provides an enforcable limit on sybil actors and their maximum return on invested capital. Pledge will not be a statistically unnoticed slight incentive used only by a handful of large private stakeholders. Community governance to adjust leverage factor LLL would be the preferred mechanism to constrain sybil behavior.\nThe new LLL parameter will range from 10,000.0 to 1.0. The initial value of the maximum pledge leverage ratio a0a0a0 should initially be set conservatively high ( =100.0) and optionally decreased slightly over time to a healthy equilibrium by community governance. At (L=100,k=500)(L=100, k=500)(L=100,k=500) approximately 680k pledge would be required to support a fully saturated pool.\nThe proposed reward formula should be visualized on a linear(yield) vs log(saturation) scale independent of kkk. The chart below shows the field of possible outcomes for various levels of pledge and stake spanning more than 3 orders of magnitude. The effect of LLL becomes obvious, pool saturation will be limited first by pledge amount and then eventually by kkk. A very important feature of this relationship is that 0 pledge will always result in 0 rewards. At L=100.0L=100.0L=100.0 to support a 100.0 saturated stake pool 1.0 pledge will be required.\nThe new equation is purposefully designed so that stakeholders of dramatically different size can all reach the exact same maximum yield. The yield ceiling feature prevents the formation of two classes of stakeholders and removes the economic benefits of custodial centralization. The yield ceiling is the egalitarian reward described but not implemented by the original paper.\nWith the minFee 30 once a pool grows to 0.5 of saturation the intermittent rewards will, on average, provide a competitive yield for delegators at 5.0 . At k=500k=500k=500 and L=100L=100L=100 this corresponds to a pool size 500k with a minimum pledge of only 5k . The yield ceiling is also compatible with a potential future implementation of the Conclave collective stake pool concept. Because of the yield ceiling large collective stake pools will only provide more predictable returns, not a materially larger yield which would compete with smaller independent pools.\nPool ranking scores in wallet browsers have a significant and powerful game theory impact on delegator pool choices, yet it is often overlooked and not transparent in wallets.\nWhen pledge becomes the most important factor for total pool size, lower leverage factors are more desirable. Lists should be sorted by leverage and presented in an descending order with the lowest leverage pools first.\nThe recommended ranking equation starts with the highest score of 10. The pools are down-ranked solely based on leverage, saturation, and fee factors.\n//equation ranking_score = 10 - max{ leverage_factor, saturation_factor } - fee_factor //variables leverage_factor = 10 * (pool_leverage/L)^A saturation_factor = 2 * (pool_stake/(saturation_stake * C))^B fee_factor = D * pool_fee_margin //parameters - A is 2.0, has range (0,10.0), can be tweaked - B is 5.0, has range (0,10.0), can be tweaked - C is 0.9, has range (0,2.0), can be tweaked - D is 50, has range (0-100) ish to be harsh, can be tweaked - pool_leverage = delegation / pledge - pool_stake = pledge + delegation - saturation_stake = total_live_stake/k (e.g. 68M₳ \"soft-cap upper limit\") - pool_fee_margin is in range (0-100)% (fixed fee + margin combined)\n//equation ranking_score = 10 - max{ leverage_factor, saturation_factor } - fee_factor //variables leverage_factor = 10 * (pool_leverage/L)^A saturation_factor = 2 * (pool_stake/(saturation_stake * C))^B fee_factor = D * pool_fee_margin //parameters - A is 2.0, has range (0,10.0), can be tweaked - B is 5.0, has range (0,10.0), can be tweaked - C is 0.9, has range (0,2.0), can be tweaked - D is 50, has range (0-100) ish to be harsh, can be tweaked - pool_leverage = delegation / pledge - pool_stake = pledge + delegation - saturation_stake = total_live_stake/k (e.g. 68M₳ \"soft-cap upper limit\") - pool_fee_margin is in range (0-100)% (fixed fee + margin combined)\nTo evaluate rank using the current reward scheme:\n//current reward scheme only ranking_score = 10 - max{ leverage_factor , saturation_factor } - fee_factor - fixed_fee_factor //variables //if fee = minFee, term drops out //if fee > minFee, term nonzero but loses relevance w/ increased stake //fixed fee matters less to rewards as stake grows, so too here fixed_fee_factor = E * (fee-minFee) / stake //parameters - E is 100, has range (0,100_000), can be tweaked - fee cannot be less than minFee\n//current reward scheme only ranking_score = 10 - max{ leverage_factor , saturation_factor } - fee_factor - fixed_fee_factor //variables //if fee = minFee, term drops out //if fee > minFee, term nonzero but loses relevance w/ increased stake //fixed fee matters less to rewards as stake grows, so too here fixed_fee_factor = E * (fee-minFee) / stake //parameters - E is 100, has range (0,100_000), can be tweaked - fee cannot be less than minFee\nA leverage-based ranking system will create interesting pool free market business dynamics. It's envisioned a pool will undergo \"business life-cycles\" based on price supply and demand (fees) and leverage (pledge raising to grow the business) as described below. Currently, the yield-based pool ranking creates a market based on fees (driving yield), but not leverage, thus pools can grow indefinitely with no \"leverage ranking costs\".\nA Leverage-Based Pool's Life-Cycle\nStart-up: Low pledge, very low leverage, very low fees, very highly ranked. Growth: Gain delegation, leverage grows, rewards gained, high rank. Sustainment: Raise fees, sustainable operation, moderate ranked. Accumulation: Delegator surplus, high leverage, raise fees, high rewards, low rank. Reset: Lower fees, increase pledge, regain low leverage and high rank. Repeat.\nStart-up: Low pledge, very low leverage, very low fees, very highly ranked.\nGrowth: Gain delegation, leverage grows, rewards gained, high rank.\nSustainment: Raise fees, sustainable operation, moderate ranked.\nAccumulation: Delegator surplus, high leverage, raise fees, high rewards, low rank.\nReset: Lower fees, increase pledge, regain low leverage and high rank. Repeat.\nWith the need to grow pledge to grow pool size and regain better ranking, the business cycle is cyclical indefinitely until the pool reaches max saturation based on the k-parameter.\nAn oversaturation attack occurs when a large stakeholder would significantly oversaturate a pool with the intention of pressuring current and future delegators to select a new pool. Limiting pool sizes based on pledge leverage provides a robust sybil enforcement, however lower saturation limits also lowers the cost of an oversaturation attack. The ultimate mitigation is not a formula modification, but instead 2-way at-will delegator acceptance staking which would allow a stake pool operator to exclude or 'veto' select stake keys. This solution has been previously discussed in the community and would benefit from its own CIP.\nThe new formula will not decrease the yields of any large stakeholders pledging to private pools. Large ADA stakeholders such as exchanges, liquidity pools, or smart contracts would not be required to pledge a vast majority of those holdings to earn yields currently only achievable with fully pledged pools. This property improves overall liquidity. The large stakeholders who voluntarily choose to divide their stake to dozens of community pools will also achieve more fault tolerance than self-operating a small number of centralized private pools. A number of wallets including Eternl are offering the capability to divide stake delegation between many pools.\nThis design decision aligns the interest of the largest stakeholders with the interests of the whole community. The only economic motivation remaining for groups with large stake including founder(s), founding organizations, exchanges, investment capital, trusts, and venture capital would be to enhance the value of the entire network by dividing delegation to secure diversification. Large stakeholders being able to divide stake without penalty to many smaller community pools will have the greatest impact on improving the effective decentralization and Nakamoto Coefficient of Cardano.\nhttps://cips.cardano.org/cips/cip7/ https://github.com/cardano-foundation/CIPs/pull/163 https://github.com/cardano-foundation/CIPs/pull/229 https://forum.cardano.org/t/cip-leverage-based-saturation-and-pledge-benefit/95632 https://forum.cardano.org/t/cip-change-the-reward-formula/33615 https://forum.cardano.org/t/an-alternative-to-a0-and-k/42784 https://dynamicstrategies.io/crewardcalculator\nhttps://cips.cardano.org/cips/cip7/\nhttps://github.com/cardano-foundation/CIPs/pull/163\nhttps://github.com/cardano-foundation/CIPs/pull/229\nhttps://forum.cardano.org/t/cip-leverage-based-saturation-and-pledge-benefit/95632\nhttps://forum.cardano.org/t/cip-change-the-reward-formula/33615\nhttps://forum.cardano.org/t/an-alternative-to-a0-and-k/42784\nhttps://dynamicstrategies.io/crewardcalculator\nTo validate any reward equation simulations must consider that an entity can choose to delegate to another entity, operate one stake pool, or operate many stake pools. Any new equation should be compared to the current equation with a0=0.3a0=0.3a0=0.3 and the current equation with minFee=0, a0=0.0a0=0.0a0=0.0. A large and increasing number of entities (1k - 1B) should be simulated for each trial of each equation. Additionally, during each epoch of each simulation for each equation block production should be sampled VRF-style from a discrete binomial or skellam distribution. Block production and rewards have statistical uncertainty.\nEach equation: Each epoch: sample block production (rewards) per pool Each entity may choose to: create/retire 1 or more pools adjust the fee/margin structure of their pool(s) delegate to the pool of a different entity\nEach equation: Each epoch: sample block production (rewards) per pool Each entity may choose to: create/retire 1 or more pools adjust the fee/margin structure of their pool(s) delegate to the pool of a different entity\nFor each equation in consideration the average (and variation) of the nakamoto coefficient, k-effective coefficient (or an entity/group based equivalent), and a sybil coefficient shall be computed for every epoch until conclusion. The sybil coefficient would quantify the fraction of stake controlled by all entities operating multiple pools excluding regulated businesses such as exchanges.\nParameter Ranges: k=[100 2000]k = [100 - 2000]k=[100 2000] L=[10 10000]L = [10 - 10000]L=[10 10000] a0=[0.0 10.0]a0 = [0.0 - 10.0]a0=[0.0 10.0] bias=[0.00 0.95]bias = [0.00 - 0.95]bias=[0.00 0.95]\nWhere equations and independent variables to test are:\nEq (1) CIP-7 Rewards: Legacy rewards equation with n-root curved pledge benefit\nEq (2) Repurposed lambda_alt_prime, and legacy sigma_prime\nEq (3) Lambda_alt: N-root curve pledge benefit\nEq (4) Crossover: An expression called crossover represents the point where the new curve crosses the line of the original curve, and the benefit in the new and original equations is identical. The crossover_factor is a divisor of saturation that calculates the pledge where the curve crosses the line. crossover_factor 1.\nThe crossover_factor is a divisor of saturation that calculates the pledge where the curve crosses the line. crossover_factor 1.\nEq (5) Curve_root: The n-root curve exponent used in Eq 2 to alter the rewards. 1 = linear, 2 = square root, 3 = cube root, etc.\nAnd the recommended testing permutations are:\nFor crossover in crossover_factor range [10,20,50]: For n in curved_root range [2,3,4]:\nFor crossover in crossover_factor range [10,20,50]: For n in curved_root range [2,3,4]:\nwith k=150k = 150k=150, bias=0.05bias=0.05bias=0.05, and L=50L = 50L=50\nwith k=150k = 150k=150, bias=0.05bias=0.05bias=0.05, and L=100L = 100L=100\nwith k=150k = 150k=150, bias=0.05bias=0.05bias=0.05, and L=1000L = 1000L=1000\nwith k=500k = 500k=500, bias=0.05bias=0.05bias=0.05, and L=50L = 50L=50\nwith k=500k = 500k=500, bias=0.05bias=0.05bias=0.05, and L=100L = 100L=100\nwith k=500k = 500k=500, bias=0.05bias=0.05bias=0.05, and L=1000L = 1000L=1000\nwith k=1000k = 1000k=1000, bias=0.05bias=0.05bias=0.05, and L=50L = 50L=50\nwith k=1000k = 1000k=1000, bias=0.05bias=0.05bias=0.05, and L=100L = 100L=100\nwith k=1000k = 1000k=1000, bias=0.05bias=0.05bias=0.05, and L=1000L = 1000L=1000\n(incomplete)\nMahmoud Nimer's proposed reward equation presented in the Ada Link's Stakepool Pledge Influence in Stake Rewards Distribution Paper seeks to compare pool pledge to total network pool pledge, and reward pool pledge growth relative to stake pool size growth.\nWhere:\nEq (1) The Nimer Rewards Equation\nEq (2) s_hat_primt: The pledge saturation equation\nEq (3) s_hat: Pool pledge / Total Network Pool Pledge\nThe change for the new equation is implemented in the ledger and has been activated through a hard fork.\nImplementation will occur in two distinct phases with the first phase being only parameter changes requiring no hard fork. During this first phase any changes will be reversible. The second phase will require a hard fork. The implementation of this proposal must be smooth, justified, staged, deliberate, and well communicated through advertising and education.\nEach change in the implementation schedule should include clear communication to the community on expectations. Transparent education on how the parameters will work and the effect on rewards is important.\nGet statements of support from a large fraction of the Cardano community.\nGet statements of support from a large fraction of the Cardano community.\nAlthough we haven't entered the Voltaire era yet, we should still reach community concensus.\nReduce minFee from 340 to 0 .\nReduce minFee from 340 to 0 .\nReducing the mandatory minimum fee to 0 will allow smaller pools to become more competitive while allowing each individual pool to select an appropriate fixed fee.\nCreate an open source simulation suite to benchmark all of the possible equation forms and parameter variations. Competitively evaluate, compare, contrast, and analyze the results of all possible equation forms and parameter variations. Include the community in selecting the winning equation form and parameter set. HARDFORK implementation of the new equation and parameter set. Measure decentralization, gather community feedback. Slightly adjust parameters by at most approximately 5 every 10 epochs until the end of Voltaire. After Voltaire adjust parameters annually/bi-annually by community vote.\nCreate an open source simulation suite to benchmark all of the possible equation forms and parameter variations.\nCreate an open source simulation suite to benchmark all of the possible equation forms and parameter variations.\nCompetitively evaluate, compare, contrast, and analyze the results of all possible equation forms and parameter variations.\nCompetitively evaluate, compare, contrast, and analyze the results of all possible equation forms and parameter variations.\nInclude the community in selecting the winning equation form and parameter set.\nInclude the community in selecting the winning equation form and parameter set.\nHARDFORK implementation of the new equation and parameter set.\nHARDFORK implementation of the new equation and parameter set.\nMeasure decentralization, gather community feedback.\nMeasure decentralization, gather community feedback.\nSlightly adjust parameters by at most approximately 5 every 10 epochs until the end of Voltaire.\nSlightly adjust parameters by at most approximately 5 every 10 epochs until the end of Voltaire.\nAfter Voltaire adjust parameters annually/bi-annually by community vote.\nAfter Voltaire adjust parameters annually/bi-annually by community vote.\nAggelos Kiayias\nAikaterini-Panagiota Stouka\nCharles Hoskinson\nChristia Ovezik\nColin Edwards\nDuncan Coutts\nElias Koutsoupias\nFrancisco Landino\nLars Br njes\nMark Stopka\nPhilipp Kant\nShawn McMurdo\nTobias Francee\nTom Stafford\nThis CIP is licensed under CC-BY-4.0\nCopyright 2022 - Michael Liesenfelt\n2023 Cardano Foundation\n\n---\n\nCIP-0052 | Cardano audit best practice guidelines\n\nThese guidelines describe the audit process in general before setting out for DApp developers what information they will need to supply to auditors as part of the process. These are guidelines rather than requirements, and different auditors may engage differently, providing complementary services. The guidelines aim to establish a common baseline, including alternative ways of satisfying high-level requirements. Appendices provide (1) a glossary, (2) an audit FAQ, (3) a list of auditors for Cardano, and (4) a sample audit report.\nThis CIP aims to promote the process of audit for DApps on Cardano, to improve the overall standard of assurance of DApps built for Cardano, and thus to contribute the improvement of the wider Cardano ecosystem.\nDApp users seek assurance about DApps that they wish to use. This comes from running automated tools on DApps and their components, as well as by audit of complete DApps. Secure evidence of these tool (level 1) and audit (level 2) results is provided by a certification service, and made available to users through a service such as a DApp store or wallet. In the longer term, results of formal verification (level 3) will also form part of the certification process.\nDApp developers seek to drive adoption through their DApps being certified. Wallets and DApp stores are enhanced by providing certification services, and the wider Cardano ecosystem is strengthened through certification becoming widespread. Best practice standards can be developed by the audit and tooling communities, and systematised by the Cardano Foundation. This document is a first step in that direction.\nAssurance can only ever be partial: a DApp can be shown to have some good features and to avoid some bad ones, but this is not a guarantee that using a DApp will not have negative consequences for a user. This is not simply because tools and audits cannot give complete coverage, but also because attacks may come at lower (e.g. network, browser) or higher (e.g. crypto-economic) levels than addressed here.\nThese are guidelines rather than requirements, and different auditors may engage differently, providing complementary services. The guidelines aim to establish a common baseline, including alternative ways of satisfying high-level requirements. Appendices provide (1) a glossary, (2) an audit FAQ, (3) a list of auditors for Cardano, and (4) a sample audit report.\n(Smart) contract: a program that runs on blockchain. In the case of Cardano, this will be a Plutus program that will contain Plutus Core components that run on blockchain as well as other code that runs off chain. All auditors will scrutinise on-chain code, some will examine off-chain code too.Some auditors might also provide orthogonal services eg. auditing a zero-knowledge protocol or an economic model.\nDApp: a complete distributed application that runs on blockchain. This will include off-chain code written in other languages, e.g. running in a browser, and will integrate or link with other services such as wallets and oracles.\nAssurance: the process of establishing various properties of systems, both positive (it does this) and negative (it doesn t do this), with different degrees of certainty.\nAudit: the process of establishing assurance by means of manual examination of artefacts, systems, processes etc. Can involve some tooling, but it is a human-led process.\nTooling: using automated processes to establish degrees of assurance of systems. Tools may be run on target DApps by parties providing such a service.\nEvidence: artefacts coming out of tooling and audit that can support assertions of assurance of systems. Examples include formal proofs, test suites, prior counterexamples and so on, as well as audit reports.\nCertification: the process and technology of giving to DApp end-users and developers secure evidence of forms of assurance about DApps and their component smart contracts. In the case of Cardano our approach is to provide information on chain as transaction metadata.\nNote: certification has also been used informally to cover the combined process of testing, audit, verification and providing evidence of these, as in level 1 certification . If it is felt that it is too confusing to use the term in both senses, then another term should be found, e.g. evidencing or witnessing.\nComponent: a discrete part of the complete system supplied by a third party, such as a wallet, (part of) which will be run on the user s system.\nService: a part of the system, such as Blockfrost, that is provided by an online server accessed through a well-defined protocol.\nScope: the parts of the DApp that are subject to audit. A repository may contain much more than Plutus code. Some audits will look at the complete app, some will just concentrate on on-chain transactions, others on the web interface around it. The possible scopes are divided in three main categories:\nOn-chain\nOff-chain\nContext: the business and economic models for the DApp\nDeployment: once a DApp is developed it is deployed by submitting the relevant on-chain scripts onto the Cardano blockchain as transaction validator scripts. The scripts being deployed might be the scripts that have been audited, or be instances of them in the case that the audited scripts are parametric.\nAn audit is a comprehensive investigation of a DApp that provides an in-depth analysis on bugs, vulnerabilities, code quality and correctness of implementation. An audit does not necessarily analyse completed DApps and often will instead analyse a fragment of a DApp. For example many auditors will only analyse the on-chain code of a DApp.\nAudits are provided by companies that specialise in the area of the developed DApp. In this case it will be experts on the Cardano blockchain and Plutus smart contracts.\nThis first part of the process is tendering, where developers will need to provide preliminary information about their DApp to candidate auditors, as described below.\nOnce a contract is agreed, the next step in the process is to provide the auditors with the necessary information to perform the audit as set out in the guidelines below. Auditors may work with developers to ensure that the documentation and other materials for audit are prepared to the standard that is required for the audit to take place. Different auditors will have different requirements, but the guidelines below establish the minimum requirements.\nAuditors will typically produce a first version of a report, which the developer can use as a guide to improving their DApp, before submitting changes to produce the final version of the code on which the final, published report is based. Once a DApp is audited, it will be deployed on the Cardano blockchain.\nA developer should contact an auditor when they have a final working version of a DApp or fragment of a DApp that they want to have audited. Once the contract is agreed, the developer will need to provide the auditor with a final version of the DApp for audit.\nHowever, it is recommended to contact a potential auditor as early as possible because many auditors also provide consultation services for the design and development of the DApps. A developer is encouraged to contact the auditor as early as possible so as to mitigate any design issues which may be very hard if not impossible to fix. Early contact is also encouraged because securing a time slot with an auditor in advance shortens the DApp s overall time to market, finally, early contact allows for scheduling and potentially avoids long delays between starting and completing an audit.\nAll audits will examine the on-chain code that is used to validate transactions submitted to advance the smart contracts that constitute part of a DApp. Audit may also cover more of the code in a DApp, including on- and off-chain code written in Plutus, as well as other languages, e.g. JavaScript running in a browser.\nAn audit is not a guarantee of unbreakable security nor a way to offset trust or responsibility. An audit will provide an in-depth review of the source code of a DApp. The audit will provide a comprehensive code review detailing any found vulnerabilities, comments on the quality of code as well as an analysis of the implementation in regards to the supplied specification. An audit cannot guarantee that all possible vulnerabilities will be found or that the deployed DApp will perform as intended. This is especially true in the cases where an audit only looks at a fragment of a DApp or when a DApp has been updated.\nDApps are used by DApp users, and built by DApp developers. Audit is performed by audit companies, using tools developed by themselves and other tool developers. Tooling can be run by tool service providers, and evidence of those and other results produced by certification providers. Audit is also impacted by components (e.g. light wallets) and services (e.g. blockfrost) provided by ecosystem members. Standards can be developed by industry consortia or governance organisations (e.g. the Cardano Foundation). In the widest sense, all holders of Ada stand to benefit from Cardano building an expectation that DApps are certified.\nIn order to provide a quote for audit, developers will need to supply\nA specification of the DApp to be audited (more details below).\nThe scope of the audit.\nAn estimate of the scale of the audit work, e.g. the number of lines in the on-chain code to be audited, or the code itself, in its current state of development.\nIn order to be audited, developers will need to supply the following documentation.\nSubmitters shall provide specification and design documents that describe in a precise and unambiguous way the architecture, interfaces and requirements characterising the DApp.\nThe documentation shall identify the expected behaviour of the code, given without direct reference to the code itself. The description should also include high-level examples demonstrating use cases of the DApp. All assumptions about how the DApp will be used will be described. The documentation shall identify and document all the interfaces with other components and services.\nSubmitters might also wish to explain mitigating actions that they have taken to protect against potential failures and attacks to the DApp.\nThe format of transactions accepted by the smart contracts should be specified using the template provided in the auxiliary document Tx-spec.md.\nTx-spec.md\nThe document should clearly specify the properties to be satisfied by the smart contract.\nProperties shall be as extensive as possible and ideally would cover functionality, robustness, safety, liveness and efficiency, e.g. cost of execution, of the smart contract.\nDiscussion should describe whether any of the properties addresses common vulnerabilities pertaining to Cardano blockchain or the smart contract domain in general.\nA formal specification is recommended but not mandatory.\nFor off-chain analysis additional information should be provided for the components and services interfaced:\nFor all interfacing components, a specification shall be given detailing their expected behaviour in relation to the DApp, including any assumptions, constraints and requirements of the component that are expected to hold by the DApp developers.\nIt also shall be stated whether any of the interfacing components have been certified.\nIdeally, submitters should submit a description of how the DApp has been tested, the results of the tests, and details of how those test results can be replicated.In particular:\nThe test cases and their results shall be recorded or reproducible in a machine-readable format to facilitate subsequent analysis.\nTests are to be performed for each targeted platform (browser, wallet etc).\nThe identity, configuration and version of all test components involved shall be documented.\nThe checksum and version of the DApp submitted for certification shall correspond to the same version making the subject of the test report.\nAn evaluation of the test coverage and test completion should be provided.\nIn the case that off-chain code is included in the scope of the audit, testing should be able to assess the performance and robustness of the DApp against significant throughput, under substantial workload, and in the scenario of a DoS attack.\nA final version of the source code should be provided that works with the use cases specified in the documentation. Information needs to be provided to allow the DApp to be built in an unambiguous and reproducible way, including any external components and services that the DApp uses. This could be in the form of\nThe URL for a commit to a repository.\nBuild information for the DApp: a pure nix build is particularly suitable, since this will identify versions of libraries, compilers, OS, etc.\nFor the on-chain code for a DApp, the specific contracts to be audited.\nVersioning information needs to be given in a way that allows end users of a DApp to determine whether or not the version of the DApp that they are using is covered by certification information held on blockchain.\nThis can be done in a number of different ways, depending on the type of audit. These include:\nThe hash of a URL for a commit to a publicly-available repository. A hash that identifies the files that contain the on-chain code that has been audited, e,g computing, from the root of the repository, listed in lexicographic order.\nThe hash of a URL for a commit to a publicly-available repository.\nA hash that identifies the files that contain the on-chain code that has been audited, e,g computing, from the root of the repository, listed in lexicographic order.\nIt is planned that DApps will be registered on the Cardano blockchain. This is currently under discussion. Once that discussion has been settled, it will also be possible to provide on-chain evidence of audit, linked to a registered entity. The mechanism for this is described in a separate document which it is intended to make into another CIP. A current draft of that document is here: Proof of Audit document.\nAuditors shall be able to carry out the following activities:\nReview the requirement specification document against the intended environment and use so as to: Identify any inconsistencies, security flaws or incomplete requirements Identify any implicit assumptions and whether they are justifiable or not Evaluate the adequacy of strategies applied by the submitter to guarantee the consistency, correctness and completeness of the requirements Identify a threat model to guarantee that any identified mitigations are indeed appropriate against a list of possible vulnerabilities for Cardano smart contracts, and which is currently being finalised.\nIdentify any inconsistencies, security flaws or incomplete requirements\nIdentify any implicit assumptions and whether they are justifiable or not\nEvaluate the adequacy of strategies applied by the submitter to guarantee the consistency, correctness and completeness of the requirements\nIdentify a threat model to guarantee that any identified mitigations are indeed appropriate against a list of possible vulnerabilities for Cardano smart contracts, and which is currently being finalised.\nThe source code shall be audited by manual and/or automated means. In particular, The source code shall be reviewed against the requirements to ensure that all of these are properly taken into account and completely fulfilled. The adequacy of the source code documentation and traceability with the requirements shall be assessed. The source code shall be free from coding patterns/programming mistakes that may introduce exploitable vulnerabilities/failures leading to security issues.\nThe source code shall be reviewed against the requirements to ensure that all of these are properly taken into account and completely fulfilled.\nThe adequacy of the source code documentation and traceability with the requirements shall be assessed.\nThe source code shall be free from coding patterns/programming mistakes that may introduce exploitable vulnerabilities/failures leading to security issues.\nProduce a detailed audit report describing scope, methodology, and results categorised by severity. In particular, Any discrepancies, deviations or spotted vulnerabilities shall be described and classified with an appropriate severity level. Recommendations to rectify the identified deficiencies shall also be provided whenever appropriate. When automated tools are used as a replacement for manual review/code inspection, they shall be documented or referenced. Note that it s the responsibility of the auditor to ensure that such tooling may not exhibit potential failures that can adversely affect the review outcome. Any strategies/methodologies used to assess the consistency, correctness and completeness of the requirements shall also be documented or referenced.\nAny discrepancies, deviations or spotted vulnerabilities shall be described and classified with an appropriate severity level. Recommendations to rectify the identified deficiencies shall also be provided whenever appropriate.\nWhen automated tools are used as a replacement for manual review/code inspection, they shall be documented or referenced. Note that it s the responsibility of the auditor to ensure that such tooling may not exhibit potential failures that can adversely affect the review outcome.\nAny strategies/methodologies used to assess the consistency, correctness and completeness of the requirements shall also be documented or referenced.\nAuditors shall provide credentials for the following competencies:\nThey shall have an in-depth knowledge of the syntax and semantics of the smart contract language to be audited, the underlying blockchain technology and associated computation and cost models.\nThey shall be competent in the strategies and methods used to elaborate threat models.\nThey shall be competent in assessing the suitability of methods (or combination of methods) used to justify the consistency, correctness and completeness of requirements against the list of common vulnerabilities pertinent to the smart contract domain and to guarantee (as far as possible) the absence of security flaws in the design.\nThey shall be competent in various test and verification methods and have solid background in the various test coverage criteria (i.e., statement, data flow, branching, compound condition, MC/DC and Path).\nThey shall also be able to assess whether the set of test cases produced for each specific test objective/property are sufficient enough to cover all the possible functional cases.\nThey shall have analytical and critical thinking ability pertaining to the: deployment and execution of smart contracts on the underlying blockchain technology; Potential attacks or sequence of events relative to the smart contract s logic that may lead to an unsafe state or invalidate some of the fundamental properties of the contract.\ndeployment and execution of smart contracts on the underlying blockchain technology;\nPotential attacks or sequence of events relative to the smart contract s logic that may lead to an unsafe state or invalidate some of the fundamental properties of the contract.\nThey shall be able to judge the adequacy of the justifications provided by submitters w.r.t., development processes (e.g., requirement elicitation techniques, threat models, test objectives and test cases, coding standard, quality management, etc) for Level 2 certification.\nDisclosure It is common but not universal practice for disclosure/publication of audit report, for example as a part of a responsible disclosure policy. A typical policy would be to publish a report after a certain period (e.g. 30-90 day) or at the point that a DApp goes live, whichever is earlier.\nThese guidelines are the result of a process of discussion between IOG staff and members of the audit and academic communities over a series of online meetings in February and March 2022. Audit organisations involved include Tweag, WellTyped, Certik, Runtime Verification, BT Block, MLabs, Quviq and Hachi/Meld, all of which supported the guidelines outlined here.\nEvidence that Cardano audits are being performed according to this proposed standard, by reference to specific audit(s) citing CIP-0052 and containing these audit elements.\nInitial set of Cardano auditors provided with CIP, with others added afterward along with contact information and verification keys.\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0054 | Cardano Smart NFTs\n\nThis CIP specifies a standard for an API which should be provided to Javascript NFTs, it also defines some additions to the 721 metadata standard defined in CIP-0025 which allow an NFT to specify it would like to receive certain current information from the blockchain.\nCurrently if an NFT creator wishes to change or otherwise evolve their NFT after minting, they must burn the token and re-mint. It would be very nice if the user were able to modify their NFT simply by sending it to themselves with some extra data contained in the new transaction metadata. This would allow implementation of something like a ROM+RAM concept, where you have the original immutable part of the NFT (in Cardano s case represented by the original 721 key from the mint transaction), and you also have a mutable part represented by any subsequent transaction metadata.\nIt would also be nice to be able to retrieve data that has been previously committed to the blockchain, separately to the NFT which wishes to access it. This would be useful for retrieving oracle data such as current Ada price quotes as well as for allowing an NFT to import another NFT s data.\nFurther to this - for on-chain programatically generated NFTs, it makes sense to mint the code to render the NFT as one token, and then have the individual NFTs contain only the input variables for that code. This CIP specifies an additional metadata option which specifies that an NFT should be rendered by another token - this will massively reduce code duplication in on-chain NFTs.\nThis combination of functionality enables many exciting new possibilities with on-chain NFTs.\nMinting metadata for Smart NFTs based on the existing CIP-0025 standard:\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array>, \"mediaType\": \"image/<mime_sub_type>\", \"description\": <string | array>, \"files\": [{ \"id\": <string> \"name\": <string>, \"mediaType\": <mime_type>, \"src\": <uri | array>, <other_properties> }], \"uses\": { \"transactions\": <string | array>, \"tokens\": <string | array>, \"renderer\": <string> } } }, \"version\": \"1.0\" } }\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array>, \"mediaType\": \"image/<mime_sub_type>\", \"description\": <string | array>, \"files\": [{ \"id\": <string> \"name\": <string>, \"mediaType\": <mime_type>, \"src\": <uri | array>, <other_properties> }], \"uses\": { \"transactions\": <string | array>, \"tokens\": <string | array>, \"renderer\": <string> } } }, \"version\": \"1.0\" } }\nHere we have added the uses key any future additions to the Smart NFT API can be implemented by adding additional keys here. We've also added an additional field to the files array - this is to specify a unique identifier to enable the files to be referenced by the Javascript API below.\nTo enable evolving NFTs where the NFT monitors a transaction history and changes in response to new transaction metadata, we define a sub-key called transactions , which can contain either a string or an array, specifying the tokens or addresses the NFT wishes to receive the transaction history for. These transaction histories will be provided to the NFT via the Javascript API detailed below.\nWe also define a special keyword own which can be used to monitor the NFT s own transaction history. So if we wish to create an evolvable NFT that can respond to additions to its own transaction history, the uses key within the metadata would look like:\n\"uses\": { \"transactions\": \"own\" }\n\"uses\": { \"transactions\": \"own\" }\nIf you wanted to create an evolving NFT which monitors its own transaction history, as well as that of an external smart contract address, the metadata would look like this:\n\"uses\": { \"transactions\": [ \"own\", \"addr1wywukn5q6lxsa5uymffh2esuk8s8fel7a0tna63rdntgrysv0f3ms\" ] }\n\"uses\": { \"transactions\": [ \"own\", \"addr1wywukn5q6lxsa5uymffh2esuk8s8fel7a0tna63rdntgrysv0f3ms\" ] }\nFinally, we also provide the option to receive the transaction history for a specific token other than the NFT itself (generally this is intended to enable import of Oracle data or control data from an external source although monitoring an address transaction history could also be used for that).\nWhen specifying an external token to monitor, you should do so via the token s fingerprint as in this example:\n\"uses\": { \"transactions\": [ \"own\", \"asset1frc5wn889lcmj5y943mcn7tl8psk98mc480v3j\" ] }\n\"uses\": { \"transactions\": [ \"own\", \"asset1frc5wn889lcmj5y943mcn7tl8psk98mc480v3j\" ] }\nTo enable modifier tokens - (that is, a token which you can hold alongside a Smart NFT which changes the Smart NFT's appearance or behaviour in some way); we provide a way for the Smart NFT to monitor the tokens held by a specific address. Similarly to the transactions key, the tokens key will also accept either a string or array.\nIn this case we define the own keyword to mean the tokens held by the same stake key that currently holds the Smart NFT itself.\nFor example, to create an evolvable NFT which also supports modifier tokens, the uses block would look like this:\n\"uses\": { \"transactions\": \"own\", \"tokens\": \"own\" }\n\"uses\": { \"transactions\": \"own\", \"tokens\": \"own\" }\nWe could also monitor a particular smart contract address, for example if we wanted to see how many tokens were listed for sale on a marketplace. The following example creates an NFT that supports modifier tokens and also monitors the tokens held by a script address:\n\"uses\": { \"tokens\": [ \"own\", \"addr1wywukn5q6lxsa5uymffh2esuk8s8fel7a0tna63rdntgrysv0f3ms\" ] }\n\"uses\": { \"tokens\": [ \"own\", \"addr1wywukn5q6lxsa5uymffh2esuk8s8fel7a0tna63rdntgrysv0f3ms\" ] }\nThe idea behind the renderer key is to reduce code duplication in on-chain Javascript by moving the generative code part of the project into a single asset which is minted once in the policy. Each individual NFT within a project is then just a set of input parameters to the generative script - this totally removes the need to fill the metadata of every mint transaction with encoded HTML and Javascript, as is the case with many on-chain Javascript NFTs now.\nWhen a Smart NFT is encountered which specifies another asset as the renderer, the site rendering the NFT should look-up the referenced asset and render that - the rendering token will then be responsible for reading the appropriate information via the Javascript API below and changing its appearance and/or behaviour based on that. In its simplest form, the rendering token could simply read the current token fingerprint and use that to seed a random number generator - this would enable a generative project to mint NFTs without even changing anything in the metadata and still have the renderer change its appearance for each one. In practice though, it's probably cooler to put actual traits like \"colour scheme\" or \"movement speed\" into the metadata and then have the renderer change its behaviour based on that.\nVia the Javascript API, the rendering token will always receive the properties of the child token which specified it as its renderer. This means if you wish to use a renderer with a token which also evolves based on its own transaction history, you will need to specify both \"renderer\" and \"transactions\" keys within the child token, and within the renderer token you do not need to specify these keys.\nFor example, to create a Smart NFT which is rendered by another token, and is also evolvable based on its own transaction history, the \"uses\" key would look like this:\n\"uses\": { \"transactions\": \"own\", \"renderer\": \"asset1frc5wn889lcmj5y943mcn7tl8psk98mc480v3j\" }\n\"uses\": { \"transactions\": \"own\", \"renderer\": \"asset1frc5wn889lcmj5y943mcn7tl8psk98mc480v3j\" }\nWhen an on-chain Javascript NFT is rendered which specifies any of the metadata options above, the website / dApp / wallet which creates the iframe sandbox, should inject the API defined here into that iframe sandbox. It is worth saying that the wallet dApp integration API from CIP-0030 should probably not be exposed inside the sandbox, to prevent cross-site-scripting attacks.\n<iframe>\n<iframe>\nThe Paginate data type along with APIError and PaginateError are copied directly from CIP-0030 and these functions should operate in a similar manner to that API.\nIt is recommended that the Smart NFT API not be injected for every NFT only the ones which specify the relevant metadata - this is an important step so that it s clear which NFTs require this additional API, and also to enable pre-loading and caching of the required data. We are aiming to expose only the specific data requested by the NFT in its metadata in this CIP we are not providing a more general API for querying arbitrary data from the blockchain.\nThere is potentially a desire to provide a more open-ended interface to query arbitrary data from the blockchain perhaps in the form of direct access to GraphQL but that may follow in a later CIP additional fields which could be added to the uses: {} metadata to enable the NFT to perform more complex queries on the blockchain.\nuses: {}\nAlthough an asynchronous API is specified so the data could be retrieved at the time when the NFT actually requests it it is expected that in most instances the site which renders the NFT would gather the relevant transaction logs in advance, and inject them into the iframe sandbox at the point where the sandbox is created, so that the data is immediately available to the NFT without having to perform an HTTP request.\n<iframe>\ncardano.nft.fingerprint: String\nThe fingerprint of the current token - in the case where we're rendering a child token, this will be the fingerprint of the child token.\ncardano.nft.metadata : Array\nThe content of the 721 key from the metadata json from the mint transaction of the current NFT - if we are rendering on behalf of a child NFT, this will be the metadata from the child NFT.\ncardano.nft.getTransactions( string which, paginate: Paginate = undefined ) : Promise<Object>\nErrors: APIError, PaginateError\nAPIError\nPaginateError\nThe argument to this function should be either an address, token fingerprint or the keyword own . It must match one of the ones specified via the transactions key in the new metadata mechanism detailed above.\ntransactions\nThis function will return a list of transaction hashes and metadata relating to the specified address or token. The list will be ordered by date with the newest transaction first, and will match the following format:\n{ \"transactions\": [ { \"txHash\": \"1507d1b15e5bd3c7827f1f0575eb0fdc3b26d69af0296a260c12d5c0c78239e0\", \"metadata\": <raw metadata from blockchain>, \"datum\": <the datum from the UTXO holding the token, if set> }, <more transactions here> ], \"fetched\": \"2022-05-01T22:39:03.369Z\" }\n{ \"transactions\": [ { \"txHash\": \"1507d1b15e5bd3c7827f1f0575eb0fdc3b26d69af0296a260c12d5c0c78239e0\", \"metadata\": <raw metadata from blockchain>, \"datum\": <the datum from the UTXO holding the token, if set> }, <more transactions here> ], \"fetched\": \"2022-05-01T22:39:03.369Z\" }\nFor simplicity, we do not include anything other than the txHash and the metadata since any other relevant details about the transaction can always be encoded into the metadata, there is no need to over-complicate by including other transaction data like inputs, outputs or the date of the transaction etc. That is left for a potential future extension of the API to include more full GraphQL support.\ncardano.nft.getTokens( string address, paginate: Paginate = undefined ) : Promise<Object>\nErrors: APIError, PaginateError\nAPIError\nPaginateError\nThis function accepts either an address or the keyword own as its argument - it must match one of the ones specified via the the tokens key in the new metadata mechanism detailed above.\ntokens\nThis function will return a list of the tokens held by the address specified in the argument, or held by the same stake key as the current token in the case of the own keyword.\n{ \"tokens\": [ { \"policyID\": \"781ab7667c5a53956faa09ca2614c4220350f361841a0424448f2f30\", \"assetName\": \"Life150\", \"fingerprint\": \"asset1frc5wn889lcmj5y943mcn7tl8psk98mc480v3j\", \"quantity\": 1, \"datum\": <the datum from the UTXO holding the token, if set> }, <more tokens here> ], \"fetched\": \"2022-05-01T22:39:03.369Z\" }\n{ \"tokens\": [ { \"policyID\": \"781ab7667c5a53956faa09ca2614c4220350f361841a0424448f2f30\", \"assetName\": \"Life150\", \"fingerprint\": \"asset1frc5wn889lcmj5y943mcn7tl8psk98mc480v3j\", \"quantity\": 1, \"datum\": <the datum from the UTXO holding the token, if set> }, <more tokens here> ], \"fetched\": \"2022-05-01T22:39:03.369Z\" }\ncardano.nft.getFileURL( string id = null, string fingerprint = null ) : Promise<String>\nErrors: APIError\nAPIError\nThis function provides access to the contents of files listed in the files[] array for this NFT - if the NFT is rendering on behalf of another NFT, the files arrays from both should be merged, with the child NFT items overwriting the rendering NFT, in the case of ID conflicts.\nfiles[]\nThe first argument specifies which entry from the files array should be retreived - if this argument is null, then the NFT's default image should be returned, which will typically come from the NFT's image metadata field rather than the files array. The second argument allows you to specify which token's files to search - it should either be the token itself (either the child token or the rendering token, in the case of tokens with a separate renderer). In the case where an NFT also uses the tokens part of this API, then the getFileURL() function will also allow you to specify any one of the fingerprints returned by the getTokens() query.\nimage\ntokens\nThe URL returned by this function should be in a format that is accessible from within the iframe sandbox - perhaps using window.URL.createObjectURL() to generate a static URL from raw data if necessary.\n<iframe>\nwindow.URL.createObjectURL()\nCurrently the NFT sites which support on-chain Javascript NFTs do so by creating a sandboxed iframe into which they inject the HTML from the NFT s metadata. From within this sandbox it is not possible to bring-in arbitrary data from external sources everything must be contained within the NFT, or explicitly bought into the sandbox via an API.\n<iframe>\nThis proposal suggests an addition to the 721 metadata key from CIP-0025, to enable an NFT to specify that it would like to receive a particular transaction history accessible to it from within the sandbox thus defining it as a Smart NFT .\nIn tandem with the additional metadata, we also define a standard for the Javascript API which is provided to the NFT within the sandbox.\nThis CIP now has a reference implementation which consists of a front-end React control which takes care of rendering an NFT - it creates the sandbox and exposes the CIP54 Javascript API to it. This works in tandem with a backend library which takes care of reading the necessary data from a dbsync instance and making it available for the front end control to render.\nThere is also an integrated development environment made available to enable realtime experimentation and debugging of Smart NFTs without having to repeatedly mint new tokens.\nFurthermore, a complete visual blockchain explorer has been made available which utilises libcip54 and SmartNFTPortal and fully supports the reference implementation of this standard.\nThe first CIP54 collection has been minted on mainnet under the policy ID 1eaf3b3ffb75ff27c43c512c23c6450b307f138281efb1d690b84652 and is available to see here. A number of other instructive example NFTs have also been provided as part of the NFT Playground website.\n1eaf3b3ffb75ff27c43c512c23c6450b307f138281efb1d690b84652\nLibcip54, SmartNFTPortal, Cardano Looking Glass and the NFT Playground are all opensource - pull requests are welcome!\nIdentify at least 1 pair of wallets, minting services, CLIs, or software utilities from separate providers which do at least 1 each of: creating NFTs according to this specification rendering NFTs according to this specification\ncreating NFTs according to this specification\nrendering NFTs according to this specification\nProvide a reference implementation of this scheme, which illustrates both: a means of creating a \"Smart NFT\" a means of rendering it Update this specification to match the new features added in the reference implementation.\na means of creating a \"Smart NFT\"\na means of rendering it\nUpdate this specification to match the new features added in the reference implementation.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0055 | Protocol Parameters (Babbage Era)\n\nThis CIP extends CIP-0028 to introduce a change to one of the Alonzo protocol parameters in the Babbage era, namely lovelacePerUTxOWord. We propose to have this updateable parameter be based on bytes instead of words (eight bytes). Additionally, two Alonzo era protocol parameters were removed, namely the decentralization parameter and the extra entropy parameter.\nlovelacePerUTxOWord\nSince the Shelley era, there has been an minimum number of lovelace requirement for every unspent transaction output. This requirement acts like a deposit, guarding the network from dust (the proliferation of small-valued unspent transaction outputs). Initially it was a constant value, since the Shelley era UTxO were simple and quite uniform. Starting in the Mary era, however, the constant value was replace with a formula to account for the variability in outputs that contained multi-assets. The formula was changed again in the Alonzo era. Both the Mary and the Alonzo era formulas provide an upper bound on the size in memory of an unspent transaction output in the Haskell implementation. We would like to simplify the formula to instead count the number of bytes in the CBOR serialization.\nTwo Alonzo era protocol parameters need to be removed for the Babbage era, since they relate to TPraos. Transitional Praos (named TPraos in the code base) is the addition of two features to Praos, which were added to provide a smooth transition from Ouroboros-BFT. In particular, Transitional Praos included an overlay schedule which could be tuned by the d parameter (d == 1 means that all the blocks are produced by the BFT nodes, d == 0 means that none of them are). It also included a way of injecting extra entropy into the epoch nonce. The extra entropy feature was used precisely once, and was explained wonderfully by one of the original authors of the Praos paper.\nTPraos\nTPraos\nd\nd == 1\nd == 0\nThe Babbage era removes both of the \"transitional\" features of TPraos, rendering the decentralization parameter and the extra entropy parameter useless.\nThe removal of the decentralization parameter and the extra entropy parameter is self explanatory. We now describe the specification of the coinsPerUTxOByte parameter.\ncoinsPerUTxOByte\nThe name of the protocol parameter is actually coinsPerUTxOWord in the Haskell implementation. It should be renamed to coinsPerUTxOByte.\ncoinsPerUTxOWord\ncoinsPerUTxOByte\nAt the moment that the hard fork combinator translates the Alonzo era ledger state to the Babbage era, the current value of coinsPerUTxOWord will be converted to\ncoinsPerUTxOWord\n⌊ coinsPerUTxOWord / 8 ⌋\n⌊ coinsPerUTxOWord / 8 ⌋\nIn the Babbage era, unspent transaction outputs will be required to contain at least\n(160 + |serialized_output|) * coinsPerUTxOByte\n(160 + |serialized_output|) * coinsPerUTxOByte\nmany lovelace. The constant overhead of 160 bytes accounts for the transaction input and the entry in the UTxO map data structure (20 words * 8 bytes).\nWe would like the formula for the minimum lovelace in a unspent transaction output be simpler and easier to reason about by all users of the Cardano network, while at the same time accounting for the size of the output.\nThe translation section explains how we will transition from the coinsPerUTxOWord parameter to the coinsPerUTxOByte parameter. Starting in the Babbage era, update proposals that want to modify coinsPerUTxOByte must bear in mind that the measurement is in bytes, not words.\ncoinsPerUTxOWord\ncoinsPerUTxOByte\ncoinsPerUTxOByte\nThe two protocol parameters that have been removed, d and extraEntropy, can no longer be used in protocol parameter updates.\nd\nextraEntropy\nThe Babbage ledger era is activated.\nDocumented parameters have been in operational use by Cardano Node and Ledger as of the Babbage ledger era.\nBabbage ledger era parameters are deemed correct by working groups at IOG.\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0057 | Plutus Contract Blueprint\n\nThis document specifies a language for documenting Plutus contracts in a machine-readable manner. This is akin to what OpenAPI or AsyncAPI are for, documenting HTTP services and asynchronous services respectively. In a similar fashion, A Plutus contract has a binary interface which is mostly defined by its datum and redeemer.\nThis document is therefore a meta-specification defining the vocabulary and validation rules with which one can specify a Plutus contract interface, a.k.a Plutus contract blueprint.\nWhile publicly accessible, on-chain contracts are currently inscrutable. Ideally, one would want to get an understanding of transactions revolving around script executions. This is both useful to visualize and to control the evolution of a contract life-cycle; but also, as a user interacting with a contract, to ensure that one is authorizing a transaction to do what it's intended to. Having a machine-readable specification in the form of a JSON-schema makes it easier (or even, possible) to enable a wide variety of use cases from a single concise document, such as:\nCode generators for serialization/deserialization of Contract's elements\nContract API Reference / Documentation, also automatically generated\nExtra automated transaction validation layers\nBetter wallet UI / integration with DApps\nAutomated Plutus-code scaffolding\nMoreover, by making the effort to write a clear specification of their contracts, DApps developers make their contracts easier to audit (as they're able to specify the expected behavior).\nThis specification introduces the notion of a Plutus contract blueprint, as a JSON document which it itself a JSON-schema as per the definition of given in JSON Schema: A Media Type for Describing JSON Documents: Draft 2020-12.\nSaid differently, Plutus blueprints are first and foremost, valid JSON schemas (according to the specification linked above). This specification defines a core vocabulary and additional keywords which are tailored to the specification of Plutus contracts. Tools supporting this specification must implement the semantic and validation rules specified in this document.\nMeta-schemas for Plutus blueprints (i.e. schemas used for validating Plutus blueprints themselves) are given in annexe.\nA Plutus contract blueprint is made of a single document describing one or more on-chain validators. By convention, the document is named plutus.json and should be located at the root of a project's repository to facilitate its discoverability.\nplutus.json\nThe document itself is a JSON object with the following fields:\nNote that examples of specifications are given later in the document to keep the specification succinct enough and not bloated with examples.\nThe preamble fields stores meta-information about the contract such as version numbers or a short description. This field is mainly meant for humans as a mean to contextualize a specification.\npreamble\nThe compiler field is optional, but allows specifying metadata about the toolkit that produced the validator and blueprint.\ncompiler\nValidators are the essence of the blueprint. This section describes each validator involved in the contract (simple applications will likely have only a single validator). A validator is mainly defined by three things: a title, arguments (i.e parameters, redeemer and/or datum) and some compiled code. Parameters refer to compile-time arguments that can be applied to a validator template. They must be instantiated to produce a final compiled code as they are embedded in the code of the validator itself. This is often the case for validators that must hold on unique external nonce to produce unique hashes.\ncompiledCode\nredeemer, datum and parameters items all share the same schema structure. They must define a schema that describes how to construct valid on-chain values for each of these fields, and they also specify a purpose (spend, mint, withdraw or publish) that indicates in which context it can figure. The purpose is either a string, or an applicator oneOf that specifies multiple (distinct) purposes. Similarly, an argument is either an object as described below, or an applicator oneOf of such objects. In case where it is defined as an applicator, purpose values between objects must be strictly non-overlapping as they are used as discriminant in chosing a schema. This allows, for example, to define different redeemer schemas for different purposes.\nredeemer\ndatum\nparameters\nschema\nspend\nmint\nwithdraw\npublish\noneOf\noneOf\npurpose\n\"spend\"\n\"mint\"\n\"withdraw\"\n\"publish\"\noneOf\noneOf\nA set of extra schemas to be re-used as references across the specification.\nPlutus blueprints ultimately describes on-chain data value that can be found at the validator's interface boundaries. This means that while we would generally operate at the level of Plutus Data, the vocabulary covers in practice any of the possible Untyped Plutus Core (abbrev. UPLC) primitives that can appear at a validator's boundary (e.g. compile-time parameters). Any UPLC primitive is therefore represented as a schema with a dataType keyword. The possible values for dataType are detailed just below. In addition, and depending on the value of dataType, we may find additional keywords in the vocabulary.\ndataType\ndataType\ndataType\ninteger\niData\nbytes\nbData\nlist\nlistData\nmap\nmapData\nconstructor\nconstrData\n#unit\n#boolean\n#integer\n#bytes\n#string\n#pair\nData\n#list\nData\nWarning\nWhile they exist for completeness, frameworks are strongly discouraged to use any of the constructs starting with a # as they refer to Plutus Core builtins types used by the Plutus virtual machines but aren't meant to figure in outward-facing interfaces. Validators should, as much as possible, stick to integer, bytes, list, map and constructor (and any composition of those) for their binary interface.\n#\ninteger\nbytes\nlist\nmap\nconstructor\nUsing these primitives, it becomes possible to represent the entire domain (i.e. possible values) which can be manipulated by Plutus contracts.\nSimilarly to JSON schemas, we provide extra validation keywords and keywords for applying subschemas with logic to further refine the definition of core primitives. Keywords allow to combine core data-types into bigger types and we'll later give some pre-defined definitions which we assume to be part of the core vocabulary and therefore, recognized by any tool supporting this standard.\nWhen presented with a validation keyword with a malformed value (e.g. \"maxLength\": \"foo\"), programs are expected to return an appropriate error.\n\"maxLength\": \"foo\"\nBeside, we define a Plutus Data Schema as a JSON object with a set of fields depending on its corresponding data-type. When we refer to a Plutus Data Schema, we refer to the entire schema definition, with its validations and with the semantic of each keywords applied.\nUnless otherwise specified, keywords are all considered optional.\nHere below are detailed all the accepted keywords for each data-type.\nNote Keywords in this section applies to any instance data-type described above.\ndataType\nThe value of this keyword must be a string, with one of the following value listed in the first column of the table above. This keyword is optional. When missing, the instance is implicitly typed as an opaque Plutus Data. When set, it defines the realm of other applicable keywords for that instance.\ntitle\nThis keyword's value must be a string. This keyword can be used to decorate a user interface and qualify an instance with some short title.\ndescription\nThis keyword's value must be a string. This keyword can be used to decorate a user interface and provide explanation about the purpose of the instance described by this schema.\n$comment\nThis keyword's value must be a string. It is meant mainly for programmers and humans reading the specification. This keyword should be ignored by programs.\nallOf\nThis keyword's value must be a non-empty array. Each item of the array MUST be a valid Plutus Data Schema. An instance validates successfully against this keyword if it validates successfully against all schemas defined by this keyword's value.\nanyOf\nThis keyword's value must be a non-empty array. Each item of the array must be a valid Plutus Data Schema. An instance validates successfully against this keyword if it validates successfully against at least one schema defined by this keyword's value.\noneOf\nThis keyword's value must be a non-empty array. Each item of the array must be a valid Plutus Data Schema. An instance validates successfully against this keyword if it validates successfully against exactly one schema defined by this keyword's value.\nnot\nThis keyword's value must be a valid Plutus Data Schema. An instance is valid against this keyword if it fails to validate successfully against the schema defined by this keyword.\n{ \"dataType\": \"bytes\" }\nNote Keywords in this section only applies to bytes. Using them in conjunction with an invalid data-type should result in an error.\nbytes\nenum\nThe value of this keyword must be an array of hex-encoded string literals. An instance validates successfully against this keyword if once hex-encoded, its value matches one of the elements of the keyword's values.\nmaxLength\nThe value of this keyword must be a non-negative integer. A bytes instance is valid against this keyword if its length is less than, or equal to, the value of this keyword.\nminLength\nThe value of this keyword must be a non-negative integer. A bytes instance is valid against this keyword if its length is greater than, or equal to, the value of this keyword.\n{ \"dataType\": \"integer\" }\nNote Keywords in this section only applies to integer. Using them in conjunction with an invalid type should result in an error.\ninteger\nmultipleOf\nThe value of \"multipleOf\" must be a integer, strictly greater than 0. The instance is valid if division by this keyword's value results in an integer.\nmaximum\nThe value of \"maximum\" must be a integer, representing an inclusive upper limit. This keyword validates only if the instance is less than or exactly equal to \"maximum\".\nexclusiveMaximum\nThe value of \"exclusiveMaximum\" must be an integer, representing an exclusive upper limit. The instance is valid only if it has a value strictly less than (not equal to) \"exclusiveMaximum\".\nminimum\nThe value of \"minimum\" must be an integer, representing an inclusive lower limit. This keyword validates only if the instance is greater than or exactly equal to \"minimum\".\nexclusiveMinimum\nThe value of \"exclusiveMinimum\" must be a integer, representing an exclusive lower limit. The instance is valid only if it has a value strictly greater than (not equal to) \"exclusiveMinimum\".\n{ \"dataType\": \"list\" }\nNote Keywords in this section only applies to list. Using them in conjunction with an invalid data-type should result in an error.\nlist\nitems\nThe value of this keyword must be either another Plutus Data Schema or a list of Plutus Data Schema. When this keyword is a single schema, it applies its subschema to all child instances of the list. When it is a list, then the list is expected to have exactly the same number of elements as specified by the keyword and each element must match against the schema corresponding to its position. The list variation is useful to represent product types such as tuples.\nmaxItems\nThe value of this keyword must be a non-negative integer. An array instance is valid against \"maxItems\" if its size is less than, or equal to, the value of this keyword.\nminItems\nThe value of this keyword must be a non-negative integer. A list instance is valid against \"minItems\" if its size is greater than, or equal to, the value of this keyword. Omitting this keyword has the same behavior as a value of 0.\nuniqueItems\nThe value of this keyword must be a boolean. If this keyword has boolean value false, the instance validates successfully. If it has boolean value true, the instance validates successfully if all of its elements are unique.\n{ \"dataType\": \"map\" }\nNote Keywords in this section only applies to map. Using them in conjunction with an invalid data-type should result in an error.\nmap\nkeys\nThe value of this keyword must be another Plutus Data Schema. This keyword applies its subschema to all keys of the map.\nvalues\nThe value of this keyword must be another Plutus Data Schema. This keyword applies its subschema to all values of the map.\nmaxItems\nThe value of this keyword must be a non-negative integer. An object instance is valid against \"maxItems\" if its number of key-value pair elements is less than, or equal to, the value of this keyword.\nminItems\nThe value of this keyword must be a non-negative integer. An object instance is valid against \"minItems\" if its number of key-value pair elements is greater than, or equal to, the value of this keyword.\n{ \"dataType\": \"constructor\" }\nNote Keywords in this section only applies to constructor. Using them in conjunction with an invalid data-type should result in an error.\nconstructor\nindex\nThis keyword's value must be a non-negative integer. An instance is valid against this keyword if it represents a Plutus constructor whose index is the same as this keyword's value. This keyword is mandatory.\nfields\nThis keyword's value must be an array of valid Plutus Data Schema; possibly empty. An instance is valid against this keyword if it represents a Plutus constructor for which each field is valid under each subschema given by this keyword's value. Fields are compared positionally. This keyword is mandatory.\n{ \"$schema\": \"https://cips.cardano.org/cips/cip57/schemas/plutus-blueprint.json\", \"$id\": \"https://github.com/aiken-lang/aiken/blob/main/examples/hello_world/plutus.json\", \"$vocabulary\": { \"https://json-schema.org/draft/2020-12/vocab/core\": true, \"https://json-schema.org/draft/2020-12/vocab/applicator\": true, \"https://json-schema.org/draft/2020-12/vocab/validation\": true, \"https://cips.cardano.org/cips/cip57\": true }, \"preamble\": { \"title\": \"aiken-lang/hello_world\", \"description\": \"Aiken contracts for project 'aiken-lang/hello_world'\", \"version\": \"1.0.0\", \"plutusVersion\": \"v2\" }, \"validators\": [ { \"title\": \"hello_world\", \"datum\": { \"title\": \"Datum\", \"purpose\": \"spend\", \"schema\": { \"anyOf\": [ { \"title\": \"Datum\", \"dataType\": \"constructor\", \"index\": 0, \"fields\": [ { \"title\": \"owner\", \"dataType\": \"bytes\" } ] } ] } }, \"redeemer\": { \"title\": \"Redeemer\", \"schema\": { \"anyOf\": [ { \"title\": \"Redeemer\", \"dataType\": \"constructor\", \"index\": 0, \"fields\": [ { \"title\": \"msg\", \"dataType\": \"bytes\" } ] } ] } }, \"compiledCode\": \"58ad0100003232322225333004323253330063372e646e64004dd7198009801002240009210d48656c6c6f2c20576f726c64210013233300100137586600460066600460060089000240206eb8cc008c00c019200022253335573e004294054ccc024cdc79bae300a00200114a226660060066016004002294088c8ccc0040052000003222333300a3370e008004016466600800866e0000d2002300d001001235573c6ea8004526165734ae855d11\", \"hash\": \"5e1e8fa84f2b557ddc362329413caa3fd89a1be26bfd24be05ce0a02\" } ] }\n{ \"$schema\": \"https://cips.cardano.org/cips/cip57/schemas/plutus-blueprint.json\", \"$id\": \"https://github.com/aiken-lang/aiken/blob/main/examples/hello_world/plutus.json\", \"$vocabulary\": { \"https://json-schema.org/draft/2020-12/vocab/core\": true, \"https://json-schema.org/draft/2020-12/vocab/applicator\": true, \"https://json-schema.org/draft/2020-12/vocab/validation\": true, \"https://cips.cardano.org/cips/cip57\": true }, \"preamble\": { \"title\": \"aiken-lang/hello_world\", \"description\": \"Aiken contracts for project 'aiken-lang/hello_world'\", \"version\": \"1.0.0\", \"plutusVersion\": \"v2\" }, \"validators\": [ { \"title\": \"hello_world\", \"datum\": { \"title\": \"Datum\", \"purpose\": \"spend\", \"schema\": { \"anyOf\": [ { \"title\": \"Datum\", \"dataType\": \"constructor\", \"index\": 0, \"fields\": [ { \"title\": \"owner\", \"dataType\": \"bytes\" } ] } ] } }, \"redeemer\": { \"title\": \"Redeemer\", \"schema\": { \"anyOf\": [ { \"title\": \"Redeemer\", \"dataType\": \"constructor\", \"index\": 0, \"fields\": [ { \"title\": \"msg\", \"dataType\": \"bytes\" } ] } ] } }, \"compiledCode\": \"58ad0100003232322225333004323253330063372e646e64004dd7198009801002240009210d48656c6c6f2c20576f726c64210013233300100137586600460066600460060089000240206eb8cc008c00c019200022253335573e004294054ccc024cdc79bae300a00200114a226660060066016004002294088c8ccc0040052000003222333300a3370e008004016466600800866e0000d2002300d001001235573c6ea8004526165734ae855d11\", \"hash\": \"5e1e8fa84f2b557ddc362329413caa3fd89a1be26bfd24be05ce0a02\" } ] }\nTHe primary goal of this CIP is to offer a mean of interoperability between tools of the ecosystem. In a world where every step of a contract development happens within a single framework -- like it's been the case with PlutusTx, this may not be seen as particularly useful. However, as soon as we start having an ecosystem of tools that operate a different levels (e.g. a language compiler, a transaction building library, an chain explorer, ...) we need some level of interoperability between them. Because the on-chain binary interface is the ultimate source of truth, it only makes sense to find an adequate way to capture it.\nJSON schemas are pervasively used in the industry for describing all sort of data models. Over the years, they have matured enough to be well understood by and familiar to a large portion of developers. Plus, tooling now exists in pretty much any major language to parse and process JSON schemas. Thus, using it as a foundation for the blueprint only makes sense.\nThis specification defines a new set of primitives types such as integer, bytes, list, map and constructor instead of the classic integer, number, string, bool, array, object, null. This is not only to reflect better the underlying structure of Plutus data which differs from JSON by many aspects, but also to allow defining or re-defining logic and validation keywords for each of those primitives.\ninteger\nbytes\nlist\nmap\nconstructor\ninteger\nnumber\nstring\nbool\narray\nobject\nnull\nNote however that apart from the keyword type, the terminology (and semantic) used for JSON schemas has been preserved to not \"reinvent the wheel\" and makes it easier to build tools on top by leveraging what already exists. Plutus schemas do not use type but use dataType instead to avoid possible confusion with JSON-schemas. A Plutus data schema is almost a JSON-schemas, but only supports a subset of the available keywords and has subtle differences for some of them (e.g. keywords for the bytes data-type operate mostly on hex-encoded strings).\ntype\ntype\ndataType\nbytes\nIn the original design specification of CIP-0057, we did not include UPLC builtins. But, there are a few legitimate cases where they might be found in the binary interface of validators. In particular, the ProtoPair builtin for constructing 2-tuples. This poses a problem of exhaustiveness (why only include ProtoPair and not the others where similar arguments could probably be made anyway). This is solved by being exhaustive in the capabilities of the blueprint specification, while discouraging their usage.\nProtoPair\nAnother point of designs here is to make Data rather transparent and promote Data's constructor variants as first-class data-types even though it's not faithfully representing what is really happening on-chain. An alternative, more faithful, representation to what's proposed would have been to have data as one of the data-type, and then keywords that identifies which of the data variant we are dealing with. So for example, instead of writing:\nData\ndata\n{ \"dataType\": \"integer\" }\n{ \"dataType\": \"integer\" }\nOne would have written:\n{ \"dataType\": \"data\", \"variant\": \"iData\" }\n{ \"dataType\": \"data\", \"variant\": \"iData\" }\nYet, because we do want Data to be the primary binary interface medium, we keep the former notation as it's more succinct and is a unambiguous shorthand. This also allows to segregate all builtins behind a common notation -- that is, prefixed with a #.\nData\n#\nOriginally, blueprints did not include any notion of purpose. However, as one of the end goal is to utilize blueprint as an input source for user interfaces, it becomes useful to:\nindicates under what circumstances is a certain validator expected to be used. provides different schemas based on the purpose\nindicates under what circumstances is a certain validator expected to be used.\nprovides different schemas based on the purpose\nYet, whereas there's a notion of purpose on-chain that is tightly coupled to the script context, different on-chain framework may handle the purpose differently. Some may chose, for example, to abstract that concern away from their users. Which is why we only make the purpose an optional field (except for datum) to leave a bit of flexibility for blueprint producers. For consumers, we recommend to treat purposes as discriminants to refine interfaces, but assume that a validator without purpose simply apply to any purpose.\ndatum\nhttps://json-schema.org/draft/2020-12/json-schema-core.html\nhttps://json-schema.org/draft/2020-12/json-schema-validation.html\nBlueprints are produced by one or (ideally) more smart-contract frameworks on Cardano. Aiken (implemented) Plu-ts (under way) OpShin (implemented) Helios (under consideration) PlutusTx (?) Plutarch (?) Scalus (?)\nBlueprints are produced by one or (ideally) more smart-contract frameworks on Cardano.\nAiken (implemented)\nPlu-ts (under way)\nOpShin (implemented)\nHelios (under consideration)\nPlutusTx (?)\nPlutarch (?)\nScalus (?)\nThere exist one or (ideally) more tools leveraging the blueprints Aiken Mesh.js Lucid Bloxbean/cardano-client-lib PyCardano Demeter\nThere exist one or (ideally) more tools leveraging the blueprints\nAiken\nMesh.js\nLucid\nBloxbean/cardano-client-lib\nPyCardano\nDemeter\nWrite specifications for a few real-world contracts, identify and fix gaps\nPoC of a toolkit generating blueprint definitions for a validator\nParse and interpret blueprints to produce smart-constructors for datums and redeemers in various languages JavaScript TypeScript Python\nJavaScript\nTypeScript\nPython\n(optional) develop a tool for rendering Plutus blueprint specifications as documentation paima/aiken-mdx\npaima/aiken-mdx\nCC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0058 | Plutus Bitwise Primitives\n\nAdd primitives for bitwise operations, based on BuiltinByteString, without requiring new data types.\nBuiltinByteString\nBitwise operations are one of the most fundamental building blocks of algorithms and data structures. They can be used for a wide variety of applications, ranging from representing and manipulating sets of integers efficiently, to implementations of cryptographic primitives, to fast searches. Their wide availability, law-abiding behaviour and efficiency are the key reasons why they are widely used, and widely depended on.\nAt present, Plutus lacks meaningful support for bitwise operations, which significantly limits what can be usefully done on-chain. While it is possible to mimic some of these capabilities with what currently exists, and it is always possible to introduce new primitives for any task, this is extremely unsustainable, and often leads to significant inefficiencies and duplication of effort.\nWe describe a list of bitwise operations, as well as their intended semantics, designed to address this problem.\nWe provide a range of applications that could be useful or beneficial on-chain, but are difficult or impossible to implement without some, or all, of the primitives we propose.\nFinite field arithmetic is an area with many applications, ranging from linear block codes to zero-knowledge proofs to scheduling and experimental design. Having such capabilities on-chain is useful in for a wide range of applications.\nA good example is multiplication over the Goldilocks field (with characteristic 264 232+12 64 - 2 32 + 1264 232+1). To perform this operation requires 'slicing' the representation being worked with into 32-bit chunks. As finite field representations are some kind of unsigned integer in every implementation, in Plutus, this would correspond to Integers, but currently, there is no way to perform this kind of 'slicing' on an Integer on-chain.\nInteger\nInteger\nFurthermore, finite field arithmetic can gain significant performance optimizations with the use of bitwise primitive operations. Two good examples are power-of-two division and computing inverses. The first of these (useful even in Integer arithmetic) replaces a division by a power of 2 with a shift; the second uses a count trailing zeroes operation to compute a multiplicative finite field inverse. While some of these operations could theoretically be done by other means, their performance is far from guaranteed. For example, GHC does not convert a power-of-two division or multiplication to a shift, even if the divisor or multiplier is statically-known. Given the restrictions on computation resources on-chain, any gains are significant.\nInteger\nHaving bitwise primitives, as well as the ability to convert Integers into a form amenable to this kind of work, would allow efficient finite field arithmetic on-chain. This could enable a range of new uses without being inefficient or difficult to port.\nInteger\nDue to the on-chain size limit, many data structures become impractical or impossible, as they require too much space either for their elements, or their overheads, to allow them to fit alongside the operations we want to perform on them. Succinct data structures could serve as a solution to this, as they represent data in an amount of space much closer to the entropy limit and ensure only constant overheads. There are several examples of these, and all rely on bitwise operations for their implementations.\nFor example, consider wanting to store a set of BuiltinIntegers on-chain. Given current on-chain primitives, the most viable option involves some variant on a BuiltinList of BuiltinIntegers; however, this is unviable in practice unless the set is small. To see why, suppose that we have an upper limit of kkk on the BuiltinIntegers we want to store; this is realistic in practically all cases. To store nnn BuiltinIntegers under the above scheme requires\nBuiltinInteger\nBuiltinList\nBuiltinInteger\nBuiltinInteger\nBuiltinInteger\nbits, where ccc denotes the constant overhead for each cons cell of the BuiltinList holding the data. If the set being represented is dense (meaning that the number of entries is a sizeable fraction of kkk), this cost becomes intolerable quickly, especially when taking into account the need to also store the operations manipulating such a structure on-chain with the script where the set is being used.\nBuiltinList\nIf we instead represented the same set as a bitmap based on BuiltinByteString, the amount of space required would instead be\nBuiltinByteString\nbits. This is significantly better unless nnn is small. Furthermore, this representation would likely be more efficient in terms of time in practice, as instead of having to crawl through a cons-like structure, we can implement set operations on a memory-contiguous byte string:\nThe cardinality of the set can be computed as a population count. This can have terrifyingly efficient implementations: the Mu a-Kurz-Lemire algorithm (the current state of the art) can process four kilobytes per loop iteration, which amounts to over four thousand potential stored integers.\nInsertion or removal is a bit set or bit clear respectively.\nFinding the smallest element uses a count leading zeroes.\nFinding the last element uses a count trailing zeroes.\nTesting for membership is a check to see if the bit is set.\nSet intersection is bitwise and.\nSet union is bitwise inclusive or.\nSet symmetric difference is bitwise exclusive or.\nA potential implementation could use a range of techniques to make these operations extremely efficient, by relying on SWAR techniques if portability is desired, and SIMD instructions for maximum speed. This would allow both potentially large integer sets to be represented on-chain without breaking the size limit, and nodes to efficiently compute with such, reducing the usage of resources by the chain. Lastly, in practice, if compression techniques are used (which also rely on bitwise operations!), the number of required bits can be reduced considerably in most cases without compromising performance: the current state-of-the-art (Roaring Bitmaps) can be used as an example of the possible gains.\nIn order to make such techniques viable, bitwise primitives are mandatory. Furthermore, succinct data structures are not limited to sets of integers, but all require bitwise operations to be implementable.\nOn-chain, space comes at a premium. One way that space can be saved is with binary representations, which can potentially represent something much closer to the entropy limit, especially if the structure or value being represented has significant redundant structure. While some possibilities for a more efficient 'packing' already exist in the form of BuiltinData, it is rather idiosyncratic to the needs of Plutus, and its decoding is potentially quite costly.\nBuiltinData\nBitwise primitives would allow more compact binary encodings to be defined, where complex structures or values are represented using fixed-size BuiltinByteStrings. The encoders and decoders for these could also be implemented more efficiently than currently possible, as there exist numerous bitwise techniques for this.\nBuiltinByteString\nFor linear structures on-chain, we are currently limited to BuiltinList and BuiltinMap, which don't allow constant-time indexing. This is a significant restriction, especially when many data structures and algorithms rely on the broad availability of a constant-time-indexable linear structure, such as a C array or Haskell Vector. While we could introduce a primitive data type like this, doing so would be a significant undertaking, and would require both implementing and costing a large API.\nBuiltinList\nBuiltinMap\nVector\nWhile for variable-length data, we don't have any alternatives if constant-time indexing is a goal, for fixed-length (or limited-length at least) data, there is a possibility, based on a similar approach taken by the finitary library. Essentially, given finitary data, we can transform any item into a numerical index, which is then stored by embedding into a byte array. As the indexes are of a fixed maximum size, this can be done efficiently, but only if there is a way of converting indices into bitstrings, and vice versa. Such a construction would allow using a (wrapper around) BuiltinByteString as a constant-time indexable structure of any finitary type. This is not much of a restriction in practice, as on-chain, fixed-width or size-bounded types are preferable due to the on-chain size limit.\nfinitary\nBuiltinByteString\nCurrently, all the pieces to make this work already exist: the only missing piece is the ability to convert indices (which would have to be BuiltinIntegers) into bit strings (which would have to be BuiltinByteStrings) and back again. With this capability, it would be possible to use these techniques to implement something like an array or vector without new primitive data types.\nBuiltinInteger\nBuiltinByteString\nTo ensure a focused and meaningful proposal, we specify our goals below.\nThe primitives provided should enable implementations of algorithms and data structures that are currently impossible or impractical. Furthermore, the primitives provided should have a high power-to-weight ratio: having them should enable as much as possible to be implemented.\nBitwise operations, via Boolean algebras, have a long and storied history of algebraic laws, dating back to important results by the like of de Morgan, Post and many others. These algebraic laws are useful for a range of reasons: they guide implementations, enable easier testing (especially property testing) and in some cases much more efficient implementations. To some extent, they also formalize our intuition about how these operations 'should work'. Thus, maintaining as many of these laws in our implementation as possible, and being clear about them, is important.\nProviding primitives alone is not enough: they should also be efficient. This is not least of all because many would associate 'primitive operation' with a notion of being 'close to the machine', and therefore fast. Thus, it is on us to ensure that the implementations of the primitives we provide have to be implementable in an efficient way, across a range of hardware.\nWhile totality is desirable, in some cases, there isn't a sensible answer for us to give. A good example is a division-by-zero: if we are asked to do such a thing, the only choice we have is to reject it. However, we need to make it as easy as possible for someone to realize why their program is failing, by emitting a sensible message which can later be inspected.\nWe also specify some specific non-goals of this proposal.\nA widespread legacy of C is the mixing of treatment of numbers and blobs of bits: specifically, the allowing of logical operations on representations of numbers. This applies to Haskell as much as any other language: according to the Haskell Report, it is in fact required that any type implementing Bits implement Num first. While GHC Haskell only mandates Eq, it still defines Bits instances for types clearly meant to represent numbers. This is a bad choice, as it creates complex situations and partiality in several cases, for arguably no real gain other than easier translation of bit twiddling code originally written in C.\nBits\nNum\nEq\nBits\nEven if two types share a representation, their type distinctness is meant to be a semantic or abstraction boundary: just because a number is represented as a blob of bits does not necessarily mean that arbitrary bit manipulations are sensible. However, by defining such a capability, we create several semantic problems:\nSome operations end up needing multiple definitions to take this into account. A good example are shifts: instead of simply having left or right shifts, we now have to distinguish arithmetic versus logical shifts, simply to take into account that a shift can be used on something which is meant to be a number, which could be signed. This creates unnecessary complexity and duplication of operations.\nAs Plutus BuiltinIntegers are of arbitrary precision, certain bitwise operations are not well-defined on them. A good example is bitwise complement: the bitwise complement of 000 cannot be defined sensibly, and in fact, is partial in its Bits instance.\nBuiltinInteger\nBits\nCertain bitwise operations on BuiltinInteger would have quite undesirable semantic changes in order to be implementable. A good example are bitwise rotations: we should be able to 'decompose' a rotation left or right by nnn into two rotations (by m1m_1m1 and m2m_2m2 such that m1+m2=nm_1 + m_2 = nm1 +m2 =n) without changing the outcome. However, because trailing zeroes are not tracked by the implementation, this can fail depending on the choice of decomposition, which seems needlessly annoying for no good reason.\nBuiltinInteger\nCertain bitwise operations on BuiltinInteger would require additional arguments and padding to define them sensibly. Consider bitwise logical AND: in order to perform this sensibly on BuiltinIntegers we would need to specify what 'length' we assume they have, and some policy of 'padding' when the length requested is longer than one, or both, arguments. This feels unnecessary, and it isn't even clear exactly how we should do this: for example, how would negative numbers be padded?\nBuiltinInteger\nBuiltinInteger\nThese complexities, and many more besides, are poor choices, owing more to the legacy of C than any real useful functionality. Furthermore, they feel like a casual and senseless undermining of type safety and its guarantees for very small and questionable gains. Therefore, defining bitwise operations on BuiltinInteger is not something we wish to support.\nBuiltinInteger\nThere are legitimate cases where a conversion from BuiltinInteger to BuiltinByteString is desirable; this conversion should be provided, and be both explicit and specified in a way that is independent of the machine or the implementation of BuiltinInteger, as well as total and round-tripping. Arguably, it is also desirable to provide built-in support for BuiltinByteString literals specified in a way convenient to their treatment as blobs of bytes (for example, hexadecimal or binary notation), but this is outside the scope of this proposal.\nBuiltinInteger\nBuiltinByteString\nBuiltinInteger\nBuiltinByteString\nWe propose several classes of operations. Firstly, we propose two operations for inter-conversion between BuiltinByteString and BuiltinInteger:\nBuiltinByteString\nBuiltinInteger\nintegerToByteString :: BuiltinInteger -> BuiltinByteString\nintegerToByteString :: BuiltinInteger -> BuiltinByteString\nConvert a non-negative number to its bitwise representation, erroring if given a negative number.\nbyteStringToInteger :: BuiltinByteString -> BuiltinInteger\nbyteStringToInteger :: BuiltinByteString -> BuiltinInteger\nReinterpret a bitwise representation to its corresponding non-negative number.\nWe also propose several logical operations on BuiltinByteStrings:\nBuiltinByteString\nandByteString :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString\nandByteString :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString\nPerform a bitwise logical AND on arguments of the same length, producing a result of the same length, erroring otherwise.\niorByteString :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString\niorByteString :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString\nPerform a bitwise logical IOR on arguments of the same length, producing a result of the same length, erroring otherwise.\nxorByteString :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString\nxorByteString :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString\nPerform a bitwise logical XOR on arguments of the same length, producing a result of the same length, erroring otherwise.\ncomplementByteString :: BuiltinByteString -> BuiltinByteString\ncomplementByteString :: BuiltinByteString -> BuiltinByteString\nComplement all the bits in the argument, producing a result of the same length.\nLastly, we define the following additional operations:\nshiftByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString\nshiftByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString\nPerforms a bitwise shift of the first argument by a number of bit positions equal to the absolute value of the second argument. A positive second argument indicates a shift towards higher bit indexes; a negative second argument indicates a shift towards lower bit indexes.\nrotateByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString\nrotateByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString\nPerforms a bitwise rotation of the first argument by a number of bit positions equal to the absolute value of the second argument. A positive second argument indicates a rotation towards higher bit indexes; a negative second argument indicates a rotation towards lower bit indexes.\npopCountByteString :: BuiltinByteString -> BuiltinInteger\npopCountByteString :: BuiltinByteString -> BuiltinInteger\nReturns the number of 111 bits in the argument.\ntestBitByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinBool\ntestBitByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinBool\nIf the position given by the second argument is not in bounds for the first argument, error; otherwise, if the bit given by that position is 111, return True, and False otherwise.\nTrue\nFalse\nwriteBitByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinBool -> BuiltinByteString\nwriteBitByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinBool -> BuiltinByteString\nIf the position given by the second argument is not in bounds for the first argument, error; otherwise, set the bit given by that position to 111 if the third argument is True, and 000 otherwise.\nTrue\ncountLeadingZeroesByteString :: BuiltinByteString -> BuiltinInteger\ncountLeadingZeroesByteString :: BuiltinByteString -> BuiltinInteger\nCounts the initial sequence of 0 bits in the argument (that is, starting from index 0). If the argument is empty, this returns 0.\ncountTrailingZeroesByteString :: BuiltinByteString -> BuiltinInteger\ncountTrailingZeroesByteString :: BuiltinByteString -> BuiltinInteger\nCounts the final sequence of 0 bits in the argument (that is, starting from the 1 bit with the highest index). If the argument is empty, this returns 0.\nWe define N+=x N x 0\\mathbb{N} {+} = \\\\{ x \\in \\mathbb{N} \\mid x \\neq 0 \\\\}N+=x N x =0. We assume that BuiltinInteger is a faithful representation of Z\\mathbb{Z}Z, and will refer to them (and their elements) interchangeably. A byte is some x 0,1, ,255x \\in \\\\{ 0,1,\\ldots,255 \\\\}x 0,1, ,255.\nBuiltinInteger\nWe observe that, given some base b N+b \\in \\mathbb{N} {+}b N+, any n Nn \\in \\mathbb{N}n N can be viewed as a sequence of values in 0,1, ,b 1\\\\{0,1,\\ldots, b - 1\\\\}0,1, ,b 1. We refer to any such sequence as a base bbb sequence. In such a 'view', given a base bbb sequence S=s0s1 skS = s_0 s_1 \\ldots s_kS=s0 s1 sk , we can compute its corresponding m N+m \\in \\mathbb{N} +m N+ as\ni 0,1, ,kbk i si\\sum_{i \\in \\\\{0,1,\\ldots,k\\\\}} b {k - i} \\cdot s_i i 0,1, ,k bk i si\nIf b 1b 1b 1 and ZZZ is a base bbb sequence consisting only of zeroes, we observe that for any other base bbb sequence SSS, Z SZ \\cdot SZ S and SSS correspond to the same number, where \\cdot is sequence concatenation.\nWe use bit sequence to refer to a base 2 sequence, and byte sequence to refer to a base 256 sequence. For a bit sequence S=b0b1 bnS = b_0 b_1 \\ldots b_nS=b0 b1 bn , we refer to 0,1, ,n\\\\{0,1,\\ldots,n \\\\}0,1, ,n as the valid bit indices of SSS; analogously, for a byte sequence T=y0y1 ymT = y_0 y_1 \\ldots y_mT=y0 y1 ym , we refer to 0,1, ,m\\\\{0,1,\\ldots,m\\\\}0,1, ,m as the valid byte indices of TTT. We observe that the length of SSS is n+1n + 1n+1 and the length of TTT is m+1m + 1m+1; we refer to these as the bit length of SSS and the byte length of TTT for clarity. We write S[i]S[i]S[i] and T[j]T[j]T[j] to represent bib_ibi and yjy_jyj for valid bit index iii and valid byte index jjj respectively.\nWe describe a 'view' of bytes as bit sequences. Let yyy be a byte; its corresponding bit sequence is Sy=y0y1y2y3y4y5y6y7S_y = y_0 y_1 y_2 y_3 y_4 y_5 y_6 y_7Sy =y0 y1 y2 y3 y4 y5 y6 y7 such that\ni 0,1, ,727 i yi=y\\sum_{i \\in \\\\{0,1,\\ldots,7\\\\}} 2 {7 - i} \\cdot y_i = y i 0,1, ,7 27 i yi =y\nFor example, the byte 555555 has the corresponding byte sequence 001101110011011100110111. For any byte, its corresponding byte sequence is unique. We use this to extend our 'view' to byte sequences as bit sequences. Specifically, let T=y0y1 ymT = y_0 y_1 \\ldots y_mT=y0 y1 ym be a byte sequence. Its corresponding bit sequence S=b0b1 bmbm+1 b8(m+1) 1S = b_0b_1 \\ldots b_m b_{m + 1} \\ldots b_{8(m + 1) - 1}S=b0 b1 bm bm+1 b8(m+1) 1 such that for any valid bit index jjj of SSS, bj=1b_j = 1bj =1 if and only if T[j/8][jmod 8]=1T[j / 8][j \\mod 8] = 1T[j/8][jmod8]=1, and is 000 otherwise.\nBased on the above, we observe that any BuiltinByteString can be a bit sequence or a byte sequence. Furthermore, we assume that indexByteString and sliceByteString 'agree' with valid byte indices. More precisely, suppose bs represents a byte sequence TTT; then indexByteString bs i is seen as equivalent to T[i]T[\\mathtt{i}]T[i]; we extend this notion to sliceByteString analogously. Throughout, we will refer to BuiltinByteStrings and their 'views' as bit or byte sequences interchangeably.\nBuiltinByteString\nindexByteString\nsliceByteString\nbs\nindexByteString bs i\nsliceByteString\nBuiltinByteString\nBuiltinInteger\nBuiltinByteString\nWe describe the translation of BuiltinInteger into BuiltinByteString, which is implemented as the integerToByteString primitive. Let iii be the argument BuiltinInteger; if this is negative, we produce an error, specifying at least the following:\nBuiltinInteger\nBuiltinByteString\nintegerToByteString\nBuiltinInteger\nThe fact that specifically the integerToByteString operation failed;\nintegerToByteString\nThe reason (given a negative number); and\nWhat exact number was given as an argument.\nOtherwise, we produce the BuiltinByteString corresponding to the base 256 sequence which represents iii.\nBuiltinByteString\nWe now describe the reverse operation, implemented as the byteStringToInteger primitive. This treats its argument BuiltinByteString as a base 256 sequence, and produces its corresponding number as a BuiltinInteger. We note that this is necessarily non-negative.\nbyteStringToInteger\nBuiltinByteString\nBuiltinInteger\nWe observe that byteStringToInteger 'undoes' integerToByteString:\nbyteStringToInteger\nintegerToByteString\nbyteStringToInteger . integerToByteString = id\nbyteStringToInteger . integerToByteString = id\nThe other direction does not necessarily hold: if the argument to byteStringToInteger contains a prefix consisting only of zeroes, and we convert the resulting BuiltinInteger i back to a BuiltinByteString using integerToByteString, that prefix will be lost.\nbyteStringToInteger\nBuiltinInteger\ni\nBuiltinByteString\nintegerToByteString\nBuiltinByteString\nThroughout, let S=s0s1 snS = s_0 s_1 \\ldots s_nS=s0 s1 sn and T=t0t1 tnT = t_0 t_1 \\ldots t_nT=t0 t1 tn be byte sequences, and let S S {\\prime}S and T T {\\prime}T be their corresponding bit sequences, with bit lengths n +1n {\\prime} + 1n +1 and m +1m {\\prime} + 1m +1 respectively. Whenever we specify a mismatched length error result, its error message must contain at least the following information:\nThe name of the failed operation;\nThe reason (mismatched lengths); and\nThe byte lengths of the arguments.\nFor any of andByteString, iorByteString and xorByteString, given inputs SSS and TTT, if n mn \\neq mn =m, the result is an error which must contain at least the following information:\nandByteString\niorByteString\nxorByteString\nThe name of the failed operation;\nThe reason (mismatched lengths); and\nThe byte lengths of the arguments.\nIf n=mn = mn=m, the result of each of these operations is the bit sequence U=u0u1 un U = u_0u_1 \\ldots u_{n {\\prime}}U=u0 u1 un , such that for all i 0,1, ,n i \\in \\\\{0, 1, \\ldots, n {\\prime}\\\\}i 0,1, ,n , U[i]=1U[i] = 1U[i]=1 under the following conditions:\nFor andByteString, when S [i]=T [i]=1S {\\prime}[i] = T {\\prime}[i] = 1S [i]=T [i]=1;\nandByteString\nFor iorByteString, when at least one of S [i],T [i]S {\\prime}[i], T {\\prime}[i]S [i],T [i] is 111;\niorByteString\nFor xorByteString, when S [i] T [i]S {\\prime}[i] \\neq T {\\prime}[i]S [i] =T [i].\nxorByteString\nOtherwise, U[i]=0U[i] = 0U[i]=0.\nWe observe that, for length-matched arguments, each of these operations describes a commutative and associative operation. Furthermore, for any given byte length kkk, each of these operations has an identity element:\nFor andByteString and xorByteString, the byte sequence of length kkk where each element is zero; and\nandByteString\nxorByteString\nFor iorByteString, the byte sequence of length kkk where each element is 255.\niorByteString\nLastly, andByteString and iorByteString have an absorbing element for each byte length kkk, which is the byte sequence of length kkk where each element is zero and 255 respectively.\nandByteString\niorByteString\nWe now describe the semantics of complementByteString. For input SSS, the result is the bit sequence U=u0u1 un U = u_0 u_1 \\ldots u_{n {\\prime}}U=u0 u1 un such that for all i {0,1, ,n }i \\in \\{0, 1, \\ldots, n {\\prime}\\}i {0,1, ,n }, we have U[i]=0U[i] = 0U[i]=0 if S [i]=1S {\\prime}[i] = 1S [i]=1 and 111 otherwise.\ncomplementByteString\nWe observe that complementByteString is self-inverting. We also note the following equivalences hold assuming b and b' have the same length; these are De Morgan's laws:\ncomplementByteString\nb\nb'\ncomplementByteString (andByteString b b') = iorByteString (complementByteString b) (complementByteString b')\ncomplementByteString (andByteString b b') = iorByteString (complementByteString b) (complementByteString b')\ncomplementByteString (iorByteString b b') = andByteString (complementByteString b) (complementByteString b')\ncomplementByteString (iorByteString b b') = andByteString (complementByteString b) (complementByteString b')\nThroughout, let S=s0s1 snS = s_0 s_1 \\ldots s_nS=s0 s1 sn be a byte sequence, and let S S {\\prime}S be its corresponding bit sequence with bit length n +1n {\\prime} + 1n +1.\nWe describe the semantics of shiftByteString and rotateByteString. Informally, both of these are 'bit index modifiers': given a positive iii, the index of a bit in the result 'increases' relative to the argument, and given a negative iii, the index of a bit in the result 'decreases' relative to the argument. This can mean that for some bit indexes in the result, there is no corresponding bit in the argument: we term these missing indexes. Additionally, by such calculations, a bit index in the argument may be projected to a negative index in the result: we term these out-of-bounds indexes. How we handle missing and out-of-bounds indexes is what distinguishes shiftByteString and rotateByteString:\nshiftByteString\nrotateByteString\nshiftByteString\nrotateByteString\nshiftByteString sets any missing index to 000 and ignores any data at out-of-bounds indexes.\nshiftByteString\nrotateByteString uses out-of-bounds indexes as sources for missing indexes by 'wraparound'.\nrotateByteString\nWe describe the semantics of shiftByteString precisely. Given arguments SSS and some i Zi \\in \\mathbb{Z}i Z, the result is the bit sequence U=u0u1 un U = u_0 u_1 \\ldots u_{n {\\prime}}U=u0 u1 un such that for all j 0,1, ,n j \\in \\\\{0, 1, \\ldots, n {\\prime}\\\\}j 0,1, ,n , we have U[j]=S [j i]U[j] = S {\\prime}[j - i]U[j]=S [j i] if j ij - ij i is a valid bit index for S S {\\prime}S and 000 otherwise.\nshiftByteString\nLet k, Zk, \\ell \\in \\mathbb{Z}k, Z such that either kkk or \\ell is 000, or kkk and \\ell have the same sign. We observe that, for any bs, we have\nbs\nshiftByteString (shiftBytestring bs k) l = shiftByteString bs (k + l)\nshiftByteString (shiftBytestring bs k) l = shiftByteString bs (k + l)\nWe now describe the semantics of rotateByteString precisely; we assume the same arguments as for shiftByteString above. The result is the bit sequence U=u0u1 un U = u_0 u_1 \\ldots u_{n {\\prime}}U=u0 u1 un such that for all j 0,1, ,n j \\in \\\\{0, 1, \\ldots, n {\\prime}\\\\}j 0,1, ,n , we have U[j]=S [n +j imod n ]U[j] = S {\\prime}[n {\\prime} + j - i \\mod n {\\prime}]U[j]=S [n +j imodn ].\nrotateByteString\nshiftByteString\nWe observe that for any k, k, \\ellk, , and any bs, we have\nbs\nrotateByteString (rotateByteString bs k) l = rotateByteString bs (k + l)\nrotateByteString (rotateByteString bs k) l = rotateByteString bs (k + l)\nWe also note that\nrotateByteString bs 0 = shiftByteString bs 0 = bs\nrotateByteString bs 0 = shiftByteString bs 0 = bs\nLastly, we note that\nrotateByteString bs k = rotateByteString bs (k `remInteger` (lengthByteString bs * 8))\nrotateByteString bs k = rotateByteString bs (k `remInteger` (lengthByteString bs * 8))\nFor popCountByteString with argument SSS, the result is\npopCountByteString\nj 0,1, ,n S [j]\\sum_{j \\in \\\\{0, 1, \\ldots, n {\\prime}\\\\}} S {\\prime}[j] j 0,1, ,n S [j]\nInformally, this is just the total count of 111 bits. We observe that for any bs and bs', we have\nbs\nbs'\npopCountByteString bs + popCountByteString bs' = popCountByteString (appendByteString bs bs')\npopCountByteString bs + popCountByteString bs' = popCountByteString (appendByteString bs bs')\nWe now describe the semantics of testBitByteString and writeBitByteString. Throughout, whenever we specify an out-of-bounds error result, its error message must contain at least the following information:\ntestBitByteString\nwriteBitByteString\nThe name of the failed operation;\nThe reason (out of bounds access);\nWhat index was accessed out-of-bounds; and\nThe valid range of indexes.\nFor testBitByteString with arguments SSS and some i Zi \\in \\mathbb{Z}i Z, if iii is a valid bit index of S S {\\prime}S , the result is True if S [i]=1S {\\prime}[i] = 1S [i]=1, and False if S [i]=0S {\\prime}[i] = 0S [i]=0. If iii is not a valid bit index of S S {\\prime}S , the result is an out-of-bounds error.\ntestBitByteString\nTrue\nFalse\nFor writeBitByteString with arguments SSS, some i Zi \\in \\mathbb{Z}i Z and some BuiltinBool bbb, if iii is not a valid bit index for S S {\\prime}S , the result is an out-of-bounds error. Otherwise, the result is the bit sequence U=u0u1 un U = u_0 u_1 \\ldots u_{n {\\prime}}U=u0 u1 un such that for all j 0,1, ,nj \\in \\\\{0, 1, \\ldots, n\\\\}j 0,1, ,n, we have:\nwriteBitByteString\nBuiltinBool\nU[j]=1U[j] = 1U[j]=1 when i=ji = ji=j and bbb is True;\nTrue\nU[j]=0U[j] = 0U[j]=0 when i=ji = ji=j and bbb is False;\nFalse\nU[j]=S [j]U[j] = S {\\prime}[j]U[j]=S [j] otherwise.\nLastly, we describe the semantics of countLeadingZeroesByteString and countTrailingZeroesByteString. Given the argument SSS, countLeadingZeroesByteString gives the result kkk such that all of the following hold:\ncountLeadingZeroesByteString\ncountTrailingZeroesByteString\ncountLeadingZeroesByteString\n0 k n +10 \\leq k n {\\prime} + 10 k n +1;\nFor all 0 i k0 \\leq i k0 i k, S [i]=0S {\\prime}[i] = 0S [i]=0; and\nIf n 0n {\\prime} \\neq 0n =0, then S [k]=1S {\\prime}[k] = 1S [k]=1.\nGiven the same argument, countTrailingZeroesByteString instead gives the result kkk such that all of the following hold:\ncountTrailingZeroesByteString\n0 k n +10 \\leq k n {\\prime} + 10 k n +1;\nFor all k i n k \\leq i n {\\prime}k i n , S [i]=0S {\\prime}[i] = 0S [i]=0; and\nIf k/neqn +1k /neq n {\\prime} + 1k/neqn +1, then S [nprime k]=1S {\\prime}[n {prime} - k] = 1S [nprime k]=1.\nLet zeroes be a BuiltinByteString consisting only of zero bytes of length len. We observe that\nzeroes\nBuiltinByteString\nlen\ncountTrailingZeroesByteString zeroes = countLeadingZeroesByteString zeroes = len * 8\ncountTrailingZeroesByteString zeroes = countLeadingZeroesByteString zeroes = len * 8\nFurthermore, for two BuiltinByteStrings bs and bs', we have\nBuiltinByteString\nbs\nbs'\ncountLeadingZeroesByteString (iorByteString bs bs') = min (countLeadingZeroesByteString bs) (countLeadingZeroesByteString bs') countTrailingZeroesByteString (iorByteString bs bs') = min (countTrailingZeroesByteString bs) (countTrailingZeroesByteString bs')\ncountLeadingZeroesByteString (iorByteString bs bs') = min (countLeadingZeroesByteString bs) (countLeadingZeroesByteString bs') countTrailingZeroesByteString (iorByteString bs bs') = min (countTrailingZeroesByteString bs) (countTrailingZeroesByteString bs')\nwhere min is the minimum value function.\nmin\nAll of the primitives we describe are linear in one of their arguments. For a more precise description, see the table below.\nintegerToByteString\nbyteStringToInteger\nandByteString\niorByteString\nxorByteString\ncomplementByteString\nshiftByteString\nBuiltinByteString\nrotateByteString\nBuiltinByteString\npopCountByteString\ntestBitByteString\nBuiltinByteString\nwriteBitByteString\nBuiltinByteString\ncountLeadingZeroesByteString\ncountTrailingZeroesByteString\nFor work in finite field arithmetic (and the areas it enables), we frequently need to move between the 'worlds' of BuiltinInteger and BuiltinByteString. This needs to be consistent, and allow round-trips. We simplify this by only requiring conversions work on non-negative integers: this means that the translations can be simpler and more efficient, and also avoids representational questions for negative numbers.\nBuiltinInteger\nBuiltinByteString\nOur choice of logical AND, IOR, XOR and complement as the primary logical operations is driven by a mixture of prior art, utility and convenience. These are the typical bitwise logical operations provided in hardware, and in most programming languages; for example, in the x86 instruction set, the following bitwise operations have existed since the 8086:\nAND: Bitwise AND.\nAND\nOR: Bitwise IOR.\nOR\nNOT: Bitwise complement.\nNOT\nXOR: Bitwise XOR.\nXOR\nLikewise, on the ARM instruction set, the following bitwise operations have existed since ARM2:\nAND: Bitwise AND.\nAND\nORR: Bitwise IOR.\nORR\nEOR: Bitwise XOR.\nEOR\nORN: Bitwise IOR with complement of the second argument.\nORN\nBIC: Bitwise AND with complement of the second argument.\nBIC\nGoing 'up a level', the C and Forth programming languages (according to C89 and ANS Forth respectively) define bitwise AND (denoted & and AND respectively), bitwise IOR (denoted | and OR respectively), bitwise XOR (denoted and XOR respectively) and bitwise complement (denoted ~ and NOT respectively) as primitive bitwise operations. These choices are mirrored by basically all 'high-level' languages; for example, Haskell's Bits type class defines these same four operations as .&., .|., xor and complement respectively.\n&\nAND\n|\nOR\n^\nXOR\n~\nNOT\nBits\n.&.\n.|.\nxor\ncomplement\nThis ubiquity in choices leads to most algorithm descriptions that rely on bitwise operations to assume that these specific four operations are 'primitive', implying that they are constant-time and constant-cost. While we could reduce the number of primitive bitwise operations (and, in fact, due to Post, we know that there exist two operations that can implement all of them), this would be both inconvenient and inefficient. As an example, consider implementing XOR using AND, IOR and complement: this would translate x XOR y into\nx XOR y\n(COMPLEMENT x AND y) IOR (x AND COMPLEMENT y)\n(COMPLEMENT x AND y) IOR (x AND COMPLEMENT y)\nThis is both needlessly complex, and also inefficient, as it requires copying the arguments twice, only to then throw away both copies. This is less of a concern if copying is 'cheap', but given that we need to operate on variable-width data (specifically BuiltinByteStrings), this seems needlessly wasteful.\nBuiltinByteString\nLike our 'baseline' bitwise operations above, shifts and rotations are widely used, and considered as primitive. For example, x86 platforms have had the following available since the 8086:\nRCL: Rotate left.\nRCL\nRCR: Rotate right.\nRCR\nSHL: Shift left.\nSHL\nSHR: Shift right.\nSHR\nLikewise, ARM platforms have had the following available since ARM2:\nROR: Rotate right.\nROR\nLSL: Shift left.\nLSL\nLSR: Shift right.\nLSR\nWhile C and Forth both have shifts (denoted with and in C, and LSHIFT and RSHIFT in Forth), they don't have rotations; however, many higher-level languages do: Haskell's Bits type class has rotate, which enables both left and right rotations.\n<<\n>>\nLSHIFT\nRSHIFT\nBits\nrotate\nWhile popCountByteString could in theory be simulated using testBitByteString and a fold, this is quite inefficient: the best way to simulate this operation would involve using something similar to the Harley-Seal algorithm, which requires a large lookup table, making it impractical on-chain. Furthermore, population counting is important for several classes of succinct data structure (particularly rank-select dictionaries and bitmaps), and is in fact provided as part of the SSE4.2 x86 instruction set as a primitive named POPCNT.\npopCountByteString\ntestBitByteString\nSSE4.2\nPOPCNT\nIn order to usefully manipulate individual bits, both testBitByteString and writeBitByteString are needed. They can also be used as part of specifying, and verifying, that other bitwise operations, both primitive and non-primitive, are behaving correctly. They are also particularly essential for binary encodings.\ntestBitByteString\nwriteBitByteString\ncountLeadingZeroesByteString and countTrailingZeroesByteString is an essential primitive for several succinct data structures: both Roaring Bitmaps and rank-select dictionaries rely on them for much of their usefulness. For finite field arithmetic, these instructions are also beneficial to have available as efficiently as possible. Furthermore, this operation is provided in hardware by several instruction sets: on x86, there exist (at least) BSF, BSR, LZCNT and TZCNT, while on ARM, we have CLZ for counting leading zeroes. These instructions also exist in higher-level languages: for example, GHC's FiniteBits type class has countTrailingZeros and countLeadingZeros. Lastly, while they can be emulated by testBitByteString, this is tedious, error-prone and extremely slow.\ncountLeadingZeroesByteString\ncountTrailingZeroesByteString\nBSF\nBSR\nLZCNT\nTZCNT\nCLZ\nFiniteBits\ncountTrailingZeros\ncountLeadingZeros\ntestBitByteString\nAt the Plutus Core level, implementing this proposal introduces no backwards-incompatibility: the proposed new primitives do not break any existing functionality or affect any other builtins. Likewise, at levels above Plutus Core (such as PlutusTx), no existing functionality should be affected.\nPlutusTx\nOn-chain, this requires a hard fork, as this introduces new primitives.\nMLabs will implement these primitives, as well as tests for these. Costing will have to be done after this is complete, but must be done by the Plutus Core team, due to limitations in how costing is performed.\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0059 | Terminology Surrounding Core Features\n\nThis CIP seeks to clarify the language around groups of features. At the very least, it provides some history.\nWhen @CharlesHoskinson conceived of Cardano, he had a vision for what features the network would support. This vision is still present on the Cardano roadmap website. In particular, the features are grouped into \"phases\", which are mostly named after poets (Goguen is the exception). The word \"era\" is used interchangeably with \"phases\" on the roadmap.\nThe word \"era\", however, has been muddled by an implementation detail in the Cardano ledger. The Shelley phase was implemented as an entire re-write of the code from the Byron phase. While the consensus layer for the Shelley phase was written with an abstraction in place for the ledger, the ledger layer was not written with any abstractions to make future phases possible.\nUpon starting into the Goguen phase, the ledger team retroactively introduce a notion of \"era\" into the ledger code, and deemed the Shelley features \"the Shelley era\". In hindsight, however, the word \"era\" in unfortunate, since the Goguen phase was completed in the ledger by what was called \"the Allegra era, the Mary era, the Alonzo era, and the Babbage era\".\nThe names Allegra and Mary were chosen for their connection to the poet Percy Shelley, and were only intended to be used as variable names for a very specific abstraction used in the ledger code. (The story is even a bit more confusing, since the Allegra and Mary era share a lot of code and are specified together in the \"Shelley-MA specification. The letters \"MA\" can hilariously refer to both \"Mary Allegra\" and \"Multi-Assets\".)\nHow did we then go from poets to Alonzo? Recall that \"Goguen\" was the only non-poet named in the phases on the Cardano roadmap. We found it fitting, therefore, to name the ledger era which introduced Plutus after the person who invented the lambda calculus (Plutus Core uses a variant of system F.).\nMoreover, going forward, we decided to use names in A, B, C, ... order, names coming from other people who walk the line between mathematics and computer science. One lack of consistency to notice is that we have used both first and last names. The inconsistency was mostly driven by the desire to find short and memorable names.\nAnother complication to the story is the notion of \"intra-era hard forks\". A new era must be introduced with a hard fork, but the ledger can also change semantics during a controlled hard fork with another mechanism, namely an intra-era hard fork. This is an implementation detail which involves bumping the major protocol version but not creating a new ledger era. The Alonzo era experienced an intra-era hard fork when going from major protocol version 5 to 6.\nYet another complication stems from the named releases. We chose to honor the late Cardano community member and Bulgarian mathematician Vasil Dabov by naming a release date after him. The ledger era after the Alonzo era was named Babbage. Babbage is a feature set, Vasil is a release date which ushered in the Babbage era.\nLastly, it is important to understand that not all of the semantic changes to the Cardano network involve the ledger, though the changes to the ledger are often the most user-facing. Changes to the consensus protocol or the networking layer may also involve a hard fork. Moreover, there is an abstraction that sits between the consensus and ledger layers, which we have named the \"protocol\" (a regrettably vague name).\nThe distinction between the ledger protocols and the ledger eras correspond roughly to how block headers are validated (protocol) versus how block bodies are validated (era). The Shelley era used the \"transitional Praos\" protocol (or TPraos for short). It consisted of Praos together with a transition system to move away from Ouroboros-BFT. The Babbage era replaced TPraos with Praos.\nA table of all the features, as of the time this CIP was submitted, can be found here.\nNote that the protocol version mentioned above is unrelated to the node-to-node and node-to-client protocol versions. The consensus layer maintains a versioning scheme for the node queries which does not necessarily align with the protocol version described in this CIP.\nNote also that the protocol version present inside of each block header indicates the maximum supported protocol version that the block producer is capable of supporting (see section 13, Software Updates, of the Shelley ledger specification).\nLet us use the following language:\nPhase - A phase in Cardano is a high level collection of features described on the Cardano roadmap.\nLedger Era - A ledger era (or era for short if there is no confusion) in Cardano is a collection of ledger features introduced at a hard fork. Moreover, starting with the Alonzo era, they will be named after mathematicians and computer scientists (preferably both!) in A, B, C, ... ordering. Some letters might prove challenging.\nIntra-era Hardfork - An intra-era hard fork in Cardano is a small and focused semantic change to the ledger which requires a hard fork.\nConsensus mechanism - A consensus mechanism in Cardano is a collection of consensus features introduced at a hard fork. Historically, these have had the name \"Ouroboros\" in them.\nLedger Protocol - A ledger protocol in Cardano is a collection of ledger features sitting between the consensus layer and the ledger layer, roughly characterized by block header validation.\nRelease Dates - When we are confident about the release of a new features, we can chose to honor Cardano community members by naming a date after them.\nIf we can agree to common language, it will greatly improve communication among ourselves and also with new community members.\nSince this is an issue of language, we will strive to use consistent language going forward, and we can correct misalignment when we find it.\nTerminology has met with positive response from community.\nTerminology has continued in use particularly in the CIP process and the Feature Table has been kept up to date.\nLedger architects have committed to standardising their language for the community.\nTable of strict definitions, with protocol versions and block heights, is produced to remove any ambiguities.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0060 | Music Token Metadata\n\nThis proposal defines an extension to CIP-25 and CIP-68 for token metadata specific to music tokens.\nMusic tokens on Cardano can be either NFTs or FTs and contain links to audio files. In order for players, indexers, and wallets to be able to properly search and categorize a user's music collection, we need to define a common schema for creating music on Cardano. If all parties creating these music tokens follow similar patterns, apps can consume this information and make proper use of it. The existing CIP-25 is a good base to build upon, but for a good music experience, we need to standardize additional fields that will be required specifically for music tokens.\nThis CIP divides the additional metadata parameters into two categories of Required and Optional. When minting a music token on Cardano, you are expected to include ALL of the required fields. If you choose to include one or more of the optional fields, they must be named exactly as defined in this CIP. This will properly allow indexing apps and music players to utilize as much of your token metadata as possible without issues.\nRequired\nOptional\nCDDL Spec Version 3 CDDL Spec Version 2 (deprecated) CDDL Spec Version 1 (deprecated)\nIn version 2 of the CIP-60 spec, album_title has been renamed to release_title. release is a more generic name that covers all types of releases from Albums, EPs, LPs, Singles, and Compilations. At the top level, we are grouping those metadata items that relate to the release under a new key release. At the file for each song, there is a new song key that holds the metadata specific to the individual song. These changes separate the music-specific metadata from the general CIP-25/CIP-68 NFT metadata. A music player can look at just the information necessary instead of having to ignore extra NFT-related fields. CIP-68 NFTs are officially supported and an example specific to CIP-68 has been added below.\nalbum_title\nrelease_title\nrelease\nrelease\nsong\nVersion 3 reorders identifiers like IPN, ISNI, etc into objects tied with the entities they are associated with. contributing_artists, artists, and featured_artists fields are explicitly defined to reduce interpretation. ipi array replaced with author array, which includes ipi key. Removed the parental_advisory field, as it was redundant (explicit is all players need to look for). lyricist is removed and merged into contributing_artist, under role. copyright adds master and composition to distinguish recording and composition copyright owners. Certain fields may be included in release within \"Album/EP\" release_type if they are qualifying GRAM (Group Registration for Works on an Album of Music) publications. This is done in order to help conserve data in otherwise redundant entries.\ncontributing_artists\nartists\nfeatured_artists\nipi\nauthor\nipi\nparental_advisory\nexplicit\nlyricist\ncontributing_artist\nrole\ncopyright\nmaster\ncomposition\nrelease\nrelease_type\nisni\nlinks\nsong\nrelease\nrelease\nsong\nsong\nsong\nrelease\nrelease_type\nsong\nsong\nrelease\ngenre\nrelease\nsong\nartists\nfeatured_artists\nartists\nfeatured_artist\nrelease\nrelease_type\nartists\nipn\nipi\nipi\nlinks\nrole\nsong\ncontributing_artists\nsong\ncontributing_artists\nauthors\nrelease\nrelease\nsong\nsong\nsong\nsong\nrelease\nrelease\nrelease\nrelease\nrelease\nsong\nsong\nsong\nrelease_type\nrelease\nsong\nsong\nsong\nrelease_type\nrelease\nsong\nsong\nrelease_type\nrelease\nsong\nfeautured_artists\nartists\nisni\nlinks\nsong\nsong\nrelease_type\nrelease\nsong\nsong\nsong\nsong\nipi\nrole\nshare\nsong\nsong\nauthors\ncontributing_artists\nsong\nauthors\nsong\nsong\n{ \"721\": { \"<policyId>\": { \"<assetName>\": { \"name\": \"<releaseName>\", \"image\": \"<mediaURL>\", \"music_metadata_version\": 3, \"release\": { \"release_type\": \"<Single/Multiple>\", \"release_title\": \"<releaseTitle>\", \"distributor\": \"<distributor>\" }, \"files\": [ { \"name\": \"<fileName>\", \"mediaType\": \"<mimeType>\", \"src\": \"<mediaURL>\", \"song\": { \"song_title\": \"<songName>\", \"song_duration\": \"PT<minutes>M<seconds>S\", \"track_number\": \"<track#>\", \"mood\": \"<mood>\", \"artists\": [ { \"name:\": \"<artistName>\", \"isni\": \"<isni>\", \"links\": { \"<linkName>\": \"<url>\", \"<link2Name>\": \"<url>\", \"<link3Name>\": \"<url>\" } }, { \"name:\": \"<artistName>\", \"isni\": \"<isni>\", \"links\": { \"<linkName>\": \"<url>\", \"<link2Name>\": \"<url>\", \"<link3Name>\": \"<url>\" } } ], \"featured_artists\": [ { \"name:\": \"<artistName>\", \"isni\": \"<isni>\", \"links\": { \"<linkName>\": \"<url>\", \"<link2Name>\": \"<url>\", \"<link3Name>\": \"<url>\" } }, { \"name:\": \"<artistName>\", \"isni\": \"<isni>\", \"links\": { \"<linkName>\": \"<url>\", \"<link2Name>\": \"<url>\", \"<link3Name>\": \"<url>\" } } ], \"authors\": [ { \"name\": \"<authorName>\", \"ipi\": \"<ipi>\", \"share\": \"<percentage>\" }, { \"name\": \"<authorName>\", \"ipi\": \"<ipi>\", \"share\": \"<percentage>\" }, { \"name\": \"<authorName>\", \"ipi\": \"<ipi>\", \"share\": \"<percentage>\" } ], \"contributing_artists\": [ { \"name\": \"<artistName>\", \"ipn\": \"<ipi>\", \"role\": [ \"<roleDescription>\", \"<roleDescription>\" ] }, { \"name\": \"<artistName>\", \"ipi\": \"<ipi>\", \"role\": [ \"<roleDescription>\", \"<roleDescription>\" ] }, { \"name\": \"<artistName>\", \"ipi\": \"<ipi>\", \"role\": [ \"<roleDescription>\", \"<roleDescription>\" ] } ], \"collection\": \"<collectionName>\", \"genres\": [ \"<genre>\", \"<genre>\", \"<genre>\" ], \"copyright\": {\"master\": \"℗ <year, copyrightHolder>\", \"composition\": \"© <year, copyrightHolder>\"} } } ] } } } }\n{ \"721\": { \"<policyId>\": { \"<assetName>\": { \"name\": \"<releaseName>\", \"image\": \"<mediaURL>\", \"music_metadata_version\": 3, \"release\": { \"release_type\": \"<Single/Multiple>\", \"release_title\": \"<releaseTitle>\", \"distributor\": \"<distributor>\" }, \"files\": [ { \"name\": \"<fileName>\", \"mediaType\": \"<mimeType>\", \"src\": \"<mediaURL>\", \"song\": { \"song_title\": \"<songName>\", \"song_duration\": \"PT<minutes>M<seconds>S\", \"track_number\": \"<track#>\", \"mood\": \"<mood>\", \"artists\": [ { \"name:\": \"<artistName>\", \"isni\": \"<isni>\", \"links\": { \"<linkName>\": \"<url>\", \"<link2Name>\": \"<url>\", \"<link3Name>\": \"<url>\" } }, { \"name:\": \"<artistName>\", \"isni\": \"<isni>\", \"links\": { \"<linkName>\": \"<url>\", \"<link2Name>\": \"<url>\", \"<link3Name>\": \"<url>\" } } ], \"featured_artists\": [ { \"name:\": \"<artistName>\", \"isni\": \"<isni>\", \"links\": { \"<linkName>\": \"<url>\", \"<link2Name>\": \"<url>\", \"<link3Name>\": \"<url>\" } }, { \"name:\": \"<artistName>\", \"isni\": \"<isni>\", \"links\": { \"<linkName>\": \"<url>\", \"<link2Name>\": \"<url>\", \"<link3Name>\": \"<url>\" } } ], \"authors\": [ { \"name\": \"<authorName>\", \"ipi\": \"<ipi>\", \"share\": \"<percentage>\" }, { \"name\": \"<authorName>\", \"ipi\": \"<ipi>\", \"share\": \"<percentage>\" }, { \"name\": \"<authorName>\", \"ipi\": \"<ipi>\", \"share\": \"<percentage>\" } ], \"contributing_artists\": [ { \"name\": \"<artistName>\", \"ipn\": \"<ipi>\", \"role\": [ \"<roleDescription>\", \"<roleDescription>\" ] }, { \"name\": \"<artistName>\", \"ipi\": \"<ipi>\", \"role\": [ \"<roleDescription>\", \"<roleDescription>\" ] }, { \"name\": \"<artistName>\", \"ipi\": \"<ipi>\", \"role\": [ \"<roleDescription>\", \"<roleDescription>\" ] } ], \"collection\": \"<collectionName>\", \"genres\": [ \"<genre>\", \"<genre>\", \"<genre>\" ], \"copyright\": {\"master\": \"℗ <year, copyrightHolder>\", \"composition\": \"© <year, copyrightHolder>\"} } } ] } } } }\n{ \"721\": { \"c00d776a22ca5db986039420b2a9b3f880d593136a9e2262fabeeb58\": { \"ZiplineFromOuterspace\": { \"name\": \"Refraktal - Zipline From Outerspace\", \"image\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"music_metadata_version\": 3, \"release\": { \"release_type\": \"Album/EP\", \"release_title\": \"Zipline From Outerspace\", \"copyright\": { \"master\": \"℗ 2024 Refraktal\", \"composition\": \"© 2024 Refraktal\" }, \"artists\": [ { \"name:\": \"Refraktal\", \"isni\": \"0000000517483974\", \"links\": { \"website\": \"https://refraktal.com\", \"exclusive_content\": \"https://refraktalnft.duckdns.org\" } } ], \"contributing_artists\": [ { \"name\": \"Sudo Scientist\", \"ipi\": \"1251891449\", \"role\": [ \"guitar on VOID and Lullaby for My Demons\", \"synth\", \"programming\" ] }, { \"name\": \"RX the Pharm Tech\", \"ipi\": \"1251891057\", \"role\": [ \"guitar on Bellywub\", \"synth\", \"programming\" ] } ], \"genre\": [ \"Electronic\", \"Experimental\", \"Psychedelic\" ] }, \"files\": [ { \"name\": \"Void\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Void\", \"song_duration\": \"PT4M21S\", \"track_number\": \"1\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Bellywub\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Bellywub\", \"song_duration\": \"PT5M31S\", \"track_number\": \"2\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Lullaby for my Demons\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Lullaby for My Demons\", \"song_duration\": \"PT3M11S\", \"track_number\": \"3\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Meliorism\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Meliorism\", \"song_duration\": \"PT4M21S\", \"track_number\": \"4\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Zipline From Outerspace\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Zipline From Outerspace\", \"song_duration\": \"PT3M36S\", \"track_number\": \"5\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT2M12S\", \"track_number\": \"6\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"7\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"8\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"9\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"10\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"11\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"12\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"13\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"14\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"15\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"16\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"17\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"18\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"19\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"20\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } } ] } } } }\n{ \"721\": { \"c00d776a22ca5db986039420b2a9b3f880d593136a9e2262fabeeb58\": { \"ZiplineFromOuterspace\": { \"name\": \"Refraktal - Zipline From Outerspace\", \"image\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"music_metadata_version\": 3, \"release\": { \"release_type\": \"Album/EP\", \"release_title\": \"Zipline From Outerspace\", \"copyright\": { \"master\": \"℗ 2024 Refraktal\", \"composition\": \"© 2024 Refraktal\" }, \"artists\": [ { \"name:\": \"Refraktal\", \"isni\": \"0000000517483974\", \"links\": { \"website\": \"https://refraktal.com\", \"exclusive_content\": \"https://refraktalnft.duckdns.org\" } } ], \"contributing_artists\": [ { \"name\": \"Sudo Scientist\", \"ipi\": \"1251891449\", \"role\": [ \"guitar on VOID and Lullaby for My Demons\", \"synth\", \"programming\" ] }, { \"name\": \"RX the Pharm Tech\", \"ipi\": \"1251891057\", \"role\": [ \"guitar on Bellywub\", \"synth\", \"programming\" ] } ], \"genre\": [ \"Electronic\", \"Experimental\", \"Psychedelic\" ] }, \"files\": [ { \"name\": \"Void\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Void\", \"song_duration\": \"PT4M21S\", \"track_number\": \"1\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Bellywub\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Bellywub\", \"song_duration\": \"PT5M31S\", \"track_number\": \"2\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Lullaby for my Demons\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Lullaby for My Demons\", \"song_duration\": \"PT3M11S\", \"track_number\": \"3\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Meliorism\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Meliorism\", \"song_duration\": \"PT4M21S\", \"track_number\": \"4\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Zipline From Outerspace\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Zipline From Outerspace\", \"song_duration\": \"PT3M36S\", \"track_number\": \"5\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT2M12S\", \"track_number\": \"6\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"7\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"8\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"9\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"10\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"11\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"12\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"13\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"14\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"15\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"16\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"17\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"18\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"19\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } }, { \"name\": \"Another Cool Song\", \"mediaType\": \"audio/wav\", \"src\": \"ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2\", \"song\": { \"song_title\": \"Another Cool Song\", \"song_duration\": \"PT3M36S\", \"track_number\": \"20\", \"isrc\": \"US-SKG-22-12345\", \"iswc\": \"T-123456789-Z\" } } ] } } } }\n{ \"constructor\": 0, \"fields\": [ { \"map\": [ {\"k\": {\"bytes\": \"373231\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"<encoded policyId>\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"<encoded assetName>\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded releaseName>\"}}, {\"k\": {\"bytes\": \"696D616765\"}, \"v\": {\"bytes\": \"<encoded mediaURL>\"}}, {\"k\": {\"bytes\": \"6D757369635F6D657461646174615F76657273696F6E\"}, \"v\": {\"int\": 3}}, {\"k\": {\"bytes\": \"72656C65617365\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"72656C656173655F74797065\"}, \"v\": {\"bytes\": \"<encoded Single/Multiple>\"}}, {\"k\": {\"bytes\": \"72656C656173655F7469746C65\"}, \"v\": {\"bytes\": \"<encoded releaseTitle>\"}}, {\"k\": {\"bytes\": \"6469737472696275746F72\"}, \"v\": {\"bytes\": \"<encoded distributor>\"}} ] } }, {\"k\": {\"bytes\": \"66696C6573\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded fileName>\"}}, {\"k\": {\"bytes\": \"6D6564696154797065\"}, \"v\": {\"bytes\": \"<encoded mimeType>\"}}, {\"k\": {\"bytes\": \"737263\"}, \"v\": {\"bytes\": \"<encoded mediaURL>\"}}, {\"k\": {\"bytes\": \"736F6E67\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"736F6E675F7469746C65\"}, \"v\": {\"bytes\": \"<encoded songName>\"}}, {\"k\": {\"bytes\": \"736F6E675F6475726174696F6E\"}, \"v\": {\"bytes\": \"<encoded PT<minutes>M<seconds>S>\"}}, {\"k\": {\"bytes\": \"747261636B5F6E756D626572\"}, \"v\": {\"int\": \"<track#>\"}}, {\"k\": {\"bytes\": \"6D6F6F64\"}, \"v\": {\"bytes\": \"<encoded mood>\"}}, {\"k\": {\"bytes\": \"61727469737473\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded artistName>\"}}, {\"k\": {\"bytes\": \"69736E69\"}, \"v\": {\"bytes\": \"<encoded ISNI>\"}}, {\"k\": {\"bytes\": \"6C696E6B73\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"<encoded linkName>\"}, \"v\": {\"bytes\": \"<encoded url>\"}}, {\"k\": {\"bytes\": \"<encoded link2Name>\"}, \"v\": {\"bytes\": \"<encoded url>\"}}, {\"k\": {\"bytes\": \"<encoded link3Name>\"}, \"v\": {\"bytes\": \"<encoded url>\"}} ] } } ] } ] } }, {\"k\": {\"bytes\": \"6665617475726564_61727469737473\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded artistName>\"}}, {\"k\": {\"bytes\": \"69736E69\"}, \"v\": {\"bytes\": \"<encoded ISNI>\"}}, {\"k\": {\"bytes\": \"6C696E6B73\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"<encoded linkName>\"}, \"v\": {\"bytes\": \"<encoded url>\"}}, {\"k\": {\"bytes\": \"<encoded link2Name>\"}, \"v\": {\"bytes\": \"<encoded url>\"}}, {\"k\": {\"bytes\": \"<encoded link3Name>\"}, \"v\": {\"bytes\": \"<encoded url>\"}} ] } } ] } ] } }, {\"k\": {\"bytes\": \"617574686F7273\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded authorName>\"}}, {\"k\": {\"bytes\": \"697069\"}, \"v\": {\"bytes\": \"<encoded IPI>\"}}, {\"k\": {\"bytes\": \"7368617265\"}, \"v\": {\"bytes\": \"<encoded percentage>\"}} ] } ] } }, {\"k\": {\"bytes\": \"636F6E747269627574696E675F61727469737473\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded artistName>\"}}, {\"k\": {\"bytes\": \"697069\"}, \"v\": {\"bytes\": \"<encoded IPI>\"}}, {\"k\": {\"bytes\": \"726F6C65\"}, \"v\": { \"array\": [ {\"bytes\": \"<encoded roleDescription>\"}, {\"bytes\": \"<encoded roleDescription>\"} ] } } ] } ] } }, {\"k\": {\"bytes\": \"636F6C6C656374696F6E\"}, \"v\": {\"bytes\": \"<encoded collectionName>\"}}, {\"k\": {\"bytes\": \"67656E726573\"}, \"v\": { \"array\": [ {\"bytes\": \"<encoded genre1>\"}, {\"bytes\": \"<encoded genre2>\"}, {\"bytes\": \"<encoded genre3>\"} ] } }, {\"k\": {\"bytes\": \"636F707972696768\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"6D6173746572\"}, \"v\": {\"bytes\": \"<encoded ℗ <year, copyrightHolder>\"}}, {\"k\": {\"bytes\": \"636F6D706F736974696F6E\"}, \"v\": {\"bytes\": \"<encoded © <year, copyrightHolder>\"}} ] } } ] } } ] } ] } } ] }} ] }} ] }} ] }, { \"int\": 1 } ] }\n{ \"constructor\": 0, \"fields\": [ { \"map\": [ {\"k\": {\"bytes\": \"373231\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"<encoded policyId>\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"<encoded assetName>\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded releaseName>\"}}, {\"k\": {\"bytes\": \"696D616765\"}, \"v\": {\"bytes\": \"<encoded mediaURL>\"}}, {\"k\": {\"bytes\": \"6D757369635F6D657461646174615F76657273696F6E\"}, \"v\": {\"int\": 3}}, {\"k\": {\"bytes\": \"72656C65617365\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"72656C656173655F74797065\"}, \"v\": {\"bytes\": \"<encoded Single/Multiple>\"}}, {\"k\": {\"bytes\": \"72656C656173655F7469746C65\"}, \"v\": {\"bytes\": \"<encoded releaseTitle>\"}}, {\"k\": {\"bytes\": \"6469737472696275746F72\"}, \"v\": {\"bytes\": \"<encoded distributor>\"}} ] } }, {\"k\": {\"bytes\": \"66696C6573\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded fileName>\"}}, {\"k\": {\"bytes\": \"6D6564696154797065\"}, \"v\": {\"bytes\": \"<encoded mimeType>\"}}, {\"k\": {\"bytes\": \"737263\"}, \"v\": {\"bytes\": \"<encoded mediaURL>\"}}, {\"k\": {\"bytes\": \"736F6E67\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"736F6E675F7469746C65\"}, \"v\": {\"bytes\": \"<encoded songName>\"}}, {\"k\": {\"bytes\": \"736F6E675F6475726174696F6E\"}, \"v\": {\"bytes\": \"<encoded PT<minutes>M<seconds>S>\"}}, {\"k\": {\"bytes\": \"747261636B5F6E756D626572\"}, \"v\": {\"int\": \"<track#>\"}}, {\"k\": {\"bytes\": \"6D6F6F64\"}, \"v\": {\"bytes\": \"<encoded mood>\"}}, {\"k\": {\"bytes\": \"61727469737473\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded artistName>\"}}, {\"k\": {\"bytes\": \"69736E69\"}, \"v\": {\"bytes\": \"<encoded ISNI>\"}}, {\"k\": {\"bytes\": \"6C696E6B73\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"<encoded linkName>\"}, \"v\": {\"bytes\": \"<encoded url>\"}}, {\"k\": {\"bytes\": \"<encoded link2Name>\"}, \"v\": {\"bytes\": \"<encoded url>\"}}, {\"k\": {\"bytes\": \"<encoded link3Name>\"}, \"v\": {\"bytes\": \"<encoded url>\"}} ] } } ] } ] } }, {\"k\": {\"bytes\": \"6665617475726564_61727469737473\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded artistName>\"}}, {\"k\": {\"bytes\": \"69736E69\"}, \"v\": {\"bytes\": \"<encoded ISNI>\"}}, {\"k\": {\"bytes\": \"6C696E6B73\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"<encoded linkName>\"}, \"v\": {\"bytes\": \"<encoded url>\"}}, {\"k\": {\"bytes\": \"<encoded link2Name>\"}, \"v\": {\"bytes\": \"<encoded url>\"}}, {\"k\": {\"bytes\": \"<encoded link3Name>\"}, \"v\": {\"bytes\": \"<encoded url>\"}} ] } } ] } ] } }, {\"k\": {\"bytes\": \"617574686F7273\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded authorName>\"}}, {\"k\": {\"bytes\": \"697069\"}, \"v\": {\"bytes\": \"<encoded IPI>\"}}, {\"k\": {\"bytes\": \"7368617265\"}, \"v\": {\"bytes\": \"<encoded percentage>\"}} ] } ] } }, {\"k\": {\"bytes\": \"636F6E747269627574696E675F61727469737473\"}, \"v\": { \"array\": [ { \"map\": [ {\"k\": {\"bytes\": \"6E616D65\"}, \"v\": {\"bytes\": \"<encoded artistName>\"}}, {\"k\": {\"bytes\": \"697069\"}, \"v\": {\"bytes\": \"<encoded IPI>\"}}, {\"k\": {\"bytes\": \"726F6C65\"}, \"v\": { \"array\": [ {\"bytes\": \"<encoded roleDescription>\"}, {\"bytes\": \"<encoded roleDescription>\"} ] } } ] } ] } }, {\"k\": {\"bytes\": \"636F6C6C656374696F6E\"}, \"v\": {\"bytes\": \"<encoded collectionName>\"}}, {\"k\": {\"bytes\": \"67656E726573\"}, \"v\": { \"array\": [ {\"bytes\": \"<encoded genre1>\"}, {\"bytes\": \"<encoded genre2>\"}, {\"bytes\": \"<encoded genre3>\"} ] } }, {\"k\": {\"bytes\": \"636F707972696768\"}, \"v\": { \"map\": [ {\"k\": {\"bytes\": \"6D6173746572\"}, \"v\": {\"bytes\": \"<encoded ℗ <year, copyrightHolder>\"}}, {\"k\": {\"bytes\": \"636F6D706F736974696F6E\"}, \"v\": {\"bytes\": \"<encoded © <year, copyrightHolder>\"}} ] } } ] } } ] } ] } } ] }} ] }} ] }} ] }, { \"int\": 1 } ] }\nImplementing this simplifies and commonizes the process for creating music tokens on Cardano. It greatly simplifies the work that apps have to make when consuming such tokens.\nThis CIP is the result of several online meetings between many different companies building music-related projects on top of Cardano. These meetings were organized as many in the community started to see fragmentation in the way music NFTs were being minted on Cardano. These meetings gave the opportunity for a bit of a reset and will allow a much brighter future for music on Cardano. As long as all projects agree on some of these basic fields, there is great flexibility in this CIP to do application-specific unique things on top of the music NFT itself. The CIP is intentionally open-ended and can be updated in future versions if there are additional fields that the wider group could benefit from.\nHas been implemented by a number of parties, including: SickCityNFT - sickcity.xyz NEWM - newm.io SoundRig - soundrig.io The Listening Room - https://thelr.io/ Jukeboys So Litty Records Arp Radio - https://arpradio.media\nSickCityNFT - sickcity.xyz\nNEWM - newm.io\nSoundRig - soundrig.io\nThe Listening Room - https://thelr.io/\nJukeboys\nSo Litty Records\nArp Radio - https://arpradio.media\nConsensus of companies building music-related Cardano projects to develop a mutually beneficial metadata vocabulary.\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0067 | Asset Name Label Registry\n\nThis proposal defines a standard to identify Cardano native assets by the asset name to put them in an asset class, as intended by their issuer.\nAs more assets are minted and different standards emerge to query data for these assets, it's getting harder for 3rd parties to determine the asset class and associated extra assumptions that may arise from this identification. For example, if an asset is identified as a non-fungible token, a third party is interested in its onchain associated metadata. This standard is similar to CIP-0010, but focuses on the asset name of a native asset.\nTo give issuers the option to classify assets, the asset_name MUST be prefixed with 4 bytes encoding the following binary value:\nasset_name\n[ 0000 | 16 bits label_num | 8 bits checksum | 0000 ]\n[ 0000 | 16 bits label_num | 8 bits checksum | 0000 ]\nThe leading and ending four 0s are brackets\nlabel_num has a fixed size of 2 bytes (Label range in decimal: [0, 65535]). If label_num 2 bytes, the remaining bits MUST be left-padded with 0s.\nlabel_num\nLabel range in decimal: [0, 65535]\nlabel_num\nchecksum has a fixed size of 1 byte. The checksum is calculated by applying the CRC-8 algorithm on the label_num (including the padded 0s).\nchecksum\nlabel_num (including the padded 0s)\nPolynomial: 0x07\n0x07\nLookup table:\n[ 0x00, 0x07, 0x0e, 0x09, 0x1c, 0x1b, 0x12, 0x15, 0x38, 0x3f, 0x36, 0x31, 0x24, 0x23, 0x2a, 0x2d, 0x70, 0x77, 0x7e, 0x79, 0x6c, 0x6b, 0x62, 0x65, 0x48, 0x4f, 0x46, 0x41, 0x54, 0x53, 0x5a, 0x5d, 0xe0, 0xe7, 0xee, 0xe9, 0xfc, 0xfb, 0xf2, 0xf5, 0xd8, 0xdf, 0xd6, 0xd1, 0xc4, 0xc3, 0xca, 0xcd, 0x90, 0x97, 0x9e, 0x99, 0x8c, 0x8b, 0x82, 0x85, 0xa8, 0xaf, 0xa6, 0xa1, 0xb4, 0xb3, 0xba, 0xbd, 0xc7, 0xc0, 0xc9, 0xce, 0xdb, 0xdc, 0xd5, 0xd2, 0xff, 0xf8, 0xf1, 0xf6, 0xe3, 0xe4, 0xed, 0xea, 0xb7, 0xb0, 0xb9, 0xbe, 0xab, 0xac, 0xa5, 0xa2, 0x8f, 0x88, 0x81, 0x86, 0x93, 0x94, 0x9d, 0x9a, 0x27, 0x20, 0x29, 0x2e, 0x3b, 0x3c, 0x35, 0x32, 0x1f, 0x18, 0x11, 0x16, 0x03, 0x04, 0x0d, 0x0a, 0x57, 0x50, 0x59, 0x5e, 0x4b, 0x4c, 0x45, 0x42, 0x6f, 0x68, 0x61, 0x66, 0x73, 0x74, 0x7d, 0x7a, 0x89, 0x8e, 0x87, 0x80, 0x95, 0x92, 0x9b, 0x9c, 0xb1, 0xb6, 0xbf, 0xb8, 0xad, 0xaa, 0xa3, 0xa4, 0xf9, 0xfe, 0xf7, 0xf0, 0xe5, 0xe2, 0xeb, 0xec, 0xc1, 0xc6, 0xcf, 0xc8, 0xdd, 0xda, 0xd3, 0xd4, 0x69, 0x6e, 0x67, 0x60, 0x75, 0x72, 0x7b, 0x7c, 0x51, 0x56, 0x5f, 0x58, 0x4d, 0x4a, 0x43, 0x44, 0x19, 0x1e, 0x17, 0x10, 0x05, 0x02, 0x0b, 0x0c, 0x21, 0x26, 0x2f, 0x28, 0x3d, 0x3a, 0x33, 0x34, 0x4e, 0x49, 0x40, 0x47, 0x52, 0x55, 0x5c, 0x5b, 0x76, 0x71, 0x78, 0x7f, 0x6a, 0x6d, 0x64, 0x63, 0x3e, 0x39, 0x30, 0x37, 0x22, 0x25, 0x2c, 0x2b, 0x06, 0x01, 0x08, 0x0f, 0x1a, 0x1d, 0x14, 0x13, 0xae, 0xa9, 0xa0, 0xa7, 0xb2, 0xb5, 0xbc, 0xbb, 0x96, 0x91, 0x98, 0x9f, 0x8a, 0x8d, 0x84, 0x83, 0xde, 0xd9, 0xd0, 0xd7, 0xc2, 0xc5, 0xcc, 0xcb, 0xe6, 0xe1, 0xe8, 0xef, 0xfa, 0xfd, 0xf4, 0xf3, ]\n[ 0x00, 0x07, 0x0e, 0x09, 0x1c, 0x1b, 0x12, 0x15, 0x38, 0x3f, 0x36, 0x31, 0x24, 0x23, 0x2a, 0x2d, 0x70, 0x77, 0x7e, 0x79, 0x6c, 0x6b, 0x62, 0x65, 0x48, 0x4f, 0x46, 0x41, 0x54, 0x53, 0x5a, 0x5d, 0xe0, 0xe7, 0xee, 0xe9, 0xfc, 0xfb, 0xf2, 0xf5, 0xd8, 0xdf, 0xd6, 0xd1, 0xc4, 0xc3, 0xca, 0xcd, 0x90, 0x97, 0x9e, 0x99, 0x8c, 0x8b, 0x82, 0x85, 0xa8, 0xaf, 0xa6, 0xa1, 0xb4, 0xb3, 0xba, 0xbd, 0xc7, 0xc0, 0xc9, 0xce, 0xdb, 0xdc, 0xd5, 0xd2, 0xff, 0xf8, 0xf1, 0xf6, 0xe3, 0xe4, 0xed, 0xea, 0xb7, 0xb0, 0xb9, 0xbe, 0xab, 0xac, 0xa5, 0xa2, 0x8f, 0x88, 0x81, 0x86, 0x93, 0x94, 0x9d, 0x9a, 0x27, 0x20, 0x29, 0x2e, 0x3b, 0x3c, 0x35, 0x32, 0x1f, 0x18, 0x11, 0x16, 0x03, 0x04, 0x0d, 0x0a, 0x57, 0x50, 0x59, 0x5e, 0x4b, 0x4c, 0x45, 0x42, 0x6f, 0x68, 0x61, 0x66, 0x73, 0x74, 0x7d, 0x7a, 0x89, 0x8e, 0x87, 0x80, 0x95, 0x92, 0x9b, 0x9c, 0xb1, 0xb6, 0xbf, 0xb8, 0xad, 0xaa, 0xa3, 0xa4, 0xf9, 0xfe, 0xf7, 0xf0, 0xe5, 0xe2, 0xeb, 0xec, 0xc1, 0xc6, 0xcf, 0xc8, 0xdd, 0xda, 0xd3, 0xd4, 0x69, 0x6e, 0x67, 0x60, 0x75, 0x72, 0x7b, 0x7c, 0x51, 0x56, 0x5f, 0x58, 0x4d, 0x4a, 0x43, 0x44, 0x19, 0x1e, 0x17, 0x10, 0x05, 0x02, 0x0b, 0x0c, 0x21, 0x26, 0x2f, 0x28, 0x3d, 0x3a, 0x33, 0x34, 0x4e, 0x49, 0x40, 0x47, 0x52, 0x55, 0x5c, 0x5b, 0x76, 0x71, 0x78, 0x7f, 0x6a, 0x6d, 0x64, 0x63, 0x3e, 0x39, 0x30, 0x37, 0x22, 0x25, 0x2c, 0x2b, 0x06, 0x01, 0x08, 0x0f, 0x1a, 0x1d, 0x14, 0x13, 0xae, 0xa9, 0xa0, 0xa7, 0xb2, 0xb5, 0xbc, 0xbb, 0x96, 0x91, 0x98, 0x9f, 0x8a, 0x8d, 0x84, 0x83, 0xde, 0xd9, 0xd0, 0xd7, 0xc2, 0xc5, 0xcc, 0xcb, 0xe6, 0xe1, 0xe8, 0xef, 0xfa, 0xfd, 0xf4, 0xf3, ]\nWe want to use the decimal label 222 for an asset name:\n222\nConvert to hex and pad with missing 0s = 0x00de Calculate CRC-8 checksum = 0x14 Add brackets and combine label = 0x000de140\nConvert to hex and pad with missing 0s = 0x00de\n0x00de\nCalculate CRC-8 checksum = 0x14\n0x14\nAdd brackets and combine label = 0x000de140\n0x000de140\nWe have the following asset name: 0x000de140\n0x000de140\nSlice off the first 4 bytes of the asset name = 0x000de140 Check if first 4 bits and last 4 bits are 0b0000 (0x0) Slice off the 2 label_num bytes and apply them to the CRC-8 algorithm. If the result matches with the checksum byte, a valid label was found and it can be returned. = 0x00de Convert to decimal = 222\nSlice off the first 4 bytes of the asset name = 0x000de140\n0x000de140\nCheck if first 4 bits and last 4 bits are 0b0000 (0x0)\n0b0000\n0x0\nSlice off the 2 label_num bytes and apply them to the CRC-8 algorithm. If the result matches with the checksum byte, a valid label was found and it can be returned. = 0x00de\nlabel_num\nchecksum\nvalid\n0x00de\nConvert to decimal = 222\n222\nThese are the reserved asset_name_label values\nasset_name_label\nasset_name_label\nThe keywords \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this section are to be interpreted as described in RFC 2119.\nThose wishing to propose an addition to this registry MUST draft a new CIP describing the standard for implementing the token. Once the CIP has achieved the Under Review status the proposer SHALL make the necessary edits to registry.json. These changes SHOULD be submitted under a separate pull request against the CIP repository and include a brief description of the standard and a link to the CIP Pull Request describing implementation details.\nUnder Review\nKeys represent labels in decimal numbers. Values represent the entire label, including brackets and checksum in hex:\ndecimal\nhex\n0 : 00000000 1 : 00001070 23 : 00017650 99 : 000632e0 533 : 00215410 2000 : 007d0550 4567 : 011d7690 11111 : 02b670b0 49328 : 0c0b0f40 65535 : 0ffff240\n0 : 00000000 1 : 00001070 23 : 00017650 99 : 000632e0 533 : 00215410 2000 : 007d0550 4567 : 011d7690 11111 : 02b670b0 49328 : 0c0b0f40 65535 : 0ffff240\nAsset name labels make it easy to identify native assets and classify them in their asset class intended by the issuer. Since the identification of these native assets is done by third parties, the design is focused on the usability for them.\nFirst, the label should be quickly parsable with a first check. That is, an initial check on an asset name that is easy and will exclude a big subset of the available token names that do not follow standard. This is why the label starts and ends with 0000 in bits. Additionally, in its hex notation, this is differentiable by a human in its readable form, a more common representation.\n0000\nSecondly, the remaining verification on whether a certain asset_name_label standard is followed should be a one shot calculation. Here we mean that the calculation of the check should be straightforward, the label should not be fitted via brute force by a third party. That's why the label contains the bit representation of the integer label it tries to follow.\nasset_name_label\nAnother thing that is important to understand is that an oblivious token issuer might not be aware of this standard. This could lead to the unintentional misinterpretation by third parties and injection attacks. We can minimize this attack vector by making the label format obscure. That is why the label also contains a checksum derived from the asset_name_label to add characters that are deterministically derived but look like nonsense. Together with the above zero \"brackets\", and the fixed size binary encoding, it make it unlikely someone follows this standard accidentally. The CRC-8 checksum is chosen for it low-impact on resources and its readily available implementations in multiple languages.\nasset_name_label\nGet support for this CIP by wallets, explorers, minting platforms and other 3rd parties.\nGet support by tools/libraries like Lucid, PlutusTx, cardano-cli, etc. to generate/verify labels.\nProvide reference implementations: Lucid TypeScript implementation of toLabel/fromLabel Lucid TypeScript implementation of CRC-8\nLucid TypeScript implementation of toLabel/fromLabel\nLucid TypeScript implementation of CRC-8\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0068 | Datum Metadata Standard\n\nThis proposal defines a metadata standard for native assets making use of output datums not only for NFTs but any asset class.\nThis proposal addresses a few shortcomings of CIP-0025:\nLack of programmability;\nDifficult metadata update / evolution;\nNon-inspectable metadata from within Plutus validators\nBesides these shortcomings CIP-0025 has some flaws in its design. For people unaware of CIP-0025 or want to use a different way of minting or want to use a different metadata format/mechanism you open up a protocol to metadata spoofing, because this standard is so established and metadata in minting transactions are interpreted by most platforms by default. Since this standard is not enforced at the protocol level there is no guarantee everyone will be aware of it or follow the rules. At the same time you limit and constraint the capabilities of the ledger if everyone was forced to follow the rules of CIP-0025.\nThis standard tackles all these problems and offers many more advantages, not only for NFTs, but also for any asset class that may follow. Additionally, this CIP will introduce a way to classify tokens so that third parties like wallets can easily know what the kind of token it is.\nThe basic idea is to have two assets issued, where one references the other. We call these two a reference NFT and an user token, where the user token can be an NFT, FT or any other asset class that is transferable and represents any value. So, the user token is the actual asset that lives in a user's wallet.\nreference NFT\nuser token\nuser token\nuser token\nTo find the metadata for the user token you need to look for the output, where the reference NFT is locked in. How this is done concretely will become clear below. Moreover, this output contains a datum, which holds the metadata. The advantage of this approach is that the issuer of the assets can decide how the transaction output with the reference NFT is locked and further handled. If the issuer wants complete immutable metadata, the reference NFT can be locked at the address of an unspendable script. Similarly, if the issuer wants the NFTs/FTs to evolve or wants a mechanism to update the metadata, the reference NFT can be locked at the address of a script with arbitrary logic that the issuer decides.\nuser token\nreference NFT\nreference NFT\nreference NFT\nreference NFT\nLastly and most importantly, with this construction, the metadata can be used by a Plutus V2 script with the use of reference inputs CIP-0031. This will drive further innovation in the token space.\nEach asset name must be prefixed by a label. The intent of this label is to identify the purpose of the token. For example, a reference NFT is identified by the label 100 and so every token considered a reference NFT should start its asset name with the hex 000643b0. This is following CIP-0067, which specifies how the label prefix should be formatted.\n000643b0\nExamples of asset names:\nFor simplicity purposes, the document will use the label (100) or ( label ) in the following documentation, but understand it should follow the CIP-0067 specification.\n(100)\n(<label>)\nThis is the registered asset_name_label value\nasset_name_label\nFor a correct relationship between the user token and the reference NFT a few conditions MUST be met.\nuser token\nreference NFT\nThe user token and reference NFT MUST be under the same policy ID.\nuser token\nreference NFT\nFor a specific user token there MUST exist exactly one reference NFT\nuser token\nreference NFT\nThe user token and associated reference NFT MUST follow the standard naming pattern. The asset name of both assets is prefixed with its respective asset_name_label followed by a pattern defined by the asset class (e.g. asset_name_label 222)\nuser token\nreference NFT\nasset_name_label\nSome remarks about the above,\nThe user token and reference NFT do not need to be minted in the same transaction. The order of minting is also not important. It may be the case that there can be multiple user tokens (multiple asset names or quantity greater than 1) referencing the same reference NFT.\nThe user token and reference NFT do not need to be minted in the same transaction. The order of minting is also not important.\nuser token\nreference NFT\nIt may be the case that there can be multiple user tokens (multiple asset names or quantity greater than 1) referencing the same reference NFT.\nuser tokens\nreference NFT\nThe datum in the output with the reference NFT contains the metadata at the first field of the constructor 0. The version number is at the second field of this constructor. The third field allows for arbitrary plutus data. This could be useful to forward relevant data to the plutus script:\nreference NFT\nbig_int = int / big_uint / big_nint big_uint = #6.2(bounded_bytes) big_nint = #6.3(bounded_bytes) metadata = { * metadata => metadata } / [ * metadata ] / big_int / bounded_bytes version = int ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data datum = #6.121([metadata, version, extra])\nbig_int = int / big_uint / big_nint big_uint = #6.2(bounded_bytes) big_nint = #6.3(bounded_bytes) metadata = { * metadata => metadata } / [ * metadata ] / big_int / bounded_bytes version = int ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data datum = #6.121([metadata, version, extra])\nNote Since version = 1\nversion >= 1\nBesides the necessary standard for the reference NFT we're introducing three specific token standards in this CIP. Note that the possibilities are endless here and more standards can be built on top of this CIP for FTs, other NFTs, rich fungible tokens, etc. The first is the 222 NFT standard with the registered asset_name_label prefix value\nreference NFT\n222\nasset_name_label\nThe user token represents an NFT (non-fungible token).\nuser token\nThe user token and reference NFT MUST have an identical name, preceded by the asset_name_label prefix.\nuser token\nreference NFT\nasset_name_label\nExample: user token: (222)Test123 reference NFT: (100)Test123\nuser token\n(222)Test123\nreference NFT\n(100)Test123\nThis is a low-level representation of the metadata, following closely the structure of CIP-0025. All UTF-8 encoded keys and values need to be converted into their respective byte's representation when creating the datum on-chain.\nfiles_details = { ? name : bounded_bytes, ; UTF-8 mediaType : bounded_bytes, ; UTF-8 src : uri, ; ... Additional properties are allowed } metadata = { name : bounded_bytes, ; UTF-8 ; The image URI must point to a resource with media type (mime type) `image/*` ; (for example `image/png`, `image/jpeg`, `image/svg+xml`, etc.) image : uri, ? description : bounded_bytes, ; UTF-8 ? files : [* files_details] ; ... Additional properties are allowed } ; A valid Uniform Resource Identifier (URI) as a UTF-8 encoded bytestring. ; The URI scheme must be one of `https` (HTTP), `ipfs` (IPFS), `ar` (Arweave) or `data` (on-chain). ; Data URLs (on-chain data) must comply to RFC2397. uri = bounded_bytes / [ * bounded_bytes ] ; UTF-8 ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data datum = #6.121([metadata, version, extra]) version = 1 / 2 / 3\nfiles_details = { ? name : bounded_bytes, ; UTF-8 mediaType : bounded_bytes, ; UTF-8 src : uri, ; ... Additional properties are allowed } metadata = { name : bounded_bytes, ; UTF-8 ; The image URI must point to a resource with media type (mime type) `image/*` ; (for example `image/png`, `image/jpeg`, `image/svg+xml`, etc.) image : uri, ? description : bounded_bytes, ; UTF-8 ? files : [* files_details] ; ... Additional properties are allowed } ; A valid Uniform Resource Identifier (URI) as a UTF-8 encoded bytestring. ; The URI scheme must be one of `https` (HTTP), `ipfs` (IPFS), `ar` (Arweave) or `data` (on-chain). ; Data URLs (on-chain data) must comply to RFC2397. uri = bounded_bytes / [ * bounded_bytes ] ; UTF-8 ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data datum = #6.121([metadata, version, extra]) version = 1 / 2 / 3\nExample datum as JSON:\n{ \"constructor\": 0, \"fields\": [ { \"map\": [ { \"k\": { \"bytes\": \"6E616D65\" }, \"v\": { \"bytes\": \"5370616365427564\" } }, { \"k\": { \"bytes\": \"696D616765\" }, \"v\": { \"bytes\": \"697066733A2F2F74657374\" } } ] }, { \"int\": 1 } ] }\n{ \"constructor\": 0, \"fields\": [ { \"map\": [ { \"k\": { \"bytes\": \"6E616D65\" }, \"v\": { \"bytes\": \"5370616365427564\" } }, { \"k\": { \"bytes\": \"696D616765\" }, \"v\": { \"bytes\": \"697066733A2F2F74657374\" } } ] }, { \"int\": 1 } ] }\nA third party has the following NFT d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken they want to lookup. The steps are\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken Look up reference NFT and find the output it's locked in. Get the datum from the output and lookup metadata by going into the first field of constructor 0. Convert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken\nreference NFT\nuser token\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken\nLook up reference NFT and find the output it's locked in.\nreference NFT\nGet the datum from the output and lookup metadata by going into the first field of constructor 0.\nConvert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.\nWe want to bring the metadata of the NFT d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken in the Plutus validator context. To do this we\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken (off-chain) Look up reference NFT and find the output it's locked in. (off-chain) Reference the output in the transaction. (off-chain) Verify validity of datum of the referenced output by checking if policy ID of reference NFT and user token and their asset names without the asset_name_label prefix match. (on-chain)\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken (off-chain)\nreference NFT\nuser token\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken\nLook up reference NFT and find the output it's locked in. (off-chain)\nreference NFT\nReference the output in the transaction. (off-chain)\nVerify validity of datum of the referenced output by checking if policy ID of reference NFT and user token and their asset names without the asset_name_label prefix match. (on-chain)\nreference NFT\nuser token\nasset_name_label\nNote Since version = 1\nversion >= 1\nThe second introduced standard is the 333 FT standard with the registered asset_name_label prefix value\n333\nasset_name_label\nThe user token is an FT (fungible token).\nuser token\nThe user token and reference NFT MUST have an identical name, preceded by the asset_name_label prefix.\nuser token\nreference NFT\nasset_name_label\nExample: user token: (333)Test123 reference NFT: (100)Test123\nuser token\n(333)Test123\nreference NFT\n(100)Test123\nThis is a low-level representation of the metadata, following closely the structure of the Cardano foundation off-chain metadata registry. All UTF-8 encoded keys and values need to be converted into their respective byte's representation when creating the datum on-chain.\n; Explanation here: https://developers.cardano.org/docs/native-tokens/token-registry/cardano-token-registry/ metadata = { name : bounded_bytes, ; UTF-8 description : bounded_bytes, ; UTF-8 ? ticker: bounded_bytes, ; UTF-8 ? url: bounded_bytes, ; UTF-8 ? decimals: int ; 'logo' does not follow the explanation of the token-registry, it needs to be a valid URI and not a plain bytestring. ; The logo URI must point to a resource with media type (mime type) `image/png`, `image/jpeg` or `image/svg+xml`. ? logo: uri, ; ... Additional properties are allowed } ; A valid Uniform Resource Identifier (URI) as a UTF-8 encoded bytestring. ; The URI scheme must be one of `https` (HTTP), `ipfs` (IPFS), `ar` (Arweave) or `data` (on-chain). ; Data URLs (on-chain data) must comply to RFC2397. uri = bounded_bytes / [ * bounded_bytes ] ; UTF-8 ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data datum = #6.121([metadata, version, extra]) version = 1 / 2 / 3\n; Explanation here: https://developers.cardano.org/docs/native-tokens/token-registry/cardano-token-registry/ metadata = { name : bounded_bytes, ; UTF-8 description : bounded_bytes, ; UTF-8 ? ticker: bounded_bytes, ; UTF-8 ? url: bounded_bytes, ; UTF-8 ? decimals: int ; 'logo' does not follow the explanation of the token-registry, it needs to be a valid URI and not a plain bytestring. ; The logo URI must point to a resource with media type (mime type) `image/png`, `image/jpeg` or `image/svg+xml`. ? logo: uri, ; ... Additional properties are allowed } ; A valid Uniform Resource Identifier (URI) as a UTF-8 encoded bytestring. ; The URI scheme must be one of `https` (HTTP), `ipfs` (IPFS), `ar` (Arweave) or `data` (on-chain). ; Data URLs (on-chain data) must comply to RFC2397. uri = bounded_bytes / [ * bounded_bytes ] ; UTF-8 ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data datum = #6.121([metadata, version, extra]) version = 1 / 2 / 3\nExample datum as JSON:\n{ \"constructor\": 0, \"fields\": [ { \"map\": [ { \"k\": { \"bytes\": \"6E616D65\" }, \"v\": { \"bytes\": \"5370616365427564\" } }, { \"k\": { \"bytes\": \"6465736372697074696F6E\" }, \"v\": { \"bytes\": \"54686973206973206D79207465737420746F6B656E\" } } ] }, { \"int\": 1 } ] }\n{ \"constructor\": 0, \"fields\": [ { \"map\": [ { \"k\": { \"bytes\": \"6E616D65\" }, \"v\": { \"bytes\": \"5370616365427564\" } }, { \"k\": { \"bytes\": \"6465736372697074696F6E\" }, \"v\": { \"bytes\": \"54686973206973206D79207465737420746F6B656E\" } } ] }, { \"int\": 1 } ] }\nA third party has the following FT d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(333)TestToken they want to lookup. The steps are\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(333)TestToken\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken Look up reference NFT and find the output it's locked in. Get the datum from the output and lookup metadata by going into the first field of constructor 0. Convert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken\nreference NFT\nuser token\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken\nLook up reference NFT and find the output it's locked in.\nreference NFT\nGet the datum from the output and lookup metadata by going into the first field of constructor 0.\nConvert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.\nWe want to bring the metadata of the FT d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(333)TestToken in the Plutus validator context. To do this we\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(333)TestToken\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken (off-chain) Look up reference NFT and find the output it's locked in. (off-chain) Reference the output in the transaction. (off-chain) Verify validity of datum of the referenced output by checking if policy ID of reference NFT and user token and their asset names without the asset_name_label prefix match. (on-chain)\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken (off-chain)\nreference NFT\nuser token\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken\nLook up reference NFT and find the output it's locked in. (off-chain)\nreference NFT\nReference the output in the transaction. (off-chain)\nVerify validity of datum of the referenced output by checking if policy ID of reference NFT and user token and their asset names without the asset_name_label prefix match. (on-chain)\nreference NFT\nuser token\nasset_name_label\nWarning Since version = 2\nversion >= 2\nThe third introduced standard is the 444 Rich-FT standard with the registered asset_name_label prefix value\n444\nasset_name_label\nRich-Fungible tokens don't fit cleanly into the other two FT/NFT classes of tokens and thus need their own standard. An example of an RFT would be a fractionalized NFT. The single reference NFT (100) represents the NFT itself, and the many (444) tokens represent the fractionalized shares. Minting 100 tokens and setting decimals to 2 would represent a single NFT that is split into 100 fractions.\n(100)\n(444)\nThe user token is an RFT (rich-fungible token).\nuser token\nThe user token and reference NFT MUST have an identical name, preceded by the asset_name_label prefix.\nuser token\nreference NFT\nasset_name_label\nExample: user token: (444)Test123 reference NFT: (100)Test123\nuser token\n(444)Test123\nreference NFT\n(100)Test123\nThis is a low-level representation of the metadata, following closely the structure of CIP-0025 with the optional decimals field added. All UTF-8 encoded keys and values need to be converted into their respective byte's representation when creating the datum on-chain.\nfiles_details = { ? name : bounded_bytes, ; UTF-8 mediaType : bounded_bytes, ; UTF-8 src : uri, ; ... Additional properties are allowed } metadata = { name : bounded_bytes, ; UTF-8 ; The image URI must point to a resource with media type (mime type) `image/*` ; (for example `image/png`, `image/jpeg`, `image/svg+xml`, etc.) image : uri, ? description : bounded_bytes, ; UTF-8 ? decimals: int, ? files : [* files_details] ; ... Additional properties are allowed } ; A valid Uniform Resource Identifier (URI) as a UTF-8 encoded bytestring. ; The URI scheme must be one of `https` (HTTP), `ipfs` (IPFS), `ar` (Arweave) or `data` (on-chain). ; Data URLs (on-chain data) must comply to RFC2397. uri = bounded_bytes ; UTF-8 ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data datum = #6.121([metadata, version, extra]) version = 3\nfiles_details = { ? name : bounded_bytes, ; UTF-8 mediaType : bounded_bytes, ; UTF-8 src : uri, ; ... Additional properties are allowed } metadata = { name : bounded_bytes, ; UTF-8 ; The image URI must point to a resource with media type (mime type) `image/*` ; (for example `image/png`, `image/jpeg`, `image/svg+xml`, etc.) image : uri, ? description : bounded_bytes, ; UTF-8 ? decimals: int, ? files : [* files_details] ; ... Additional properties are allowed } ; A valid Uniform Resource Identifier (URI) as a UTF-8 encoded bytestring. ; The URI scheme must be one of `https` (HTTP), `ipfs` (IPFS), `ar` (Arweave) or `data` (on-chain). ; Data URLs (on-chain data) must comply to RFC2397. uri = bounded_bytes ; UTF-8 ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data datum = #6.121([metadata, version, extra]) version = 3\nExample datum as JSON:\n{ \"constructor\": 0, \"fields\": [ { \"map\": [ { \"k\": { \"bytes\": \"6E616D65\" }, \"v\": { \"bytes\": \"5370616365427564\" } }, { \"k\": { \"bytes\": \"6465736372697074696F6E\" }, \"v\": { \"bytes\": \"54686973206973206D79207465737420746F6B656E\" } }, { \"k\": { \"bytes\": \"696D616765\" }, \"v\": { \"bytes\": \"697066733A2F2F74657374\" } }, { \"k\": { \"bytes\": \"646563696D616C73\" }, \"v\": { \"int\": 2 } } ] }, { \"int\": 1 } ] }\n{ \"constructor\": 0, \"fields\": [ { \"map\": [ { \"k\": { \"bytes\": \"6E616D65\" }, \"v\": { \"bytes\": \"5370616365427564\" } }, { \"k\": { \"bytes\": \"6465736372697074696F6E\" }, \"v\": { \"bytes\": \"54686973206973206D79207465737420746F6B656E\" } }, { \"k\": { \"bytes\": \"696D616765\" }, \"v\": { \"bytes\": \"697066733A2F2F74657374\" } }, { \"k\": { \"bytes\": \"646563696D616C73\" }, \"v\": { \"int\": 2 } } ] }, { \"int\": 1 } ] }\nA third party has the following RFT d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(444)TestToken they want to lookup. The steps are\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(444)TestToken\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken Look up reference NFT and find the output it's locked in. Get the datum from the output and lookup metadata by going into the first field of constructor 0. Convert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken\nreference NFT\nuser token\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken\nLook up reference NFT and find the output it's locked in.\nreference NFT\nGet the datum from the output and lookup metadata by going into the first field of constructor 0.\nConvert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.\nWe want to bring the metadata of the RFT d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(444)TestToken in the Plutus validator context. To do this we\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(444)TestToken\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken (off-chain) Look up reference NFT and find the output it's locked in. (off-chain) Reference the output in the transaction. (off-chain) Verify validity of datum of the referenced output by checking if policy ID of reference NFT and user token and their asset names without the asset_name_label prefix match. (on-chain)\nConstruct reference NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken (off-chain)\nreference NFT\nuser token\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken\nLook up reference NFT and find the output it's locked in. (off-chain)\nreference NFT\nReference the output in the transaction. (off-chain)\nVerify validity of datum of the referenced output by checking if policy ID of reference NFT and user token and their asset names without the asset_name_label prefix match. (on-chain)\nreference NFT\nuser token\nasset_name_label\nThe keywords \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this section are to be interpreted as described in RFC 2119.\nAll CIPs proposing to modify or extend this standard MUST include the language or a reference link to the extension and modification language found in the Extension Boilerplate.\nIn order to prevent conflicting updates in the future; the addition of new asset classes following, or as part of, this standard MUST be submitted as a new CIP providing their own justification, implementation, rationale, and community review prior to official acceptance. Newly proposed asset_name_labels SHOULD NOT be added to CIP-0067 until the accompanying CIP has matured through the community review and feedback stage to a point that it is considered in the Under Review status and is assigned a tentative CIP number by the CIP Editors panel.\nasset_name_labels\nUnder Review\nA brief reference to new asset classes MAY be added to this document after the accompanying CIP achieves the accepted status. Documentation describing these token asset classes MUST be fully encapsulated within their individual CIPs and a link MUST be provided to that CIP within this document.\naccepted\nIf a modification or change is deemed necessary to one of the asset classes contained within this document: namely Asset Name Labels: 100, 222, 333, or 444; which do not fundamentally change the nature, use, or reference of the tokens; it MAY be made as a modification of this document. However, any change proposed that presents a non-backwards compatible change MUST include an accompanying version field iteration and both specifications for the proposed, current, and historical versions of the format MUST be maintained to assist future implementors who may encounter a version of these tokens from any point in time with the following format:\nversion\n#### Versions 1. [6d897eb](https://github.com/cardano-foundation/CIPs/tree/6d897eb60805a58a3e54821fe61284d5c5903764/CIP-XXXX) 2. [45fa23b](https://github.com/cardano-foundation/CIPs/tree/45fa23b60806367a3e52231e552c4d7654237678/CIP-XXXX) 3. [bfc6fde](https://github.com/cardano-foundation/CIPs/tree/bfc6fde340280d8b51f5a7131b57f4cc6cc5f260/CIP-XXXX) 4. **Current**\n#### Versions 1. [6d897eb](https://github.com/cardano-foundation/CIPs/tree/6d897eb60805a58a3e54821fe61284d5c5903764/CIP-XXXX) 2. [45fa23b](https://github.com/cardano-foundation/CIPs/tree/45fa23b60806367a3e52231e552c4d7654237678/CIP-XXXX) 3. [bfc6fde](https://github.com/cardano-foundation/CIPs/tree/bfc6fde340280d8b51f5a7131b57f4cc6cc5f260/CIP-XXXX) 4. **Current**\nEach time a new version is introduced the previous version's link MUST be updated to match the last commit corresponding to the previous version.\nIf a change is proposed that would fundamentally alter the nature of one or more of the asset_name_labels and their associated tokens contained within this document, namely Asset Name Labels: 100, 222, 333, or 444; these changes MUST be submitted via a new, separate CIP with its own justification, implementation, rationale, and community review prior to official acceptance. These separate CIPs MUST include a plan for the obsolescence of any previous versions of the affected tokens. asset_name_labels MUST only be marked obsolete once a modifying CIP achieves the accepted status.\nasset_name_labels\nasset_name_labels\naccepted\nNFT (222) & FT (333) asset classes\nAdded new RFT asset class (444)\nAdded [* bounded_bytes] support to the image and src tags on the metadata\nWithout separation of reference NFT and user token you lose all flexibility and moving the user token would be quite cumbersome as you would need to add the metadata everytime to the new output where the user token is sent to. Hence, you separate metadata and user token and lock the metadata inside another UTxO, so you can freely move the user token around.\nreference NFT\nuser token\nuser token\nuser token\nuser token\nuser token\nIn order to reference the correct UTxO containing the metadata, it needs to be authenticated, otherwise metadata spoofing attacks become possible. One way to achieve that is by adding an NFT (reference NFT) to the UTxO. This NFT needs to under the same Policy ID as the user token, followed by an asset name pattern defined in the standard. This way you create a secure link between reference NFT and user token without the need for any extra data, and you can make use of this off-chain and on-chain.\nreference NFT\nuser token\nreference NFT\nuser token\nThe security for the link is derived from the minting policy itself, so it's important to write the validator with the right constraints and rules since this CIP solely defines the interface to keep flexibility as high as possible.\nTo keep metadata compatibility with changes coming in the future, we introduce a version field in the datum.\nversion\nOpen-source more practical implementations/projects which make use of this CIP.\nIntroduce a version integer datum field to increment for new asset classes or changes to the on-chain format.\nversion\nAgree on a binary encoding for asset name labels in CIP-0067.\nGet support for this CIP by wallets, explorers, tools, minting platforms and other 3rd parties.\nMinimal reference implementation making use of Lucid ( off-chain), PlutusTx (on-chain): Implementation\nCIP 25 - Media NFT Metadata Standard\nCIP 31 - Reference inputs\nCIP 67 - Asset Name Label Registry\nRFC 3986 - Uniform Resource Identifier (URI)\nRFC 2397 - The \"data\" URL scheme\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0069 | Script Signature Unification\n\nThis CIP unifies the arguments given to all types of Plutus scripts currently available (spending, certifying, rewarding, minting) by removing the argument of a datum.\nFor a while now every builder, myself included have struggled with the mutual dependency issue (two validators that need to know each other's hash) when designing dapps and it is widely considered a substantial barrier to safe protocols and one that limits our design space considerably.\nThe exact change would be to have every validator take as argument the redeemer and then the script context. Datums, only relevant to locking validators would be able to be provided by either looking them up in the ScriptContext or by extending the Spending constructor of TxInfo to carry (TxOutRef, Datum).\nSpending\nTxInfo\n(TxOutRef, Datum)\nAs it stands the scripts being made on cardano often suffer this problem, and the tokens usually are made to be able to be minted at any time. This leads to further checks being made on the frontend and further fragilitiy of the systems we create. When a mutual dependency arises we are forced to choose which script gets to statically know what's the hash of the other, and which has to be provided 'during runtime'.\nUse Case 1: Minting validator checks datum given to spending validator. The spending validator requires the token be present as witness of the datum's correctness.\nUse Case 1: Minting validator checks datum given to spending validator. The spending validator requires the token be present as witness of the datum's correctness.\nUse Case 2 (taken from Optim's Liquidity Bonds): Unique NFT is minted to give a unique identifier to a loan, that then gets reused by Bond Tokens. The spending validators require that NFT be present.\nUse Case 2 (taken from Optim's Liquidity Bonds): Unique NFT is minted to give a unique identifier to a loan, that then gets reused by Bond Tokens. The spending validators require that NFT be present.\nUse Case 3 (taken from Minswap's Dex V1): NFT is minted for the same reason as above. It allows a minting policy to later mint LP tokens with that unique id token name.\nUse Case 3 (taken from Minswap's Dex V1): NFT is minted for the same reason as above. It allows a minting policy to later mint LP tokens with that unique id token name.\nWe see a similar pattern repeating over and over again as witnessed by dapp developers and auditors alike. By allowing the multi-purpose scripts (spending and any other) we increase the security of Cardano by giving us more confidence and allowing to design protocols that have their architecture driven by Cardano's features, not limited by Cardano's language.\nThis primarily manifests in the ability to use a single validator for both minting and spending but the proposed solution makes it possible to use one validator for any and all purposes at once.\nOne of the major footguns of Plutus scripts is if a user sends to the script with a wrong or missing datum. This has happened in the case of the Nami wallet having a bug that caused the wrong address to be chosen. There are other instances of user error where they send from a CEX to a script address. A wrong datum can be handled by the Plutus scripts themselves by having an alternative execution branch if type does not match the expected datum type. But in the case of no datum the script is not run and fails in phase 1 validation. The other motivation of this CIP is to be able to create spend scripts that can handle the no datum case.\nI see three major use cases when it comes to running spend scripts without datums:\nUse Case 1: A script that acts as a wallet for users. By having no datum spending the user can send directly from exchanges or have friends send to their smart contract wallet with no datum needed.\nUse Case 1: A script that acts as a wallet for users. By having no datum spending the user can send directly from exchanges or have friends send to their smart contract wallet with no datum needed.\nUse Case 2: As a DAO treasury. The funds in this script would be controlled by a DAO and anyone can donate/contribute to the DAO without a datum.\nUse Case 2: As a DAO treasury. The funds in this script would be controlled by a DAO and anyone can donate/contribute to the DAO without a datum.\nUse Case 3: Allow dApp protocols to have a claim or withdraw mechanism (similar to Ethereum for tokens sent to a contract without call) for claiming tokens sent without a datum.\nUse Case 3: Allow dApp protocols to have a claim or withdraw mechanism (similar to Ethereum for tokens sent to a contract without call) for claiming tokens sent without a datum.\nI'd be remiss if I didn't mention CIP-0112 which has been expanded to improve native script capabilities to provide an alternative solution for use case 1 and 2. But for use case 3, CIP-0112 does not enable a \"claim or withdraw mechanism\" for contracts.\nAll the script purposes have a form of Redeemer - ScriptContext - a except the Spending one. It has the following form: Datum - Redeemer - ScriptContext - a. This is enforced by the Cardano ledger.\nRedeemer -> ScriptContext -> a\nDatum -> Redeemer -> ScriptContext -> a\nWe propose to make the following modification:\nThe signature of all scripts will be ScriptContext - a. The ScriptInfo type is a union type with a variant for each script purpose. It is the same as ScriptPurpose, except for the additional optional datum in spending scripts.\nScriptContext -> a\nScriptInfo\nScriptPurpose\n-- | The context that the currently-executing script can access. data ScriptContext = ScriptContext { scriptContextTxInfo :: TxInfo -- ^ information about the transaction the currently-executing script is included in , scriptContextRedeemer :: V2.Redeemer -- ^ Redeemer for the currently-executing script , scriptContextScriptInfo :: ScriptInfo -- ^ the purpose of the currently-executing script, along with information associated -- with the purpose } -- | Like `ScriptPurpose` but with an optional datum for spending scripts. data ScriptInfo = MintingScript V2.CurrencySymbol | SpendingScript V3.TxOutRef (Haskell.Maybe V2.Datum) | RewardingScript V2.Credential | CertifyingScript Haskell.Integer -- ^ 0-based index of the given `TxCert` in `txInfoTxCerts` TxCert | VotingScript Voter | ProposingScript Haskell.Integer -- ^ 0-based index of the given `ProposalProcedure` in `txInfoProposalProcedures` ProposalProcedure\n-- | The context that the currently-executing script can access. data ScriptContext = ScriptContext { scriptContextTxInfo :: TxInfo -- ^ information about the transaction the currently-executing script is included in , scriptContextRedeemer :: V2.Redeemer -- ^ Redeemer for the currently-executing script , scriptContextScriptInfo :: ScriptInfo -- ^ the purpose of the currently-executing script, along with information associated -- with the purpose } -- | Like `ScriptPurpose` but with an optional datum for spending scripts. data ScriptInfo = MintingScript V2.CurrencySymbol | SpendingScript V3.TxOutRef (Haskell.Maybe V2.Datum) | RewardingScript V2.Credential | CertifyingScript Haskell.Integer -- ^ 0-based index of the given `TxCert` in `txInfoTxCerts` TxCert | VotingScript Voter | ProposingScript Haskell.Integer -- ^ 0-based index of the given `ProposalProcedure` in `txInfoProposalProcedures` ProposalProcedure\nThe datum in SpendingScript is optional, which will allow the execution of spending scripts without a datum. One more change will be needed on the ledger side in order to make the Datum optional for spending scripts. The ledger UTXOW rule needs to be relaxed, this ledger rule checks if a utxo has an existing datum if the address's payment credential is a phase 2 validation script.\nSpendingScript\nThe ScriptPurpose type used in the Redeemers Map is left the same. It is used to uniquely identify a Plutus script within a transaction.\nUnifying of the script signature is a very elegant solution to the problem, streamlining the experience of developing on cardano. It begs the question if it should be added as an argument to all validators, to further emphasize that fact.\nThis CIP turns all scripts into 1 arg scripts with a Script Context union type for each purpose.\nThis change is not backwards compatible; it must be introduced in a new Plutus language version. Node code must be modified.\nThe change has been implemented in the Plutus codebase, integrated in the ledger and released through a hard-fork. Included within the Chang #1 hardfork\nIncluded within the Chang #1 hardfork\nThe Cardano Ledger and Cardano Plutus teams would need to implement this in following repositories: IntersectMBO/plutus IntersectMBO/cardano-ledger\nThe following languages that compile to uplc would need to update to support the new ScriptContext argument that is passed in for the next Plutus Version: Aiken Helios Opshin Plu-ts Plutarch Scalus\nThis CIP is licensed under Apache-2.0\n2023 Cardano Foundation\n\n---\n\nCIP-0071 | Non-Fungible Token (NFT) Proxy Voting Standard\n\nThis proposal uses plutus minting policies to create valid \"ballots\" that are sent alongside datum \"votes\" to a centralized smart contract \"ballot box\" in order to perform verifiable on-chain voting in NFT projects that do not have a governance token.\nThis proposal is intended to provide a standard mechanism for non-fungible token (NFT) projects to perform on-chain verifiable votes using only their NFT assets. There are several proposed solutions for governance that involve using either a service provider (e.g., Summon) with native assets or the issuance of proprietary native assets. However, there are several issues with these approaches:\nAirdrops of governance tokens require minUTxO ada attached, costing the NFT project ada out of pocket\nFungible tokens do not have a 1:1 mechanism for tracking votes against specific assets with voting power\nSale of the underlying NFT is not tied to sale of the governance token, creating separate asset classes and leaving voting power potentially with those who are no longer holders of the NFT\nThis standard provides a simple solution for this by using the underlying NFT to mint a one-time \"ballot\" token that can be used only for a specific voting topic. Projects that adopt this standard will be able to integrate with web viewers that track projects' governance votes and will also receive the benefits of verifiable on-chain voting without requiring issuance of a new native token.\nWe anticipate some potential use cases:\nEnforcing an exact 1:1 vote based on a user's existing NFT project holdings\nEnforcing vote validity by rejecting invalid vote options (e.g., disallowing write-ins)\nCreating \"super-votes\" based on an NFT serial number (e.g., rare NFTs in the 9,000-10,000 serial range get 2x votes)\nWarning This specification is not intended for for governance against fungible tokens that cannot be labeled individually.\nThe basic analogy here is that of a traditional state or federal vote. Imagine a citizen who has a state ID (e.g., Driver's License) and wants to vote, as well as a central voting authority that counts all the ballots.\nCitizens go to to a precinct and show their ID to the appropriate authority Citizens receive a ballot with choices for the current vote Citizens mark their selections on the ballot Citizens sign their ballot with their name Citizens submit their ballot into a single \"ballot box\" Central voting authorities process the vote after polls close Citizens await the election results through a trusted news outlet\nCitizens go to to a precinct and show their ID to the appropriate authority\nCitizens receive a ballot with choices for the current vote\nCitizens mark their selections on the ballot\nCitizens sign their ballot with their name\nCitizens submit their ballot into a single \"ballot box\"\nCentral voting authorities process the vote after polls close\nCitizens await the election results through a trusted news outlet\nThis specification follows the same process, but using tokens:\nA holder of a project validates their NFT by sending it to self The holder signs a Plutus minting policy to create a \"ballot\" NFT linked to their unique NFT The holder marks their desired vote selections on the ballot The holder signs a tx that sends the \"ballot\" NFT to a \"ballot box\" (smart contract) with their \"vote\" (datum) Authorized vote counting wallets process UTxOs and their datums in the \"ballot box\" smart contract after polls close Authorized vote counters report the results in a human-readable off-chain format to holders\nA holder of a project validates their NFT by sending it to self\nThe holder signs a Plutus minting policy to create a \"ballot\" NFT linked to their unique NFT\nThe holder marks their desired vote selections on the ballot\nThe holder signs a tx that sends the \"ballot\" NFT to a \"ballot box\" (smart contract) with their \"vote\" (datum)\nAuthorized vote counting wallets process UTxOs and their datums in the \"ballot box\" smart contract after polls close\nAuthorized vote counters report the results in a human-readable off-chain format to holders\nNote Because of the efficient UTxO model Cardano employs, steps #1 through #4 occur in a single transaction.\nEvery holder that participates in the vote will have their project NFT in a wallet that can be spent from (either hardware or software, typically accessed via CIP-30). To create a ballot, the voting authority will create a Plutus minting policy with a specific combination of:\ntype BallotMintingPolicy = { referencePolicyId: MintingPolicyHash, // Reference policy ID of the original NFT project pollsClose: Time, // Polls close (as a Unix timestamp in milliseconds) assetNameMapping: func(ByteArray) -> ByteArray // Some function (potentially identity) to map reference NFT name 1-for-1 to ballot NFT name };\ntype BallotMintingPolicy = { referencePolicyId: MintingPolicyHash, // Reference policy ID of the original NFT project pollsClose: Time, // Polls close (as a Unix timestamp in milliseconds) assetNameMapping: func(ByteArray) -> ByteArray // Some function (potentially identity) to map reference NFT name 1-for-1 to ballot NFT name };\nThis Plutus minting policy will perform the following checks:\nPolls are still open during the Tx validFrom/validTo interval The ballot NFTs were validly minted (at the least, the user sent-to-self the reference assets and the vote weight/choices are correct) The minted assets are sent directly to the ballot box smart contract in the minting transaction (see the potential attack below)\nPolls are still open during the Tx validFrom/validTo interval\nThe ballot NFTs were validly minted (at the least, the user sent-to-self the reference assets and the vote weight/choices are correct)\nThe minted assets are sent directly to the ballot box smart contract in the minting transaction (see the potential attack below)\nFor the voter, each vote they wish to cast will require creating a separate \"ballot\" NFT. In the process, their reference NFT never leaves the original wallet. Sample Helios language pseudocode (functions elided for space) is as follows:\nfunc main(redeemer: Redeemer, ctx: ScriptContext) -> Bool { tx: Tx = ctx.tx; minted_policy: MintingPolicyHash = ctx.get_current_minting_policy_hash(); redeemer.switch { Mint => { polls_are_still_open(tx.time_range) && ballots_are_validly_minted(tx.minted, minted_policy, tx.outputs) && assets_locked_in_ballot_box(tx, tx.minted) }, // Burn code elided for space... } }\nfunc main(redeemer: Redeemer, ctx: ScriptContext) -> Bool { tx: Tx = ctx.tx; minted_policy: MintingPolicyHash = ctx.get_current_minting_policy_hash(); redeemer.switch { Mint => { polls_are_still_open(tx.time_range) && ballots_are_validly_minted(tx.minted, minted_policy, tx.outputs) && assets_locked_in_ballot_box(tx, tx.minted) }, // Burn code elided for space... } }\nNote ballots_are_validly_minted() includes all required and custom checks (e.g., the holder has sent the reference NFT to themselves in tx.outputs) to validate newly minted ballots\nballots_are_validly_minted()\ntx.outputs\nTo cast the vote, the user sends the ballot NFT just created to a \"ballot box\". Note that for reasons specified in the \"attacks\" section below this needs to occur during the same transaction that the ballot was minted in.\nThe datum is a simple object representing the voter who cast the vote and the vote itself:\ntype VoteDatum = { voter: PubKeyHash, vote: object };\ntype VoteDatum = { voter: PubKeyHash, vote: object };\nThe voter element is extremely important in this datum so that we know who minted the ballot NFT and who we should return it to. At the end of the ballot counting process, this user will receive their ballot NFT back.\nvoter\nNote that we are trying to avoid being overly prescriptive here with the specific vote type as we want the only limitations on the vote type to be those imposed by Cardano. Further iterations of this standard should discuss the potential for how to implement ranked-choice voting (RCV) inside of this vote object, support multiple-choice vote selection, and more.\nvote\nEssentially, the \"ballot box\" is a smart contract with arbitrary logic decided upon by the authorized vote counter. Some examples include:\nA ballot box that can be redeemed at any time by a tx signed by the authorized vote counter A ballot box that can be redeemed only after polls close A ballot box that can be redeemed once a majority of voters have sent in a ballot A ballot box that can be redeemed only by the specific wallet specified in the voter datum of each UTxO A ballot box that can be redeemed only after polls close, has to burn the ballots it redeems, and has to send the minUTxO back to the voter address\nA ballot box that can be redeemed at any time by a tx signed by the authorized vote counter\nA ballot box that can be redeemed only after polls close\nA ballot box that can be redeemed once a majority of voters have sent in a ballot\nA ballot box that can be redeemed only by the specific wallet specified in the voter datum of each UTxO\nvoter\nA ballot box that can be redeemed only after polls close, has to burn the ballots it redeems, and has to send the minUTxO back to the voter address\nBecause the ballot creation and vote casting process has already occurred on-chain we want to provide the maximum flexibility in the protocol here so that each project can decide what is best for their own community. Helios code for the simple case enumerated as #1 above would be:\nconst EXPECTED_SIGNER: PubKeyHash = PubKeyHash::new(#0123456789abcdef) func main(ctx: ScriptContext) -> Bool { ctx.tx.is_signed_by(EXPECTED_SIGNER) }\nconst EXPECTED_SIGNER: PubKeyHash = PubKeyHash::new(#0123456789abcdef) func main(ctx: ScriptContext) -> Bool { ctx.tx.is_signed_by(EXPECTED_SIGNER) }\nGiven the flexible nature of the \"ballot box\" smart contract enumerated above, we propose a simple algorithm for counting votes and returning the ballots to the user:\nEnsure the polls are closed (can be either on or off-chain) Iterate through all UTxOs in forward-time-order locked in the \"ballot box\" and for each: Determine which assets are inside of that UTxO Mark their most recent vote to match the vote object in the UTxOs datum Ensure any required quorums or thresholds were reached Report on the final ballot outcome\nEnsure the polls are closed (can be either on or off-chain)\nIterate through all UTxOs in forward-time-order locked in the \"ballot box\" and for each: Determine which assets are inside of that UTxO Mark their most recent vote to match the vote object in the UTxOs datum\nDetermine which assets are inside of that UTxO\nMark their most recent vote to match the vote object in the UTxOs datum\nvote\nEnsure any required quorums or thresholds were reached\nReport on the final ballot outcome\nJavascript-like pseudocode using the Lucid library for the above algorithm would be as follows:\nfunction countVotes(ballotPolicyId, ballotBox) { var votesByAsset = {}; const votes = await lucid.utxosAt(ballotBox); for (const vote of votes) { const voteResult = Data.toJson(Data.from(vote.datum)); for (const unit in vote.assets) { if (!unit.startsWith(ballotPolicyId)) { continue; } const voteCount = Number(vote.assets[unit]); // Should always be 1 votesByAsset[unit] = { voter: voteResult.voter, vote: voteResult.vote, count: voteCount } } } return votesByAsset; }\nfunction countVotes(ballotPolicyId, ballotBox) { var votesByAsset = {}; const votes = await lucid.utxosAt(ballotBox); for (const vote of votes) { const voteResult = Data.toJson(Data.from(vote.datum)); for (const unit in vote.assets) { if (!unit.startsWith(ballotPolicyId)) { continue; } const voteCount = Number(vote.assets[unit]); // Should always be 1 votesByAsset[unit] = { voter: voteResult.voter, vote: voteResult.vote, count: voteCount } } } return votesByAsset; }\nThere is no requirement that the \"ballot counter\" redeem all \"ballots\" from the \"ballot box\" and send them back to the respective voters, but we anticipate that this is what will happen in practice. We encourage further open-sourced code versions that enforce this requirement at the smart contract level.\nEven if the ballot NFT is returned to the user, this will leave users with ada locked alongside these newly created assets, which can impose a financial hardship for certain project users.\nWe can add burn-specific code to our Plutus minting policy so that ballot creation does not impose a major financial burden on users:\nfunc main(redeemer: Redeemer, ctx: ScriptContext) -> Bool { tx: Tx = ctx.tx; minted_policy: MintingPolicyHash = ctx.get_current_minting_policy_hash(); redeemer.switch { // Minting code elided for space... Burn => { tx.minted.get_policy(minted_policy).all((asset_id: ByteArray, amount: Int) -> Bool { if (amount > 0) { print(asset_id.show() + \" asset ID was minted not burned (quantity \" + amount.show() + \")\"); false } else { true } }) } } }\nfunc main(redeemer: Redeemer, ctx: ScriptContext) -> Bool { tx: Tx = ctx.tx; minted_policy: MintingPolicyHash = ctx.get_current_minting_policy_hash(); redeemer.switch { // Minting code elided for space... Burn => { tx.minted.get_policy(minted_policy).all((asset_id: ByteArray, amount: Int) -> Bool { if (amount > 0) { print(asset_id.show() + \" asset ID was minted not burned (quantity \" + amount.show() + \")\"); false } else { true } }) } } }\nThe Helios code above simply checks that during a burn (as indicated by the Plutus minting policy's redeemer), the user is not attempting to mint a positive number of any assets. With this code, any Cardano wallet can burn any ballot minted as part of this protocol. Why so permissive? We want to ensure that each vote is bringing the minimal costs possible to the user. In providing this native burning mechanism we can free up the minUTxO that had been locked with the ballot, and enable the user to potentially participate in more votes they might not have otherwise. In addition, users who really do not like the specific commemorative NFTs or projects that choose to skip the \"commemorative\" aspect of ballot creation now have an easy way to dispose of \"junk\" assets.\nredeemer\nThere are several existing open-source protocols (e.g., VoteAire) that use metadata to record votes in Cardano transactions without requiring any additional minting or smart contracts. However, since the vote counting occurs off-chain by validating metadata the vote counter is the ultimate arbiter of what is a \"valid\" vote. In this specification, the validity of the vote is ensured in the Ballot creation process, so that any vote in the ballot box is guaranteed to be valid. We strongly believe that moving the entire process into flexible on-chain logic will improve the transparency of the voting process and meet the needs of the use cases discussed in \"Motivation\" and \"Ballot Box\".\nThere is a question as to whether we should enforce the requirement that votes be burned when they are counted by the vote counter. However, we do not want that to be a standard as many users of NFT communities have expressed an interest in receiving commemorative NFTs (similar to an \"I Voted\" sticker). Instead, we propose that the ballot Plutus minting policy be burn-able by anyone who holds the NFT in their wallet. This way, locked ada can be reclaimed if the user has no further use for the commemorative NFT (see an example of this in the Implementation).\nImagine a user who decides to create ballots for the current vote, but not actually cast the vote by sending it to the ballot box. According to checks #1 and #2 in the Plutus Minting Policy, this would be possible. After the ballot was created, the user could sell the reference NFT and wait until just before the polls close to surreptitiously cast a vote over the wishes of the new project owner. Check #3 in the minting policy during the mint transaction itself prevents this attack.\nA user could wait until their first vote casting transaction completes, then create more ballots and resubmit to the ballot box. The result would be the creation of more assets that count toward the ultimate vote. However, Cardano helps us here by identifying tokens based on the concatenation of policy ID and asset identifier. So long as the mapping function in the Plutus minting policy for ballots is idempotent, each subsequent time the user votes the policy will create an additional fungible token with the same asset identifier. Then, the ballot counter can ignore any prior votes based on each unique asset identifier to avoid duplicate votes (see \"'Ballot Counter' - Authorized Wallet\").\nA user could attempt to create multiple ballots of the same name for a given reference NFT. If the reference NFT is actually a fungible token and not an NFT, then our assumptions will have been broken and this is an unsupported use case. But if our assumption that this is an NFT project are correct, then simply checking that the quantity minted is equal to the quantity spent inside of the Plutus minting policy will prevent this. The example code attached does just that.\nDuring the construction of the ballot NFTs we allow the user to specify their vote alongside a voter field indicating where their \"ballot\" NFT should be returned to once the vote is fully counted. Unfortunately, this is not strictly checked inside the Plutus minting policy's code (largely due to CPU/memory constraints). So, we rely on the user to provide an accurate return address, which means that there is the potential for someone who has not actually voted to receive a commemorative NFT. This does not impact the protocol though, as the \"ballot\" NFT was legally minted, just returned to the incorrect location. That user actually received a gift, as they can now burn the ballot and receive some small amount of dust.\nvoter\nThere are several potential disadvantages to this proposal that may be avoided by the use of a native token or other voting mechanism. We enumerate some here explicitly so projects can understand where this protocol may or may not be appropriate to use:\nProjects concerned with token proliferation and confusing their user base with the creation of multiple new assets might want to avoid this standard as it requires one new token policy per vote/initiative\nProjects wishing to create a \"secret ballot\" that will not be revealed until after polls close should not use this because the datum votes appear on-chain (and typically inline) Performing an encrypted vote on-chain with verifiable post-vote results is an exercise left to the standard's implementer\nPerforming an encrypted vote on-chain with verifiable post-vote results is an exercise left to the standard's implementer\nProjects wishing for anonymity in their votes should not use this standard as each vote can be traced to a reference asset\nIn no particular order, we recommend the following implementation details that do not impact the protocol, but may impact user experience:\nThe mapping function described in the Plutus minting policy for ballots should likely be some sort of prefixing or suffixing (e.g., \"Ballot #1 - \"), and NOT the identity function. Although the asset will be different than the reference NFT due to its differing policy ID, users are likely to be confused when viewing these assets in a token viewer.\nThe \"ballot\" NFT should have some sort of unique metadata (if using CIP-25), datum (if using CIP-68) or other identification so that the users can engage with the ballot in a fun, exciting way and to ensure there is no confusion with the reference NFT.\nThe \"vote\" represented by a datum will be easier to debug and analyze in real-time if it uses the new \"inline datum\" feature from Vasil, but the protocol will still work on Alonzo era transactions.\nThe \"ballot box\" smart contract should likely enforce that the datum's \"voter\" field is respected when returning the ballots to users after voting has ended to provide greater transparency and trust for project participants.\nDue to the nature of Plutus minting policies and smart contracts, which derive policy identifiers and payment addresses from the actual source code, once a vote has been started it cannot change versions or code implementations. However, because the mechanism we propose here is just a reference architecture, between votes projects are free to change either the \"ballot\" Plutus minting policy or the \"ballot box\" smart contract as they see fit. There are no prior CIPs with which to conform with for backward interoperability.\nCIP-0025: NFT Metadata Standard\nCIP-0030: Cardano dApp-Wallet Web Bridge\nCIP-0068: Datum Metadata Standard\nHelios Language: On-Chain Cardano Smart Contract language used in example code\nLucid: Transaction construction library used in code samples and pseudocode\nVoteAire Specification: Open-source voting specification using metadata off-chain\nPresentation to, and adoption by, projects that may benefit from ranked-choice voting\nOpen-source implementations from other NFT projects that make use of this CIP\nMinimal reference implementation making use of Lucid (off-chain), Plutus Core using Helios (on-chain): Implementation\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0072 | Cardano dApp Registration & Discovery\n\ndApp developers do not have a standardised method to record immutable, persistent claims about their dApp(s) that their users can verify. A dApp developer needs to \"register\" their dApp by providing a set of claims about their dApp(s) that can be verified by the user. This CIP describes a standardised method for dApp developers to register their dApp(s) on-chain and for users to verify the claims made by dApp developers.\nThis proposal aims to standardise the process of dApp registration and verification, and to provide a set of claims that dApp developers can use to register their dApp(s).\ndApps can express a plethora of information. Some of this information could be claims about who the developer is, what the dApp's associated metadata is, and more. This data lacks standardisation, persistence, and immutability. Data without these features, means that dApp users cannot verify if the dApp's expressed information is consistent across time. The goal of this CIP is to formalise how dApps register their information with a new transaction metadata format that can record the dApp's metadata, ownership, and potentially developer's identity. This formalisation means dApp users can verify if the data expressed by a dApp is consistent with what was registered on-chain.\nAlso, having this formalisation facilitates any actor in the ecosystem to index and query this data, and provide a better user experience when it comes to dApp discovery and usage.\nanchor - A hash written on-chain (rootHash) that can be used to verify the integrity (by way of a cryptographic hash) of the data that is found off-chain.\ndApp - A decentralised application that is described by the combination of metadata, certificate and a set of used scripts.\nmetadata claim - Generically any attempt to map off-chain metadata to an on-chain subject. This specification looks at dApp specific metadata claims. Caution: it is highly recommended that dApp developers provide links to a specific snapshot (version) without removing all previous snapshots / version links. Some stores may choose not to show a dApp if all off-chain historical versions are not available but instead only latest snapshot.\nclient - Any ecosystem participant which follows on-chain data to consume metadata claims (i.e. dApp stores, wallets, auditors, block explorers, etc.).\ndApp Store - A dApp aggregator application which follows on-chain data looking for and verifying dApp metadata claims, serving their users linked dApp metadata.\npublishers - Entities which publish metadata claims on-chain, in the case of dApps the publishers are likely the dApp developer(s).\nDevelopers and publishers of dApps can register their dApps by submitting a transaction on-chain that can be indexed and verified by stores, auditors and other ecosystem actors.\nStores and auditors should be able to follow the chain and find when a new dApp registration is anchored on-chain. They should then perform integrity and trust validations on the dApp's certificate and metadata.\nintegrity: The dApp's off-chain metadata should match the metadata anchored on-chain.\nintegrity\ntrust: The dApp's certificate should be signed by a trusted entity. It's up to the store/auditor to decide which entities are trusted and they should maintain and publish their own list of trusted entities. Although this entities might be well known, it's not the responsibility of this CIP to maintain this list. These entities could be directly associated with developer/publisher or not.\ntrust\n{ \"subject\": \"b37aabd81024c043f53a069c91e51a5b52e4ea399ae17ee1fe3cb9c44db707eb\", \"rootHash\": \"7abcda7de6d883e7570118c1ccc8ee2e911f2e628a41ab0685ffee15f39bba96\", \"metadata\": [ \"https://foundation.app/my_dapp_7abcda/tart/\", \"7abcda7de6d883e7570118c1ccc8ee2e91\", \"e4ea399ae17ee1fe3cb9c44db707eb/\", \"offchain.json\" ], \"type\": { \"action\": \"REGISTER\", \"comment\": \"New release adding zapping support.\" } }\n{ \"subject\": \"b37aabd81024c043f53a069c91e51a5b52e4ea399ae17ee1fe3cb9c44db707eb\", \"rootHash\": \"7abcda7de6d883e7570118c1ccc8ee2e911f2e628a41ab0685ffee15f39bba96\", \"metadata\": [ \"https://foundation.app/my_dapp_7abcda/tart/\", \"7abcda7de6d883e7570118c1ccc8ee2e91\", \"e4ea399ae17ee1fe3cb9c44db707eb/\", \"offchain.json\" ], \"type\": { \"action\": \"REGISTER\", \"comment\": \"New release adding zapping support.\" } }\nsubject: Identifier of the claim subject (dApp). A UTF-8 encoded string, max 64 chars. The uniqueness of this property cannot be guaranteed by the protocol and multiple claims for the same subject may exist, therefore it is required to exist some mechanism to assert trust in the veracity of this property.\nsubject\ntype: The type of the claim. This is a JSON object that contains the following properties:\ntype\naction: The action that the certificate is asserting. It can take the following values: REGISTER: The certificate is asserting that the dApp is registered for the first time or is providing an update. DE_REGISTER: The certificate is asserting that the dApp is deprecated / archived. So, no further dApp's on-chain update is expected.\naction\nREGISTER: The certificate is asserting that the dApp is registered for the first time or is providing an update.\nREGISTER\nDE_REGISTER: The certificate is asserting that the dApp is deprecated / archived. So, no further dApp's on-chain update is expected.\nDE_REGISTER\nrootHash: The blake2b-256 hash of the entire offchain metadata tree object. This hash is used by clients to verify the integrity of the metadata tree object. When reading a metadata tree object, the client should calculate the hash of the object and compare it with the rootHash property. If the two hashes don't match, the client should discard the object. The metadata tree object is a JSON object that contains the dApp's metadata. The metadata tree object is described in the next section. Please note that off-chain JSON must be converted into RFC 8765 canonical form before taking the hash!\nrootHash\nrootHash\nmetadata: Chunks of URLs that make up the dApp's metadata are arranged in an array to accommodate the 64-character limit per chunk, allowing for the support of longer URLs. The metadata itself is a JSON object compatible with RFC 8785, containing detailed information about the dApp\nmetadata\nOn-chain CDDL for registration / de-registration (Version 2.0.0)\nwhich also can be expressed using JSON schema:\ndApp on-chain certificate JSON Schema (Version 2.0.0)\nWhen submitting the transaction metadata pick the following value for transaction_metadatum_label:\ntransaction_metadatum_label\n1667: dApp Registration\n1667\nThe dApp Registration certificate itself doesn't enforce a particular structure to the metadata you might fetch off-chain. However, we recommend that you use the following structure:\nOff-chain dApp Registration certificate schema (Version 2)\nThis schema describes the minimum required fields for a store to be able to display and validate your dApp.\n{ \"version\": \"1.0.0\", \"subject\": \"abcdef1234567890\", \"projectName\": \"My dApp\", \"link\": \"https://www.exampledapp.com\", \"companyName\": \"Amazing dApp Inc.\", \"companyEmail\": \"contact@myamazingdapp.com\", \"companyWebsite\": \"https://www.myamazingdapp.com\", \"logo\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUAAA...\", \"categories\": [\"DeFi\", \"Games\"], \"screenshots\": [ \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...\", \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEASABIAAD...\" ], \"social\": [ { \"name\": \"GitHub\", \"link\": \"https://github.com/exampledapp\" }, { \"name\": \"Twitter\", \"link\": \"https://twitter.com/exampledapp\" } ], \"description\": { \"short\": \"This is a short description of the dApp, giving an overview of its features and capabilities.\" }, \"releases\": [ { \"releaseNumber\": \"1.0.0\", \"releaseName\": \"Initial Release\", \"securityVulnerability\": false, \"comment\": \"First major release with all core features.\", \"scripts\": [ { \"id\": \"script1\", \"version\": \"1.0.0\" } ] } ], \"scripts\": [ { \"id\": \"script1\", \"name\": \"Example Script\", \"purposes\": [\"SPEND\", \"MINT\"], \"type\": \"PLUTUS\", \"versions\": [ { \"version\": \"1.0.0\", \"plutusVersion\": 1, \"scriptHash\": \"abc123\" } ] } ] }\n{ \"version\": \"1.0.0\", \"subject\": \"abcdef1234567890\", \"projectName\": \"My dApp\", \"link\": \"https://www.exampledapp.com\", \"companyName\": \"Amazing dApp Inc.\", \"companyEmail\": \"contact@myamazingdapp.com\", \"companyWebsite\": \"https://www.myamazingdapp.com\", \"logo\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUAAA...\", \"categories\": [\"DeFi\", \"Games\"], \"screenshots\": [ \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...\", \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEASABIAAD...\" ], \"social\": [ { \"name\": \"GitHub\", \"link\": \"https://github.com/exampledapp\" }, { \"name\": \"Twitter\", \"link\": \"https://twitter.com/exampledapp\" } ], \"description\": { \"short\": \"This is a short description of the dApp, giving an overview of its features and capabilities.\" }, \"releases\": [ { \"releaseNumber\": \"1.0.0\", \"releaseName\": \"Initial Release\", \"securityVulnerability\": false, \"comment\": \"First major release with all core features.\", \"scripts\": [ { \"id\": \"script1\", \"version\": \"1.0.0\" } ] } ], \"scripts\": [ { \"id\": \"script1\", \"name\": \"Example Script\", \"purposes\": [\"SPEND\", \"MINT\"], \"type\": \"PLUTUS\", \"versions\": [ { \"version\": \"1.0.0\", \"plutusVersion\": 1, \"scriptHash\": \"abc123\" } ] } ] }\nEach store might have their own requirements for the metadata. For example, some stores might require a field for logo, or screenshots links. The store's should adviertise what fields they require in their documentation so that developers are aware and they can include them in the metadata.\nThere are multiple options to store metadata offchain. The most common options are:\nCIP-26 compliant servers\nIPFS\nBitbucket\nAny REST JSON API\nWe quickly reached a conclusion that it is better to separate them and keep scope of CIP smaller. During discussions it became clear that while there is some overlap of certifications / audits with dApp registration, this overlap is small and can be even removed. At one point we wanted to couple certifications CIP to this CIP (e.g. via some link or dApp version) but we analyzed how dApp developers are currently following the process and we noticed that in many cases certification / audit comes before an official dApp release on main-net. Having established it, we removed this link and not only that dApp registration and certifications are different CIPs but they are very loosely coupled. Loose coupling has also disadvantages as it leads to a situation that in order to attest that a dApp is certified / audited, implementators will have to scan for all script hashes belonging to a dApp and check whether those have been certified / explicitly mentioned in the audit.\nThis one is rather obvious but for the sake of completeness worth documenting. We analyzed how much we should put on-chain vs off-chain and we quickly reached the conclusion that it is better to keep small amount of data on-chain and larger chunk off-chain for which is what exactly CIP-26 is meant for.\nWe believe that CIP-26 is geared towards storing this type of off-chain metadata format but we don't want by any means to stipulate / police this form of storage. In fact it is possible to use offchain metadata storage alternatives such as CIP-26 compatible server / direct http(s) hosting / IPFS, etc.\nWe went back and forth whether we should actually store link (links) to off-chain metadata, eventually we settled on a solution that this is required because there could be a situation that a dApp registration may have off-chain metadata stored somewhere but some stores have it, others don't have it. Now it is required that a dApp developer points to at least one store that has off-chain metadata (as a reference metadata).\nRelease Name is a field, which dApp developers can use on top of Release Version, it has been debated whether field should be mandatory or optional but eventually it has been agreed that we do not want to enforce this field, Release Name is an optional field, Release Version, however, needs to follow semver and is a mandatory field.\nAt the begining neither on-chain, nor off-chain JSON has been following RFC 8785 (canonical JSON) but we reached a point that, due to consistency checks, we need to take hash of both on-chain and off-chain and this forced us to stipulate that both on-chain and off-chain metadata documents need to be converted according to RFC 8785 before taking a blake2b-256 hash of them.\nAs any transaction in cardano network has a signature, which has a role to verify that a certain dApp owner made changes.\nSmart contracts are ownerless, it has been debated that there could be multiple claims to the same dApps from different parties.\nThe standard doesn't prevent anyone from making a claim, so it's up to the different operator to their diligence work and make their own choices of whom they trust. The signature should give the most confidence as anyone can collect known public keys from known development companies. Future CIP revisions can include DID's and Verifiable Credentials to tackle this problem in a more elegant way.\nSince DIDs / Verifiable Credetials are not yet widely used in Cardano ecosystem, usage of them in this initial CIP version has been de-scoped.\nCategories is a predefined enum with values defined in the CIP / schema, it is NOT a free text field, rationale for this is that dApp developers will have no idea what ontology / classification to use, which will likely result in many duplicates of the same thing.\nCategories\nIt may have been visible that we have a purpose field, which can be: \"SPEND\" or \"MINT\", those fields directly map to what is allowed by a Cardano Smart Contract. As of the time of writing CIP - PlutusTx does not allow a script to be both of type: \"SPEND\" and \"MINT\", however, there are new languages on Cardano being worked on where they already allow one validator to be both spending UTxOs and minting tokens - all with the same script hash. To be open for the future it has been agreed to turn purpose field into purposes and make it a JSON array.\npurpose\npurpose\npurposes\nOn Cardano, there are parametrised scripts, meaning that before compilation takes place, it is possible to pass certain parameters instead of using Datum. The consequence of this will be that as we pass different parameters, script hash will be changing. This is especially troublesome for things like certifications / audits but also dApp registration. This topic is being debated as part of CIP: https://github.com/cardano-foundation/CIPs/pull/385, however, it doesn't seem that there has been conclusion how to tackle this problem. For the moment, a new script hash (despite changing only a parameter) requires a re REGISTRATION to the existing dApp with a requirement to add new version(s) in the dApp's off-chain metadata.\nDatum\nThere are cases on Cardano main-net that script hashes are changing every day, most probably due to parameterised scripts. It is responsibility of the developers to issue an REGISTRATION command and provide on-chain and off-chain metadata following the change, for scripts that are changing daily / hourly it is envisaged that this process be automated by a dApp developer.\nREGISTRATION\nIt has been argued that since anybody can make claims to dApps, this CIP instead of using metadata should use tokens. dApp developers would mint token, which would ensure they are the owners of a given dApp. It is a certainly an interesting approach but shortcomings of the current solution can also be lifted by moving to DID based system and benefit of metadata is that it is easily queriable off chain and currently stores can attest / validate multiple claims for the same dApp. Forcing dApp developers to MINT tokens would complicate this CIP and potentially hinder it's adoption.\nIt has been suggested that we do not use metadata but rather Datums. Metadata cannot enforce format and Datums could. It has been rejected as using Datums requires a smart contract and we want to keep this solution as accessible as possible. It is a GUI concern since if there is a some app that can attest validity and conformance to JSON schema - dApp Registration / Update MUST never be done that does not conform to the schema.\nWe made a decision to change the schema so that scripts and releases are no longer required. This could help to get initial registration from dApp developers faster and some stores simply do not require dApps to add their scripts in order to be listed.\nWe briefly discussed tags and we will likely introduce tags in the future. An array of tags to help stores / dApp developers categories where their dApp should show. This will complement categories field.\ncategories\nWe added DE_REGISTER in additon to already existing REGISTER. The idea is that once dApp devs want to deprecate dApp they can now issue DE_REGISTER request.\nREGISTER\nType field can be PLUTUS or NATIVE, we made it optional and there are already two dApps at least on Cardano at the time of writing, which are only using NATIVE scripts. This optional field helps to differentiante between NATIVE script based and NON_NATIVE dApps.\nType\nPLUTUS\nNATIVE\nWe discussed scenario what to do in case a dApp team wants to deprecate a particular version. Upon a few iteration we settled on doing this in off-chain section.\nIt is not uncommon to see a dApp release a version and then release a fix in the new version and flag the previous version as having security vulnerability. We are intoducing an optional field in the offchain json on the release level: `securityVulnerability\": true.\nWe are introducing a field in the on-chain JSON only, which allows dApp development teams to provide a free text field comment about changes they are making in a given (re-)registration request.\nWe will evaluate for a few months if we have not missed any details, collect feedback from dApp developers, stores. We reserve right to make small changes in this time, while the proposal is in the PROPOSED status / state. Once Acceptance Criteria are met and all comments / feedback from dApp developers is addressed, we will update the proposal to be in ACTIVE state.\nPROPOSED\nAcceptance Criteria\nACTIVE\nAt least 3 non trivial dApps from 3 different teams register on-chain / off-chain via following this CIP\nAt least one Implementator (main-net) implements the store indexing this CIP metadata from on-chain\nDappsOnCardano dApp Store: https://github.com/Cardano-Fans/crfa-dapp-registration-and-certification-service for DappsOnCardano.com\nLace's Wallet dApp Store: https://github.com/input-output-hk/lace\nCC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0074 | Set minPoolCost to 0\n\nA minPoolCost of 340 ADA/epoch makes popularity the basis for pool desirability, causing preferred traits like pledge and performance to be overshadowed. This has promoted stake to centralize with operators who are effective at campaigning, but do not necessarily have any stake in the system of their own. We want to create a fair marketplace for stake pools that allows the network to decentralize with time; minPoolCost is averse to that goal.\nPopularity is currently the basis for desirability and defines which pools will receive high rewards and which won't. A pool with high popularity, low pledge, and low performance will offer significantly higher rewards than a pool with low popularity, high pledge, and high performance. With equal fee structures, a pool with 19.5MM pledge that is just starting out will be less desirable than a saturated pool with no pledge; a pool with 6MM pledge and perfect performance will be less desirable than a pool with 0 pledge and 90 performance. This makes it apparent we are not incentivizing a secure stable network and the network cannot self-correct if stake ends up in the wrong place.\nThe entirety of IOG's research is without a minPoolCost or any other minimum fee. The game theory design relied on stake pool operators (SPOs) having the ability to compete in a fair marketplace of pool desirability by modifying their pledge, cost, and margin in relation to other pools. Adding a minimum value to cost has removed the opportunity for operators to control their desirability outside of first gaining popularity. As a pool gains popularity it disproportionately gains desirability, allowing the operator to lower pledge and raise margin, while still maintaining higher desirability than a less popular pool with better fee structure and higher pledge. Operators can lie about their costs while still gaining utility. As long as the operator can maintain popularity they can exceed K indefinitely by opening more and more pools with a reward bonus instead of penalty. This significantly lowers the cost of a sybille attack, while making sybille behavior highly desirable and profitable. A sybille attacker, or anybody for that matter, should need stake in the system to compete for delegation, not just a rigorous marketing campaign.\nThe network is incentivizing the wrong behavior from SPOs which has made the network highly leveraged.\n\"The higher the leverage of the system, the worse its security [...] The lower the leverage of a blockchain system, the higher its degree of decentralization.\"\nHigh security should always be the priority.\nStake pool desirability will become a function of pledge, performance, cost, and margin instead of only popularity. Operators will need to consolidate and grow pledge while moderating fees to remain competitive with other pools on the market.\nSPOs define their cost as an absolute value when submitting their registration.cert. They do not reference minPoolCost. Changing minPoolCost will not change any predefined costs. Pools that wish to leave their cost as-is can do so without any input. Pools that wish to lower their cost below 340 ADA/epoch will have to submit an updated registration.cert.\n\"minPoolCost\": 0\n98 of all pools have their cost at 340 ADA/epoch or within + 10. As the price of ADA went from 0.3to0.3 to 0.3to3, almost no operators modified their cost. As the price of ADA dropped from 3to3 to 3to0.3, almost no operators modified their cost. This shows that the minPoolCost is not related to the real cost of operating a pool and the cost of a pool is no longer related to its utility.\nWe have a large body of research accompanied by simulations showing that removing minPoolCost will increase decentralization of the network and begin incentivizing the right behavior from SPOs and the delegating community.\nA minPoolCost is not in Cardano's design specification. ALL published research is in favor of setting minPoolCost to 0.\nDesign Specification for Delegation and Incentives in Cardano https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-delegation.pdf\nReward Sharing Schemes for Stake Pools https://arxiv.org/pdf/1807.11218.pdf\nPreventing Sybil attacks https://iohk.io/en/blog/posts/2018/10/29/preventing-sybil-attacks/\nStake Pools in Cardano https://iohk.io/en/blog/posts/2018/10/23/stake-pools-in-cardano/\nIncentive Paper Lecture Series (Parts 1-7) https://www.youtube.com/playlist?list=PLFLTrdAG7xRbAqhF3Tg8BeAea7Ard-ttn\nThe general perspective on staking in Cardano\nA protocol parameter update assigning minPoolCost to 0.\nminPoolCost\n0\nAgreement by the Ledger team as defined in CIP-0084 under Expectations for ledger CIPs including \"expert opinion\" on changes to rewards & incentives.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0075 | Fair Stake Pool Rewards\n\nThe current reward sharing scheme of Cardano is unfair and anticompetitive. As a result, Cardano has become more centralized over time. The high minimum fixed fee and current pledge benefit favor large stakepools and leave small stakepools at a significant disadvantage. The current scheme allows large pools with low pledge to be more attractive than smaller pools with higher pledge leading to centralization and potential Goldfinger attacks. Furthermore, k, the parameter representing the optimal number of stakepools, is set too low resulting in an ineffective pledge benefit, the formation of multipools, and a low incentive for stakepools to increase pledge over time. Finally, the current setting of a0, the pledge influence parameter, gives an unnecessarily large boost in rewards to fully pledged private pools resulting in significantly less rewards for public pools and their delegators.\nThis proposal retains most of the original reward sharing scheme, but makes changes to ensure fairness, increase decentralization, and reduce the viability of Goldfinger attacks. By removing the minimum fixed fee, adjusting the pledge benefit, increasing k, and reducing a0, a more egalitarian network can be achieved.\nMinimum Fixed Fee\nProtocol parameter minPoolCost.\nMargin\nStakepool parameter PoolRate (percentage fee).\nPublic Stakepool\nA stakepool with a margin that is less than 100 .\nPrivate Stakepool\nA stakepool that is not seeking delegation with a margin of 100 .\nStakepool Operator (SPO)\nThe operator of a stakepool, can be a single person or an entity.\nMultipool\nA group or brand of stakepools operated by a single entity or stakepool operator.\nROS\nReturn on staking (often annualized and represented as a percentage of the initial investment).\nLeverage\nThe ratio between a stakepool s total stake and pledge.\nStakepool Viability Point\nThe amount of pledge required for a stakepool with zero delegations to distribute nonzero rewards to delegators (assuming minimum stakepool fees and ignoring luck in VRF block production, i.e., rewards are exactly proportional to stake).\nStakepool Competitive Point\nThe amount of pledge required for a stakepool with zero delegations to offer the same ROS as a fully-saturated stakepool with zero pledge (assuming minimum stakepool fees).\nStakepool Saturation Point\nThe maximum amount of total stake in a stakepool before total stakepool rewards are capped and ROS diminishes.\nUnsaturated Pledge Benefit Penalty\nThe amount of potential pledge benefit (in ADA or represented as a percentage) a stakepool loses for being unsaturated. The penalty is larger the smaller the stakepool.\nMinimum Attack Vector (MAV)\nAlso known as the Nakamoto Coefficient, the MAV is the minimum number of entities required to capture more than 50 of a network. In the context of Cardano, this refers to the minimum number of SPOs required to capture more than 50 of active stake.\nGoldfinger Attack\nAn attack on a cryptocurrency protocol where the objective is to attack the protocol or network in order to make profit by shorting the native cryptocurrency.\nSybil Attack\nAn attack on an online system where an entity tries to take over the network by creating many identities or nodes.\nIn section 10.8 Rewards Distribution Calculation of A Formal Specification of the Cardano Ledger (git revision 1.1-486-g301fede) the current rewards calculation equation is described by the function maxPoolmaxPoolmaxPool:\nmaxPool=R1+a0 (o+p a0 o pz0 oz0z0)maxPool = \\frac{R}{1 + a0} \\cdot (o + p \\cdot a0 \\cdot \\frac{o - p \\frac{z0 - o}{z0}}{z0})maxPool=1+a0R (o+p a0 z0o pz0z0 o )\nwhere: maxPoolmaxPoolmaxPool = maximum rewards for a stake pool, R=((reserve rho)+fees) (1 tau),o=min(poolstake/totalstake,z0)=z0R = ((reserve \\cdot rho) + fees) \\cdot (1 - tau), o = min(poolstake / totalstake, z0) = z0R=((reserve rho)+fees) (1 tau),o=min(poolstake/totalstake,z0)=z0 for fully saturated pool, p=min(pledge/totalstake,z0)p = min(pledge / totalstake, z0)p=min(pledge/totalstake,z0), and z0=1/kz0 = 1 / kz0=1/k\nCurrent protocol parameters: k=500,rho=0.003,a0=0.3k = 500, rho = 0.003, a0 = 0.3k=500,rho=0.003,a0=0.3, and tau=0.2tau = 0.2tau=0.2\nThe current reward sharing scheme which includes the rewards calculation equation and the minimum fixed fee are inadequate in promoting decentralization as evident by Cardano s currently low MAV relative to k. This is due to its anticompetitive features which are discussed in this section.\nThe minimum fixed fee or minPoolCost was apparently included for additional sybil attack protection. However, it s effect has been the opposite allowing stakepools with low pledge to offer greater rewards than pools with higher pledge in some cases. Moreover, the minimum fixed fee is certainly problematic as it places an unfair burden on small pools by enforcing a disproportionally larger fee than that of larger stakepools, reducing the ROS of small pools and incentivizing delegation to larger stakepools. This leads to centralization of the network around established stakepools, leaving less opportunity for smaller stakepools that may have greater pledge or community presence.\nThe current pledge benefit in the rewards equation is a function of the total stake in a pool that is significantly biased towards large stakepools that are close to saturation. Specifically, the current equation penalizes the pledge benefit of small pools. The smaller the pool, the larger the penalty. This unsaturated pledge benefit penalty combined with the minimum fixed fee leads to illogical rewards where large pools with low pledges can offer delegators higher ROS than smaller pools with significantly higher pledges. As a result, delegators are incentivized to delegate to larger stakepools even if they have lower pledges leading to network centralization.\nThe unsaturated pledge benefit penalty is part of the current rewards calculation equation and can be described by the equation:\nUnsaturatedPledgeBenefitPenalty=R1+a0 p a0 pz0 oz0z0Unsaturated Pledge Benefit Penalty = \\frac{R}{1 + a0} \\cdot p \\cdot a0 \\cdot \\frac{p \\frac{z0 - o}{z0}}{z0}UnsaturatedPledgeBenefitPenalty=1+a0R p a0 z0pz0z0 o\nSee UnsaturatedPledgeBenefitPenalty.xlxs to calculate the current unsaturated pledge benefit penalty for a stakepool.\nThe minimum fixed fee and current pledge benefit introduce a potential security threat to the Cardano protocol: Goldfinger attacks. The current reward scheme puts all small stakepools at a disadvantage regardless of pledge centralizing the network around established stakepools rather than pools with the most attractive pledge and fee combination. When SPOs with low stake (pledge) in the protocol are allowed to dominate consensus, they have a potential alternative incentive to attack the network in order make profit by shorting ADA. Because these stakepools have low stake in the protocol (operate with low or even zero pledge), they would be able make profit without any significant loss other than future staking rewards. With leverage, the attackers could make significantly more profit shorting ADA than years of staking.\nThe current setting of k, the parameter representing the optimal number of stakepools, is set too low to provide an effective pledge benefit leaving little incentive for pools to increase pledge over time. Specifically, with a small k value, a fully pledged pledge benefit is far from achievable for most SPOs. An ineffective pledge benefit leads to the formation of multipools with high leverage, as operators can split their pledge into multiple stakepools without a significant decrease in ROS in the resulting stakepools.\nThe current setting of a0, the pledge influence parameter, gives an unnecessarily large boost in rewards to very high pledge and fully pledged private pools. Specifically, the current setting of a0 results in approximately 30 greater rewards for fully pledged private pools. This boost in rewards unfortunately results significantly less rewards for public pools that commonly have low pledges relative to the saturation point. Given that most Cardano users are delegators and not SPOs, this exclusive boost for high pledge pools decreases Cardano s overall attractiveness as a staking protocol. Additionally, having a high a0 setting only accelerates the wealth (ADA) disparity between large entities operating private pools and delegators who make up majority of the ecosystem.\nThe proposed rewards calculation equation is a modification of the current equation that removes the unsaturated pledge benefit penalty:\nmaxPool=R1+a0 (o+p a0 oz0)maxPool = \\frac{R}{1 + a0} \\cdot (o + p \\cdot a0 \\cdot \\frac{o}{z0})maxPool=1+a0R (o+p a0 z0o )\nwhere: maxPoolmaxPoolmaxPool = maximum rewards for a stake pool, R=((reserve rho)+fees) (1 tau),o=min(poolstake/totalstake,z0)=z0R = ((reserve \\cdot rho) + fees) \\cdot (1 - tau), o = min(poolstake / totalstake, z0) = z0R=((reserve rho)+fees) (1 tau),o=min(poolstake/totalstake,z0)=z0 for fully saturated pool, p=min(pledge/totalstake,z0)p = min(pledge / totalstake, z0)p=min(pledge/totalstake,z0), and z0=1/kz0 = 1 / kz0=1/k\nThe proposed parameter values are the following:\nThe main goal of this proposal is to ensure fairness in stakepool rewards. This is achieved by including these principles in the design:\nEliminate all anticompetitive features. These include any parts of the design that treat stakepools differently based on anything other than pledge or declared fees. Ensure that the pledge benefit is fair and corresponds to a consistent boost in ROS no matter pool size. In other words, assuming the same fees, two pools with the same pledge should always offer the same ROS. Ensure that the pledge benefit is effective and incentivizes increasing pledge over time. Reduce the large rewards disparity between private pools and delegators and increase Cardano s overall attractiveness as a staking protocol.\nEliminate all anticompetitive features. These include any parts of the design that treat stakepools differently based on anything other than pledge or declared fees.\nEnsure that the pledge benefit is fair and corresponds to a consistent boost in ROS no matter pool size. In other words, assuming the same fees, two pools with the same pledge should always offer the same ROS.\nEnsure that the pledge benefit is effective and incentivizes increasing pledge over time.\nReduce the large rewards disparity between private pools and delegators and increase Cardano s overall attractiveness as a staking protocol.\nThe current reward sharing scheme includes two notable anticompetitive features. These features are the minimum fixed fee and the unsaturated pledge benefit penalty. This proposal removes these features to ensure fairness and promote adequate competition between stakepools. Removing these anticompetitive features promotes delegation to pools with most attractive pledge and fee combinations rather than established large pools and multipools. This results in fairer competition among stakepools and lower possibility of Goldfinger attacks as the pledge benefit is effective at all stakepool sizes. Greater decentralization is also possible as small stakepools will be able to offer competitive returns and potentially extract delegation from low pledge multipools.\nTo ensure a more effective pledge benefit and incentivize increasing pledge over time, this proposal increases the current value of k from 500 to 1000. This allows a fully pledged pledge benefit to be closer for all SPOs and will force multipools to split pledge and reduce pledge benefit if they wish to continue operating with the same leverage. Additionally, a change in the value of k will give many stagnant delegators an incentive to reconsider their delegations giving smaller stakepools an opportunity at increasing delegation.\nFinally, to reduce the large rewards disparity between private pools and delegators, this proposal reduces the setting of a0 from 0.3 to 0.2. The current setting of a0 results in approximately 30 greater rewards for fully pledged private pools. This proposal reduces this disparity to 20 to create a fairer rewards distribution. The result is an overall increase in rewards for delegators as most public pools operate with low pledges relative to the saturation point. Given that delegators make up majority of users, this reduction in a0 will make Cardano a much more competitive staking investment in contrast to other blockchains.\nThese proposed changes to Cardano s reward sharing scheme are aimed at ensuring fairness, increasing decentralization, and creating a more egalitarian staking ecosystem.\nStakepool viability and competitive points can give some insight into the fairness of the reward scheme. These points are essentially start-up costs required to run viable and competitive stakepools. These points are very high and out of reach for many SPOs with the current scheme. This proposal effectively minimizes these points.\nCurrent stakepool viability point: ~625,000 ADA\nCurrent stakepool competitive point: ~19,000,000 ADA\nProposal stakepool viability point: 1 ADA\nProposal stakepool competitive point: 1 ADA\nSee FairStakepoolRewards.xlxs to compare stakepool ROS between the current and proposed scheme.\nThis proposal includes parameter changes, one parameter removal, and a change to the rewards calculation. Because of the parameter removal and changes to the rewards calculation, a hardfork will be necessary for implementation.\nEach stage will be an individual protocol update. The first two updates will be protocol parameter updates. The third and final update will require a hardfork.\nBefore implementation, engineering and research teams must review the feasibility and potential consequences of the proposal, create the implementation for each update, and decide on the time interval between updates.\nThe protocol update is created, including all necessary changes. The raw transaction for the protocol update is built. Transaction is signed. Transaction is submitted. Protocol update is confirmed.\nThe protocol update is created, including all necessary changes.\nThe raw transaction for the protocol update is built.\nTransaction is signed.\nTransaction is submitted.\nProtocol update is confirmed.\nImplementation can be staged to reduce shock to the network:\nDecrease minPoolCost from 340 ADA to 100 ADA and increase k from 500 to 750. Increase k from 750 to 1000, decrease minPoolCost from 100 ADA to 0 ADA, and decrease a0 from 0.3 to 0.2. Remove minPoolCost from the protocol and implement the new rewards calculation equation.\nDecrease minPoolCost from 340 ADA to 100 ADA and increase k from 500 to 750.\nIncrease k from 750 to 1000, decrease minPoolCost from 100 ADA to 0 ADA, and decrease a0 from 0.3 to 0.2.\nRemove minPoolCost from the protocol and implement the new rewards calculation equation.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0080 | Transaction Serialization Deprecation Cycle\n\nThis CIP specifies a policy for the backwards compatibility of the serialization scheme of Cardano transactions.\nTransactions on Cardano are sent on the wire using CBOR and are specified with CDDL. The first scheme was introduced with the Byron phase. This scheme was changed dramatically with the introduction of the Shelley phase. As of the time of the writing of this CIP, however, every new scheme has been backwards compatible with the original scheme from the Shelley phase. The intention is still to maintain backwards compatibility to the extent reasonable, and to make explicit our policy for breaking backwards compatibility when deemed necessary.\nProblems with serialization fall into two categories:\nflaws in the implementation\nflaws is the CDDL specification\nNote that at the time of the writing of this CIP, there is only one implementation of the Cardano node, and we do not yet need to consider inconsistencies between different implementations.\nThe policy for maintaining backwards compatibility with the transaction serialization will be as follows.\nA serious flaw in the serialization is an issue which could have a large and negative impact on the network, and which requires a hard fork to fix. These will almost always be problems with the serialization and not the specification. It is up to human discretion to determine what constitutes a serious flaw, mostly likely by the core developers.\nBackwards compatibility can be abandoned in the case of a serious flaw, and the fix will occur at the next available hard fork.\nA non-serious flaw in the serialization is an issue which is not safety critical, but is problematic enough to merit breaking backwards compatibility. This is again a human judgment.\nBackwards compatibility can be abandoned in the case of a non-serious flaw, but there must be a deprecation cycle:\nIn the case of a soft fork (meaning that the change is backwards incompatible for block producers but not block validators), a new format can be introduced at the next major or minor protocol version, at which time the old format can be abandoned.\nIn the case of a hard fork (meaning that the change is backwards incompatible for both block producers and block validators), a new format can be introduced at the next major protocol version, but the old format must be supported for at least six months. After six months, the old format can be abandoned at the next possible fork.\nA good example of a non-serious flaw is the CDDL specification of the transaction output in the Alonzo ledger era:\nalonzo_transaction_output = [ address, amount : value, ? datum_hash : $hash32 ]\nalonzo_transaction_output = [ address, amount : value, ? datum_hash : $hash32 ]\nThere is nothing inherently wrong with this scheme, but it caused a problem in the Babbage ledger era with the addition of inline datums and script references. In particular, there were two new optional fields, and there was mutual exclusivity. In order to maintain backwards compatibility, Babbage introduced this scheme:\ntransaction_output = alonzo_transaction_output / babbage_transaction_output babbage_transaction_output = { 0 : address , 1 : value , ? 2 : [ 0, $hash32 // 1, data ] , ? 3 : script_ref }\ntransaction_output = alonzo_transaction_output / babbage_transaction_output babbage_transaction_output = { 0 : address , 1 : value , ? 2 : [ 0, $hash32 // 1, data ] , ? 3 : script_ref }\nIn other words, a new format was created, but the legacy format was still supported. The new format, babbage_transaction_output, was introduced 2022-09-22 with the Vasil hard fork, The old format, alonzo_transaction_output, can be retired after 2023-03-22.\nbabbage_transaction_output\nalonzo_transaction_output\nNote that this example required a hard fork.\nA good example of a non-serious flaw requiring a soft fork is the removal of zero-valued multi-assets in the mint field of the transaction body.\nIn the Babbage ledger era, a multi-asset value was defined as:\nvalue = coin / [coin,multiasset<uint>]\nvalue = coin / [coin,multiasset<uint>]\nZero values can be confusing inside of things like explorers, so in the Conway era they are removed:\nnatNum = 1 .. 4294967295 value = coin / [coin,multiasset<natNum>]\nnatNum = 1 .. 4294967295 value = coin / [coin,multiasset<natNum>]\nNotice that block validators will not notice this change, though block producers will notice it.\nWe should strive to maintain backwards compatibility.\nSerious flaws can be fixed immediately (at the next hard fork), and can break backwards compatibility.\nNon-Serious flaws can be fixed (at the next hard fork), but the old format must be supported for at least six months with support ending at the next hard fork event after the six months have passed.\nIt seems clear that security issues merit breaking backwards compatibility and should be fixed as soon as possible. The six month compatibility window for non-serious flaws is mostly arbitrary, but we need to allow enough time for people to migrate. It would be great to have more explicit definitions for \"serious\" and \"non-serious\" flaws, but this seems very difficult.\nThe proposal is accepted and recognized by the ledger team.\nN/A\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0082 | Improved Rewards Scheme Parameters\n\nThe current parameter settings of Cardano's rewards sharing scheme leave much to be desired in terms of fairness and promoting decentralization. minPoolCost puts small stakepools at a significant disadvantage. Replacing minPoolCost with a minPoolRate will ensure a level playing field for stakepools while providing sufficient sybil attack resistance. Additionally, the current setting of k, the optimal number of stakepools, is too low to provide an adequate pledge benefit. Increasing k will make the pledge benefit more effective and get delegations moving in hopes of helping single pool operators gain delegations.\nThe parameter changes in this proposal are an optimization of the current settings and are meant to improve the fairness and decentralization of Cardano. Furthermore, all changes suggested in this proposal have been specifically voiced by the Cardano community.\nStakepool Operator (SPO)\nThe operator of a stakepool, can be a single person or an entity.\nMultipool\nA group or brand of stakepools operated by a single entity or stakepool operator.\nROS\nReturn on staking (often annualized and represented as a percentage of the initial investment).\nStakepool Viability Point\nThe amount of pledge required for a stakepool with zero delegations to distribute nonzero rewards to delegators (assuming minimum stakepool fees and ignoring luck in VRF block production, i.e., rewards are exactly proportional to stake).\nStakepool Competitive Point\nThe amount of pledge required for a stakepool with zero delegations to offer the same ROS as a fully-saturated stakepool with zero pledge (assuming minimum stakepool fees).\nStakepool Saturation Point\nThe maximum amount of total stake in a stakepool before total stakepool rewards are capped and ROS diminishes.\nMinimum Attack Vector (MAV)\nAlso known as the Nakamoto Coefficient, the MAV is the minimum number of entities required to capture more than 50 of a network. In the context of Cardano, this refers to the minimum number of SPOs required to capture more than 50 of active stake.\nSybil Attack\nAn attack on an online system where an entity tries to take over the network by creating many identities or nodes.\nCardano s declining MAV is a real problem as the network matures and gains more users. Ideally MAV should increase or stay consistent over time. However, this is currently not the case. This trend points to potential problems with the parameters set in the rewards sharing scheme. Now that staking on the network has matured, it is time to re-examine the rewards sharing scheme parameters and seek optimization. This proposal suggests changes that aim to increase the fairness and decentralization of the Cardano network.\nminPoolCost sets a minimum fixed fee that a stakepool must collect from an epoch s rewards before distributing to delegators. This parameter was added to the rewards sharing scheme to provide additional sybil attack protection. While this parameter has been successful at deterring sybil attacks on the Cardano network, it has been at the expense of fairness and decentralization. minPoolCost imposes a proportionally greater staking fee on small stakepools in contrast to larger pools. This discrepancy results in many small pools being unviable and the network centralizing around established pools. It is not uncommon for small stakepools with higher pledge to offer inferior rewards to that of large stakepools with much lower pledge. In this way, minPoolCost reduces the effectiveness of pledge in the rewards scheme. In order to create a level playing field for stakepools and increase the effectiveness of pledge, minPoolCost must be removed from the protocol.\nThe current pledge benefit favors established pools close to saturation as a means of sybil attack protection. Specifically, this property combats high pledge sybil pools (~1 mil ADA pledge) that could be set up by large entities such as centralized exchanges. In contrast to minPoolCost, the pledge benefit s built-in sybil attack protection is significantly less aggressive and does not affect the viability of stakepools. In this way, it is a useful mechanism to combat sybil pools. However, the pledge benefit on its own has no means to combat zero fee sybil pools. Without minPoolCost, zero fee sybil pools could offer greater rewards than that of established stakepools that must set fees to pay for continued operation. In order to combat zero fee sybil pools without minPoolCost, minPoolRate must be added to the protocol. minPoolRate will set a minimum margin or percentage fee that operators can extract from an epoch s staking rewards. This new parameter will protect established stakepools by ensuring that they collect sufficient revenue from operation while offering a competitive ROS. Additionally, minPoolRate can be updated to reflect current economic conditions. As minPoolRate enforces a proportional minimum fee, it does not affect the viability of stakepools. Unlike minPoolCost, minPoolRate will be able to provide sufficient sybil attack protection with the pledge benefit while maintaining a level playing field for stakepools. minPoolRate was originally proposed in CIP-0023.\nk represents the optimal number of stakepools that the Cardano network can support. This is achieved through a saturation mechanism where after exceeding a saturation point a stakepool will earn no additional rewards. A stakepool larger than the saturation point will offer lower rewards than a stakepool at the saturation point. The parameter k is used to tune the saturation point. In addition to tuning the saturation point, k also affects the pledge benefit. The maximum pledge benefit exists when a pool is fully saturated. Therefore, increasing k will increase the number of optimal stakepools, decrease the saturation point, and make it easier for stakepools to achieve the maximum pledge benefit. Increasing the effect of the pledge benefit on rewards is imperative, as pledge currently has little effect on rewards allowing low pledge pools to proliferate through marketing alone. Furthermore, increasing k can be used to get stale delegations moving. This is especially useful in the case of saturated multipools, where delegators would have to reconsider their delegation potentially assisting small and medium sized pools. Any delegation flow from multipools to single pool operators will increase the decentralization of Cardano.\nThis CIP proposes several parameter changes. In order to give SPOs and delegators enough time to react to the changes, this CIP is divided into 4 stages. The proposed time interval between stages is 3 epochs or 15 days. However, it is up to the implementors to determine the best time interval between stages. Parameters are changed in the following stages:\nIn order to ensure the compatibility of existing stakepool registration certificates with this CIP, the variable poolRateEff must be added to the protocol. This variable will be the effective margin used during stakepool fee calculation. Following the hardfork, poolRate will only be the value set by SPOs. poolRate will be superseded by minPoolRate if its value is lower than minPoolRate. This is demonstrated in the definition of poolRateEff:\npoolRateEff=max(poolRate,minPoolRate)poolRateEff = max(poolRate, minPoolRate)poolRateEff=max(poolRate,minPoolRate)\nPropose changes voiced by the community. Make Cardano staking fairer by eliminating aggressive anticompetitive features. Increase the effect of the pledge benefit on staking rewards. Get stale delegations moving and allow users to reconsider their delegation. Maintain sufficient sybil attack protection.\nPropose changes voiced by the community.\nMake Cardano staking fairer by eliminating aggressive anticompetitive features.\nIncrease the effect of the pledge benefit on staking rewards.\nGet stale delegations moving and allow users to reconsider their delegation.\nMaintain sufficient sybil attack protection.\nStage 1 reduces minPoolCost from 340 ADA to 170 ADA. 170 ADA is proposed because it is half of the current minPoolCost and is close to what the USD value of minPoolCost was during the launch of Shelley. This value is more than sufficient to allow established community pools to stay profitable while enabling smaller pools to be more competitive. This value also maintains sybil attack resistance.\nStage 2 introduces the largest change to the network by deleting minPoolCost from the protocol in favor of a minPoolRate of 3 . 3 is proposed, as it allows established community pools to stay profitable when competing against minimum fee pools. This in combination with the pledge benefit provide sufficient sybil attack resistance while leveling the playing field for all stakepools. As pool size is significantly less important following this stage, pledge becomes a more important factor in choice of delegation. In contrast, Lido, Ethereum s most popular liquid staking DApp, applies a 10 fee on participants' staking rewards.\nStage 3 increases k from 500 to 750. The purpose of stage 3 and stage 4 is two-fold. Firstly, increasing k increases the pledge benefit. The more effective the pledge benefit, the greater Cardano s sybil attack resistance. Secondly, increasing k may get stale delegations moving again by oversaturating large pools. This will cause many delegators to reconsider their delegation, potentially helping smaller community pools find delegations. Increasing k is split into two stages to give SPOs sufficient time to react to the change. Furthermore, it is imperative that k is increased after stage 2, as small stakepools will only be competitive after minPoolCost has been removed.\nStage 4 increases k from 750 to 1000. Stage 4 will further improve the pledge benefit and get more delegations moving. This is the final stage of this proposal.\nBelow are test cases for the current rewards scheme and each stage of this proposal. The calculated values assume all pools are operating with minimum fees. Sybil pools are assumed to be small pools with no delegations. Community pools are assumed to be pools with significant delegation. As demonstrated below, this proposal allows community pools to have sufficient revenue (higher than the USD value of minPoolCost at the launch of Shelley) while creating a level playing field for all stakepools. Sybil attack resistance is maintained at every stage, as community pool ROS remains higher than sybil pool ROS. Data used for calculations was approximated from epoch 385. See ImprovedRewardsSchemeParameters.xlxs for more test cases.\nStakepool Viability Point: ~670,000 ADA\nStakepool Competitive Point: ~20,000,000 ADA\nStakepool Viability Point: ~335,000 ADA\nStakepool Competitive Point: ~16,500,000 ADA\nStakepool Viability Point: 1 ADA\nStakepool Competitive Point: 1 ADA\nStakepool Viability Point: 1 ADA\nStakepool Competitive Point: 1 ADA\nStakepool Viability Point: 1 ADA\nStakepool Competitive Point: 1 ADA\nThis proposal includes several parameter changes and changes to ledger rules. Specifically, stage 2 of this proposal will require a hardfork to introduce a new parameter, delete a parameter, and modify the stakepool fee calculation equation. As mentioned in the specification section, the stakepool fee calculation equation must be modified in order to ensure current stakepool registration certificates are compatible with this CIP.\nThe raw transaction for the parameter update is built. Transaction is signed. Transaction is submitted. Parameter update is accepted by majority of the network. Parameter update is confirmed.\nThe raw transaction for the parameter update is built.\nTransaction is signed.\nTransaction is submitted.\nParameter update is accepted by majority of the network.\nParameter update is confirmed.\nNecessary research and development is completed for the changes to the ledger rules. New version of cardano-node supporting the changes to the ledger rules is released. Raw transaction signaling the hardfork is built. Transaction is signed. Transaction is submitted. Hardfork is accepted by majority of the network. Hardfork and changes to ledger rules are confirmed.\nNecessary research and development is completed for the changes to the ledger rules.\nNew version of cardano-node supporting the changes to the ledger rules is released.\nRaw transaction signaling the hardfork is built.\nTransaction is signed.\nTransaction is submitted.\nHardfork is accepted by majority of the network.\nHardfork and changes to ledger rules are confirmed.\nEach stage will be an individual update. Stages 1, 3, and 4 will be parameter updates. Stage 2 will require a hardfork.\nBefore implementation, engineering and research teams must review the feasibility and potential consequences of the proposal, create the implementation for each update, and decide on the time interval between updates.\nAs previously mentioned, implementation will occur in the following stages:\nminPoolCost is decreased to 170 ADA minPoolCost is deleted, minPoolRate of 3 is implemented (requires hardfork) k is increased to 750 k is increased to 1000\nminPoolCost is decreased to 170 ADA\nminPoolCost is deleted, minPoolRate of 3 is implemented (requires hardfork)\nk is increased to 750\nk is increased to 1000\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0083 | Encrypted Transaction message/comment metadata (Addendum to CIP-0020)\n\nThis CIP is an addendum to the original CIP-0020, which is active since mid 2021 and widely used across many entities. It describes the JSON schema to add encrypted messages/comments/memos as transaction metadata. It is fully backwards compatible and requires no changes in existing tools, explorers, wallets. Tools/Wallets that do not have an implementation to decrypt this format will just show the encrypted base64 as the message, but it will not break any existing processes.\nTransaction messages/comments/memos via CIP-0020 are widely used across the Cardano Blockchain. For example DEXs are using it to comment there payouts to the customers. Individual users are using it to send funds across the network to other users with attached information about it. Users are buying goods and pay directly in ADA, attaching payment informations via an added message.\nTheses and many other usecases are actively happening on the blockchain right now and are a valuable addition to the core functions.\nMetadata is attached as a CBOR bytearray in the auxiliary dataset of a transaction. So the encoding is just done from UTF8-Text to Hex-Code/Bytes and after that it is sent in plaintext over the network/blockchain. To seek further adoption of blockchain usage, privacy features are a must in the future. Having cleartext information in a TCP packet might not be an issue for many things, but it is an issue if you wanna convince users to use the blockchain and their transaction feature like users using it now with bank transfers.\nIt is easy for 3rd-party entities like Internet Service Providers, Datacenters or basically any Man-In-The-Middle to collect data that is sent in cleartext. Data such as bank-account-numbers, email-addresses, telephone numbers, etc. which are parts of transaction messages.\nAs pointed out above, everyone that is having access to the datastream and of course the publicly distributed ledger can extract the cleartext data of the transaction messages. Because there must not even be a specific approach to get such transaction message data out of a TCP stream, just a simple filter for email addresses (example) is enough. Even with a simple encryption of such messages - and publicly known passphrase - it is much more complicated for the Man-In-The-Middle listener to collect data on the fly.\nTargeted benefits:\nBy using a default passphrase, Man-In-The-Middle \"sniffer\" cannot extract/parse data like email-addresses, invoice-numbers on the fly that easily. They would need to search for a cardano-node transmission and decrypt each message. Public explorers like Cexplorer.io, Cardanoscan, etc. can still show the decrypted message content via there https connection to the user. So no cleartext transmission at all.\nDifferent users can transfer funds with encrypted messages attached between each other, using a preshared passphrase. Only theses users need to know the content. Example: A user buys goods from an online-store, the store provides a preshared-passphrase to the user on their website or via email, the user sends the payment with payment-information encrypted with this passphrase to the store.\nKeeping the usecase of a transaction private does not only belong to different entities, but to a single user too. Example: If a user sends funds to a Dex or wants to lend some fund to a friend, he just can add information like 'Sent xxx ADA to bob for xxx' to the outgoing transaction as a documentation using an own choosen private passphrase. This information is stored on the chain and so in the wallet, only the user itself can review the use case of these transactions.\nBackwards compatible with CIP-0020\nEasy implementation by using well known tools like OpenSSL\nThis addition to the original CIP-0020 should not be seen as the end-all-be-all security solution for privacy on the blockchain. There's better options and upcoming Midnight for that. The transaction messages are also not intended to act like chat messages on the chain.\nThe specification update for encrypted messages takes advantage of the simple original design, which is leaving room for additional json-keys not affecting the parsing of the content at all. The only outcome if a receiver does not process the encrypted content is, that the encrypted message is shown instead of an maybe autodecrypted one. But even the encrypted base64 strings fit into the max. 64char long string restriction. So it does not break any tools. More on the autodecryption later.\n{ \"674\": { \"enc\": \"<encryption-method>\", \"msg\": [ \"base64-string 1\", \"base64-string 2\", \"base64-string 3\" ... ] } }\n{ \"674\": { \"enc\": \"<encryption-method>\", \"msg\": [ \"base64-string 1\", \"base64-string 2\", \"base64-string 3\" ... ] } }\nThe format is identical to the original one, with a simple addition of the enc (encryptionmode) entry.\nenc\nThe value given in the enc field references the type of encryption is used. Starting with a simple implementation named basic. There is room to add additional encryption method in the future like using ChaCha20/Poly1305 or using public/private key encryption. Also there is the possibility to not encode the metadata in the standard JSON format, but using CBOR encoding instead.\nenc\nbasic\nThis is not really an encryption mode, but included as a backwards compatible entry to signal this message as an unencrypted one. The entry is not needed and fully optional for unencrypted messages.\nSalted__\nOpenSSL was choosen, because its fast and widely available also for all kind of different platforms, web frontends, etc. Encryption algo is AES-256-CBC (salted) using pdkdf2 to derive the key from the given passphrase. 10000 Iterations is the default value for this encryption method. The format of the encoded output is base64 format.\npdkdf2\nThe encryption is based on a given passphrase, which can be choosen by the user. However, a default-passphrase \"cardano\" should be used to encrypt/decrypt if no other passphrase is provided or known.\nOpenSSL uses PKCS#7 as padding. The adopted cipher accepts only multiple of 16-byte blocks. Not fitting messages to be encrypted are filled with the number of padding bytes that are needed to form multiple of 16-bytes. So if 1 byte of padding is required, the padding \"01\" is added. If 2 bytes of padding are needed, \"02 02\" is added. If no padding is required, an extra block of 0x10 bytes is added, meaning sixteen \"10\" bytes. In order to be interoperable with OpenSSL this kind of padding is a requirement.\nAs pointed out above, its way harder for man-in-the-middle listeners, to decrypt every single message on the fly. So by using a default passphrase, tools can encrypt messages and explorers/wallets can autodecrypt such messages trying to use the default passphrase. In that way, the displayed message is automatically readable to the user. If a more protected communication is needed, the sender can choose a custom passphrase and communicate that to the receiver as a preshared passphrase.\nWhat part is uses for the encryption?\nThe whole content of the unencrypted normal transaction metadata msg: key is used, thats the array with the message string(s). (Example below)\nmsg:\nYes, example implementations for node.js, PHP, bash, etc. can be found in the codesamples folder. They are showing how to encrypt/decrypt text with the right parameters set for this basic mode.\nwarning\nMessage decryption should be done on the user frontend if possible, not via server callbacks.**\nFirst, generate a normal metadata transaction message.\nnormal-message-metadata.json:\n{ \"674\": { \"msg\": [\"Invoice-No: 123456789\",\"Order-No: 7654321\",\"Email: john@doe.com\"] } }\n{ \"674\": { \"msg\": [\"Invoice-No: 123456789\",\"Order-No: 7654321\",\"Email: john@doe.com\"] } }\nThe encryption is done on the whole content of the msg: key, so this is\nmsg:\n[\"Invoice-No: 123456789\",\"Order-No: 7654321\",\"Email: john@doe.com\"]\n[\"Invoice-No: 123456789\",\"Order-No: 7654321\",\"Email: john@doe.com\"]\nin our example.\nEncrypt this content via openssl, the default passprase cardano, iteration set to 10000 and key-derivation via pbkdf2:\necho -n '[\"Invoice-No: 123456789\",\"Order-No: 7654321\",\"Email: john@doe.com\"]' | openssl enc -e -aes-256-cbc -pbkdf2 -iter 10000 -a -k \"cardano\"\necho -n '[\"Invoice-No: 123456789\",\"Order-No: 7654321\",\"Email: john@doe.com\"]' | openssl enc -e -aes-256-cbc -pbkdf2 -iter 10000 -a -k \"cardano\"\nThe encrypted result are the base64 encoded strings:\nU2FsdGVkX1/5Y0A7l8xK686rvLsmPviTlna2n3P/ADNm89Ynr1UPZ/Q6bynbe28Y /zWYOB9PAGt+bq1L0z/W2LNHe92HTN/Fwz16aHa98TOsgM3q8tAR4NSqrLZVu1H7\nU2FsdGVkX1/5Y0A7l8xK686rvLsmPviTlna2n3P/ADNm89Ynr1UPZ/Q6bynbe28Y /zWYOB9PAGt+bq1L0z/W2LNHe92HTN/Fwz16aHa98TOsgM3q8tAR4NSqrLZVu1H7\nCompose the JSON by using the base64 encoded encrypted strings now for the msg: part.\nmsg:\nAlso add the value basic for the enc: key, to mark this transaction message as encrypted with basic mode.\nbasic\nenc:\nencrypted-message-metadata.json:\n{ \"674\": { \"enc\": \"basic\", \"msg\": [ \"U2FsdGVkX1/5Y0A7l8xK686rvLsmPviTlna2n3P/ADNm89Ynr1UPZ/Q6bynbe28Y\", \"/zWYOB9PAGt+bq1L0z/W2LNHe92HTN/Fwz16aHa98TOsgM3q8tAR4NSqrLZVu1H7\" ] } }\n{ \"674\": { \"enc\": \"basic\", \"msg\": [ \"U2FsdGVkX1/5Y0A7l8xK686rvLsmPviTlna2n3P/ADNm89Ynr1UPZ/Q6bynbe28Y\", \"/zWYOB9PAGt+bq1L0z/W2LNHe92HTN/Fwz16aHa98TOsgM3q8tAR4NSqrLZVu1H7\" ] } }\nConsole one-liner:\njq \".\\\"674\\\".msg = [ $(jq -cjrM .\\\"674\\\".msg normal-message-metadata.json | openssl enc -e -aes-256-cbc -pbkdf2 -iter 10000 -a -k \"cardano\" | awk {'print \"\\\"\"$1\"\\\",\"'} | sed '$ s/.$//') ]\" <<< '{\"674\":{\"enc\":\"basic\"}}' | tee encrypted-message-metadata.json | jq\njq \".\\\"674\\\".msg = [ $(jq -cjrM .\\\"674\\\".msg normal-message-metadata.json | openssl enc -e -aes-256-cbc -pbkdf2 -iter 10000 -a -k \"cardano\" | awk {'print \"\\\"\"$1\"\\\",\"'} | sed '$ s/.$//') ]\" <<< '{\"674\":{\"enc\":\"basic\"}}' | tee encrypted-message-metadata.json | jq\nA decryption can be done in a similar way:\njq -crM \".\\\"674\\\".msg[]\" encrypted-message-metadata.json | openssl enc -d -aes-256-cbc -pbkdf2 -iter 10000 -a -k \"cardano\"\njq -crM \".\\\"674\\\".msg[]\" encrypted-message-metadata.json | openssl enc -d -aes-256-cbc -pbkdf2 -iter 10000 -a -k \"cardano\"\nWhich results in the original content of the msg key:\n[\"Invoice-No: 123456789\",\"Order-No: 7654321\",\"Email: john@doe.com\"]\n[\"Invoice-No: 123456789\",\"Order-No: 7654321\",\"Email: john@doe.com\"]\nThis design is simple, so many tools on the cardano blockchain can adopt it easily and a few have already started to implement it. The original CIP-0020 design allowed the addition of new entries like the \"enc\": key for encrypted messages in this CIP. Therefore the encoding format of the encrypted message was choosen to be UTF-8 instead of bytearrays, because it would break the backwards compatibility to CIP-0020. But maybe more important, it gives the user a simple text-format to handle such messages. Users can copy and paste the base64 encoded string(s) using there own tools for creation and verification. For example, a user can simply copy the encrypted format from an explorer and verify it with an external own local tool. Such messages are usally pretty short. Yes, the benefit of using bytearrays is to have less data (around -33 over base64), but the decision was made to sacrifice this benefit in favor of the base64 format for the reasons pointed out before.\n\"enc\":\nThere is also for example CIP-8, but CIP-8 doesn't really fulfill the simplicity of just providing encrypted messages. CIP-8 is focused on Signing, which is not needed for encryption. The method to generate encrypted messages here is not intended to verify the owner of a message via signing. There is no need that everything on Cardano must be difficult. Also using such CBOR encoded structures would break all currently implemented transaction message solutions. This CIP uses openssl and base64 encoding, and endusers could even copy&paste such text into other tools, etc. Future updates may include the option to mix encrypted and unencrypted messages by adding another key like msgclear to support such a mixed message style format.\nmsgclear\nWallets/Tools can implement an autodecryption attempt with the default passphrase on such messages, to give the user a more streamlined experience. The communication should be done via https or similar to make sure the message cleartext is not exposed again during the transmission. Additionally the Tools can prompt for an input and decrypt the message once the user has requested it, this decryption should be done on the user frontend for further security.\nLike with CIP-0020, it is up to the wallet-/display-/receiver-implementor to parse and check the provided metadata. As for the current state, its not possible to have the same label \"674\" more than once in a cardano transaction. So a check about that can be ignored at the moment. This CIP provides the correct implementation format, the parsing should search for the \"674\" metadata label, the \"msg\" and the \"enc\" key underneath it. There should also be a check, that the provided data within that \"msg\" key is an array. All other implementations like a missing \"msg\" key, or a single string instead of an array, should be marked by the display-implementor as \"invalid\". Additional keys within the \"674\" label should not affect the parsing of the \"msg\" and the \"enc\" key. As written above, we will likely see more entries here in the future like a \"version\" key for example, so additional keys should not harm the parsing of the \"msg\" and \"enc\" key.\nAn encrypted transaction message should be considered valid if the following apply:\nLabel = 674.\nhas property \"enc\".\nenc property contains a supported method like basic\nbasic\nhas property \"msg\".\nmsg property contains an array of strings, even for a single-line message.\nEach line has a maximum length of 64 characters.\nIf there are additional properties, they don't invalidate the message. They can just be ignored.\nIf any of the above is not met, ignore the metadata as an encrypted transaction message. Can still be displayed as general metadata to the transaction.\nThe implementation format in this CIP should be the ground base for encrypted transaction messages/comments/memos and should be respected by creator-/sender-implementations as well as in wallet-/receiver-/display-implementations.\nThe acceptance criteria to be Active should already have been met, because the following Implementors using this CIP on the Cardano Blockchain:\nActive\nCardano Explorer (https://cexplorer.io)\nStakePoolOperator Scripts (https://github.com/gitmachtl/scripts)\nAdaStat.net (https://adastat.net)\nEternl Wallet (https://eternl.io)\nCexplorer.io: With the implementation of the encrypted message decoding.\nStakePool Operator Scripts: It works on the commandline like any other script of the collection by just adding the \"enc: basic\" parameter, you can provide an individual passphrase by using the \"pass: passphrase \" parameter. This automatically generates the needed metadata.json structure with the encrypted message in it and attaches it to the transaction itself.\n\"enc: basic\"\n\"pass:<passphrase>\"\nEternl.io:\nAdaStat.net: With the implementation of the encrypted message decoding using a pure frontend solution.\nThe following Projects have committed to also implement it:\nCNTools (https://cardano-community.github.io/guild-operators/#/Scripts/cntools)\nJorManager (https://bitbucket.org/muamw10/jormanager/)\nCardanoscan.io (https://cardanoscan.io)\nThe plan is to reach out to other projects - which already supporting the normal transaction messages - too. And of course also to new ones.\nThere are various code samples available in the codesamples folder to make it as easy as possible for integrators to implement it.\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0084 | Cardano Ledger Evolution\n\nThis CIP provides guidance for future CIPs concerning the Cardano ledger.\nThe ledger is responsible for processing transactions and updating the shared state of the network. It also processes block headers and handles the state transformation from one epoch to the next (e.g. computing the staking rewards).\nMost of the state maintained by the ledger relates to the Extended UTxO accounting model and support for Ouroboros, the proof-of-stake consensus mechanism used in Cardano.\nThis CIP aims to give guidance for future CIPs related to the ledger, making it a registered category of the CIP process1.\nMany thanks to Arnaud Bailly and Michael Peyton Jones for all their help reviewing and providing feedback on the first versions of this CIP.\nContext for the terminology used in this document is given in CIP-59.\nThe ledger is specified as a state transition system using a small step operational semantics. We refer to this framework as the Small Step Semantics for Cardano, or the STS for short. An understanding of the existing STS specifications for the existing ledger eras is often required to fully understand the implications of changes to the ledger (though an understanding of the Haskell implementation is a fair substitute).\nThe STS framework leaves both cryptographic primitives and the serialization format abstract. The STS specifications need to be complete enough to realize a full implementation of the ledger given the cryptographic primitives and serialization format. The cryptographic primitives are described as appendices to the STS specifications, and the serialization format is given as a CDDL file. There SHOULD be one STS specification per ledger era.\nFrom the Byron to the Babbage ledger eras, the STS frameworks were written in LaTeX\\LaTeXLATE X. Starting in the Conway ledger era, literate Agda will be used. During the transition from LaTeX\\LaTeXLATE X to literate Agda, we will take advantage of the ability to substitute LaTeX\\LaTeXLATE X in place of Agda code when needed for expedience. With time, the Agda specification will not only be used to provide PDF specifications, but also reference implementations.\nA ledger era is a collection of features added to the ledger which are introduced at a hard fork. The existing ledger eras, with very simplistic descriptions, are given below.\nNote that there is no Allegra specification. The Allegra era consists entirely of the addition of timelocks to the MultiSig script introduced in the Shelley ledger era (See figure 12 of the Mary specification).\nNote that small, isolated changes can be made within a ledger era by way of an intra-era hard fork. See CIP-59 for more details.\nSome provenance about the ledger calculations is provided by ledger events. Sometimes these events have clear triggers (e.g. Plutus script execution) and sometimes they provide intermediate calculations performed by the ledger rules (e.g. the reweard calculation). The events come with zero cost to a running node if not used and are not stored in the ledger state. Documentation about the existing events can be found here.\nSince most ledger CIPs will involve backwards incompatible changes, the following two definitions are helpful:\nHard fork - A hard fork is a change to the protocol (not restricted to the ledger) resulting in a single new block definition becoming valid.\nAlternatively, a hard fork is a backwards incompatible change for both block producers and block validators.\nSoft fork - A soft fork is a change to the protocol (not restricted to the ledger) resulting in fewer blocks being valid.\nAlternatively, a soft fork is a backwards incompatible change for block producers, but a backwards compatible change for block validators.\nTransactions and blocks are serialized with CBOR and specified with CDDL file.\nSerialization changes to the ledger are discussed in CIP-80.\nNote that the serialisation format of the ledger state is unspecified and left as an implementation detail (unlike the format of blocks).\nThe ledger and Plutus scripts have a common interface, described in CIP-35. CIPs relating to this inteface are relevant to both the ledger and to the Plutus CIP categories.\nAdditionally, there is significant overlap with the Ledger category around the ledger-script interface and the protocol parameters. CIPs which change these parts of Cardano should generally use the Plutus category and not the Ledger category, although the Editors may ask the Ledger reviewers to comment.\nThe criterion for deciding if a change to the ledger merits a CIP is as follows: changes to the ledger require going through the CIP process whenever every implementation of the Cardano ledger needs to be standardized on the details.\nBug fixes are an exception to this criterion, they do not merit a CIP except in the case that the fix is substantially complicated. A \"bug fix\" is a change to behavior where:\nThe implemented behavior does not match the specification; or\nThe specified behavior is clearly wrong (in the judgment of relevant experts)\nSerialization changes are another possible exception to the criterion. Many serialization changes can be handled as a part of the normal development process without the need for a CIP. Dramatic changes to the serialization, however, may benefit from the CIP process.\nThe ledger rules MUST be standardized in order for consensus to be maintained, but things like the ledger events are more open to debate.\nChanges to the protocol parameter values do not require a CIP since they are a governance issue (see CIP-1694).\nFamiliarity with the existing ledger specifications is required to propose changes to the ledger.\nThe CIP specifications for ledger CIPs must be sufficiently detailed for inclusion in a formal ledger specification.\nThough proposals can be accepted solely on the basis of peer and Ledger team review, some areas (e.g. changes to the incentives model) might only considered ready for implementation if accompanied by an opinion from an expert designated by the implementor (e.g. with a proper game theoretic analysis).\nThe following table gives the current set of reviewers for Ledger CIPs.\nEven though there is currently only one implementation, this provides us with a clear definition of what is essential to the ledger. It also provides a clear path for future implementations.\nThis decision should be left to the community as more use cases emerge.\nIt is not always clear which seemingly small details can make a large difference to the many consumers of the ledger. It is better that the CIP process achieve consensus on all the details than for these decisions to be made during the implementation phase.\nThis CIP requires the acceptance of the Ledger team, which it has in virtue of its authorship.\nNo implementation is required.\nThis CIP is licensed under CC-BY-4.0.\nSee CIP-1. While nothing new is added to the usual CIP process, expectations for ledger CIPs are made explicit and some background information is provided.\nSee CIP-1. While nothing new is added to the usual CIP process, expectations for ledger CIPs are made explicit and some background information is provided.\nSee CIP-1. While nothing new is added to the usual CIP process, expectations for ledger CIPs are made explicit and some background information is provided.\n2023 Cardano Foundation\n\n---\n\nCIP-0085 | Sums-of-products in Plutus Core\n\nPlutus Core does not contain any native support for datatypes. Instead, users who want to use structured data must encode it; a typical approach is to use Scott encoding. This CIP proposes to add native support for datatypes to Plutus Core using sums-of-products (SOPs), and to use that support more efficient scripts, and better code-generation for compilers targeting Plutus Core.\nThe first designs of Plutus Core had native support for datatypes. In the interest of keeping the language as small and simple as possible, this was removed in favour of encoding datatype values using Scott encoding.\nExperiments at the time showed that the performance penalty of using Scott encoding over native datatypes was not too large, and so we could realistically use Scott encoding. But we might expect that we should be able to do better with a native implementation, and indeed we can.\nThis section talks about Haskell, but the same problem applies in other languages.\nGiven a Haskell value, how do you translate it into the equivalent Plutus Core term ('lifting')? It's clear what to do for a Haskell value of one of the Plutus Core builtin types (just turn it into a constant), but it's much more complicated for a Haskell value that is a datatype value: you have to know how to do all the complicated Scott-encoding work.\nFor example:\n1 lifts to (con integer 1) (easy enough)\n1\n(con integer 1)\nJust 1 lifts to (delay (lam case_Nothing (lam case_Just [case_Just (con integer 1)]))) (much more complicated)\nJust 1\n(delay (lam case_Nothing (lam case_Just [case_Just (con integer 1)])))\nThis means that it's difficult to specify how to do lifting for structured data. For example, if we want users (or the ledger!) to create Plutus Core terms representing structured data, it will be difficult to explain how to do it.\nThere is also the opposite direction: how can we turn a Plutus Core term back into a Haskell value ('unlifting)'? Again, it's clear how to do this for builtin types and very unclear how to do it for datatypes. Doing anything with a Scott-encoded datatype usually requires applying it to arguments and evaluating it. It's not reasonable to require the Plutus Core evaluator just to work out what a term means.\nFor example:\n(con integer 1) clearly unlifts to 1\n(con integer 1)\n1\n(delay (lam case_Nothing (lam case_Just [caseJust (con integer 1)]))) should unlift to Just , but this is hard to see. Scott encodings are not even canonical (there can be many terms that represent the same Scott-encoded value), so it is hard to know what they represent in general.\n(delay (lam case_Nothing (lam case_Just [caseJust (con integer 1)])))\nJust\nIn practice this makes Plutus Core terms very opaque and difficult to work with.\nData\nThe design of the EUTXO model rests on passing arguments to the script for the redeemer, datum, and script context. But we hit upon the problem of how to represent this information.\nThe first design was to encode each argument as a Plutus Core term (using Scott-encoding for structued data), and pass it directly to the script. However, this had several problems:\nAt that point, the on-chain form of Plutus Core was typed, and we would ideally want to typecheck the application before peforming it. But this could be expensive. As we saw above, Plutus Core terms representing structured data are very opaque, so using them for redeemers and datums would make those values very opaque to users. Creating the script context would require the ledger to take a Haskell value and lift it into a Plutus Core term, which as we've seen is non-trivial.\nAt that point, the on-chain form of Plutus Core was typed, and we would ideally want to typecheck the application before peforming it. But this could be expensive.\nAs we saw above, Plutus Core terms representing structured data are very opaque, so using them for redeemers and datums would make those values very opaque to users.\nCreating the script context would require the ledger to take a Haskell value and lift it into a Plutus Core term, which as we've seen is non-trivial.\nThe solution that we chose was to pick a single generic structured data type which would be used for data interchange between the ledger and scripts. This is the Data type.\nData\nHowever, Data is not the natural representation of structured values inside a script, that would be as datatypes. So this means we often want a decoding step where the script translates Data into its own representation.\nData\nData\nThe Plutus Core interpreter has got a lot faster over the last few years (at least one order of magnitude, possibly two by now). However, there continues to be relentless pressure on script resource usage. This will always be the case: no matter how fast we make things, things can always be faster or cheaper and it will improve throughput and save people money.\nThat means that significant performance gains are always compelling.\nAs discussed above, both lifting and unlifting are currently very tricky or impossible.\nThe Data type is used in many places as a representation of structured data:\nData\nThe script context is represented as Data Some language toolchains that target PLC use Data for all structured data, rather than Scott encoding or similar.\nThe script context is represented as Data\nData\nSome language toolchains that target PLC use Data for all structured data, rather than Scott encoding or similar.\nData\nHowever, working with Data is not very pleasant. Deconstructing a datatype encoded as Data requires multiple steps, each of which has to go through the builtins machinery, which imposes additional overhead. Our benchmarks show that this is very 3x slower even than Scott encoding datatypes.\nData\nData\nThe following new term constructors are added to the Plutus Core language:1\nt ::= ... -- Packs the fields into a constructor value tagged with i | (constr i t...) -- Inspects the tag on t and passes its fields to the corresponding case branch | (case t t...)\nt ::= ... -- Packs the fields into a constructor value tagged with i | (constr i t...) -- Inspects the tag on t and passes its fields to the corresponding case branch | (case t t...)\nTags start from 0, with 0 being the first case branch, and so on.\nThe small-step reduction rules for these terms are:\nNote that CASE_SUB does not reduce branches and CASE_EVAL preserves only the selected case branch. This means that only the chosen case branch is evaluated, and so case is not strict in the other branches.2\nCASE_SUB\nCASE_EVAL\ncase\nAlso, note that case branches are arbitrary terms, and can be anything which can be applied. That means that, for example, it is legal to use a builtin as a case branch, or an expression which evaluates to a function. However, the most typical case will probably be a literal lambda.\nA new Plutus Core minor language version 1.1.0 is added to indicate support for these terms. They are illegal before that version.\nAll steps in the CEK machine have costs, all of which are constant. There are cost model parameters which set the costs for each step. This will therefore be new cost model paramters governing the costs for the steps for evaluating constr and case.\nconstr\ncase\nThere is a potential problem because constr and case have a variable number of children, unlike all the existing constructs. The risk is that we could end up doing a linear amount of work but only paying a constant cost.\nconstr\ncase\nHowever:\nconstrs arguments are all evaluated in turn, so we are sure to pay a linear price.\nconstr\ncase's branches are not all evaluated, but the only place we could do a linear amount work is in selecting the chosen branch. But this can be avoided in the implementation by storing the cases in a datastructure with constant-time indexing.\ncase\nMost Plutus Core programs spend a significant amount of their time operating on datatypes, so we would expect that a performance improvement here would make a significant difference.\nIndeed, this seems to be true. Our benchmarks show that this CIP leads to:\nA 0-30 real-time speedup in example programs that do generic computation work, versus Scott encoding3 (the 0 is a primality tester that is mostly doing arithmetic, the 30 is mostly doing data manipulation). A small global slowdown of 2-4 .\nA 0-30 real-time speedup in example programs that do generic computation work, versus Scott encoding3 (the 0 is a primality tester that is mostly doing arithmetic, the 30 is mostly doing data manipulation).\nA small global slowdown of 2-4 .\nNote that the speedup from point 1 is inclusive of the slowdown from point 2.\nToday, lifting is complicated, and unlifting is mostly impossible. With this CIP lifting becomes very simple, and unlifting becomes not only possible but simple. Our prototype implementation contains code-generation to generate lifting and unlifting code in Haskell, but it is straightforward enough that you could easily do it in any language.\nThere are still some limitations, e.g. you cannot lift or unlift structures that contain functions, but this is not that unusual (similarly, you typically can't serialise such structures).\nThe costs of this proposal are substantial.\nPlutus Core today has only 8 kinds of term, this proposal adds 2 more, an increase of 25 . Additionally, the new term types are the first term types which have a variable number of children. Both of these changes increase implementation complexity throughout the system: everything that processes Plutus Core terms must now handle the new cases, and handle the variable numbers of children.\nThis change also modestly expends our novelty budget. Plutus Core is a deliberately conservative language: for the most part it is just the untyped lambda calculus. The proposed new features for sums-of-products are also conservative, and follow a very typical pattern. But they are not quite as standard as the untyped lambda calculus.\nMoreover, a change like this would be very painful to reverse, so we should seriously consider the costs before proceeding.\nThis makes it different to all other Plutus Core terms, but we believe it is the expected behaviour: no language has strict case destructuring.\nConsider e.g.\ncase (Just 1) of Just x -> print \"Success!\" Nothing -> error \"Failure!\"\ncase (Just 1) of Just x -> print \"Success!\" Nothing -> error \"Failure!\"\nEven in a strict language like Rust, nobody would expect this to evaluate the Nothing branch and evaluate to an error.\nNothing\nFurthermore, this additional laziness has significant performance benefits, see Appendix 2 for more discussion.\nData\nData is expressive enough to encode datatypes, indeed this is why it is possible to encode the script context into it. It's performant enough that people who work with the script context as Data are able to get by. Moreover, lifting and unlifting to Data is very easy. So why not just use it to encode all datatypes?4\nData\nData\nData\nThere are two reasons:\nData cannot contain functions, so it's less expressive than either SOPs or Scott-encoding. The performance is much worse.\nData cannot contain functions, so it's less expressive than either SOPs or Scott-encoding.\nData\nThe performance is much worse.\nTo justify 2, we benchmarked some list processing using datatypes implemented with SOPs and with Data, and the Data version is over 3x worse (not accounting for overhead, so the real figure may be higher).\nData\nData\nThe current solution is a sums-of-products solution, i.e. we have an introduction and elimination form for objects that have both a tag and a list of fields. We could instead separate these two parts out and have an introduction and elimination form for sums, likewise for products.\nThat would look something like this:\nt ::= ... -- Tags a term with a tag | (tag i t) -- Inspects a tagged term for the tag, and passes the -- inner term to the case function corresponding to the tag | (case t t...) -- Constructs a product with the given fields | (prod t...) -- Accesses the i'th field of the given product | (index i t)\nt ::= ... -- Tags a term with a tag | (tag i t) -- Inspects a tagged term for the tag, and passes the -- inner term to the case function corresponding to the tag | (case t t...) -- Constructs a product with the given fields | (prod t...) -- Accesses the i'th field of the given product | (index i t)\nThis is cleaner in some ways, and was the first prototype we implemented, but it has the following problems:\nIt adds two more term constructors to the language\nIt is significantly slower in practice, because the combined operation of processing a sum-of-products is extremely common\nThe design presented here allows the case branches to be arbitrary terms, which must be evaluated and then applied to the fields.\nA more complex design would be to have the bindings for the variables be part of the case expression instead. That is, the current design does this:\n(case (constr 1 a b) (\\x -> t1) (\\x y -> t2))\n(case (constr 1 a b) (\\x -> t1) (\\x y -> t2))\n(the literal lambdas here could be arbitrary terms)\nwhereas we could do this:\n(case (constr 1 x y) (alt x t1) (alt x y t2))\n(case (constr 1 x y) (alt x t1) (alt x y t2))\nThe effect of this is that we know statically that we have a function, and we can jump right into evaluating the body of the case branch instead of first having to evaluate it to a function and apply the arguments.\nThis saves us a small but meaningful amount of overhead at the cost of making the implementation significantly more complex.\nWe prototyped this version and it was noticeably faster (~10 ). However, performance investigations showed that we can realize a significant amount of the performance gain through other means, leaving only about 3 unclaimed.5 We think that this is an acceptable cost for a simpler implementation.\nconstr\ncase\nBoth constr and case in this proposal are saturated, meaning that they have all of the arguments that they need explicitly provided in the AST. This is not the only option.\nconstr\ncase\nWe could easily have unsaturated constr, by making (constr i) a function that then needs to be applied to its arguments. This would be mildly more complicated to implement, since we wouldn't know how many arguments to expect, and so we would need to always be able to add aditional arguments to a constr value, but this would be manageable.\nconstr\n(constr i)\nconstr\nUnsaturated case is more complicated. While the tag on the scrutinee constr value tells us which branch we are going to take in the end, we don't know how many branches we need in total. In principle we could extend the tags on constructors to include not only the selected tag but also the maximum tag. That would allow us to know how many case branches we need. A more serious problem is that we would not be able to be non-strict in the branches any more, as they would be passed as function arguments and hence forced.\ncase\nconstr\nThe main advantage of unsaturated constr and case is that it would avoid the need for n-ary terms in the language, as both would then have a fixed number of children. However, it makes them more complex to work with, and likely less efficient to implement. Finally, this is simply a less common design, and so conservatism suggests sticking to the more standard approach unless there is a compelling reason not to.\nconstr\ncase\nplutus changes Specification Production implementation Costing of the new operations\nplutus\nSpecification\nProduction implementation\nCosting of the new operations\ncardano-ledger changes Implementation of new ledger language including SOPs\ncardano-ledger\nImplementation of new ledger language including SOPs\nFurther benchmarking Ensure that regressions on existing scripts do not occur Check additional real-world examples\nEnsure that regressions on existing scripts do not occur\nCheck additional real-world examples\nRelease New Plutus language version supported in a released node version New ledger language supported in a released node version\nNew Plutus language version supported in a released node version\nNew ledger language supported in a released node version\nThis plan will be implemented by Michael Peyton Jones and the Plutus Core team with assistance from the Ledger team. The changes will be in the PlutusV3 ledger language at the earliest, which will probably arrive in the Conway ledger era.\nPlutusV3\nWhile Typed Plutus Core is not part of the specification of Cardano, it is still interesting and informative to give the changes here. Plutus Core was originally conceived of as a typed language, and making sure that we can express the changes cleanly in a typed setting means that we ensure that the semantics make sense and that it will continue to be easy to compile to Plutus Core from typed languages.\nWe add one new type constructors and one auxiliary constructor to the type language of Typed Plutus Core:\n-- List of types. This is an auxiliarry syntactic form, not a type! tyl ::= [ty...] ty ::= ... -- Sum-of-products type, has n children, each of which is a list of types | (sop tyl...)\n-- List of types. This is an auxiliarry syntactic form, not a type! tyl ::= [ty...] ty ::= ... -- Sum-of-products type, has n children, each of which is a list of types | (sop tyl...)\nThis corresponds to a sum-of-products type, and it has one list of types for each constructor, giving the argument types.\nWe add the following new term constructors to the Typed Plutus Core language:\nt ::= ... | (constr ty i t...) | (case ty t t...)\nt ::= ... | (constr ty i t...) | (case ty t t...)\nThese are identical to their untyped cousins, except that they include a type annotation for the type of the whole term. These make the typing rules much simpler, as otherwise we lack enough information to pin down the whole type.\nThe typing rules for the new terms are:\nThe reduction rules are essentially the same since the types do not affect evaluation.\nPerformance testing of the various prototypes revealed the following interesting facts:\nScott encoding is surprisingly performant. It's tricky to say why sums-of-products is so much better. Binding case variables makes a modest difference, but we can achieve the same in other ways.\nScott encoding is surprisingly performant.\nIt's tricky to say why sums-of-products is so much better.\nBinding case variables makes a modest difference, but we can achieve the same in other ways.\nLet's look at these in turn.\nTo see why Scott encoding is so good, let's look at what happens when we 1) construct, and 2) destruct a typical datatype value, using sums-of-products but without binding case variables. We'll do this in the typed setting for clarity.\nLet s consider four programs, corresponding to creation and destruction of data XYZ = MkXY X Y | MkZ Z.\ndata XYZ = MkXY X Y | MkZ Z\nP1: Scott-encoded construction\n[ -- Scott-encoded constructor for MkXY (\\(x : X) (y : Y) . (/\\ R . \\(xyc : X -> Y -> R) (zc : Z -> R) . xyc x y) xx yy ]\n[ -- Scott-encoded constructor for MkXY (\\(x : X) (y : Y) . (/\\ R . \\(xyc : X -> Y -> R) (zc : Z -> R) . xyc x y) xx yy ]\nP2: Scott-encoded destruction\n[ { P1 r } -- case alternatives (\\(x : X) (y : Y) -> b1) (\\(z : Z) -> b2 ]\n[ { P1 r } -- case alternatives (\\(x : X) (y : Y) -> b1) (\\(z : Z) -> b2 ]\nP3: Explicit construction\nconstr 0 xx yy\nconstr 0 xx yy\nP4: Explicit matching\ncase P3 (\\(x : X) (y : Y) . b1) (\\(z : Z) . b2)\ncase P3 (\\(x : X) (y : Y) . b1) (\\(z : Z) . b2)\nP1 vs P3\nP1 Evaluate the function, evaluate the argument, apply the argument (x2 for two arguments) Return a closure containing the two arguments in the environment\nEvaluate the function, evaluate the argument, apply the argument (x2 for two arguments)\nReturn a closure containing the two arguments in the environment\nP3 Allocate an array Evaluate the argument and put it into the array (x2 for two arguments) Return the value containing the tag and the array\nAllocate an array\nEvaluate the argument and put it into the array (x2 for two arguments)\nReturn the value containing the tag and the array\nP2 vs P4\nP2 Evaluate the scrutinee Force the scrutinee (type instantiation) Evaluate the resulting function, evaluate the argument, apply the argument (x2 for two branch arguments) Evaluate the branch function, evaluate the argument, apply the argument (x2 for two constructor arguments) Enter the branch body\nEvaluate the scrutinee\nForce the scrutinee (type instantiation)\nEvaluate the resulting function, evaluate the argument, apply the argument (x2 for two branch arguments)\nEvaluate the branch function, evaluate the argument, apply the argument (x2 for two constructor arguments)\nEnter the branch body\nP4 Evaluate the scrutinee Look at the tag Evaluate the case branch (x2 for two branch arguments) Apply the branch function to the constructor arguments (x2 for two constructor arguments) Enter the branch body\nEvaluate the scrutinee\nLook at the tag\nEvaluate the case branch (x2 for two branch arguments)\nApply the branch function to the constructor arguments (x2 for two constructor arguments)\nEnter the branch body\nScott encoding isn't doing anything clearly unnecessary: it's quite efficient at constructing values (because it just returns a function closure right away), and it's quite efficient at deconstructing values (because it just loads the constructor arguments into an environment and then applies the branch).\nIn particular, both ways we have to do a similar amount of work in a) evaluating the branch function, b) applying it to its arguments.\nThere are a few advantages for sums-of-products. The most important is that it does not evaluate all the case branches, only the one it needs. Whereas Scott-encoding passes the case branches as simple function arguments, so they are always evaluated before we proceed.\nThis makes a surprising amount of difference. We are performing so few steps already for each datatype match that adding a few more to evaluate unused case branches makes a difference.\nBinding case variables allows us to get rid of some of the work identified in the previous sections. A constr that binds case variables has two differences:\nconstr\nThe body of the case branch is known, rather than potentially being an arbitrary lambda that we will need to resolve by doing evaluation. The constructor arguments can be loaded into the environment all in one go, rather than taking requiring multiple steps through the evaluator for each evaluation.\nThe body of the case branch is known, rather than potentially being an arbitrary lambda that we will need to resolve by doing evaluation.\nThe constructor arguments can be loaded into the environment all in one go, rather than taking requiring multiple steps through the evaluator for each evaluation.\nIn practice it seems that difference 1 makes a significant amount of difference, because even if the case branch is a literal lambda, we are forced to allocate a value for the lambda. Optimizing the evaluator to avoid this allocation removes a significant part of the advantage.\nDifference 2 does not seem to make as large a difference. If we did see a big difference here then we might want to investigate adding multi-lambdas to Plutus Core in order to gain this benefit in other places. In practice, however, it does not seem to be that significant, and prototypes of multi-lambdas have not performed well.\nThroughout, the following commits are referenced:\nmaster: df9b23f59852d11776fde382720df830c6163238\nmaster\ndf9b23f59852d11776fde382720df830c6163238\nsums-of-products: e98b284204070053b2e64bb66c7aa0832520afec\nsums-of-products\ne98b284204070053b2e64bb66c7aa0832520afec\nThese represent somewhat arbitrary snapshots. The sums-of-products branch represents the current prototype, and master is it's merge-base with plutus's master branch. These may be updated at a future date.\nsums-of-products\nmaster\nplutus\nmaster\nThese are benchmarks taken from the nofib benchmark suite used by GHC. They are defined in plutus-benchmark/nofib. They are not totally comprehensive, but they represent a reasonable survey of programs that do various kinds of general computation.\nnofib\nplutus-benchmark/nofib\nThese benchmarks are re-compiled from Haskell each time, so the comparison does not represent faster evaluation of the same script, but rather than we can now compile datatype operations using the new terms, which are faster overall.\nmaster\nsums-of-products\nThe results indicate that the speedup is more associated with programs that do lots of datatype manipulation, rather than those that do a lot of numerical work (which in Plutus Core means calling lots of builtin functions). However, we don't see any regression even in the primality tester, which is very numerically heavy (the noise threshold for the benchmarks is ~1 ).\nThese benchmarks are some real-world examples taken from plutus-use-cases. They are defined in plutus-benchmark/validation. They thus represent real-world workloads.\nplutus-use-cases\nplutus-benchmark/validation\nThe validation benchmarks are not recompiled, they are specific saved Plutus Core programs. These benchmarks thus only show changes in the performance of the Plutus Core evaluator itself.\nmaster\nsums-of-products\nThis is an average slowdown of 4 , which is not good at all. We do not want to have a negative impact on scripts that don't use the new constructs.\nHowever, this slowdown is very difficult to avoid. The GHC Core (GHC's intermediate language for Haskell programs) for both versions looks nearly identical, with the only differences being the introduction of new code for the new cases. We believe that this indicates that GHC simply produces slightly slower code when we have more constructs, even if those code paths are not used. In particular, there are some threshold effects when you cross certain numbers of constructors.\nWe tested this by doing an experiment that simply adds new unused constructors to the Plutus Core term type and evaluator frame type. This caused a slowdown of on average 2 . That's not enough to completely explain the loss, but we suspect that similar causes account for the rest (investigation is ongoing).\nThis is still bad -- a slowdown is still a slowdown -- but it's less bad because it's unavoidable if we ever want to increase the size of the language. We will pay this cost whenever we decide expand the language. Especially if the cost comes from threshold effects, it may be a one-time cost that we just have to pay on some occasion.\nThis benchmark compares lists implemented three ways: using SOPs, using builtin lists, and using Data. It is defined in plutus-benchmark/lists. The benchmark task is summing a list of 100 integers. All three versions are using the Plutus Tx compiler, so any overhead is identical, and the only difference is how the list operations are implemented in the end. There almost certainly is a decent amount of overhead (we did not attempt to measure it here), so the proportional difference in the underlying operations may in fact be greater.\nData\nplutus-benchmark/lists\nData\nUsing Data is much worse. This is not terribly surprising: pattern-matching on a datatype encoded using Data requires multiple builtin calls:\nData\nData\nA call to ChooseData to identify the type of Data\nChooseData\nData\nA call to UnConstrData to get the tag and arguments as a builtin pair\nUnConstrData\nA call to Fst to get the tag\nFst\nSome number of calls to builtin operations on integers to work out which branch to take given the tag\nA call to Snd to get the args\nSnd\nSome number of calls to Head/Tail to extract the arguments to be used\nHead\nTail\nOn the other hand, for SOPs this is a single machine step.\nThis CIP is licensed under CC-BY-4.0.\nSee Appendix 1 for changes to Typed Plutus Core. See 'Why is case not strict?' for more discussion. See \"Can't we just implement datatypes using Data itself?\" for comparison with using Data directly. The Aiken language does this. See Appendix 2 for more details.\nSee Appendix 1 for changes to Typed Plutus Core.\nSee Appendix 1 for changes to Typed Plutus Core.\nSee 'Why is case not strict?' for more discussion.\nSee 'Why is case not strict?' for more discussion.\nSee \"Can't we just implement datatypes using Data itself?\" for comparison with using Data directly.\nSee \"Can't we just implement datatypes using Data itself?\" for comparison with using Data directly.\nData\nData\nThe Aiken language does this.\nThe Aiken language does this.\nSee Appendix 2 for more details.\nSee Appendix 2 for more details.\n2023 Cardano Foundation\n\n---\n\nCIP-0086 | NFT Metadata Update Oracles\n\nThis proposal extends the CIP-25 standard for defining and updating token metadata via transaction metadata, by providing a new mechanism to update token metadata without having to mint or burn tokens, while maintaining full backward compatibility with CIP-25. The new mechanism is capable of expressing metadata updates more efficiently than CIP-25 updates.\nOn Cardano s eUTxO ledger, native tokens exist without any inherently attached metadata. The ledger does not provide a direct method for preserving any information associated with an asset class of native tokens, as transactions move the tokens from one UTxO to another.\nThe Media NFT Metadata Standard (CIP-25) proposed an indirect way to attach metadata to an asset class using the metadata field (CBOR tag 721) of its minting transactions. When there are multiple minting transactions for the same asset class, the latest minting transaction s metadata overrides all previous metadata defined for that asset class.\nCIP-25 is widely supported across the Cardano community by blockchain indexers, wallet providers, marketplace applications, and other stakeholders. It has served the community quite well so far, in particular for non-fungible tokens (NFTs) with static metadata (i.e. intended to be mostly immutable after minting).\nHowever, the CIP-25 metadata update mechanism is suboptimal because it requires tokens of the asset class to be minted or burned whenever its metadata is updated. This is incongruous with the often purely-informational purpose of metadata update transactions. While it may sometimes be convenient to combine token minting and metadata updates within the same transaction (e.g. to save on transaction fees), there is a broad range of applications where metadata updates need to be more frequent and independent from minting events.\nIn practice, there are three main drawbacks to the CIP-25 update mechanism:\nThe metadata authority for an asset class must retain the ability to mint/burn more tokens of the asset class if it wants to retain the ability to post metadata updates/corrections. This compromises the token issuers guarantees to token holders that the token supply (or NFT collections) will not be diluted in the future. Token holders are being asked to trust that the metadata authority will not abuse its power to mint new tokens of the same asset class. Executing a minting policy script to mint or burn tokens every time that an asset class metadata is updated incurs script execution fees, particularly if the script is Plutus-based, far exceeding the fees corresponding to the actual informational content of the metadata update. This cost can become severe for large collections of thousands of NFTs, and it may be prohibitive for implementing any dynamic fields for such large NFT collections. An asset class metadata can only be updated as a whole. This is inefficient (both in terms of transaction fees and ledger bloat) when only a small subset of the metadata fields needs to be updated for a large collection of NFTs, and it can lead to errors if the unmodified fields are improperly copied over to the update.\nThe metadata authority for an asset class must retain the ability to mint/burn more tokens of the asset class if it wants to retain the ability to post metadata updates/corrections. This compromises the token issuers guarantees to token holders that the token supply (or NFT collections) will not be diluted in the future. Token holders are being asked to trust that the metadata authority will not abuse its power to mint new tokens of the same asset class.\nExecuting a minting policy script to mint or burn tokens every time that an asset class metadata is updated incurs script execution fees, particularly if the script is Plutus-based, far exceeding the fees corresponding to the actual informational content of the metadata update. This cost can become severe for large collections of thousands of NFTs, and it may be prohibitive for implementing any dynamic fields for such large NFT collections.\nAn asset class metadata can only be updated as a whole. This is inefficient (both in terms of transaction fees and ledger bloat) when only a small subset of the metadata fields needs to be updated for a large collection of NFTs, and it can lead to errors if the unmodified fields are improperly copied over to the update.\nWe encountered these drawbacks during the development of an NFT gaming application, where thousands of NFTs correspond to game assets with properties (e.g. horse age, physical and mental abilities) that evolve both over time and as the NFTs participate in various game-related events on-chain. These properties do not need to interact with smart contract logic directly, which makes transaction metadata a more appropriate place to define them, rather than UTxO datums with complicated state-evolution logic.\nOur NFT gaming application is leveraging the Cardano immutable ledger, rather than an off-chain database, as the single source of truth to track the evolution of the game assets most important properties. This reassures users that the history of metadata states for a game NFT cannot be retroactively erased or altered by the game admins although the game admins have the authority to modify metadata going forward, they are retroactively accountable for all past updates.\nWe expect that this proposal will be useful to many other applications that need to track dynamic on-chain metadata for large NFT collections, particularly in the growing NFT gaming sector. It may also be useful for artists to update on-chain metadata about their art NFTs (e.g. royalty payment receiving address) without having to burn or mint anything.\nOur proposal extends CIP-25 with a new update mechanism:\nA metadata oracle can be assigned for a given policy ID.\nFor a policy with a metadata oracle assigned, the metadata oracle can post CIP-86 updates to add, remove, or modify any fields of the token s CIP-25 metadata, without needing to mint or burn any tokens in the metadata update transactions.\nThe current metadata for a token can be deterministically reconstructed by starting from the latest CIP-25 update to the token, and applying the subsequent CIP-86 metadata updates in ascending blockchain transaction order.\nBoth CIP-25 and CIP-86 updates affect the token metadata:\nA CIP-25 update removes all fields in the current metadata state and replaces them with a new complete definition of the metadata state.\nA CIP-86 update selectively adds, removes, or modifies fields relative to the current metadata state. It does not apply to any asset class that has not had tokens previously minted or any asset class that has not had CIP-25 metadata previously defined, before the CIP-86 update.\nHowever, we recommend that CIP-86 adopters use the new metadata update mechanism exclusively to manage all updates to their token metadata after the initial CIP-25 metadata is set.\nOur proposal only affects the 86 top-level CBOR tag of the metadata field in Cardano transactions. (Note: we re using 86 as a stand-in for the CIP number that will eventually be assigned to this proposal.)\n86\n86\nA token metadata oracle is defined via two addresses:\nAn oracle update address that is authorized to post metadata updates. For example, this address could be controlled by a bot that automatically posts token metadata updates as needed to the blockchain.\nAn oracle main address that is used to update the oracle update address. This address should be controlled via private keys held in cold storage or hardware wallets.\nA metadata oracle can be explicitly assigned to a policy ID by setting the following metadata in a transaction that mints or burns tokens for that policy:\n{ \"86\": { \"assign_metadata_oracle\": { \"<policyId>\": { \"main_address\": \"<Shelley_address>\", \"update_address\": \"<Shelley_address>\" } } } }\n{ \"86\": { \"assign_metadata_oracle\": { \"<policyId>\": { \"main_address\": \"<Shelley_address>\", \"update_address\": \"<Shelley_address>\" } } } }\nWhile a metadata oracle is not explicitly assigned to a native-script-based policy ID, the policy ID is implicitly assigned a metadata oracle with both addresses set to an address derived from the native script. Specifically, they are both set to an Enterprise address constructed by setting the payment key to the verification key from the native script and keeping the staking key empty.\nThe metadata oracle assignment for a policy ID can be updated via a transaction signed by the oracle main address key. The oracle assignment transaction must be signed by the signing key of the main oracle address, but it may contain any inputs and outputs, as these are ignored for the purposes of metadata oracle assignment.\nThe schema for updating an oracle assignment is the same as for the initial assignment in the minting transaction:\n{ \"86\": { \"assign_metadata_oracle\": { \"<policyId>\": { \"main_address\": \"<Shelley_address>\", \"update_address\": \"<Shelley_address>\" } } } }\n{ \"86\": { \"assign_metadata_oracle\": { \"<policyId>\": { \"main_address\": \"<Shelley_address>\", \"update_address\": \"<Shelley_address>\" } } } }\nThe main_address or the update_address fields for a policyID can be omitted, in which case the addresses for the omitted fields remain the same for that policy ID.\nmain_address\nupdate_address\n<policyID>\nIf a metadata oracle was implicitly assigned to a policy ID before the assignment update, then the implicit assignment is replaced by the new explicit assignment.\nA simple metadata update transaction must be signed by the signing key of the oracle update address, but it may contain any inputs and outputs, as these are ignored for the purposes of updating token metadata.\nThe schema for simple metadata updates in CIP-86 is similar to the CIP-25 schema, but it is nested under 86.simple_metadata_update in the transaction metadata object.\n86.simple_metadata_update\n{ \"86\": { \"simple_metadata_update\": { \"<policyId>\": { \"<tokenName>\": { \"<metadataField>\": \"<metadataValue>\" } } } } }\n{ \"86\": { \"simple_metadata_update\": { \"<policyId>\": { \"<tokenName>\": { \"<metadataField>\": \"<metadataValue>\" } } } } }\nTo remove a metadata field, set its value explicitly to null in the metadata update.\nnull\nThe schema for regex metadata updates is as follows:\n{ \"86\": { \"regex_metadata_update\": { \"<policyId>\": { \"<tokenNameRegex>\": { \"<metadataField>\": \"<metadataValue>\" } } } } }\n{ \"86\": { \"regex_metadata_update\": { \"<policyId>\": { \"<tokenNameRegex>\": { \"<metadataField>\": \"<metadataValue>\" } } } } }\nA regex metadata update transaction must be signed by the signing key of the oracle update address, but it may contain any inputs and outputs, as these are ignored for the purposes of updating token metadata.\nThe only difference from the simple metadata update is that here the token names are defined in terms of PCRE regular expressions (regex). The regex metadata update applies to any previously minted token whose policy ID matches policyID and whose token name matches the tokenNameRegex regular expression.\n<policyID>\n<tokenNameRegex>\nFor example, the following metadata update would apply to every Equine pioneer horse between EquinePioneerHorse05000 and EquinePioneerHorse05999:\nEquinePioneerHorse05000\nEquinePioneerHorse05999\n{ \"86\": { \"regex_metadata_update\": { \"30ed3d95db1d6bb2c12fc5228a2986eab4553f192a12a4607780e15b\": { \"^EquinePioneerHorse05\\\\d{3}$\": { \"age\": 2 } } } } }\n{ \"86\": { \"regex_metadata_update\": { \"30ed3d95db1d6bb2c12fc5228a2986eab4553f192a12a4607780e15b\": { \"^EquinePioneerHorse05\\\\d{3}$\": { \"age\": 2 } } } } }\nThe regular expression pattern in tokenNameRegex is defined according to the grammar in:\n<tokenNameRegex>\nEuropean Computer Manufacturers Association, \"ECMAScript Language Specification 5.1 Edition\", ECMA Standard ECMA-262, June 2011. Section 15.10. https://www.ecma-international.org/wp-content/uploads/ECMA-262_5.1_edition_june_2011.pdf\nTabular metadata updates use a condensed rectangular format to specify new values for a fixed set of fields for a large number of assets. Specifically, for each policy ID we provide an object with the following three fields:\nfield_paths contains an array of paths pointing to possibly-nested fields within a token metadata object. Each of these field paths is a dot-separated list of field names (e.g. \"images.background.sunset.url\") that lead from the top of the metadata object (for asset classes of the policy) into a targeted field within that object.\nfield_paths\n\"images.background.sunset.url\"\ntoken_names contains an array of token names.\ntoken_names\nvalues contains a table of values, represented by an array of arrays. For each token name in token_names, the outer array in values contains one element (an inner array) of metadata values to which the fields targeted by field_paths should be updated for that token name under the policy ID. The outer array of values must be equal in length to token_names and each inner array of values must be equal in length to field_paths.\nvalues\ntoken_names\nvalues\nfield_paths\nvalues\ntoken_names\nvalues\nfield_paths\n{ \"86\": { \"tabular_metadata_update\": { \"<policyId>\": { \"field_paths\": [ \"<fieldPath>\" ], \"token_names\": [ \"<tokenName>\" ], \"values\": [ [\"<metadataValue>\"] ] } } } }\n{ \"86\": { \"tabular_metadata_update\": { \"<policyId>\": { \"field_paths\": [ \"<fieldPath>\" ], \"token_names\": [ \"<tokenName>\" ], \"values\": [ [\"<metadataValue>\"] ] } } } }\nA tabular metadata update transaction must be signed by the signing key of the oracle update address, but it may contain any inputs and outputs, as these are ignored for the purposes of updating token metadata.\nFor example, the following update would apply updates to six metadata fields of five Equine horse NFTs:\n{ \"86\": { \"tabular_metadata_update\": { \"<policyId>\": { \"field_paths\": [ \"age\", \"stats.acceleration\", \"stats.agility\", \"stats.endurance\", \"stats.speed\", \"stats.stamina\" ], \"token_names\": [ \"EquinePioneerHorse00000\", \"EquinePioneerHorse00012\", \"EquinePioneerHorse00315\", \"EquinePioneerHorse01040\", \"EquinePioneerHorse09175\" ], \"values\": [ [3,34,16,18,51,33], [2,24,48,12,32,18], [3,33,34,41,14,31], [4,19,22,21,21,50], [1,24,11,36,22,14] ] } } } }\n{ \"86\": { \"tabular_metadata_update\": { \"<policyId>\": { \"field_paths\": [ \"age\", \"stats.acceleration\", \"stats.agility\", \"stats.endurance\", \"stats.speed\", \"stats.stamina\" ], \"token_names\": [ \"EquinePioneerHorse00000\", \"EquinePioneerHorse00012\", \"EquinePioneerHorse00315\", \"EquinePioneerHorse01040\", \"EquinePioneerHorse09175\" ], \"values\": [ [3,34,16,18,51,33], [2,24,48,12,32,18], [3,33,34,41,14,31], [4,19,22,21,21,50], [1,24,11,36,22,14] ] } } } }\nIt is equivalent to the following simple metadata update:\n{ \"86\": { \"simple_metadata_update\": { \"<policyId>\": { \"EquinePioneerHorse00000\": { \"age\": 3, \"stats\": { \"acceleration\": 34, \"agility\": 16, \"endurance\": 18, \"speed\": 51, \"stamina\": 33 } }, \"EquinePioneerHorse00012\": { \"age\": 2, \"stats\": { \"acceleration\": 24, \"agility\": 48, \"endurance\": 12, \"speed\": 32, \"stamina\": 18 } }, \"EquinePioneerHorse00315\": { \"age\": 3, \"stats\": { \"acceleration\": 33, \"agility\": 34, \"endurance\": 41, \"speed\": 14, \"stamina\": 31 } }, \"EquinePioneerHorse01040\": { \"age\": 4, \"stats\": { \"acceleration\": 19, \"agility\": 22, \"endurance\": 21, \"speed\": 21, \"stamina\": 50 } }, \"EquinePioneerHorse09175\": { \"age\": 1, \"stats\": { \"acceleration\": 24, \"agility\": 11, \"endurance\": 36, \"speed\": 22, \"stamina\": 14 } } } } } }\n{ \"86\": { \"simple_metadata_update\": { \"<policyId>\": { \"EquinePioneerHorse00000\": { \"age\": 3, \"stats\": { \"acceleration\": 34, \"agility\": 16, \"endurance\": 18, \"speed\": 51, \"stamina\": 33 } }, \"EquinePioneerHorse00012\": { \"age\": 2, \"stats\": { \"acceleration\": 24, \"agility\": 48, \"endurance\": 12, \"speed\": 32, \"stamina\": 18 } }, \"EquinePioneerHorse00315\": { \"age\": 3, \"stats\": { \"acceleration\": 33, \"agility\": 34, \"endurance\": 41, \"speed\": 14, \"stamina\": 31 } }, \"EquinePioneerHorse01040\": { \"age\": 4, \"stats\": { \"acceleration\": 19, \"agility\": 22, \"endurance\": 21, \"speed\": 21, \"stamina\": 50 } }, \"EquinePioneerHorse09175\": { \"age\": 1, \"stats\": { \"acceleration\": 24, \"agility\": 11, \"endurance\": 36, \"speed\": 22, \"stamina\": 14 } } } } } }\nLike CIP-25, CIP-86 supports two different methods of representing policy IDs and token name:\nIn version 1, policy IDs and token names must be expressed as text (see cddl/version_1.cddl).\nIn version 2, policy IDs and token names must be expressed as raw bytes (see cddl/version_2.cddl).\nBy default, all CIP-86 metadata updates use version 1. However, version 2 can be used if the version field of the object under the top-level \"86\" CBOR tag is set to 2.\nversion\n\"86\"\n2\nFor example:\n{ \"86\": { \"version\": 2, \"simple_metadata_update\": { \"<policyIdRawBytes>\": { \"<tokenNameRawBytes>\": { \"<metadataField>\": \"<metadataValue>\" } } } } }\n{ \"86\": { \"version\": 2, \"simple_metadata_update\": { \"<policyIdRawBytes>\": { \"<tokenNameRawBytes>\": { \"<metadataField>\": \"<metadataValue>\" } } } } }\nRegex updates are disallowed in version 2, because it is unclear how to apply regular expressions to non-UTF-8 bytestrings (or their corresponding hex encodings).\nUp to network consensus, the Cardano blockchain imposes a total ordering on transactions added to the chain each transaction can be indexed by the slot number of the block that added it to the chain, and the transaction s index within that block. Network nodes may disagree about which blocks have been added most recently to the blockchain; however, the disagreement about whether a particular transaction was added at a particular position in the total order decreases exponentially as more and more blocks are added to the chain.\nTo reconstruct the metadata state for a given asset class, scan through the sequence of transactions in a Cardano node s blockchain, applying the CIP-25 and CIP-86 updates in the order that they are encountered in this sequence. Should a transaction contain both CIP-25 and CIP-86 updates, then the CIP-25 updates should be applied first, followed by the CIP-86 updates. If a transaction contains both a CIP-25 update and a CIP-86 explicit oracle assignment, then the CIP-25 update will be applied as usual and the oracle addresses (main and/or update) will be set to those of the explicit CIP-86 oracle assignment. If the Cardano node rolls back some blocks from the chain tip, then roll back the updates from those blocks as well.\nWe recommend that token metadata oracle operators wait for their metadata update transactions to be confirmed at a sufficient block depth before submitting any subsequent metadata updates for the same asset classes. Doing so should minimize any confusion about the order of simultaneous pending metadata updates while the Cardano network settles toward consensus.\nA transaction can contain CIP-86 token metadata updates of different types, plus oracle assignment updates. In this case, apply the updates in the following sequence:\nApply the CIP-86 regex update. Apply the CIP-86 tabular update. Apply the CIP-86 simple update. Apply the CIP-86 oracle assignment update.\nApply the CIP-86 regex update.\nApply the CIP-86 tabular update.\nApply the CIP-86 simple update.\nApply the CIP-86 oracle assignment update.\nWe recommend that token metadata oracle operators not mix multiple update types in the same transaction, unless they have a clear understanding of the outcome of applying the updates in the above sequence.\nThe CIP-86 token metadata indexer begins with a configuration of the policy IDs for which it will be tracking metadata. Optionally, it can be configured to track metadata for all tokens on Cardano.\nThe indexer monitors the blockchain for minting transactions. If such a minting transaction mints tokens of a tracked policy ID and contains an explicit oracle assignment and/or CIP-25 metadata for that policy ID, then the indexer caches that assignment and metadata in its database. If the transaction does not contain an explicit oracle assignment for the policy ID, and there is no prior oracle assignment, then the indexer caches the implicit oracle assignment for the policy ID in its database.\nThe indexer continues to monitor minting transactions for the policy IDs that it s tracking, applying oracle assignment updates and CIP-25 metadata updates accordingly in its database. A CIP-25 metadata update is applied as a wholesale replacement of the metadata cached in the indexer database for the respective asset classes.\nFor each oracle main address currently assigned to a policy ID, the indexer monitors the blockchain for transactions that contain oracle assignment updates in their metadata and are signed by the signing key of the main address. The indexer updates its database to reflect these explicit oracle assignments and removes any implicit assignments that were replaced by explicit assignments.\nFor each oracle update address currently assigned to a policy ID, the indexer monitors the blockchain for transactions that contain CIP-86 token metadata updates and are signed by the signing key of the update address. The indexer applies these metadata updates in the order defined in Order of application for updates. CIP-86 metadata updates are applied to the asset classes and metadata fields that they target, while keeping all other fields the same.\nTo be able to handle blockchain rollbacks, the indexer keeps track of past metadata states for its policy IDs, going back 2160 blocks (~12 hours) from the current blockchain tip. Cardano s securityParam Shelley Genesis parameter prevents nodes from rolling back more than 2160 blocks. If the Cardano node informs the indexer of a rollback, the indexer restores the past metadata state that existed immediately before all the metadata updates in the rolled-back blocks were applied.\nsecurityParam\nFor compatibility with existing applications that are already relying on CIP-25 metadata indexers, the CIP-86 indexer provides a similar API so that those applications can get and display the current CIP-86 token metadata in the same way that they have been for CIP-25 metadata. The indexer indicates that it is following the CIP-86 standard.\nWe pursued the following design goals in our solution:\nMaintain backward compatibility with CIP-25 tokens that only use CIP-25 updates should have token metadata displayed identically by CIP-25 indexers and CIP-86 indexers. Ensure that the current token metadata can be reconstructed by only looking at the blockchain, without accessing any external resources. Allow token metadata to be updated by designated authorities without minting or burning tokens. Allow existing token issuers to opt into and out of the CIP-86 update mechanism without having to remint all of their existing tokens. Allow designated authorities to securely rotate any keys that they use in their automated and networked processes (e.g. oracle update address used by a bot to update token metadata). Allow token metadata to be updated more efficiently than by a wholesale replacement of the entire metadata object for an asset class. Minimize the time and resource usage required for an indexer to apply CIP-25 and CIP-86 updates for an asset class and then serve the resulting token metadata to applications. Gracefully handle blockchain rollbacks that modify the sequence of CIP-25 and CIP-86 metadata updates for an asset class.\nMaintain backward compatibility with CIP-25 tokens that only use CIP-25 updates should have token metadata displayed identically by CIP-25 indexers and CIP-86 indexers.\nEnsure that the current token metadata can be reconstructed by only looking at the blockchain, without accessing any external resources.\nAllow token metadata to be updated by designated authorities without minting or burning tokens.\nAllow existing token issuers to opt into and out of the CIP-86 update mechanism without having to remint all of their existing tokens.\nAllow designated authorities to securely rotate any keys that they use in their automated and networked processes (e.g. oracle update address used by a bot to update token metadata).\nAllow token metadata to be updated more efficiently than by a wholesale replacement of the entire metadata object for an asset class.\nMinimize the time and resource usage required for an indexer to apply CIP-25 and CIP-86 updates for an asset class and then serve the resulting token metadata to applications.\nGracefully handle blockchain rollbacks that modify the sequence of CIP-25 and CIP-86 metadata updates for an asset class.\nWe maintain full backward compatibility with CIP-25:\nCIP-25 updates are respected and applied in the same way as before by the CIP-86 indexer.\nCIP-86 updates are namespaced under a different top-level CBOR tag than CIP-25, in order to prevent any clashes between field names, policy IDs, and token names.\nThe CIP-86 indexer provides the accumulated CIP-25 and CIP-86 metadata via the same API as the CIP-25 indexer.\nWe recognize CIP-86 updates only if they are issued by an oracle currently assigned for the corresponding policy IDs. This assignment is either directly authorized by the token issuer (explicitly or implicitly) or else indirectly authorized by the token issuer via the delegated authority that the token issuer placed in the originally assigned oracle to transfer the assignment to other oracles. Therefore, all authority to post valid CIP-86 updates about a token originates from the token issuer.\nAn alternative design could have oracles that declare themselves as sources of metadata for tokens, without authorization from anyone, and then users could voluntarily subscribe to the metadata oracles that they wish to follow. However, such an approach would make it very difficult for the Cardano ecosystem to converge on a single objective metadata state for a token, as each user would have his own subjective view of token metadata based on his oracles subscriptions. This alternative approach could be interesting to allow secondary/auxiliary metadata to be defined for tokens, but it is unsuitable for the primary metadata that CIP-25 and CIP-86 seek to manage.\nWe use two addresses to identify an oracle when assigning it to a policy ID. It would be simpler to use a single oracle address, but we chose to separate the main address (authorized to reassign the oracle) from the update address (authorized to post updates) for an oracle. We did this to mitigate the risk of using the update address in an automated process on a network machine, and to allow the update address to be safely rotated via a transaction that can only be signed on a cold storage or hardware wallet device using the main address.\nInstead of using addresses to identify oracles, we could have identified oracles by minting policies (not the minting policies to which oracles are assigned). In this alternative design, a minting policy X could have an assigned oracle identified by minting policy A. Under such an assignment, a CIP-86 update for a token under X would be valid if the update transaction consumed a utxo that contained a token of minting policy A. In other words, the holder of an A token would be allowed to post metadata updates about any X tokens.\nX\nA\nX\nA\nA\nX\nThis minting-policy-based alternative for identifying oracles may be more advantageous for more flexibly managing oracle authorization (including rate-limiting, time-boxing, etc.) and proving data provenance to on-chain scripts. These advantages are relevant for oracles that provide information in utxo datums meant for use in smart contracts, but they are not as relevant for this CIP, where we seek to provide a method to provide updateable token metadata via transaction metadata (as a direct extension of CIP-25). Furthermore, it is easier to track transactions originating from a given address in an indexer than to keep track of all the people who control various authorization tokens, at a given time.\nIn the first draft of this proposal, we prohibited CIP-86 oracle assignment updates and metadata updates from occurring in transactions that mint tokens, in order to avoid awkward clashes with CIP-25 metadata transactions. This was removed because, while it likely does not make sense to create CIP-25 and CIP-86 metadata for the same token in one transaction, it could feasibly make sense to want to update the metadata for other tokens while minting another one.\nWe also originally required CIP-86 updates to occur in transactions that only send ADA from an oracle address (main or update, as appropriate), to prevent unforeseen interactions with other mechanisms that may have negative consequences. This requirement was removed for two reasons. First, the reasoning above does not establish any specific issues with other transaction types. Second, it is too restrictive and creates unnecessary additional transactions for long sequences of updates, causing the update issuer to spend unnecessary transaction fees. Therefore, we decided to remove these unnecessary restrictions on CIP-86 update transactions.\nThe implicit method of assigning a metadata oracle is needed to allow existing token issuers to opt into the CIP-86 update mechanism. Their minting policies may no longer allow any more token minting or burning, which would prevent the token issuers from being able to explicitly assign an oracle via a CIP-25 update for those policies. The implicit assignment method bootstraps the CIP-86 update mechanism for these policies.\nThe implicit address is of the Enterprise address type, to avoid having to deal with staking keys. If needed, the metadata oracle operator can send ADA to the Enterprise address, and then spend it if the operator still controls the payment key of that Enterprise address.\nOpting out of the CIP-86 update mechanism can be done by explicitly assigning an oracle with addresses from which ADA cannot be spent (e.g. Plutus AlwaysFails). If the minting policy does not allow any more minting or burning, then this is an irreversible opt-out.\nWhen managing large collections of thousands of NFTs, one often needs to set a given field to the same value for many NFTs. Doing this individually for each NFT via CIP-25 updates or CIP-86 simple updates is inefficient, so we have proposed the regex metadata update as a succinct way to specify a mapping from multiple token names to a single metadata update.\nAnother common use case for dynamic token metadata involves having a set of volatile fields that should receive relatively frequent updates, but where those updates should be different for each NFT. Labeling each field value update with its field name for each NFT is very verbose, especially if the field is deeply nested within the metadata schema for the NFT. For this use case, we have proposed the tabular metadata update format as a way to avoid this repetition field names/paths are defined once in the column names of a rectangular table and applied consistently for each row of updated metadata field values.\nRectangular tables are a standard format used in the data analytics field for these situations.\nThis proposal may be considered active if:\nThe solution meets the design goals listed in the Rationale section to a satisfactory degree. The indexer and simple tools to construct CIP-86 update transactions (as described in the Specification section) are fully implemented and provided in an open-source (Apache 2.0 licensed) repository with sufficient documentation. The CIP-86 metadata format, indexer, and/or indexer API are used by several stakeholders in the Cardano ecosystem, including dApps, blockchain explorers, analytics platforms, etc.\nThe solution meets the design goals listed in the Rationale section to a satisfactory degree.\nThe indexer and simple tools to construct CIP-86 update transactions (as described in the Specification section) are fully implemented and provided in an open-source (Apache 2.0 licensed) repository with sufficient documentation.\nThe CIP-86 metadata format, indexer, and/or indexer API are used by several stakeholders in the Cardano ecosystem, including dApps, blockchain explorers, analytics platforms, etc.\nEquine and MLabs are collaborating on developing the indexer described in this CIP and the Equine NFT gaming application will be using CIP-86 updates to manage metadata updates for its large collection of thousands of NFTs under multiple minting policies.\nWe will include detailed documentation, example configurations, and tutorials on how to adapt the tools to new projects.\nWe are actively engaged in discussions with other stakeholders in the Cardano ecosystem that are interested in adopting this CIP to their projects, platforms, and tools.\nThis CIP is licensed under the Creative Commons Attribution 4.0 International Public License (CC-BY-4.0).\n2023 Cardano Foundation\n\n---\n\nCIP-0088 | Token Policy Registration\n\nCurrently, token projects (NFT or FT) have no mechanism to register the intent, details, or feature set of their minting policy on-chain. This CIP will aim to create a method that is backwards compatible and enable projects to declare, and update over time, the supported feature set and metadata details pertaining to their tokens.\nThis CIP will aim to make use of a hybrid (on and off-chain) information schema to enable maximum flexibility and adaptability as new and novel use cases for native assets expand and grow over time.\nThis CIP was borne out of a distaste for the lack of on-chain token policy intent registration that has been cited as both a centralization and security concern at various points over the preceding two years of native asset history on Cardano.\nExample 1: The Cardano Token Registry\nMany Fungible Token (FT) projects require special treatment of their native assets such as decimal places for proper display and formatting, project information, and token logo. As it stands, these projects must currently register via a GitHub repository (Cardano Token Registry) in order to have their token properly appear in wallets. This GitHub repository is currently managed by the Cardano Foundation (CF). While there is no reason to believe that the CF would take any malicious or nefarious action, forcing token projects to register in this fashion introduces a point of centralization, potential gate keeping, and ultimately reliance on the interaction of a 3rd party (the human at CF responsible for merging pull requests) in order for projects to register, or update, their information.\nNote: The original intent of the CIP-26 Cardano Token Registry was never to have a sole provider or controller of the repository. However, time has shown that there is little interest from the community or various service providers to contribute or participate in this or alternative solutions.\nThis CIP attempts to provide a decentralized solution to this problem.\nExample 2: Token Metadata Insecurity\nOne of the stated rationales for CIP-68 was that the \"original\" Cardano NFT Metadata Standard (CIP-25) was insecure in some example use cases. This is due to the link between the transactional metadata and the minting of native assets. For example, a smart contract that cannot/does not validate against transaction metadata (i.e. Liquidity Pool tokens) could have a malicious user inject CIP-25 metadata into the transaction, potentially inserting illicit, illegal, or otherwise nefarious metadata information tied to the tokens which may be picked up and displayed to end users unwittingly by explorers and wallets.\nExample 3: De-duplication of Data\nSimilar to Example 1 above, when it comes to Non-Fungible Token (NFT) projects currently operating on the chain there is usually a desire to provide some level of information that pertains to all tokens under the given policy. Examples include project/collection names, social media handles, and miscellaneous project registration information. At current, this is generally solved by adding \"static\" fields in the metadata of every token. By moving this project-specific information a layer higher, not only do we achieve a path to dynamism but also reduce ledger bloat size by de-duplicating data.\nSimilarly, there are currently multiple marketplaces and decentralized exchanges (DEXes) in operation in the Cardano ecosystem. At current, most DEXes pull information from the Cardano Token Registry but there is no similar function for NFT projects. As such, much information must be manually provided to individual marketplaces by the token projects creating an undue burden on the project creators to provide a largely static amount of information via different web forms and authentication schemes rather than simply publishing this information to the blockchain directly.\nCPS-0001 presents a problem of metadata discoverability and trust. This CIP attempts to address and solve several of the issues proposed in CPS-0001 but is most likely not a \"complete\" solution and is rather narrowed (for the time being) to the scope of token projects although with some refinement to the schema could potentially be expanded to support additional scopes.\nThe primary purpose of this CIP is to enhance discoverability of token projects utilizing and parsing only the information contained on-chain. In the first version of this CIP we address the discoverability of top-level information related to \"Token Projects\" such as NFT and FT projects needing to provide social media handles, human-friendly names, etc.\nThe goal of both minimizing redundant data stored on-chain and enhancing discoverability of projects for platforms like DExes and NFT Marketplaces is specifically referenced in Example #3 above.\nNote that while some external chain indexing and validation will ultimately be required, there is no off-chain, centralized or decentralized trusted repository of additional information required (although aspects of the metadata provided may rely on off-chain storage solutions).\nThis CIP aims to ensure metadata \"correctness\" on two different fronts.\nActual Data Correctness This CIP utilizes a strongly-typed, numerically indexed data structure that should minimize common errors and omissions observed in less strictly-typed standards. Parsers of the data presented within the scope of this standard should ignore any non-specified or non-conforming data. Data Provenance Specifically in the context of correctness via proving provenance of the metadata, this CIP aims to address correctness via the same data witness standards utilized by CIP-26 although with a slightly modified data structure. Currently existing solutions for things like NFT Project verification standards rely on trust methods such as publishing a special message on your website, send us a DM from your Twitter account, and other less secure means of validating provenance of the data.\nActual Data Correctness This CIP utilizes a strongly-typed, numerically indexed data structure that should minimize common errors and omissions observed in less strictly-typed standards. Parsers of the data presented within the scope of this standard should ignore any non-specified or non-conforming data.\nThis CIP utilizes a strongly-typed, numerically indexed data structure that should minimize common errors and omissions observed in less strictly-typed standards. Parsers of the data presented within the scope of this standard should ignore any non-specified or non-conforming data.\nData Provenance Specifically in the context of correctness via proving provenance of the metadata, this CIP aims to address correctness via the same data witness standards utilized by CIP-26 although with a slightly modified data structure. Currently existing solutions for things like NFT Project verification standards rely on trust methods such as publishing a special message on your website, send us a DM from your Twitter account, and other less secure means of validating provenance of the data.\nSpecifically in the context of correctness via proving provenance of the metadata, this CIP aims to address correctness via the same data witness standards utilized by CIP-26 although with a slightly modified data structure. Currently existing solutions for things like NFT Project verification standards rely on trust methods such as publishing a special message on your website, send us a DM from your Twitter account, and other less secure means of validating provenance of the data.\nAs mentioned in the Data Provenance note on Data Correctness above, this CIP minimizes the trust required by relying on a verifiable witness signature versus currently existing solutions which largely rely on off-chain trust mechanisms for proof of provenance. Therefore, we increase trust in the data by describing a relatively simple means of data validation while decreasing the need for trust outside the scope of the on-chain metadata.\nWhere applicable the 0 (zero) index of all specification documents is reserved for an optional integer version identifier to enable future extensions to this and CIP-specific sub-standards.\nA numeric-indexed structure is used to support required and optional fields in a format that is compatible with both CBOR and JSON transport formats with minimal changes to the data structure and to minimize the possibility of misspelling or capitalization issues.\nThis standard is likely to need frequent extension and modification, particularly relating to CIP-Specific Information. Any group or individual wishing to extend or modify this standard MUST comply to the following criteria:\nNew CIPs SHOULD achieve the Active status prior to being included and documented in this directory after undergoing the regular community feedback and review process.\nActive\nAny change or modification to required fields in the root standard or a CIP's specific details MUST be written as a new version, MUST increment the version number by 1, and MUST include new, versioned documentation for the CIP while leaving previous version documentation intact for backwards compatibility.\nrequired\nversion\n1\nSubmissions for addition to this CIP MUST be made via a separate, dedicated pull request against this repository so that the format and documentation pertaining to CIP-88 specifically can undergo community review and feedback prior to inclusion here.\nWhenever possible, extensions to this CIP SHOULD attempt to introduce new indices and object definitions without changing or modifying existing data structures to enable new functionality and existing implementations to operate until newer standards can be adopted and enhance functionality rather than change it.\nNew CIP submissions MUST follow the same paradigms and documentation examples as those found within the CIPs directory including: a README.md document describing the fields, values, and rationale a CBOR CDDL specification file a JSON format schema file (optional) a JSON example file showing all defined fields (optional)\na README.md document describing the fields, values, and rationale\na CBOR CDDL specification file\na JSON format schema file (optional)\na JSON example file showing all defined fields (optional)\nVersion: 1\nVersion: 1\nThe Token Registration Payload Object (TRPO) consists of 4 required fields and optional additional fields to provide context and information. The top-level metadata label of 867 has been chosen for the purposes of this standard.\nThe following fields are required in all token registration submissions.\nCurrently, this CIP concerns itself with the scope of Tokens with relation to CPS-0001 as described in the Motivation section. However, the specification is left flexible to encapsulate additional scopes and contexts (Stake Pools, dApps, etc.) should the specification become adopted and the community desire to expand the scope of this CIP.\nScopes\n[0, h'policyID', [h'policyHex']]\nNative Scripts: Native scripts should be specified as an array with the first entry indicating the type (Native Script), the second entry indicating the script hash (Policy ID) and the third entry consisting of an array with one or more 64-byte strings constituting the hex-encoded CBOR representation of the Native Script itself. In this way, CIP-88 registration may be submitted on-chain prior to any tokens being minted and can be used by validators to confirm the legitimacy of the certificate without any secondary information source.\nNative Scripts: Native scripts should be specified as an array with the first entry indicating the type (Native Script), the second entry indicating the script hash (Policy ID) and the third entry consisting of an array with one or more 64-byte strings constituting the hex-encoded CBOR representation of the Native Script itself. In this way, CIP-88 registration may be submitted on-chain prior to any tokens being minted and can be used by validators to confirm the legitimacy of the certificate without any secondary information source.\nExample:\n[0, h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be', h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050']\n[0, h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be', h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050']\nThe Feature Set is a simple array of unsigned integer values representing the CIP standards that should be applied to the subject scope.\nExample:\n[25, 27]\n[25, 27]\nIn order to minimize issues relating to capitalization and misspellings, we should use a well-defined map of integer values for validation methods that will be utilized by third party observers and processors to authenticate the payload. The validation method entry should always be provided as an array with the first element being an unsigned integer representing the method and additional entries providing additional context to the validation as required.\nProposed Validation Methods\n[0]\n[1, [h'<policyId>',h'<assetId>']]\nExamples:\n[0], [1, [h' policyId ',h' assetId ']]\n[0]\n[1, [h'<policyId>',h'<assetId>']]\nThe nonce value is utilized to prevent a replay attack vector. The nonce value should be an unsigned integer value that is always at least one greater than the previously registered value.\nExample:\n12345\n12345\nTo be utilized and expanded upon in a separate CIP, this should be a valid URI pointing to a source of additional, potentially dynamic information relating to the project and/or the tokens minted under it.\nExample:\n[ \"https://\", \"oracle.mytokenproject.io/\" ]\n[ \"https://\", \"oracle.mytokenproject.io/\" ]\nThis entry, if present, should be a CIP ID indexed object containing additional information pertaining to that CIP. When and where possible the CIP-Specific registration should follow the CBOR-like declaration syntax to ensure that the content is well-formed and easily parseable.\nNote: CIP-0068 Tokens\nDue to a lack of clarity in the original language of CIP-0068, standards for fungible, non-fungible, and \"rich\" fungible tokens have been added to the core standard. To accommodate for this, projects should use the CIP-68 data when using the 222 (NFT) or 444 (RFT) tokens for top-level project information. Projects utilizing the 333 (FT) style tokens should utilize the CIP-26 data structure to provide fungible token context.\n222\n444\n333\nBeacon Token Registration\nWhere applicable to a specific CIP, the CIP-specific registration may refer to a \"beacon token\". This is standardized in this CIP as a two-element array consisting of the hex-encoded policy ID and asset ID of the token to be used as a beacon token for the purposes of smart contract interactions. e.g. [ h' policy_id ', h' asset_id ' ]\n[ h'<policy_id>', h'<asset_id>' ]\nMultiple Feature Set Example (CBOR):\n{ 25: { 0: 1, 1: { 0: \"Cool NFT Project\", 1: [ \"This is a description of my project\", \"longer than 64 characters so broken up into a string array\" ], 2: [ \"https://\", \"static.coolnftproject.io\", \"/images/icon.png\" ], 3: [ \"https://\", \"static.coolnftproject.io\", \"/images/banner1.jpg\" ], 4: 0, 5: [ [ \"twitter\", [ \"https://\", \"twitter.com/spacebudzNFT\" ] ], [ \"discord\", [ \"https://\", \"discord.gg/spacebudz\" ] ] ], 6: \"Virtua Metaverse\" } }, 27: { 0: 1, 1: { 1: \"0.05\", 2: [ \"addr_test1qqp7uedmne8vjzue66hknx87jspg56qhkm4gp6ahyw7kaahevmtcux\", \"lpy25nqhaljc70094vfu8q4knqyv6668cvwhsq64gt89\" ] } } }\n{ 25: { 0: 1, 1: { 0: \"Cool NFT Project\", 1: [ \"This is a description of my project\", \"longer than 64 characters so broken up into a string array\" ], 2: [ \"https://\", \"static.coolnftproject.io\", \"/images/icon.png\" ], 3: [ \"https://\", \"static.coolnftproject.io\", \"/images/banner1.jpg\" ], 4: 0, 5: [ [ \"twitter\", [ \"https://\", \"twitter.com/spacebudzNFT\" ] ], [ \"discord\", [ \"https://\", \"discord.gg/spacebudz\" ] ] ], 6: \"Virtua Metaverse\" } }, 27: { 0: 1, 1: { 1: \"0.05\", 2: [ \"addr_test1qqp7uedmne8vjzue66hknx87jspg56qhkm4gp6ahyw7kaahevmtcux\", \"lpy25nqhaljc70094vfu8q4knqyv6668cvwhsq64gt89\" ] } } }\nThe Witness Array included in the on-chain metadata should consist of an array of arrays with two elements, the public key of the signing key and the signed key witness. If a script requires multiple signatures, enough signatures to meet the criteria of the script should be included and required for proper validation of an updated token registration.\nThe signing payload should be the hex-encoded Token Registration Payload Object.\nExample\n[ [ h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb', h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08' ], [ h'26bacc7b88e2b40701387c521cd0c50d5c0cfa4c6c6d7f0901395757', h'secondSignatureByteString' ] ]\n[ [ h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb', h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08' ], [ h'26bacc7b88e2b40701387c521cd0c50d5c0cfa4c6c6d7f0901395757', h'secondSignatureByteString' ] ]\nBelow is a complete example of the hypothetical metadata payload for an NFT project registering their policy on-chain.\n{ 867: { 0: 1, 1: { 1: [ 0, h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be', h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050' ], 2: [ 25, 27 ], 3: [0], 4: 12345, 5: [ \"https://\", \"oracle.mycoolnftproject.io/\" ], 6: { 25: { 0: 1, 1: { 0: \"Cool NFT Project\", 1: [ \"This is a description of my project\", \"longer than 64 characters so broken up into a string array\" ], 2: [ \"https://\", \"static.coolnftproject.io\", \"/images/icon.png\" ], 3: [ \"https://\", \"static.coolnftproject.io\", \"/images/banner1.jpg\" ], 4: 0, 5: [ [ \"twitter\", [ \"https://\", \"twitter.com/spacebudzNFT\" ] ], [ \"discord\", [ \"https://\", \"discord.gg/spacebudz\" ] ] ], 6: \"Virtua Metaverse\" } }, 27: { 0: 1, 1: { 1: \"0.05\", 2: [ \"addr_test1qqp7uedmne8vjzue66hknx87jspg56qhkm4gp6ahyw7kaahevmtcux\", \"lpy25nqhaljc70094vfu8q4knqyv6668cvwhsq64gt89\" ] } } } }, 2: [ [ h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb', h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08' ], [ h'26bacc7b88e2b40701387c521cd0c50d5c0cfa4c6c6d7f0901395757', h'secondWitnessByteString' ] ] } }\n{ 867: { 0: 1, 1: { 1: [ 0, h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be', h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050' ], 2: [ 25, 27 ], 3: [0], 4: 12345, 5: [ \"https://\", \"oracle.mycoolnftproject.io/\" ], 6: { 25: { 0: 1, 1: { 0: \"Cool NFT Project\", 1: [ \"This is a description of my project\", \"longer than 64 characters so broken up into a string array\" ], 2: [ \"https://\", \"static.coolnftproject.io\", \"/images/icon.png\" ], 3: [ \"https://\", \"static.coolnftproject.io\", \"/images/banner1.jpg\" ], 4: 0, 5: [ [ \"twitter\", [ \"https://\", \"twitter.com/spacebudzNFT\" ] ], [ \"discord\", [ \"https://\", \"discord.gg/spacebudz\" ] ] ], 6: \"Virtua Metaverse\" } }, 27: { 0: 1, 1: { 1: \"0.05\", 2: [ \"addr_test1qqp7uedmne8vjzue66hknx87jspg56qhkm4gp6ahyw7kaahevmtcux\", \"lpy25nqhaljc70094vfu8q4knqyv6668cvwhsq64gt89\" ] } } } }, 2: [ [ h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb', h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08' ], [ h'26bacc7b88e2b40701387c521cd0c50d5c0cfa4c6c6d7f0901395757', h'secondWitnessByteString' ] ] } }\n{ 867: { 0: 1, 1: { 1: [ 0, h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be', h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050' ], 2: [ 26 ], 3: [0], 4: 12345, 5: [ \"https://\", \"oracle.tokenproject.io/\" ], 6: { 26: { 0: 1, 1: [ { 0: [ h\"d894897411707efa755a76deb66d26dfd50593f2e70863e1661e98a0\", h\"7370616365636f696e73\" ], 1: \"spacecoins\", 2: [ \"the OG Cardano community token\", \"-\", \"whatever you do, your did it!\", \"\", \"Learn more at https://spacecoins.io!\" ], 3: \"SPACE\", 4: 0, 5: [ \"https://\", \"spacecoins.io\" ], 6: [ \"ipfs://\", \"bafkreib3e5u4am2btduu5s76rdznmqgmmrd4l6xf2vpi4vzldxe25fqapy\" ], 7: [ [ \"ipfs://\", \"bafkreibva6x6dwxqksmnozg44vpixja6jlhm2w7ueydkyk4lpxbowdbqly\" ], \"3507afe1daf05498d764dce55e8ba41e4acecd5bf42606ac2b8b7dc2eb0c305e\" ], 8: [ h\"d894897411707efa755a76deb66d26dfd50593f2e70863e1661e98a0\", h\"7370616365636f696e74\" ] } ] } } }, 2: [ [ h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb', h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08' ] ] } }\n{ 867: { 0: 1, 1: { 1: [ 0, h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be', h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050' ], 2: [ 26 ], 3: [0], 4: 12345, 5: [ \"https://\", \"oracle.tokenproject.io/\" ], 6: { 26: { 0: 1, 1: [ { 0: [ h\"d894897411707efa755a76deb66d26dfd50593f2e70863e1661e98a0\", h\"7370616365636f696e73\" ], 1: \"spacecoins\", 2: [ \"the OG Cardano community token\", \"-\", \"whatever you do, your did it!\", \"\", \"Learn more at https://spacecoins.io!\" ], 3: \"SPACE\", 4: 0, 5: [ \"https://\", \"spacecoins.io\" ], 6: [ \"ipfs://\", \"bafkreib3e5u4am2btduu5s76rdznmqgmmrd4l6xf2vpi4vzldxe25fqapy\" ], 7: [ [ \"ipfs://\", \"bafkreibva6x6dwxqksmnozg44vpixja6jlhm2w7ueydkyk4lpxbowdbqly\" ], \"3507afe1daf05498d764dce55e8ba41e4acecd5bf42606ac2b8b7dc2eb0c305e\" ], 8: [ h\"d894897411707efa755a76deb66d26dfd50593f2e70863e1661e98a0\", h\"7370616365636f696e74\" ] } ] } } }, 2: [ [ h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb', h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08' ] ] } }\nFor this specification, I have drawn inspiration from CIP-36: Catalyst/Voltaire Registration Metadata Format which succinctly and canonically publishes data to the main chain (L1) via a metadata transaction and without any required modification or customization to the underlying ledger.\nBy leveraging the existing signing keys present in native asset scripts from the beginning of the Mary Era on Cardano we can enable all projects to update and provide additional, verified information about their project in a canonical, verifiable, and on-chain way while also providing for additional off-chain information.\nThis makes this CIP backwards compatible with all existing standards (CIP-25, 26, 27, 68, etc) while also providing the flexibility for future-proofing and adding additional context and information in the future as additional use cases, utility, and standards evolve.\nThis CIP should receive feedback, criticism, and refinement from: CIP Editors and the community of people involved with token projects (both NFT and FT) to review any weaknesses or areas of improvement.\nGuidelines and examples of publication of data as well as discovery and validation should be included as part of criteria for acceptance.\nSpecifications should be updated to be written in both JSON Schema and CBOR CDDL format for maximum compatibility.\nImplementation and use demonstrated by the community: Token Projects, Blockchain Explorers, Wallets, Marketplaces/DEXes.\nPublish instructions and tooling for publication and verification of certificates\nDevelop standard for validation of Smart Contract minted tokens\nPublish open source tooling and instructions related to the publication and verification of data utilizing this standard. Achieve \"buy in\" from existing community actors and implementors such as: blockchain explorers, token marketplaces, decentralized exchanges, wallets.\nPublish open source tooling and instructions related to the publication and verification of data utilizing this standard.\nAchieve \"buy in\" from existing community actors and implementors such as: blockchain explorers, token marketplaces, decentralized exchanges, wallets.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0091 | Don't force Built-In functions\n\nThe Untyped Plutus Language Core (UPLC) has established itself as the target language for a host of emerging Smart Contract Languages. These languages implement type safety by checking the types of variables at compile time. In the compiled output, type information is absent and no longer required or checked. This proposal suggests replacing or enhancing the set of builtin functions with untyped builtin functions, whose arguments are devoid of any type instantiations. This change aims to improve performance and reduce resource costs.\nMany currently available UPLC builtin functions require forcing between 1 and 3 times to eliminate type instantiations checked at a higher level language of the toolstack (PLC), which most third-party tools do not use. These forces only burn cycles of nodes that evaluate contracts, since there is no actual type instantiation happening internally. By removing the need for these no-op force operations, this proposal aims to enhance performance and reduce resource costs.\nThere is one data point as to how much performance improvement this may bring in the non-optimized case here. However, the performance improvement in the optimized case is generally constant: One can bind the forced builtins to a variable at the outermost layer of the UPLC program and from there on just use the forced builtins.\nFor all existing UPLC Builtin Functions x that require n 0 forces for evaluation, this proposal suggests to implement the builtin function x' without any required forces.\nThis proposal suggests that all existing UPLC Builtin Functions x be replaced by x'. Generally, this proposal also suggests that no further Builtin Functions be defined that require force.\nforce\nThis proposal reduces the resources needed to evaluate builtin functions by removing the need to apply no-op force operations to them. However, the actual performance impact might be negligible, and the main impact could be on simplifying the language and making it easier for compiler writers. These are weaker reasons than widespread performance improvements. Implementing this proposal may also require a new Plutus ledger language, as described in CIP-35, due to the non-backwards-compatible changes.\nThe implementation will break the backward compatibility for future Plutus Smart Contracts.\nplutus changes Specification Production implementation Costing of the new operations\nplutus\nSpecification\nProduction implementation\nCosting of the new operations\ncardano-ledger changes Specification, including specification of the script context translation to a Plutus Core term Implementation of new ledger language, including new ledger-script interface\ncardano-ledger\nSpecification, including specification of the script context translation to a Plutus Core term\nImplementation of new ledger language, including new ledger-script interface\nFurther benchmarking Check additional real-world examples\nCheck additional real-world examples\nRelease New Plutus language version supported in a released node version New ledger language supported in a released node version\nNew Plutus language version supported in a released node version\nNew ledger language supported in a released node version\nIt is currently not planned to implement this proposal.\nThis CIP is licensed under [CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode\n2023 Cardano Foundation\n\n---\n\nCIP-0093 | Authenticated Web3 HTTP requests\n\nThe proposed Cardano Improvement Proposal (CIP) outlines a conventional structure for data payloads that are signed by wallets, which can be used by decentralized application (dApp) servers to authenticate user requests. By leveraging the Cardano blockchain as an identity provider, dApp servers can securely and trustlessly verify user identities without relying on centralized servers or third-party services. This CIP aims to provide a standard approach for implementing wallet signature authentication in dApps, improving the security and reliability of user interactions with decentralized systems.\nThe cardano wallets have the ability to sign arbitrary piece of data as we can see in the Message signing CIP-0008. All wallets implement the method api.signData(addr: Address, payload: Bytes): Promise DataSignature defined in Cardano dApp-Wallet Web Bridge CIP-0030.\napi.signData(addr: Address, payload: Bytes): Promise<DataSignature>\ndApps generate arbritary payloads as byte arrays. These payloads are signed and included in the protected headers of the signatures. The wallets are responsible for showing the payload data to the user, who will proceed to sign or reject the payload. It's a common practice to encode a string or a JSON string but there isn't any standard for the way to construct and to show this data.\nThe current implementations for web3 applications use static strings. This is dangerous because if a bad actor intercepts the signed message then it can be used in a replay attack by the bad actor. That's why it is very important to produce a dynamic payload rather a static string.\nAnother problem with the current approach is how the wallets show the information contained in the payload. The payload is a encoded byte array and it could contain anything. If Alice want to call an endpoint and Bob has the ability to change the message before Alice gets it. Alice must be notified somehow that she is signing a potentially malicious payload. A simple hex-encoded representation of the payload isn't enough to ensure a safe interaction.\nThis specification involves multiple parties: Wallet/Client, dApp Server and Blockchain.\nWallet/Client: The Wallet/Client is responsible for managing the user's cryptographic keys. Anyone can create a wallet using the CIP-0030 API interface, but it may produce invalid or malicious data sent to the dApp. This CIP aims to validate the ownership and veracity of the data provided by wallets. Additionally, it establishes guidelines for mitigating common wallet attacks, improving security for user interaction. dApp Server: The dApp Server represents the server-side infraestructure that supports decentralized applications (dApps). It communicates with the blockchain to retrieve stored data and validate wallet status. It must enforce minimum payload requirements to ensure authenticity and protect users from malicious actors. Blockchain: The Blockchain is the underlying distributed ledger technology that forms the foundation of decentralized systems. It is a decentralized and immutable ledger that securely records all transactions and data in a chronological and transparent manner. The Blockchain can be utilized for authentication, providing user identity, and for authorization, tracking user history and current status.\nWallet/Client: The Wallet/Client is responsible for managing the user's cryptographic keys. Anyone can create a wallet using the CIP-0030 API interface, but it may produce invalid or malicious data sent to the dApp. This CIP aims to validate the ownership and veracity of the data provided by wallets. Additionally, it establishes guidelines for mitigating common wallet attacks, improving security for user interaction.\nWallet/Client: The Wallet/Client is responsible for managing the user's cryptographic keys. Anyone can create a wallet using the CIP-0030 API interface, but it may produce invalid or malicious data sent to the dApp. This CIP aims to validate the ownership and veracity of the data provided by wallets. Additionally, it establishes guidelines for mitigating common wallet attacks, improving security for user interaction.\ndApp Server: The dApp Server represents the server-side infraestructure that supports decentralized applications (dApps). It communicates with the blockchain to retrieve stored data and validate wallet status. It must enforce minimum payload requirements to ensure authenticity and protect users from malicious actors.\ndApp Server: The dApp Server represents the server-side infraestructure that supports decentralized applications (dApps). It communicates with the blockchain to retrieve stored data and validate wallet status. It must enforce minimum payload requirements to ensure authenticity and protect users from malicious actors.\nBlockchain: The Blockchain is the underlying distributed ledger technology that forms the foundation of decentralized systems. It is a decentralized and immutable ledger that securely records all transactions and data in a chronological and transparent manner. The Blockchain can be utilized for authentication, providing user identity, and for authorization, tracking user history and current status.\nBlockchain: The Blockchain is the underlying distributed ledger technology that forms the foundation of decentralized systems. It is a decentralized and immutable ledger that securely records all transactions and data in a chronological and transparent manner. The Blockchain can be utilized for authentication, providing user identity, and for authorization, tracking user history and current status.\n+-----------+ +---------------+ +----------------+ | Wallet/ | | dApp Server | | Blockchain | | Client | | | | | +-----------+ +---------------+ +----------------+ | | | | | | | 1. Create payload | | |------------+ | | | | | | |<-----------+ | | | | | | 2. Request signature | | |------------+ | | | | | | |<-----------+ | | | | | | 3. Send signed payload | | |----------------------------->| | | | | | | 4. Verify signature | | |------------+ | | | | | | |<-----------+ | | | | | | 5. Check blockchain (optional)| | |------------------------------>| | | |\n+-----------+ +---------------+ +----------------+ | Wallet/ | | dApp Server | | Blockchain | | Client | | | | | +-----------+ +---------------+ +----------------+ | | | | | | | 1. Create payload | | |------------+ | | | | | | |<-----------+ | | | | | | 2. Request signature | | |------------+ | | | | | | |<-----------+ | | | | | | 3. Send signed payload | | |----------------------------->| | | | | | | 4. Verify signature | | |------------+ | | | | | | |<-----------+ | | | | | | 5. Check blockchain (optional)| | |------------------------------>| | | |\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"NOT RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.\nThe payload MUST be encoded as a JSON string. JSON strings are semi-structured data that are human-readable. So that with a straightforward decoding into text, it will be understandable for the readers. This feature also allows the users to debug it in an easy manner, for example, with the browser debugger tools.\nThe content of the payload will be included in the protected header of the COSESign1 signature, hence the content effects directly the behavior and security of the system. The payload MUST have the following fields:\nThe field uri MUST contain the full path to the endpoint, where the payload will be processed. Sometimes, the endpoint uri field is not enough to determine its purpose. The user should understand perfectly the objective of the payload which he or she is signing. That's why the payload MUST contain an action field with a descriptive text containing the purpose of the payload. For example, if someone calls the endpoint /users without an action field, they can create or delete users. However, by including the action field in the payload, it not only provides additional clarity but also effectively limits the scope of the payload. In order to improve globalization, the payload MAY include an actionText field that represents the action in the locale of the user. When present, the wallet MUST display this field to the user. By including the actionText field, the wallet facilitates the processing of the action field, eliminating the need for the server to be aware of the user's locale and the possible variants of the action text. The payload MUST include either a UNIX timestamp or a slot number. The slot field represents a specific time in the blockchain and serves as a reference for synchronization between the client and the server. The timestamp or slot number is also used as a nonce and serves as an indicator for payload expiration in case the payload is compromised.\nThe field uri MUST contain the full path to the endpoint, where the payload will be processed.\nThe field uri MUST contain the full path to the endpoint, where the payload will be processed.\nuri\nSometimes, the endpoint uri field is not enough to determine its purpose. The user should understand perfectly the objective of the payload which he or she is signing. That's why the payload MUST contain an action field with a descriptive text containing the purpose of the payload. For example, if someone calls the endpoint /users without an action field, they can create or delete users. However, by including the action field in the payload, it not only provides additional clarity but also effectively limits the scope of the payload.\nSometimes, the endpoint uri field is not enough to determine its purpose. The user should understand perfectly the objective of the payload which he or she is signing. That's why the payload MUST contain an action field with a descriptive text containing the purpose of the payload. For example, if someone calls the endpoint /users without an action field, they can create or delete users. However, by including the action field in the payload, it not only provides additional clarity but also effectively limits the scope of the payload.\nuri\naction\n/users\nIn order to improve globalization, the payload MAY include an actionText field that represents the action in the locale of the user. When present, the wallet MUST display this field to the user. By including the actionText field, the wallet facilitates the processing of the action field, eliminating the need for the server to be aware of the user's locale and the possible variants of the action text.\nIn order to improve globalization, the payload MAY include an actionText field that represents the action in the locale of the user. When present, the wallet MUST display this field to the user. By including the actionText field, the wallet facilitates the processing of the action field, eliminating the need for the server to be aware of the user's locale and the possible variants of the action text.\nactionText\nactionText\nThe payload MUST include either a UNIX timestamp or a slot number. The slot field represents a specific time in the blockchain and serves as a reference for synchronization between the client and the server. The timestamp or slot number is also used as a nonce and serves as an indicator for payload expiration in case the payload is compromised.\nThe payload MUST include either a UNIX timestamp or a slot number. The slot field represents a specific time in the blockchain and serves as a reference for synchronization between the client and the server. The timestamp or slot number is also used as a nonce and serves as an indicator for payload expiration in case the payload is compromised.\ntimestamp\nslot\nslot\ntimestamp\nslot\nAdditional fields MAY be included in the payload, and these fields can be string fields or objects. Depending on the specific process or use case, including additional fields in the protected header of the signature can provide valuable functionality and security enhancements. For example, in a registration request, it may be useful to include the email information as an additional field in the protected header. By doing so, the payload can be uniquely associated with that specific email, ensuring its integrity and preventing tampering.\n{ \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\": \"object\", \"properties\": { \"uri\": { \"type\": \"string\", \"format\": \"uri\" }, \"action\": { \"type\": \"string\" }, \"actionText\": { \"type\": \"string\" }, \"timestamp\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"string\", \"pattern\": \"^\\\\d+$\" } ] }, \"slot\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"string\", \"pattern\": \"^\\\\d+$\" } ] } }, \"required\": [\"uri\", \"action\"], \"oneOf\": [{ \"required\": [\"timestamp\"] }, { \"required\": [\"slot\"] }], \"additionalProperties\": { \"type\": [\"string\", \"object\"] } }\n{ \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\": \"object\", \"properties\": { \"uri\": { \"type\": \"string\", \"format\": \"uri\" }, \"action\": { \"type\": \"string\" }, \"actionText\": { \"type\": \"string\" }, \"timestamp\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"string\", \"pattern\": \"^\\\\d+$\" } ] }, \"slot\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"string\", \"pattern\": \"^\\\\d+$\" } ] } }, \"required\": [\"uri\", \"action\"], \"oneOf\": [{ \"required\": [\"timestamp\"] }, { \"required\": [\"slot\"] }], \"additionalProperties\": { \"type\": [\"string\", \"object\"] } }\n{ \"uri\": \"http://example.com/signin\", \"action\": \"Sign in\", \"timestamp\": 1673261248, }\n{ \"uri\": \"http://example.com/signin\", \"action\": \"Sign in\", \"timestamp\": 1673261248, }\n{ \"uri\": \"http://example.com/signup\", \"action\": \"Sign up\", \"timestamp\": \"1673261248\", \"email\": \"email@example.com\" } { \"uri\": \"http://example.com/signup\", \"action\": \"SIGN_UP\", \"actionText\": \"Registrar\", \"slot\": 94941399 }\n{ \"uri\": \"http://example.com/signup\", \"action\": \"Sign up\", \"timestamp\": \"1673261248\", \"email\": \"email@example.com\" } { \"uri\": \"http://example.com/signup\", \"action\": \"SIGN_UP\", \"actionText\": \"Registrar\", \"slot\": 94941399 }\nThe wallets can improve the overall security implementing the following guidelines. We RECOMMEND to show in a structured way the payload information for sake of clarity. This information should be well understood by the users before the payload is signed.\nThe uri field provides information about the hostname of the application. This hostname MUST be included in the wallet allow list. If a known domain A tries to sign a payload for an unknown domain B, you will be prompted with permission popup making more obvious the cross-domain interaction. When possible, the wallet SHOULD warn the user if a payload is for a different domain.\nuri\nThe wallet SHOULD update the timestamp field to the current time just before the signature. This field ideally should match the moment just before the signature such that the server receives fresh payload.\ntimestamp\nThe server has ultimate responsibility of processing correctly the requests. We use the content to validate the payload. The request will be processed with the following steps:\nThe server MUST check the action and the endpoint included in the request. Each route to an endpoint MUST have an associated action and a URI. The first step is to check that they match with the parameterized action. The server MUST check the expiration of the payload. The expiration SHOULD be enough to give time to the user to introduce the wallet password but it SHOULD NOT be too long, we RECOMMEND not more than 5 minutes. The server MUST validate the COSESign1 signature and check that the address inside the protected map of the signature corresponds to the public key in the COSEKey.\nThe server MUST check the action and the endpoint included in the request. Each route to an endpoint MUST have an associated action and a URI. The first step is to check that they match with the parameterized action.\nThe server MUST check the action and the endpoint included in the request. Each route to an endpoint MUST have an associated action and a URI. The first step is to check that they match with the parameterized action.\nThe server MUST check the expiration of the payload. The expiration SHOULD be enough to give time to the user to introduce the wallet password but it SHOULD NOT be too long, we RECOMMEND not more than 5 minutes.\nThe server MUST check the expiration of the payload. The expiration SHOULD be enough to give time to the user to introduce the wallet password but it SHOULD NOT be too long, we RECOMMEND not more than 5 minutes.\nThe server MUST validate the COSESign1 signature and check that the address inside the protected map of the signature corresponds to the public key in the COSEKey.\nThe server MUST validate the COSESign1 signature and check that the address inside the protected map of the signature corresponds to the public key in the COSEKey.\nAdditionally the server COULD extract the payload content and pass it through the server logic.\nCIP-0008 enhances authentication by enabling individuals to prove ownership of addresses, identities, and off-chain data through message signing. It provides a reliable means of authentication by allowing individuals to attach a public key to their data and sign messages associated with that data, thereby establishing ownership and ensuring the integrity of the authentication process.\nAdditionally, This specification provides the general guidelines and necessary recommendations for performing secure authenticated web3 requests in the Cardano ecosystem. It covers the two main desired characteristics for a secure payload: It must expire and it must be non-static. Moreover, the signature method proposed in this CIP does not require users to spend funds in a transaction, which further lowers the cost and barriers to entry for users.\nAnother important aspect for security is how wallets process the payload. They can improve the security using the data inside the payload to warn the users about possible malicious interactions. This specification emphasizes the importance of informing users clearly about the purpose of the payloads and how wallets can use the URI field to apply allow-lists and/or cross-domain policies. It establishes also the requirements and recommendations for server side processing. The server must also ensure the validity of the signature and the payload, as well as of its purpose in order to accomplish the authentication.\nIn addition to the aforementioned aspects, this CIP also aims to promote decentralization and enhance security and privacy by enabling users to sign and verify transactions without relying on external servers or third parties. By allowing users to create and sign their own payloads, this specification reduces the dependency on centralized authorities and enhances the security and privacy of the transactions.\nDuring discussions about this specification, the possibility of modifying CIP-0008 to incorporate the standards defined here was considered. However, a thorough evaluation revealed that this approach would require extensive modifications to CIP-0008 and CIP-0030, leading to significant changes in the Cardano wallet API. Moreover, it would result in a lengthy waiting period for browser wallet developers to implement the necessary requirements. This could potentially bypass important security measures outlined in this CIP, such as the requirement for human readability.\nWhile this alternative approach brings advantages, such as defining the payload in CBOR, which aligns well with Cardano, it also presents challenges. JSON and CBOR offer different levels of expressiveness, and the choice between the two depends on the specific needs of the application. JSON provides a more flexible and widely supported data format, whereas CBOR offers a more compact and efficient representation, particularly beneficial when working with the blockchain.\nConsidering these factors, it was concluded that deploying this standard as it currently stands, while coexisting with a future version that allows users to choose between JSON and CBOR payloads, would be the most practical approach. This would provide sufficient time for modifying CIP-0008 and CIP-0030, enabling browser wallet developers to fulfill the requirements for human readability and make necessary adjustments to the wallet API. Consequently, a version 2 of this CIP can be introduced, incorporating COSESign and CBOR, accommodating both realms, and ensuring broad support.\nThe payload signature ensures wallet ownership without incurring transaction fees. However, requiring the user to enter their spending password for every authenticated request can be inconvenient for the user experience. To address this, it is recommended to restrict the use of the payload signature to only important requests such as login, sign up, or other critical operations depending on the dApp requirements.\nA common practice is to request the user's signature for the login process, and once authenticated, the dApp can issue a session token, such as a JSON Web Token (JWT), to manage the session. By implementing this approach, future non-critical requests can be performed using standard web 2.0 methods, eliminating the need to enter the spending password for each step. This significantly enhances the usability of the application, providing a smoother user experience.\njmagan/passport-cardano-web3\njmagan/cardano-express-web3-skeleton\njmagan/cardano-nextjs-web3-skeleton\nAt least one library should implement this authentication method.\nThe 80 users should have wallets implementing the following requirements: It MUST detect when the payload is formatted using this specification. The information contained in the payload MUST be parsed and formatted in the signing pop-up. The wallet SHOULD update the timestamp just before the payload is signed. The wallet MUST detect if the URI is in the allow list. The wallet SHOULD warn the user against cross-domain requests.\nIt MUST detect when the payload is formatted using this specification. The information contained in the payload MUST be parsed and formatted in the signing pop-up. The wallet SHOULD update the timestamp just before the payload is signed. The wallet MUST detect if the URI is in the allow list. The wallet SHOULD warn the user against cross-domain requests.\nIt MUST detect when the payload is formatted using this specification.\nThe information contained in the payload MUST be parsed and formatted in the signing pop-up.\nThe wallet SHOULD update the timestamp just before the payload is signed.\nThe wallet MUST detect if the URI is in the allow list.\nThe wallet SHOULD warn the user against cross-domain requests.\nA detailed documentation about web3 standards should be published. This documentation will include this standard and further best practices for web3 technologies.\nCreate a library for processing payload according to this specification.\nOpen a conversation about this specification and its possible improvements.\nTalk about further web3 standards and new specifications.\nWrite the documentation for web3 developers.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0094 | On-chain SPO polls\n\nThe Cardano Foundation proposes a mechanism for polling Cardano stake pool operators on specific topics. Polls are done on-chain through transaction metadata and authenticated through stake pool credentials (Ed25519 cold key). The goal is to gather opinions on governance matters such as protocol parameter updates. This standard is an inclusive interim solution while the work on a larger governance framework such as CIP-1694 continues.\nGovernance is difficult. Discussions on CIP-1694 can attest to that quite clearly. There are constant debates within the Cardano community about changing protocol parameters, and the decision ultimately falls -- at this stage still -- onto the three genesis entities: Input Output, The Cardano Foundation and Emurgo. Yet, at this stage, few governance tools are at their disposal to make educated decisions. Besides Twitter polls, newsletter surveys, and SPO town halls on Discord, we have identified a gap and an opportunity to engage with the Cardano community through means currently at our disposal.\nConducting an on-chain poll between SPOs can also be seen as an experiment and an evaluation of the network's participation and engagement in the governance questions. Even though we only propose to poll one particular group of the Cardano community (the SPOs), such events can help to provide actual data to fuel the conversations around CIP-1694.\nIn summary, the goals are:\nto make some first experimental baby steps in the realm of governance; to be achievable now (or an in immediate future); to capture participation data from SPOs; to raise awareness amongst SPOs regarding their future role in governance; to keep the Voltaire dynamics up in the ecosystem while other efforts are being pursued; to improve relations between the Cardano Foundation & SPOs for better mutual understanding and fruitful conversations.\nto make some first experimental baby steps in the realm of governance;\nto be achievable now (or an in immediate future);\nto capture participation data from SPOs;\nto raise awareness amongst SPOs regarding their future role in governance;\nto keep the Voltaire dynamics up in the ecosystem while other efforts are being pursued;\nto improve relations between the Cardano Foundation & SPOs for better mutual understanding and fruitful conversations.\nPolls will be multiple-choice questions by The Cardano Foundation with pre-defined answers to choose from.\nHere's an example of a question and answers:\nPineapples on pizza? yes no\nyes\nno\nThe serialised question and answers will be posted on-chain and signed by one of the delegate genesis keys owned by The Cardano Foundation. Answers will be provided on-chain by participating SPOs via transaction metadata referring to:\nThe question and answers\nThe index of the chosen answer from the available choices\nA digital signature (EdDSA) from the SPO's current cold key\nNote In this document, every time we refer to a serialized object, we refer to its canonical CBOR representation. In particular, keys in a map are always ordered alphabetically.\nA question is posted in a transaction's metadata using the metadata label 94 and the following metadata structure:\n94\nquestion = { 0: prompt , 1: [ * choice ] , ? \"_\": nonce } prompt = [ * text .size (0..64) ] choice = [ * text .size (0..64) ] nonce = uint\nquestion = { 0: prompt , 1: [ * choice ] , ? \"_\": nonce } prompt = [ * text .size (0..64) ] choice = [ * text .size (0..64) ] nonce = uint\nA nonce is optionally included to provide non-replayability should the same question and answers be asked multiple times over different periods. The transaction carrying a question must be signed by one of the genesis delegate keys to be considered valid. This genesis key signature isn't captured in the metadata but in the transaction itself as an extra signatory.\nFor example:\nSimilarly, an answer to a question is posted as transaction's metadata using the label 94 and the following metadata structure:\n94\nanswer = { 2: question_hash , 3: choice } question_hash = bytes .size 32\nanswer = { 2: question_hash , 3: choice } question_hash = bytes .size 32\nSome remarks:\nThe field 2 (question_hash) is a blake2b-256 hash digest, whose preimage is the entire serialised question metadata payload (with the 94 top-level label). The field 3 represents the 0-based index of the chosen answer from the available choices (from field 1 of the target poll).\nThe field 2 (question_hash) is a blake2b-256 hash digest, whose preimage is the entire serialised question metadata payload (with the 94 top-level label).\n2\nquestion_hash\n94\nThe field 3 represents the 0-based index of the chosen answer from the available choices (from field 1 of the target poll).\n3\n1\nFor example:\nThe transaction carrying the answer metadata must then be signed using a stake pool operator cold key. Because cold key are not payment keys, it is required to specify an extra required signer on the transaction (transaction's field number 14 as per Babbage's CDDL) to prevent malicious nodes from potentially propagating transactions without the necessary key witnesses.\nAlternatively, operators that are unable to sign arbitrary transactions due to hardware limitations can opt for stake pool update-registration certificate and attach the transaction metadata to it. Because an update-registration requires a signature from the cold key, the extra required signer field is redundant in that situation.\nRegardless of the method, the signature shall be produced in an air-gapped environment only.\nWarning\nOnly the first answer to a poll for each credential shall be considered. If multiple answers are found, only the first answer submitted (transaction & block ordering tallying) shall be considered.\nIt is possible to optionally attach extra context to the transaction as metadata following the procedure described in CIP-0020. Beside the structure specified in CIP-0020, such extra metadata is free-form and can be used to signal an intention behind a choice, or to voice a concern, or simply to give extra context. This is totally optional though we encourage SPOs to use this to inform their delegators of their choices.\nA poll starts when a valid transaction with a question is posted on-chain. Answers can be submitted until the end of the following epoch, so there is always at least one whole epoch to answer the poll.\nAfter one or more epochs in which the Stake Pool Operators have cast their answers, there follows a period of one or more epochs in which Cardano delegators may respond: If they disagree with the choice of their current stake pool, they can delegate to another pool. This changes the stake weight and thus influences the result. At the current state, the epochs for the answer and redelegation phase are only defined off-chain. In the future, they could also be defined as part of the signed question.\nIndirectly, this results in the possibility of participation for all Ada holders.\nThe outcome of a poll will depend on its level of participation (in terms of stake). It is essential to understand that we explicitly call this a poll / survey and not a vote to dispel any possible confusion. So it is akin to 1 Lovelace = 1 Voice although we may chose to interpret data using different equations (e.g. giving more weight to pledged stake). How the data is interpret is deemed out of the scope of this proposal which aims mainly at producing the data-points. Further conversations and debates will be needed regarding interpretation of the data-points.\n1 Lovelace = 1 Voice\nThis proposal does not introduce a change in the current governance scheme: it is still up to the three genesis entities to make a final call based on the poll results. Poll results will provide new data points to feed into the conversation. But, regardless of the outcome, any decision will be explained and motivated by other auditable sources of information. And on-chain polls will provide such an auditable source.\nThe proposed process will permanently record questions and their answers on-chain by leveraging existing transaction metadata. Note that we consciously do not record any element as datums. There are several reasons for this:\nDatums offer extra programmability (for being available in Plutus script context); this is not needed at this stage. Following a keep-it-simple strategy, we propose relying on well-known and well-supported transaction features (a.k.a metadata) for producers and consumers. Storing data in datums / UTxO has a non-negligible cost; naive datum storage would create thousands of new dummy UTxO on each poll. Transactions are cheaper to store and consume. Polls rely on slot order when tallying answers, which means that chain sync is needed anyway, and there's no strong argument for having this information readily available in the UTxO graph.\nDatums offer extra programmability (for being available in Plutus script context); this is not needed at this stage.\nFollowing a keep-it-simple strategy, we propose relying on well-known and well-supported transaction features (a.k.a metadata) for producers and consumers.\nStoring data in datums / UTxO has a non-negligible cost; naive datum storage would create thousands of new dummy UTxO on each poll. Transactions are cheaper to store and consume.\nPolls rely on slot order when tallying answers, which means that chain sync is needed anyway, and there's no strong argument for having this information readily available in the UTxO graph.\nThere have been several (on-and-off-the-record) discussions regarding using the cold key (Ed25519) vs the VRF key as authentication instruments; and arguments for both.\nOn the one hand, some prefer the use of the cold key because:\nThe cold key is meant to authenticate stake-pools activity (e.g. certificate registrations/updates).\nIt is ultimately the cold key that identifies a pool; its hash is the pool id.\nThe VRF is more likely to be compromised, hence granting rights to participate in a poll to potential adversaries.\nCold keys are Ed25519 keys, which allows piggybacking on the existing protocol's capabilities for transaction witnesses (extra required signer + verification key witnesses).\nOn the other hand, arguments for using the VRF key were already discussed as part of CIP-0022:\nBecause it's a hotkey, the VRF is usually more accessible, so it is more likely to lead to higher participation in surveys and no exposure of the cold key is needed.\nBlocks contain VRF proofs, which serve as explicit pool identifiers.\nIt is only necessary to check that a key is correct at the moment of the poll, making VRF keys perfectly suitable.\nWe originally opted for a hybrid solution (as visible in input-output-hk#5050) but later decided to drop the VRF option to rely solely on cold key signing (see input-output-hk#5132). The reason for that regards the possible uncertainty of promoting (ab)use of VRF proving in the cardano-cli on such a short time period (see also Insecurity of secret key re-usage).\nThis has the unfortunate effect of making this participation procedure harder for SPOs relying on cold storage but we are open to the idea of proxy-keys authenticated off-chain through a challenge similar to CIP-0022.\nThere's a third on-chain element which we could use for identifying SPOs which is a digital signature from their KES credentials. It is however a bit more annoying to leverage mainly because KES are meant to expire and are only loosely tied to pools by operational certificate. Thus, verifying KES signatures on a survey requires a more complex setup and monitoring to keep track of operational certificates and their validity at the time of the survey.\nIf this CIP was meant to NOT be an interim solution, this is something we would likely consider. However, given the timeframe we're looking at and the overall trade-offs in complexity, we have opted out of using the KES as an authentication mechanism in this iteration.\nAnother possible alternative to what's described in the CIP would be to have SPOs register a proxy Ed25519 key and use that proxy key onward. The validity of the proxy key registration would be conditionned to the production of an associated VRF proof or a digital signature from the cold key (very much like it's done for operational certificate).\nYet, like the KES alternative, this option is in conflict with some of the design goals of this CIP: simplicity. All the more so given that we want to maximise participation of SPOs to the various surveys. We aim to make the process of participating to the survey as simple as possible, without compromising on security.\nNote Both alternative options for KES Signing and Proxy Keys may be re-considered in a future version of the survey. Especially if the solution turns out to be not as temporary as intended. Fortunately, the current design decisions do not preclude this from happening as it shall be possible to introduce two new witness types 6 and 7 for those purpose. The KES registration can be handled through a separate on-chain event.\n6\n7\nQuestions are meant to be unique, achieved using an optional nonce. It is up to the genesis entity conducting the poll to ensure the formulated question is unique. If the same question is asked several times, the nonce provides non-replayable protection.\nThen, because every answer contains a (unique) hash of the question, answers are unique too. Yet, it still means that the same answer can be recast multiple times (possibly, by another system actor), so we do not allow answers to be changed/cast multiple times. The only exception is when answers are authenticated again using a cold key.\nExposure to SPOs' secret credentials must be limited, and their manipulation shall be done carefully. This potential attack vector is why we propose to extend the cardano-cli and have support for these features scrutinised by existing core maintainers and other open source actors.\ncardano-cli\nOther tools are then free to replicate the approach taken in the cardano-cli, but we recommend that SPOs proceed with extreme caution when using third-party tools. In particular, any tool should be able to work fully offline to produce the required metadata. Final transaction construction and submission shall be made in any suitable environment, yet the metadata's production shall be done only in air-gapped systems.\nThe Cardano Foundation has conducted a first trial poll on mainnet (CardanoScan / AdaStat)\nVisible agreement and engagement from a large set of SPOs Multiple SPOs workshops ~800 stake pools participating on the first mainnet poll ~11B stake answered the first mainnet poll\nMultiple SPOs workshops\n~800 stake pools participating on the first mainnet poll\n~11B stake answered the first mainnet poll\nProvide a reference implementation for the signing method cardano-cli has been updated to provide support for constructing and signing relevant transactions. Created scripts to crawl the chain for results.\nProvide a reference implementation for the signing method\ncardano-cli has been updated to provide support for constructing and signing relevant transactions.\ncardano-cli\nCreated scripts to crawl the chain for results.\nPossibly add support for KES signing as an alternative to EdDSA from the cold key and the VRF proving.\nPossibly add support for KES signing as an alternative to EdDSA from the cold key and the VRF proving.\ncncli has been updated with similar support\ncncli\nCardanoScan now lists available and past polls directly on their web UI.\nCardanoScan\nAdaStat now lists available and past polls directly on their web UI.\nAdaStat\ncardano-signer might be updated with similar support\ncardano-signer\nAnnounce a testnet run (on Preprod) and invite SPOs to a workshop session to conduct a testnet poll. See the Preprod poll on AdaStat.\nSee the Preprod poll on AdaStat.\nPossibly do a second test run, but on mainnet this time.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0095 | Web-Wallet Bridge - Conway ledger era\n\nThis document describes an interface between webpage/web-based stacks and Cardano wallets. This specification defines the API of the javascript object that needs to be injected into web applications.\nThese definitions extend CIP-30 | Cardano dApp-Wallet Web Bridge to provide support for CIP-1694 | A First Step Towards On-Chain Decentralized Governance focussed web-based stacks. Here we aim to support the requirements of Ada holders and DReps in the Conway Ledger era, this specification is based on the Conway Ledger Era Specification.\nFor the many contributors to this proposal, see Acknowledgements.\nCIP-1694 introduces many new concepts, entities and actors to Cardano; describing their implementation at the ledger level. This creates the need for new tooling with respect to governance. For the average ecosystem participant, the details should be abstracted away, enabling them to leverage the new ledger features more effectively. This specification allows for creation of web-based tools for the utilization of CIP-1694's governance features.\nWhilst CIP-30 facilitated the launch of dApp development on Cardano, it's functionality is limited in scope. It was written well before the emergence of the Conway Ledger Era and thus lacks the required methods to support full user interaction. We believe that expecting existing CIP-30 implementors to upgrade implementations is unfeasible, thus we must extend it's functionality with this API.\nThis proposal enables Ada holders, and DReps to engage web-based tooling through wallets. Thus the primary stakeholders for this proposal are tool developers and wallet providers. Here we aim to outline all endpoints needed to be exposed to web based tools to support all the needs Ada holders and DReps to engage with CIP-1694's governance design.\nWe define the following section as an extension to the specification described within CIP-30. Although currently CIP-30 acts as the defacto Cardano dApp-wallet connector, this specification could be applied to similar standards.\nNote This specification will evolve as the proposed ledger governance model is finalized.\nFrom CIP-30's Data Types we inherit:\nA string representing an address in either bech32 format, or hex-encoded bytes. All return types containing Address must return the hex-encoded bytes format, but must accept either format for inputs.\nAddress\nA hex-encoded string of the corresponding bytes.\nA hex-encoded string representing CBOR corresponding to T defined via CDDL either inside of the Shelley Multi-asset binary spec or, if not present there, from the CIP-0008 signing spec. This representation was chosen when possible as it is consistent across the Cardano ecosystem and widely used by other tools, such as cardano-serialization-lib, which has support to encode every type in the binary spec as CBOR bytes.\nT\ntype DataSignature = {| signature: cbor<COSE_Sign1>, key: cbor<COSE_Key>, |};\ntype DataSignature = {| signature: cbor<COSE_Sign1>, key: cbor<COSE_Key>, |};\nAn extension is an object with a single field \"cip\" that describe a CIP number extending the API (as a plain integer, without padding). For example:\n\"cip\"\n{ \"cip\": 30 }\n{ \"cip\": 30 }\ntype PubDRepKey = string;\ntype PubDRepKey = string;\nA hex-encoded string representing 32 byte Ed25519 DRep public key, as described in CIP-0105 | Conway Era Key Chains for HD Wallets.\ntype DRepID = string;\ntype DRepID = string;\nA hex-encoded string representing a registered DRep's ID which is a Blake2b-224 hash digest of the above mentioned 32 byte Ed25519 public key, as described in CIP-1694 Registered DReps.\ntype PubStakeKey = string;\ntype PubStakeKey = string;\nA hex-encoded string representing 32 byte Ed25519 public key used as a staking credential.\nFor the methods described in Governance Extension API, we inherit APIError, DataSignError and TxSignError from CIP-30's Error Types.\nNote We choose to reword some descriptions from CIP-30, to improve clarity.\nWe repurpose this error type from CIP-30, extending it's functionality. We extend the Refused error code to also include the case of the extension no longer being enabled.\nRefused\nAPIErrorCode { InvalidRequest: -1, InternalError: -2, Refused: -3, AccountChange: -4, } type APIError { code: APIErrorCode, info: string }\nAPIErrorCode { InvalidRequest: -1, InternalError: -2, Refused: -3, AccountChange: -4, } type APIError { code: APIErrorCode, info: string }\nInvalidRequest - Inputs do not conform to this specification or are otherwise invalid.\nInvalidRequest\nInternalError - An internal wallet error occurred during execution of this API call.\nInternalError\nRefused - The request was refused due to lack of access - e.g. wallet disconnects or extension is no longer enabled.\nRefused\nAccountChange - The account has changed. The client application should call wallet.enable() to reestablish connection to the new account. The wallet should not ask for confirmation as the user was the one who initiated the account change in the first place.\nAccountChange\nwallet.enable()\nWe repurpose this error type from CIP-30, extending it's functionality. We extend the ProofGeneration error code to also include cases where DRep secret key is not available. We also add one new error code DeprecatedCertificate.\nProofGeneration\nDeprecatedCertificate\nTxSignErrorCode = { ProofGeneration: 1, UserDeclined: 2, DeprecatedCertificate: 3, }; type TxSignError = { code: TxSignErrorCode; info: String; };\nTxSignErrorCode = { ProofGeneration: 1, UserDeclined: 2, DeprecatedCertificate: 3, }; type TxSignError = { code: TxSignErrorCode; info: String; };\nProofGeneration - User has accepted the transaction sign, but the wallet was unable to sign the transaction. This is because the wallet does have some of the private keys required.\nProofGeneration\nUserDeclined - User declined to sign the transaction.\nUserDeclined\nDeprecatedCertificate - Returned regardless of user consent if the transaction contains a deprecated certificate.\nDeprecatedCertificate\nWe repurpose this error type from CIP-30, extending it's functionality. We extend the ProofGeneration error code to also include cases where DRep secret key is not available.\nProofGeneration\nDataSignErrorCode { ProofGeneration: 1, AddressNotPK: 2, UserDeclined: 3, } type DataSignError = { code: DataSignErrorCode, info: String }\nDataSignErrorCode { ProofGeneration: 1, AddressNotPK: 2, UserDeclined: 3, } type DataSignError = { code: DataSignErrorCode, info: String }\nProofGeneration - Wallet could not sign the data; because the wallet does not have the secret key to the associated with the address or DRep ID.\nProofGeneration\nAddressNotPK - Address was not a P2PK address and thus had no SK associated with it.\nAddressNotPK\nUserDeclined - User declined to sign the data.\nUserDeclined\nThese are the CIP-95 methods that should be returned as part of the API object, namespaced by cip95 without any leading zeros.\ncip95\nFor example: api.cip95.getPubDRepKey()\napi.cip95.getPubDRepKey()\nTo access these functionalities a client application must present this CIP-95 extension object, as part of the extensions object passed at enable time:\ncardano.{wallet-name}.enable({ extensions: [{ cip : 95 }]})\ncardano.{wallet-name}.enable({ extensions: [{ cip : 95 }]})\napi.cip95.getPubDRepKey(): Promise<PubDRepKey>\nThe connected wallet account provides the account's public DRep Key, derivation as described in CIP-0105.\nThese are used by the client to identify the user's on-chain CIP-1694 interactions, i.e. if a user has registered to be a DRep.\nThe wallet account's public DRep Key.\nAPIError\nInvalidRequest\nAPIError\nInternalError\nAPIError\nRefused\nAPIError\nAccountChange\napi.getRegisteredPubStakeKeys(): Promise<PubStakeKey[]>\nThe connected wallet account's registered public stake keys. These keys may or may not control any Ada, but they must all have been registered via a stake key registration certificate. This includes keys which the wallet knows are in the process of being registered (already included in a pending stake key registration certificate).\nIf none of the wallets stake keys are registered then an empty array is returned.\nThese keys can then be used by the client to identify the user's on-chain CIP-1694 interactions, and also create vote delegation certificates which can be signed via .signTx().\n.signTx()\nAn array of the connected user's registered public stake keys.\nAPIError\nInvalidRequest\nAPIError\nInternalError\nAPIError\nRefused\nAPIError\nAccountChange\napi.cip95.getUnregisteredPubStakeKeys(): Promise<PubStakeKey[]>\nThe connected wallet account's unregistered public stake keys. These keys may or may not control any Ada. This includes keys which the wallet knows are in the process of becoming unregistered (already included in a pending stake key unregistration certificate).\nIf the wallet does not know the registration status of it's stake keys then it should return them as part of this call. If all of the wallets stake keys are registered then an empty array is returned.\nThese keys can then be used by the client to identify the user's on-chain CIP-1694 interactions, i.e if a user has delegated to a DRep.\nAn array of the connected user's unregistered stake keys.\nAPIError\nInvalidRequest\nAPIError\nInternalError\nAPIError\nRefused\nAPIError\nAccountChange\napi.signTx(tx: cbor<transaction>, partialSign: bool = false): Promise<cbor<transaction_witness_set>>\nThis endpoint requests the wallet to inspect and provide appropriate witnesses for a supplied transaction. The wallet should articulate this request from client application in a explicit and highly informative way.\nHere we extend the capabilities of CIP-30's .signTx(). To allow signatures with drep_credential and recognition of Conway ledger era transaction fields and certificates.\n.signTx()\ndrep_credential\nAs read from cardano-ledger Conway draft specification.\nSupporting wallets should be able to recognize and inspect all the following certificates and data contained in transaction bodies, in any combination.\nstake_registration\nstake_deregistration\nstake_delegation\npool_registration\npool_retirement\nreg_cert\nunreg_cert\nvote_deleg_cert\nstake_vote_deleg_cert\nstake_reg_deleg_cert\nvote_reg_deleg_cert\nstake_vote_reg_deleg_cert\nauth_committee_hot_cert\nresign_committee_cold_cert\nreg_drep_cert\nunreg_drep_cert\nupdate_drep_cert\nvoting_procedure\nproposal_procedure\ncoin\npositive_coin\nAll other potential transaction field inclusions 0-18, should be able to be recognized by supporting wallets.\nIn the Conway ledger era two certificate types are deprecated genesis_key_delegation and move_instantaneous_rewards_cert. If the wallet receives a transaction containing a deprecated certificate it should return a TxSignError with an error code of DeprecatedCertificate.\ngenesis_key_delegation\nmove_instantaneous_rewards_cert\nTxSignError\nDeprecatedCertificate\ngenesis_key_delegation\nmove_instantaneous_rewards_cert\nAlthough constitutional committee certificates and stake pool certificates should be able to be recognized they should not be able to be correctly witnessed by wallets following this API. Wallet's should only support witnesses using payment, stake and DRep keys.\nThe portions of the witness set that were signed as a result of this call are returned. This encourages client apps to verify the contents returned by this endpoint before building the final transaction.\npartialSign\nAPIError\nInvalidRequest\ntrue\nfalse\nAPIError\nInternalError\ntrue\nfalse\nAPIError\nRefused\ntrue\nfalse\nAPIError\nAccountChange\ntrue\nfalse\nTxSignError\nProofGeneration\nfalse\nTxSignError\nUserDeclined\ntrue\nfalse\nTxSignError\nDeprecatedCertificate\ntrue\nfalse\nIf partialSign is true, the wallet only tries to sign what it can. If partialSign is false and the wallet could not sign the entire transaction, TxSignError shall be returned with the ProofGeneration code.\npartialSign\ntrue\npartialSign\nfalse\nTxSignError\nProofGeneration\napi.cip95.signData(addr: Address | DRepID, payload: Bytes): Promise<DataSignature>\nErrors: APIError, DataSignError\nAPIError\nDataSignError\nThis endpoint requests the wallet to inspect and provide a DataSignature for the supplied data. The wallet should articulate this request from client application in a explicit and highly informative way.\nHere we extend the capabilities of CIP-30's .signData(). To allow for signatures using DRep keys.\n.signData()\nThis endpoint utilizes the CIP-0008 | Message Signing for standardization/safety reasons. It allows the client app to request the user to sign a payload conforming to said spec.\nHere we define how each key is identified by an Address in relation to CIP-0019 | Cardano Addresses, these are all Shelley key-hash-based addresses.\nAddress\nWe allow for DRepID to be passed in the addr field to signal signature using the associated DRep key.\nDRepID\naddr\nTo construct an address for DRep Key, the client application should construct a type 6 address. Using an appropriate Network Tag and a hash of a public DRep Key.\naddr\nDRepID\nThese keys will be used to sign the COSE_Sign1's Sig_structure with the following headers set:\nCOSE_Sign1\nSig_structure\nalg (1) - must be set to EdDSA (-8)\nalg\nEdDSA\nkid (4) - Optional, if present must be set to the same value as in the COSE_key specified below. It is recommended to be set to the same value as in the \"address\" header.\nkid\nCOSE_key\n\"address\"\n\"address\" - must be set to the raw binary bytes of the address as per the binary spec, without the CBOR binary wrapper tag\n\"address\"\nThe payload is not hashed and no external_aad is used.\nexternal_aad\nThe return shall be a DataSignature with signature set to the hex-encoded CBOR bytes of the COSE_Sign1 object specified above and key shall be the hex-encoded CBOR bytes of a COSE_Key structure with the following headers set:\nDataSignature\nsignature\nCOSE_Sign1\nkey\nCOSE_Key\nkty (1) - must be set to OKP (1).\nkty\nOKP\nkid (2) - Optional, if present must be set to the same value as in the COSE_Sign1 specified above.\nkid\nCOSE_Sign1\nalg (3) - must be set to EdDSA (-8).\nalg\nEdDSA\ncrv (-1) - must be set to Ed25519 (6).\ncrv\nEd25519\nx (-2) - must be set to the public key bytes of the key used to sign the Sig_structure.\nx\nSig_structure\nAPIError\nInvalidRequest\nAPIError\nInternalError\nAPIError\nRefused\nAPIError\nAccountChange\nDataSignError\nProofGeneration\nDataSignError\nAddressNotPK\nDataSignError\nUserDeclined\nWhilst this CIP is in it's unmerged proposed state, it remains very fluid and substantial changes can happen, so we would advise against any implementation. Once more feedback is received, maturing this design, implementations can safely emerge, alongside this proposal's merger into the CIPs repository. Once merged only small necessary changes should be made, ideally in backwards compatible fashion.\nThis, in tandem with, maturing implementations should move this proposal to an active state where only small backwards compatible changes can be made. If any large changes are needed once active then a new proposal should be made to replace this one. This we believe aligns with the (new) extendibility design of CIP-0030.\nThis describes a potential flow of connection between CIP-95 compatible client application and wallet, then a subsequent login.\nConnection: User indicates to the client their intent to connect, causing client offer a list of supported wallets, user selects their desired wallet. The client will then invoke .{wallet-name}.enable({extensions: [{ \"cip\": 95 }]}) from the shared cardano namespace, ensuring to pass in the CIP-95 extension object. Wallet Confirmation: The wallet indicates through its UI the clients intent to connect, the user should then grant permission. Share Credentials: The client invokes both .getRegisteredPubStakeKeys() and .getPubDRepKey(), causing the connected wallet to share relevant credentials. Chain Lookup: The client uses a chain indexer to work out the governance state of the provided credentials. The results of the lookup are then shown to the user, acting as a login.\nConnection: User indicates to the client their intent to connect, causing client offer a list of supported wallets, user selects their desired wallet. The client will then invoke .{wallet-name}.enable({extensions: [{ \"cip\": 95 }]}) from the shared cardano namespace, ensuring to pass in the CIP-95 extension object.\n.{wallet-name}.enable({extensions: [{ \"cip\": 95 }]})\ncardano\nWallet Confirmation: The wallet indicates through its UI the clients intent to connect, the user should then grant permission.\nShare Credentials: The client invokes both .getRegisteredPubStakeKeys() and .getPubDRepKey(), causing the connected wallet to share relevant credentials.\n.getRegisteredPubStakeKeys()\n.getPubDRepKey()\nChain Lookup: The client uses a chain indexer to work out the governance state of the provided credentials. The results of the lookup are then shown to the user, acting as a login.\nlogin\nAssume a DRep Aggregator and Delegation specialized client app, that aggregates DRep metadata from DRep registration certificates and renders this metadata to show prospective delegators. Assume that connection to a users wallet has already been made via cardano.{wallet-name}.enable({extensions: [{ cip: 95 }]}).\ncardano.{wallet-name}.enable({extensions: [{ cip: 95 }]})\nChoose DRep: User browses DReps and selects one which align's with their values to delegate too. It is up to the client application to choose and manage which stake key should be used for this delegation, this could be with or without user input. Construct Delegation: The client application uses CIP-30 endpoints to query the wallet's UTxO set and payment address. A DRep delegation certificate (vote_deleg_cert) is constructed by the app using the chosen DRep's ID and wallet's stake credential. A transaction is constructed to send 1 ADA to the wallet's payment address with the certificate included in the transaction body. Inspect and Sign: The app passes the transaction to the wallet via .signTx(). The wallet inspects the content of the transaction, informing the user of the client app's intension. If the user confirms that they are willing to sign, the wallet returns the appropriate witnesses, of payment key and stake key. Submit: The app will add the provided witnesses into the transaction body and then pass the witnessed transaction back to the wallet for submission via .submitTx(). Feedback to user: The wallet returns the submitted transaction's hash, the app can use this to track the status of the transaction on-chain and provide feedback to the user.\nChoose DRep: User browses DReps and selects one which align's with their values to delegate too. It is up to the client application to choose and manage which stake key should be used for this delegation, this could be with or without user input.\nConstruct Delegation: The client application uses CIP-30 endpoints to query the wallet's UTxO set and payment address. A DRep delegation certificate (vote_deleg_cert) is constructed by the app using the chosen DRep's ID and wallet's stake credential. A transaction is constructed to send 1 ADA to the wallet's payment address with the certificate included in the transaction body.\nvote_deleg_cert\nInspect and Sign: The app passes the transaction to the wallet via .signTx(). The wallet inspects the content of the transaction, informing the user of the client app's intension. If the user confirms that they are willing to sign, the wallet returns the appropriate witnesses, of payment key and stake key.\n.signTx()\nSubmit: The app will add the provided witnesses into the transaction body and then pass the witnessed transaction back to the wallet for submission via .submitTx().\n.submitTx()\nFeedback to user: The wallet returns the submitted transaction's hash, the app can use this to track the status of the transaction on-chain and provide feedback to the user.\nAssume a DRep Registration specialized client app, that allows people to register as a DRep. Assume that connection to a users wallet has already been made via cardano.{wallet-name}.enable({extensions: [{ cip: 95 }]}) and that the user is not a registered DRep.\ncardano.{wallet-name}.enable({extensions: [{ cip: 95 }]})\nUser Indicates Intent: User indicates to the client that they wish to register as a DRep. The client asks the user to provide metadata anchor, this is bundled with DRepID the client generates from the wallet's public DRep Key provided via .getPubDRepKey(). Construct Registration: The client application uses CIP-30 endpoints to query the wallet's UTxO set and payment address. A DRep registration certificate (reg_drep_cert) is constructed by the app using the wallet's DRep ID and the provided metadata anchor. A transaction is constructed to send 1 ADA to the wallet's payment address with the certificate included in the transaction body. Inspect and sign: The app passes the transaction to the wallet via .signTx(). The wallet inspects the content of the transaction, informing the user of the client app's intension. If the user confirms that they are happy to sign, the wallet returns the appropriate witnesses, of payment key and DRep key (drep_credential). Submit: The app will add the provided witnesses into the transaction body and then pass the witnessed transaction back to the wallet for submission via .submitTx(). Feedback to user: The wallet returns the submitted transaction's hash, the app can use this to track the status of the transaction on-chain and provide feedback to the user.\nUser Indicates Intent: User indicates to the client that they wish to register as a DRep. The client asks the user to provide metadata anchor, this is bundled with DRepID the client generates from the wallet's public DRep Key provided via .getPubDRepKey().\n.getPubDRepKey()\nConstruct Registration: The client application uses CIP-30 endpoints to query the wallet's UTxO set and payment address. A DRep registration certificate (reg_drep_cert) is constructed by the app using the wallet's DRep ID and the provided metadata anchor. A transaction is constructed to send 1 ADA to the wallet's payment address with the certificate included in the transaction body.\nreg_drep_cert\nInspect and sign: The app passes the transaction to the wallet via .signTx(). The wallet inspects the content of the transaction, informing the user of the client app's intension. If the user confirms that they are happy to sign, the wallet returns the appropriate witnesses, of payment key and DRep key (drep_credential).\n.signTx()\ndrep_credential\nSubmit: The app will add the provided witnesses into the transaction body and then pass the witnessed transaction back to the wallet for submission via .submitTx().\n.submitTx()\nFeedback to user: The wallet returns the submitted transaction's hash, the app can use this to track the status of the transaction on-chain and provide feedback to the user.\nThe principle aim for this design is to reduce the complexity for wallet implementors whilst maintaining backwards compatibility with CIP-30 implementations. This is motivated by the necessity for users to be able to interact with the age of Voltaire promptly, by keeping the wallet's providers ask small we aim to reduce implementation time.\nThis design aims to make the tracking of a user's governance state an optional endeavour for wallet providers. This is achieved by placing the responsibility on clients to track a user's governance state, i.e. if a wallet user is a DRep, what DRep a wallet user has delegated to, etc.\nDespite only defining the minimal set of endpoints required, we do not wish to discourage the creation of subsequent CIPs with a wider range of governance functionality. Nor does this specification aim to discourage wallet providers from fully integrating governance features, side-stepping the necessity for this API (matching how staking is achieved).\nWeb-based stacks, with wallet connectivity, are a familiar place for users to be able to interact with Cardano. These tools lower the technical bar to engage with the ecosystem. Thus we believe encouraging further adoption of this approach is beneficial.\nThe primary alternative approach is for wallet providers to integrate this functionality fully inside of wallet software, matching how staking is often implemented. We deem this approach as preferable from a security standpoint, we would encourage wallet providers to pursue this. But we understand that this adds significant overhead to wallet designs, so we offer this API as an alternative.\nThis proposal only caters to two types of governance actor described in CIP-1694; Ada holders and DReps, this decision was three fold. Primarily, this is to allow these groups to utilize a web-based client to participate in Cardano's governance. These groups are likely less comfortable utilizing command-line interfaces than other groups, thus making alternatives from them is a priority. Secondly, the other types of actor (constitutional committee members and SPOs) are identified by different credentials than Ada holders and DReps, making their integration in this specification more complex. These alternative credentials are unlikely to be stored within standard wallet software which may interface with this API. Thirdly, Ada holders and DReps likely represent the majority of participants thus we aim to cast a wide net with this specification.\nIn this specification we have placed explicit boundaries on what should not be supported with .signTx(). Those being not witnessing stake pool or constitutional committee, certificates and not inspecting genesis key delegation or MIR certificates.\n.signTx()\nFrom speaking to CIP-30 implementors it seems reasonable that there does not exist implementations or motivation to support witnessing stake pool certificates via wallet web bridges. This is because stake pool operators much prefer the utility and security advantages not operating via light wallets. Due to the Lack of Specificity of CIP-30 we felt it necessary to explicitly state the lack of support in this extension.\nConstitutional committee certificates are not supported by this specification's .signTx() for two reasons. First, this specification is only focussed on the need's of Ada holders and DReps. Secondly, the credentials used by the constitutional committee, are a hot and cold key setup. Hot and cold keys are not suited for standard light wallets.\n.signTx()\nGenesis key delegation and move instantaneous reward certificates (see in Shelley spec) are not supported here because they have been deprecated in the Conway ledger era. Furthermore, due to the lack of accessibility (require access to genesis keys) for these certificates it is extremely unlikely any CIP-30 implementations supported these.\nThe endpoints specified here aim to maintain the role of the wallet as: sharing public keys, transaction inspecting, transaction signing and transaction submission.\nIn a previous design we had stipulated the precise information that must be shown to user by wallets at signature time. This was discussed during the wallets and tooling hackathon and consensus was reached that is not the place of these APIs to prescribe such details to wallets. Rather this specification should be describing the interface between web-based stacks and wallets and not telling wallets what UI elements should be used. It is in a wallet's best interest to always adequately inform the user, with varying levels of detail based on the wallet's discretion.\nBy not placing the burden of transaction construction onto the wallet, we move the application specific complexity from wallet implementations and onto applications. This has a number of benefits, primarily this should lower the bar for wallet adoption. But this also helps in the creation of iterative updates, all wallet implementers do not need to update if the format of these transactions is adjusted during development.\nHere we also benefit from imitating the existing flows which have been utilized by CIP-30 compliant systems. Reusing existing flows is beneficial for developer adoption as it enables straight forward code reuse.\nOne argument against this design is that, if wallets are required to be able to inspect and thus understand these application specific transactions then they may as well build the transaction. Ultimately, we have taken the design decision to leave transaction construction to the applications.\nWhilst CIP-30 facilitated the launch of dApp client development on Cardano, it's functionality is limited in scope. Although it does offer generic functions, these cannot satisfy the problem that this proposal tackles in a backwards compatible manner. Thus extending it's functionality is a necessity.\nThe CIP-30 specification has required amendments to add clarification to it's ambiguity. There is further ambiguity around what is and is not supported via .signTx(). The specification does not explicitly list the transaction artifacts wallets has to be able to inspect and witness. Whilst for most use cases this is likely fine and has served the community well. We forsee issues around large ledger upgrades which introduce new types of transaction fields and certificate. Without explicit mention of what is and is not supported deltas between expected and actual functionality become common and hazardous. This is why we choose to explicitly list those items that wallet have to support when complying with this API.\n.signTx()\nWith this specification we chose to extend CIP-30's functionalities. There would be two competing designs to this approach. One; move to have this specification included within CIP-30. Two; deploy this specification as it's own standalone web-bridge.\nIt would be undesirable to include this functionality within the base CIP-30 API because it would force all wallets supporting CIP-30 to support this API. This is undesirable because not all client apps or wallets will have the need or desire to support this specification.\nThe reason we chose to not deploy this specification on its own is because it is unlikely that clients implementing this API will not want to also use the functionality offered by CIP-30. Additionally, CIP-30 offers a extensibility mechanism meaning that the initial handshake connection is defined and thus wont be needed to be defined within this specification.\nFurthermore, another benefit of utilizing the CIP-30 extensibility mechanism is the potential for siloing of wallet capabilities between client apps. By having to request access to each extension wallets and users are able to silo which extensions they allow to each client application. An example of this could be only allowing the CIP-95 API with governance related applications and not decentralized extensions.\nThe primary issue with just using CIP-30 to inspect, sign and submit Conway transactions/certificates is that wallet implementations are likely incompatible. This is because such certificates/transactions were not part of the ledger design at time of original CIP-30 implementation. Furthermore, CIP-30 was written and implemented before voting credentials were defined and thus it would be impossible to provide signatures with this credential to votes, DRep registrations and DRep retirements.\nAlthough it is likely that some of the capabilities of this API can be achieved by existing CIP-30 implementations it is not certain how much. We would like to avoid the potential mismatching of capabilities between CIP-30 implementations, as this creates unpredictable wallet behavior for client applications. Such behavior was a primary motivator to introduce such an extendability mechanism to CIP-30.\nIn this specification we have chosen to explicitly namespace all endpoint except .signTx() where we omit the namespacing. By not namespacing .signTx() we intend to offer client apps an override of the CIP-30 .signTx(). We chose to do this because this .signTx() extends the CIP-30 functionality in a backwards compatible way. All other endpoints are namespaced to avoid possible collisions with other future extensions.\n.signTx()\n.signTx()\n.signTx()\n.signTx()\nIn this design we chose to extend the capabilities of CIP-30's .signTx() and .signData() rather than introducing new endpoints for signing and submission. This was a result of community discussion at the wallets and tooling hackathon, leading to a more straight forward design. Originally we had individual endpoints for sign and submitting of DRep registration, DRep retirement, votes, governance actions and vote delegations.\n.signTx()\n.signData()\nWhilst individual endpoints seem like a simpler solution they would likely introduce more complexities for wallet implementors. As constraining what transactions an endpoint would require additional validation complexities and error handling from wallets. For example; what should a wallet do if it is passed a DRep registration certificate via .signTx()? should it witness or reject and only witness with a dedicated DRep registration endpoint?\n.signTx()\nFurthermore individual restrictive endpoints limit how much can be done in a single transaction. These methods would not allow multiple certificates to be supplied at once. Thus client apps would be limited to a single governance artifact per transaction. This is limiting as it means users have to submit multiple transactions to achieve what is possible in one.\nBy providing updated CIP-30 endpoints we essentially use the CIP-95 extension object as a flag to signal to apps at connection time a wallet's compatibility with Conway leger era. This goes against how past ledger feature upgrades have rolled out. Rather in the past, existing CIP-30 implementors have just updated their implementations, we believe this to be an error prone approach. This undoubted introduces deltas between what a client application expects a wallet to be able to do and what it can do. There is no way for a client application to know what a wallet is capable of.\nDespite this we do not discourage CIP-30 implementors from updating their implementations to support Conway artifacts to .signData() and .signTx(). But if so they must support the CIP-95 object flag at connection time, so that clients are aware of this functionality.\n.signData()\n.signTx()\nAlthough multi-stake key wallets are not widely adopted across Cardano, we make an effort to support them here. This is because a single stake key can delegate to a single DRep. By allowing users to spread stake across multiple stake keys it allows different weighted delegation to different DReps, this could be a very desirable feature.\nThis specification does not cater for wallets who manage non-key-based stake credentials and those who wish to handle non-key-based DRep credentials. This does limit the usefulness of this specification. But the complexities that would be introduced by generalization this specification to these credentials is unlikely to yield much benefit since these types of wallet are not prevalent in Cardano.\nAlthough this means that we are likely excluding tooling for DAOs from being supported through this standard. The argument could be made that such entities generally prefer to use more advanced wallet tooling rather than relying on interaction with web-based stacks, thus it is not even certain DAOs would want to use such a standard.\nUnlike the CIP-30 specification we have made an decision, where possible to represent keys as raw hex rather than encoded representations. We believe that it is not the role of the wallet to encode such credentials, encoding needs should be at the application's discretion. Introducing different encodings into this API would add unneeded complexity.\nFurthermore, in this API we chose to return public keys over key-hashes or addresses. Again we believe wallets should just serve public key information and it is up to the application to encode and derive addresses as needed. This simplifies the overall design and makes implementations easier for wallets.\nFor stake keys we have chosen to implement two endpoints where wallets can share registered and unregistered stake keys. Originally we had a single endpoint which only allowed sharing of registered stake keys. This was problematic for wallets which had no registered stake keys, and thus the second endpoint was introduced.\nWe chose to keep a single endpoint for DRep keys, although it would have been possible to introduce a second to allow for wallets to activity of their DRep keys. This was just for the simplicity of the API. Furthermore, due to the design of this proposal it is unlikely that wallets will implement methods to track a user's DRep state.\nThis proposal should not effect the backwards compatibility of either clients or wallet implementors.\nCIP-62? | Cardano dApp-Wallet Web Bridge Catalyst Extension is another extension to the CIP-30 API, this proposal is independent of CIP-95. The CIP-95 specification does not rely on any of the implementation defined in CIP-62?. We have attempted to avoid any collisions of naming between these proposals, this was motivated by a desire to make wallet implementations more straight forward for wallets implementing both APIs.\nThe burden of transaction building to be placed on dApps or wallets? As we are replacing CIP-30's signTx it makes sense to follow the same flow and place the burden on the client applications.\nAs we are replacing CIP-30's signTx it makes sense to follow the same flow and place the burden on the client applications.\nDoes supporting governance action submission a necessary burden for the scope of this proposal? Since moving burden of transaction construction from wallet to app, this becomes much less of an issue as the complex error checking should now be done by the application.\nSince moving burden of transaction construction from wallet to app, this becomes much less of an issue as the complex error checking should now be done by the application.\nshould provide support for combination certificates? Yes we will support ALL conway ledger era Tx/Certs, this will allow for CIP95 to be \"the Conway compatible\" wallet web bridge.\nYes we will support ALL conway ledger era Tx/Certs, this will allow for CIP95 to be \"the Conway compatible\" wallet web bridge.\nIs it necessary to provide a method to prove ownership of DRep key? Yes, this will be a useful add.\nYes, this will be a useful add.\nIs it sensible to place multi-stake key burden onto clients? Yes, seems like a reasonable approach. If wallets want to manage it, they can only provide the keys they wish.\nYes, seems like a reasonable approach. If wallets want to manage it, they can only provide the keys they wish.\nDo we need to share stake keys or can we just reuse reward addresses? Reusing CIP30's .getRewardAddresses() may act as an alternative, but it is unclear how implementors have supported this function and thus its reuse maybe a mistake. It is a more reasonable approach to share public key material instead of addresses as it gives the client application more freedom.\nReusing CIP30's .getRewardAddresses() may act as an alternative, but it is unclear how implementors have supported this function and thus its reuse maybe a mistake.\n.getRewardAddresses()\nIt is a more reasonable approach to share public key material instead of addresses as it gives the client application more freedom.\nShould this proposal cater for non-key-based stake credential? We can leave this for a future iteration.\nWe can leave this for a future iteration.\nMove DRep key definitions be moved into another CIP? Yes, this is a cleaner approach, as we keep the purity of this proposal to being a wallet web bridge.\nYes, this is a cleaner approach, as we keep the purity of this proposal to being a wallet web bridge.\nShould there be a way for the optional sharing of governance state, from wallet to client? We leave this for future CIPs.\nWe leave this for future CIPs.\nShould DRep key be moved into CIP-1852? Yes it will be moved to it's own CIP with reference added to CIP-1852.\nYes it will be moved to it's own CIP with reference added to CIP-1852.\nThe interface is supported by three wallet providers. Nufi Lace Yoroi demos wallet\nNufi\nLace\nYoroi\ndemos wallet\nThe interface is used by one web application to allow users to engage with the Conway ledger design. SanchoNet GovTool GovTool cip95-cardano-wallet-connector drep-campaign-platform\nSanchoNet GovTool\nGovTool\ncip95-cardano-wallet-connector\ndrep-campaign-platform\nThe interface is supported via libraries. Cardano JS-SDK purescript-cip95 Mesh SDK\nCardano JS-SDK\npurescript-cip95\nMesh SDK\nProvide a public Discord channel for open discussion of this specification. See wallets-sanchonet channel in the IOG Technical Discord under (to view you have to opt-in to the Sanchonet group in the start-here channel).\nSee wallets-sanchonet channel in the IOG Technical Discord under (to view you have to opt-in to the Sanchonet group in the start-here channel).\nwallets-sanchonet\nAuthor to engage with wallet providers for feedback.\nAuthor to run a hackathon workshop with wallet providers. In person and online hackathon run 2023.07.13, outcomes presented here: CIP-95 pull request comment.\nIn person and online hackathon run 2023.07.13, outcomes presented here: CIP-95 pull request comment.\nResolve all Open Questions.\nAuthor to provide test dApp to test against. See cip95-cardano-wallet-connector.\nSee cip95-cardano-wallet-connector.\nAuthor to provide a reference wallet implementation. See cip95-demos-wallet.\nSee cip95-demos-wallet.\nAuthor to produce a set of test vectors for wallets to test against.\nAuthor to move DRep key definitions to a separate CIP. via the addition of CIP-105 | Conway era Key Chains for HD Wallets via CIPs PR #597.\nvia the addition of CIP-105 | Conway era Key Chains for HD Wallets via CIPs PR #597.\nOn 2023.07.13 a online and in person community hackathon took place, aims of this event included maturation of the design of this specification.\nWe would like to thank the following attendees for providing their valuable insights:\nPiotr Czeglik - Lace\nMircea Hasegan - Lace\nAlex Apeldoorn - Lace\nMichal Szorad - Yoroi\nJavier Bueno - Yoroi\nEd Eykholt - Blocktrust\nVladimir Volek - Five Binaries\nMarek Mahut - Five Binaries\nMarkus Gufler - Cardano Foundation\nMichal Ciborowski - BinarApps\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0099 | Proof of Onboarding\n\nSince at least 2021 when Cardano entered the Mary Era and implemented Native Assets, projects and creators building on the network have sought a means to distribute their tokens efficiently to users. Of particular frustration has been the ability to onboard new users at real-world events. Solutions for this historically have been to create and preload \"paper wallets\" where a seed phrase and accompanying password is generated by the project, pre-populated with tokens and ADA, and then delivered over to attendees of said event.\nThe creation of this addendum to the CIP-13 Cardano URI scheme seeks to minimize this friction and take advantage of existing technology to enable a new era of user onboarding, particularly at real world events through the use of a defined URI scheme enabling instant and frictionless communication between a project's \"token fountain\" and the user's wallet to reward, incentivize, and onboard new users easier than ever.\nThis CIP defines an extension to the CIP-13 URI Scheme as well as an API specification to facilitate and streamline communications between wallets and project servers.\nBy leveraging the power of Cardano-specific URIs (CIP-13) and the modern technological advances of mobile devices and wallets we can provide a framework for Cardano projects to attend real world events, incentivize or reward attendees via their Native Assets, and have facts and figures to help support and analyze the impact that their attendance had (Proof of Onboarding).\nDistributing Native Assets (and/or ADA) to attendees of IRL events has historically been a pain point in the ecosystem. Some implemented solutions have included: Pre-generating wallet seed phrases and pre-populating these wallets with a minimum amount of ADA as well as the desired Native Assets, (re)creating token fountain/faucet designs which can be cumbersome and not user-friendly to instruct individuals to install a wallet, visit a website, enter a code and claim tokens.\nThe Cardano Token Claim URI schema is proposed to allow wallets (particularly mobile wallets) to implement a QR-friendly URI structure allowing for easy onboarding and distribution of Native Assets and/or ADA to individuals in a variety of situations but not least of which being at IRL events specifically tailored and geared to onboarding new users to the ecosystem.\nExamples:\n<!-- Token Claim URIs --> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=consensus2023\">Claim $HOSKY</a> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io%2Fconsensus23&code=ABC123\">Claim $HOSKY</a> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.nftxlv.com&code=ABC123&invoice=123456\">Claim NFTxLV Commermorative NFT!</a>\n<!-- Token Claim URIs --> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=consensus2023\">Claim $HOSKY</a> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io%2Fconsensus23&code=ABC123\">Claim $HOSKY</a> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.nftxlv.com&code=ABC123&invoice=123456\">Claim NFTxLV Commermorative NFT!</a>\nFor a token claim URI (authority = claim), a versioning path, a faucet_url and a required code.\nclaim\nfaucet_url\ncode\nAdditional query parameters may be provided and should be passed through to the provided faucet_url without modification.\nfaucet_url\ncardanourn = \"web+cardano:\" claimtokenref claimtokenref = \"//claim\" claimversion claimquery claimversion = \"/v1\" claimquery = ( \"?\" claimurl) ( \"&\" claimcode) claimurl = \"faucet_url=\" text claimcode = \"code=\" text\ncardanourn = \"web+cardano:\" claimtokenref claimtokenref = \"//claim\" claimversion claimquery claimversion = \"/v1\" claimquery = ( \"?\" claimurl) ( \"&\" claimcode) claimurl = \"faucet_url=\" text claimcode = \"code=\" text\nAll arguments for Token Claim URIs should be URL-encoded\nVersion 1 URIs\nVersion 1 URIs must include a faucet_url and a code as required parameters.\nfaucet_url\ncode\nURIs may include additional arguments to suit the needs of the project's faucet API.\nThe token claim URI should consist of a required versioning path (i.e. /v1) as well as one or more required or optional URL-encoded arguments.\npath\n/v1\nAll Token Claim URIs must include a URL-encoded faucet_url argument as well as a code argument.\nfaucet_url\ncode\nThe wallet provider should send a POST request to the provided Faucet URL that includes:\nFaucet URL\nThe change/receipt wallet address of the user\nAny additional arguments specified in the URI as key: value pairs\nExample:\nURI: web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io%2Fconsensus23&code=ABC123 Version: 1 URL: https://claim.hosky.io/consensus23 CODE: ABC123 JSON POST Data:\nURI: web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io%2Fconsensus23&code=ABC123 Version: 1 URL: https://claim.hosky.io/consensus23 CODE: ABC123 JSON POST Data:\n{ \"address\": \"addr1abc...xyz\", \"code\": \"ABC123\" }\n{ \"address\": \"addr1abc...xyz\", \"code\": \"ABC123\" }\naddress\nThe wallet should always send the recipient address in bech32 format. If a particular token faucet implementation wishes to restrict or limit access to their faucet based on staking key or individual wallet address, this should be handled at the server end.\nSupported Addresses\nShelley-era enterprise address consisting of only a payment key\nenterprise\nShelley-era staking address consisting of a payment and staking key\nstaking\ncode\nThe code is required. Specifying a code allows for reliable tracking and/or limiting of claims to the faucet host. Codes can be used to identify attendees of particular events (i.e. CODE = consensus2023) or can be a unique, one-time code per user (i.e. CODE = abc123xyz987). In this way we leave the code to be flexible to match a variety of analytical use cases depending upon the needs of the implementing project.\ncode\nconsensus2023\nabc123xyz987\nWallets should prompt/warn users prior to sending potentially sensitive information (wallet address + code) via the token claim URI. An informational pop-up or confirmation modal should be displayed to users such as: We are about to send your address and code 123456 to https://claim.hosky.io. Are you sure you want to proceed?\nWallets should prompt/warn users prior to sending potentially sensitive information (wallet address + code) via the token claim URI. An informational pop-up or confirmation modal should be displayed to users such as: We are about to send your address and code 123456 to https://claim.hosky.io. Are you sure you want to proceed?\nWe are about to send your address and code 123456 to https://claim.hosky.io. Are you sure you want to proceed?\nThe envisioned process flow for the POO Protocol is as follows:\nThe project set asides some amount of budget (tokens + Lovelace [minUTxO]) for a given marketing push or IRL event If desired, one or more codes are generated to help track and analyze claiming figures QR Code(s) may be generated, printed, and otherwise displayed or given to users during the course of events Users scan the code with their mobile light wallet The light wallet makes a POST request to the API endpoint specified in the Cardano URI containing the user's wallet address and the included code (if present) The project API returns a documented status code indicating the success or failure of the operation If a successful status is detected and returned, the project issues tokens to the specified address per their campaign settings\nThe project set asides some amount of budget (tokens + Lovelace [minUTxO]) for a given marketing push or IRL event\nIf desired, one or more codes are generated to help track and analyze claiming figures\ncodes\nQR Code(s) may be generated, printed, and otherwise displayed or given to users during the course of events\nUsers scan the code with their mobile light wallet\nThe light wallet makes a POST request to the API endpoint specified in the Cardano URI containing the user's wallet address and the included code (if present)\nThe project API returns a documented status code indicating the success or failure of the operation\nIf a successful status is detected and returned, the project issues tokens to the specified address per their campaign settings\nThe URI format consists of the CIP-13 web+cardano:// scheme, followed by the claim authority, then a version path.\nweb+cardano://\nclaim\nversion\nNOTE: ALL ARGUMENTS SHOULD BE URL-ENCODED\nVersion 1 URIs must include /v1 as the path of the URI.\n/v1\nVersion 1 URIs must include two required arguments:\nfaucet_url as a fully-typed URL (i.e. https://claim.hosky.io)\nfaucet_url\ncode as either a campaign identifier or unique, one-time use code\ncode\nVersion 1 URIs may include additional query parameters that should be passed through to the api server.\nVersion 1 Examples:\n<!-- A Cardano Claim URI with campaign identifier code --> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=consensus2023\">Thanks for attending Consensus 2023!</a> <!-- A Cardano Claim URI with unique, one-time use code --> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=dff6508d8dfb4e128fd67e9ff54af147\">Claim your $HOSKY now!</a> <!-- A Cardano Claim URI with a campaign-specific code and optional user_id argument --> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=NFTxLV2023&user_id=Idjiot1337\">Get your $HOSKY!</a>\n<!-- A Cardano Claim URI with campaign identifier code --> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=consensus2023\">Thanks for attending Consensus 2023!</a> <!-- A Cardano Claim URI with unique, one-time use code --> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=dff6508d8dfb4e128fd67e9ff54af147\">Claim your $HOSKY now!</a> <!-- A Cardano Claim URI with a campaign-specific code and optional user_id argument --> <a href=\"web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=NFTxLV2023&user_id=Idjiot1337\">Get your $HOSKY!</a>\nLight wallets that detect and support web+cardano URIs as well as mobile wallets who detect either a QR code or other link with this format should parse the URI and send a POST request to the specified URL containing a JSON payload including:\nweb+cardano\nPOST\nThe user's wallet receive address\nThe code\nAdditional URI query parameters passed through\nURI: web+cardano://claim/v1?faucet_url=https 3A 2F 2Fclaim.nftxlv.com&code=NFTxLV2023\nweb+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.nftxlv.com&code=NFTxLV2023\nFaucet URL: https://claim.nftxlv.com\nhttps://claim.nftxlv.com\nPOST JSON Data:\n{ \"address\": \"addr1abc...xyz\", \"code\": \"NFTxLV2023\" }\n{ \"address\": \"addr1abc...xyz\", \"code\": \"NFTxLV2023\" }\nURI: web+cardano://claim/v1?faucet_url=https 3A 2F 2Fclaim.nftxlv.com&code=NFTxLV2023\nweb+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.nftxlv.com&code=NFTxLV2023\nFaucet URL: https://claim.hosky.io\nhttps://claim.hosky.io\nPOST JSON Data:\n{ \"address\": \"addr1abc...xyz\", \"code\": \"ABC123\" }\n{ \"address\": \"addr1abc...xyz\", \"code\": \"ABC123\" }\nURI: web+cardano://claim/v1?faucet_url=https 3A 2F 2Fclaim.nftxlv.com&code=NFTxLV2023&user_id=Adam1337\nweb+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.nftxlv.com&code=NFTxLV2023&user_id=Adam1337\nFaucet URL: https://claim.nftxlv.com\nhttps://claim.nftxlv.com\nPOST JSON Data:\n{ \"address\": \"addr1abc...xyz\", \"code\": \"NFTxLV2023\", \"user_id\": \"Adam1337\" }\n{ \"address\": \"addr1abc...xyz\", \"code\": \"NFTxLV2023\", \"user_id\": \"Adam1337\" }\nThe API server is expected to return one of the following defined status blocks in application/json format. Any other responses from the API server should be considered invalid and discarded or display an error.\napplication/json\nThe expected API that any token fountain implementation should follow and wallet integrators should expect is documented on Swagger!\nVersion 1\nFirst Successful Request\n{ \"code\": 200, \"lovelaces\": \"2000000\", \"queue_position\": 23, \"status\": \"accepted\", \"tokens\": { \"a0028f350aaabe0545fdcb56b039bfb08e4bb4d8c4d7c3c7d481c235.484f534b59\": \"29433292000000\" } }\n{ \"code\": 200, \"lovelaces\": \"2000000\", \"queue_position\": 23, \"status\": \"accepted\", \"tokens\": { \"a0028f350aaabe0545fdcb56b039bfb08e4bb4d8c4d7c3c7d481c235.484f534b59\": \"29433292000000\" } }\nSubsequent Successful Request (Address + Code Match) prior to token distribution\n{ \"code\": 201, \"lovelaces\": \"2000000\", \"queue_position\": 1, \"status\": \"queued\", \"tokens\": { \"a0028f350aaabe0545fdcb56b039bfb08e4bb4d8c4d7c3c7d481c235.484f534b59\": \"29433292000000\" } }\n{ \"code\": 201, \"lovelaces\": \"2000000\", \"queue_position\": 1, \"status\": \"queued\", \"tokens\": { \"a0028f350aaabe0545fdcb56b039bfb08e4bb4d8c4d7c3c7d481c235.484f534b59\": \"29433292000000\" } }\nSubsequent Successful Request (Address + Code Match) after token(s) are distributed\n{ \"code\": 202, \"lovelaces\": \"2000000\", \"status\": \"claimed\", \"tokens\": { \"a0028f350aaabe0545fdcb56b039bfb08e4bb4d8c4d7c3c7d481c235.484f534b59\": \"29433292000000\" }, \"tx_hash\": \"TX1234\" }\n{ \"code\": 202, \"lovelaces\": \"2000000\", \"status\": \"claimed\", \"tokens\": { \"a0028f350aaabe0545fdcb56b039bfb08e4bb4d8c4d7c3c7d481c235.484f534b59\": \"29433292000000\" }, \"tx_hash\": \"TX1234\" }\nThe provided address is not a valid Cardano address\n{ \"code\": 400, \"status\": \"invalidaddress\" }\n{ \"code\": 400, \"status\": \"invalidaddress\" }\nNo code was provided in the request\n{ \"code\": 400, \"status\": \"missingcode\" }\n{ \"code\": 400, \"status\": \"missingcode\" }\nThe wallet provided is from the wrong network (testnet/mainnet)\n{ \"code\": 400, \"status\": \"invalidnetwork\" }\n{ \"code\": 400, \"status\": \"invalidnetwork\" }\nThe specified code does not exist\n{ \"code\": 404, \"status\": \"notfound\" }\n{ \"code\": 404, \"status\": \"notfound\" }\nAn address was already used (if not code present) or the code presented was found but the address did not match\n{ \"code\": 409, \"status\": \"alreadyclaimed\" }\n{ \"code\": 409, \"status\": \"alreadyclaimed\" }\nFor time-limited fountains, a code of 410 means that the period for redemption has expired\n{ \"code\": 410, \"status\": \"expired\" }\n{ \"code\": 410, \"status\": \"expired\" }\nFor time-limited fountains, a code of 425 means that the period for redemption has not begun yet\n{ \"code\": 425, \"status\": \"tooearly\" }\n{ \"code\": 425, \"status\": \"tooearly\" }\nRate limiting settings and details are left to the discretion and implementation of individual projects. A status code of 429 or this status response should be considered as a rate limiting response.\n{ \"code\": 429, \"status\": \"ratelimited\" }\n{ \"code\": 429, \"status\": \"ratelimited\" }\nImplementations should of course be prepared to handle situations where a server is non-responsive for any reason and be prepared to handle any other, non-specified error codes including 500 codes.\nIf there is sufficient justification in the future for modification of this standard to the point that a \"Version 2\" would be necessary, those changes MUST be submitted as a new, separate CIP to this repository and follow all applicable CIP standards for acceptance. Examples of \"major\" changes that might justify a new version of this CIP include: fundamentally altering the URI structure, adding or removing a required field, or any other non-backwards compatible changes to the Process Flow.\nMinor changes for grammar, clarity, or functionality that fall within the scope of \"Version 1\" of this document may be made by editing this document directly. Such changes include: grammatical or exposition changes to improve readability or clarity of communication, improvements to documented code examples, additional or optional server response information, etc.\nBy creating a well-defined standard for both a CIP-13 URI scheme and the expected API response(s) we can create a framework that both wallets and projects can utilize to encourage and onboard new users into the ecosystem via Native Asset incentive models without needlessly and constantly reinventing the wheel for each product or project.\nFurthermore, the aforementioned \"paper wallet\" technique has many drawbacks including:\nThe person(s) responsible for generating the paper wallets at some point have access to the seed phrases generated, leading to a potential security vulnerability\nProjects would need to preload these wallets with funds/tokens; this makes it difficult and/or impossible to reliably know how many of the paper wallets were ever actually claimed\nFor those wallets that go forever unclaimed, this essentially creates a permanent \"burn\" of both Lovelace and the native assets of the project; less than ideal\nBy utilizing this framework, projects can have accurate, measurable analytics into the success of various real-world marketing and event efforts: Proof of Onboarding.\nDemonstrate a working MVP\nOpen source an MVP example of token faucet server-side code\nReceive feedback and iterate based on community feedback\nVESPR Mobile Wallet supports the Proof of Onboarding Protocol.\nYoroi Mobile Wallet supports the Proof of Onboarding Protocol.\nHOSKY Project has released an open source server-side implementation software that may be used as a proof of concept for any interested projects.\nMultiple projects at multiple, global events have successfully deployed Proof of Onboarding.\nOnboard additional wallet providers, server/service providers, and redemption methods.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0100 | Governance Metadata\n\nThis Cardano Improvement Proposal (CIP) introduces a standardized and flexible metadata format for governance events in the Cardano ecosystem. While the ledger does not enforce the structure of metadata, providing a standard for metadata will enable better tooling, improved interoperability, and more consistent display across wallets, blockchain explorers, and other tools. This CIP aims to strike a balance between flexibility and structure to facilitate high-quality tooling, without limiting expressivity with regards to user expression.\nFor the many contributors to this proposal, see Acknowledgements.\nWith the advent of the Voltaire era, Cardano is moving towards a decentralized governance model. CIP-1694 addresses a potential technical implementation of ledger rules for creating, voting on, and ratifying proposed changes to the ledger. The ledger has no mechanism or desire to validate this metadata, and as a result, the official specification leaves the format of this metadata unspecified.\nTo facilitate rich user experiences for the various governance actors, however, it would be beneficial to have a suggested universal format for this metadata, allowing deep and interconnected discovery and exploration of this metadata. This CIP seeks to provide that standard format, and a minimal set of fields for various governance actions, leaving the true depth of metadata to be defined later through the extensibility mechanism outlined below.\nWhile this specification is written with CIP-1694 in mind, many of the ideas should be equally suitable for any other governance proposal, provided that proposal has a mechanism for attaching metadata to a governance action.\nExplicitly, here are the goals this CIP is trying to satisfy:\nStandardize a format for rich metadata related to cardano governance\nStandardize a minimal and uncontroversial set of fields to support \"Minimal Viable Governance\"\nLeave that format open to extension and experimentation\nEnable tooling and ecosystem developers to build the best user experiences possible\nProvide a set of best practices that tooling and ecosystem developers can rely on for the health and integrity of this data\nThis section outlines the high level format and requirements of this standard.\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in [RFC 2119](https://www.rfc-editor.org/rfc/rfc2119).\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in [RFC 2119](https://www.rfc-editor.org/rfc/rfc2119).\nOn chain metadata actions in CIP-1694 (and likely any alternative proposals) have the notion of an \"Anchor\"; this is the URL and a hash for additional, non-ledger metadata about the action.\nTools which publish governance related transactions SHOULD publish metadata via these fields.\nWhile that content MAY be in any format, following any standard or non-standard, for the remainder of this document SHOULD/MUST will refer to documents that are following this specification.\nThe content hosted at the anchor URL MUST be a JSON-LD document according to the rest of this specification.\nThis document SHOULD include @context and @type fields below to aid in interpretation of the document.\n@context\n@type\nThe JSON document SHOULD be formatted for human readability, for the sake of anyone who is manually perusing the metadata.\nThat content SHOULD be hosted on a content addressable storage medium, such as IPFS or Arweave, to ensure immutability and long term archival.\nThe hash in the anchor MUST be the hash of the the raw bytes of the content, using the hashing algorithm specified in the hashAlgorithm field. Currently only blake2b-256 is supported.\nhashAlgorithm\nFor the purposes of hashing and signature validation, we should use the canonical RDF triplet representation, as outlined in the JSON-LD specification.\nJSON-LD is a standard for describing interconnected JSON documents that use a shared vocabulary.\nIn a JSON-LD document, every field is uniquely tied to some globally unique identifier by means of an Internationalized Resource Identifier (IRI). Different machine-consumers can then know that they agree on the interpretation of these fields.\nThe shared vocabulary of fields is standardized within the scope of a document via a @context field. This allows a compositional / extensible approach to versioning, similar to the recent changes to CIP-0030. Rather than specifying a version number and forcing competing standards to compete for what the \"next\" version number will include, instead a wide variety of standards are allowed and encouraged. Tool authors MAY support those which are the most beneficial or common. This creates an organic, collaborative evolution of the standard.\n@context\nNote: Any URI's in the @context field SHOULD be content-addressable and robustly hosted; losing access to the schema is less dangerous than losing access to the metadata itself, but should still prefer strong and immutable storage options for the preservation of context.\nA governance metadata document MAY include a @context field.\n@context\nA governance metadata document MAY include a @type field, referring to a specific type of document from the included @contexts.\n@type\n@context\nThe @context field, if included, MUST be a valid JSON-LD @context field; the basics are described below. It MAY be a string, in which case it MUST be the IRI of a jsonld document describing the shared vocabulary to assume when interpreting the document. It MAY be an object, where each key refers to a property name, and the value is either an IRI to a schema describing that field, or an object with that definition inlined. It MAY be an array, including multiple contexts, which are merged in order with a \"most-recently-defined-wins\" mechanism. For a full understanding of the @context field, refer to the JSON-LD specification.\n@context\nIt MAY be a string, in which case it MUST be the IRI of a jsonld document describing the shared vocabulary to assume when interpreting the document.\nIt MAY be an object, where each key refers to a property name, and the value is either an IRI to a schema describing that field, or an object with that definition inlined.\nIt MAY be an array, including multiple contexts, which are merged in order with a \"most-recently-defined-wins\" mechanism.\nFor a full understanding of the @context field, refer to the JSON-LD specification.\nEach IRI in the @context field SHOULD refer to a schema hosted via a robust, content addressable, and immutable storage medium such as IPFS, Arweave, etc.\n@context\nIf the metadata document is missing the @context field, it will be assumed to refer to [./cip-0100.common.jsonld]\n@context\nFuture CIPs may standardize common contexts, and SHOULD attempt to reuse common terminology and SHOULD avoid naming collisions.\nTool authors MAY choose which contexts to support, but MUST make a best effort to display the metadata in the presence of unrecognized context, up to and including gracefully falling back to a raw display of the JSON document.\nExtensions to the governance metadata standard can take one of two forms:\nCIP-0100 itself can be updated through the normal CIP process to provide additional clarity on any concepts that are giving people trouble with adoption, or to correct inaccuracies.\nA new CIP, which defines new JSON-LD vocabulary to extend this one, which seeks broad adoption.\nA new context document, used in the wild, but not officially standardized, and which doesn't seek broad adoption.\nIn CIP-1694 (and likely any alternative or future evolution of it), there are a number of certificates that can be attached to a transaction pertaining to governance; each of these is equipped with an \"anchor\", which is a URI at which can be found additional metadata.\nWhile this metadata can be published anywhere, external hosting may be unavailable to some users. Therefore, we recognize the transaction metadata as an effective tool for a \"common town square\" for hosting and discoverability, and reserve metadatum label 1694 for publishing governance related metadata on-chain.\nWith the help of CIP-?, the anchor can then refer to the metadata of another transaction, or even the metadata of the transaction being published itself.\nWhen published on-chain, the CBOR encoding of this metadata, when published on-chain, follows the standard convention used for converting JSON to CBOR in transaction metadata.\nAdditionally, someone may wish to augment a previous piece of metadata with new information, divorced from the transaction that initially published it; this may be useful, for example, provide additional arguments in favor of or against a proposal, provide translations to other languages, provide a layman's explanation of a highly technical document, etc.\nThese use the same format, but leverage the references field to point to the documents that they wish to extend.\nreferences\nThese can, in theory, be published anywhere, but the use of metadatum label 1694 mentioned above is particularly useful in this case.\nWe expect a rich ecosystem of CIPs to emerge defining different extensions, so this CIP provides only an initial base-line MVP of fields, defined in the following JSON-LD and JSON Schema files:\nJSON-LD Context\nJSON Schema\nThe rest of this document will provide a high level description of how these fields should be interpreted\nThe following properties are considered common to all types of transactions, and the minimal set needed for \"minimum viable governance\":\nhashAlgorithm: The algorithm used to hash the document for the purposes of the on-chain anchor; currently only supports blake2b-256\nhashAlgorithm\nauthors: The primary contributors and authors for this metadata document An array of objects Each object MAY have a name property, which serves as a display name Each object MUST have a witness field, which contains a signature of the body of the document The witness may define a @context, describing the manner in which the document has been witnessed A default witness scheme context is described in a later section Tooling authors SHOULD validate witnesses they understand If witnesses aren't validated, tooling authors SHOULD emphasize this to the user If absent or invalid, tooling authors SHOULD make this clear to the user\nauthors\nAn array of objects\nEach object MAY have a name property, which serves as a display name\nname\nEach object MUST have a witness field, which contains a signature of the body of the document\nwitness\nbody\nThe witness may define a @context, describing the manner in which the document has been witnessed\n@context\nA default witness scheme context is described in a later section\nTooling authors SHOULD validate witnesses they understand\nIf witnesses aren't validated, tooling authors SHOULD emphasize this to the user\nIf absent or invalid, tooling authors SHOULD make this clear to the user\nbody: The material contents of the document, separate for the purposes of signing references An array of objects Each object specifies a @type, which is either \"GovernanceMetadata\" or \"Other\" Each object specifies a label, which serves as a human readable display name Each object specifies a uri; when the type is set to \"GovernanceMetadata\", the URI should point to another CIP-0100 compliant document comment Freeform text attached to the metadata; richer structures for justifying the transaction this is attached to is left to future CIPs Tooling authors SHOULD emphasize that these comments represent the authors views, and may contain bias. externalUpdates A array of objects Each object can have a @context defining how to interpret it by default, assumed to just be an object with a title and uri field The purpose is to allow \"additional updates\", that aren't written yet, such as a blog or RSS feed Tooling authors MAY fetch and parse this metadata according to this standard, If so, Tooling authors MUST emphasize that this information is second-class, given that it might have changed\nbody\nreferences An array of objects Each object specifies a @type, which is either \"GovernanceMetadata\" or \"Other\" Each object specifies a label, which serves as a human readable display name Each object specifies a uri; when the type is set to \"GovernanceMetadata\", the URI should point to another CIP-0100 compliant document\nreferences\nAn array of objects\nEach object specifies a @type, which is either \"GovernanceMetadata\" or \"Other\"\n@type\nEach object specifies a label, which serves as a human readable display name\nlabel\nEach object specifies a uri; when the type is set to \"GovernanceMetadata\", the URI should point to another CIP-0100 compliant document\nuri\ncomment Freeform text attached to the metadata; richer structures for justifying the transaction this is attached to is left to future CIPs Tooling authors SHOULD emphasize that these comments represent the authors views, and may contain bias.\ncomment\nFreeform text attached to the metadata; richer structures for justifying the transaction this is attached to is left to future CIPs\nTooling authors SHOULD emphasize that these comments represent the authors views, and may contain bias.\nexternalUpdates A array of objects Each object can have a @context defining how to interpret it by default, assumed to just be an object with a title and uri field The purpose is to allow \"additional updates\", that aren't written yet, such as a blog or RSS feed Tooling authors MAY fetch and parse this metadata according to this standard, If so, Tooling authors MUST emphasize that this information is second-class, given that it might have changed\nexternalUpdates\nA array of objects\nEach object can have a @context defining how to interpret it\n@context\nby default, assumed to just be an object with a title and uri field\ntitle\nuri\nThe purpose is to allow \"additional updates\", that aren't written yet, such as a blog or RSS feed\nTooling authors MAY fetch and parse this metadata according to this standard,\nIf so, Tooling authors MUST emphasize that this information is second-class, given that it might have changed\nAdditionally, we highlight the following concepts native to json-ld that are useful in the context of governance metadata:\n@language The @context field SHOULD specify a @language property, which is an ISO 639-1 language string, to define a default language for the whole document Specific sub-fields can specify different languages The @context field may specify a @container property set to @language, in which case the property becomes a map with different translations of the property Tooling authors MAY provide automatic translation, but SHOULD make the original prose easily available\nThe @context field SHOULD specify a @language property, which is an ISO 639-1 language string, to define a default language for the whole document\n@context\n@language\nSpecific sub-fields can specify different languages\nThe @context field may specify a @container property set to @language, in which case the property becomes a map with different translations of the property\n@context\n@container\n@language\nTooling authors MAY provide automatic translation, but SHOULD make the original prose easily available\nWhen publishing a governance action, the certificate has an \"anchor\", defined as a URI and a hash of the content at the URI.\nFor CIP-0100 compliant metadata, the hash in the anchor should be the blake2b-256 hash of the raw bytes received from the wire. Hashing directly the original bytes ensures that there are no ambiguities, since the process doesn't depend on parsing the metadata, which can be the source of conflicts in different implementations.\nA metadata has a number of authors, each of which MUST authenticate their endorsement of the document in some way.\nThis is left extensible, through the use of a new context, but for the purposes of this CIP, we provide a simple scheme.\nEach author should have a witness. The witness will be an object with an witnessAlgorithm (set to ed25519), a publicKey (set to a base-16 encoded ed25519 public key), and a signature, set to the base-16 encoded signature of the body field.\nwitnessAlgorithm\npublicKey\nsignature\nBecause the overall document may change, it is neccesary to hash a subset of the document that is known before any signatures are collected. This is the motivation behind the body field.\nbody\nThe signature is an ed25519 signature using the attached public key, and the payload set to the blake2b-256 hash of the body field. Specifically, this field is canonicalized in the following way.\nbody\nCanonicalize the whole document according to this specification.\nIdentify the node-ID of the body node\nbody\nFilter the canonicalized document to include the body node, and all its descendents\nEnsure the file ends in a newline\nHash the resulting file with blake2b-256\nThis section outlines a number of other best practices for tools and user experiences built on top of this standard, as brainstormed by the Cardano community.\nIf the hash in the anchor doesn't match the computed hash of the content, it is imperative to make that obvious to the user. Without this being obvious, there are severe and dramatic attack vectors for manipulating user votes, delegators, etc. NOTE: The term \"MUST\" in the RFC-2119 sense isn't used here because it's unenforcable anyway, but if these hashes aren't checked, you SHOULD inform the user that you are not checking the integrity of the data. You MAY do this by displaying a prominent warning, or potentially fully barring access to the content.\nWithout this being obvious, there are severe and dramatic attack vectors for manipulating user votes, delegators, etc.\nNOTE: The term \"MUST\" in the RFC-2119 sense isn't used here because it's unenforcable anyway, but if these hashes aren't checked, you SHOULD inform the user that you are not checking the integrity of the data.\nYou MAY do this by displaying a prominent warning, or potentially fully barring access to the content.\nYou SHOULD provide a way to access the raw underlying data for advanced or diligent users. This MAY be in the form of a JSON viewer, or a simple link to the content.\nThis MAY be in the form of a JSON viewer, or a simple link to the content.\nYou SHOULD gracefully degrade to a simple raw content view if the metadata is malformed in some way, or not understood.\nYou SHOULD provide links and cross references whenever the metadata refers to another object in some way For example, a proposal may link to the sponsoring DReps, which may have their own view within the tool you're building\nFor example, a proposal may link to the sponsoring DReps, which may have their own view within the tool you're building\nIf you are hosting the content for the user, you SHOULD use a content-addressable hosting platform such as IPFS or Arweave\nIf the content is self-hosted, you SHOULD take care to warn the user about changing the content For example, you CAN detect well-known content-addressable file storage platforms such as IPFS or Arweave, and display an extra warning if the content is not hosted on one of those\nFor example, you CAN detect well-known content-addressable file storage platforms such as IPFS or Arweave, and display an extra warning if the content is not hosted on one of those\nHere are the goals this CIP seeks to achieve, and the rationale for how this specific solution accomplishes them:\nStandardize a format for rich metadata related to cardano governance Standardizing on JSON-LD provides an industry standard, yet highly flexible format for effectively arbitrary structured data\nStandardizing on JSON-LD provides an industry standard, yet highly flexible format for effectively arbitrary structured data\nStandardize a minimal and uncontroversial set of fields to support \"Minimal Viable Governance\" This CIP specifies a minimal number of fields: hash-algorithm, authors, justification, external-updates, and @language Each of these fields is essential for the global accessibility of this data, and enables tooling that promotes a well-informed voting populace\nThis CIP specifies a minimal number of fields: hash-algorithm, authors, justification, external-updates, and @language\nEach of these fields is essential for the global accessibility of this data, and enables tooling that promotes a well-informed voting populace\nLeave that format open to extension and experimentation JSON-LD has, built in, a mechanism for extending and experimenting with new field types Anyone can extend this metadata, even independent of the CIP process, with their own field definitions Tooling authors can, independently, choose which extensions to support natively, while also surfacing fields they don't recognize in more generic ways.\nJSON-LD has, built in, a mechanism for extending and experimenting with new field types\nAnyone can extend this metadata, even independent of the CIP process, with their own field definitions\nTooling authors can, independently, choose which extensions to support natively, while also surfacing fields they don't recognize in more generic ways.\nEnable tooling and ecosystem developers to build the best user experiences possible The @context field of a JSON-LD document allows tooling authors to confidently and consistently interpret a known field within the metadata, with reduced risk of misinterpreting or misrepresenting the authors intent This metadata can also reference other objects and documents in the ecosystem, providing for rich exploration needed for an informed voting populace.\nThe @context field of a JSON-LD document allows tooling authors to confidently and consistently interpret a known field within the metadata, with reduced risk of misinterpreting or misrepresenting the authors intent\n@context\nThis metadata can also reference other objects and documents in the ecosystem, providing for rich exploration needed for an informed voting populace.\nProvide a set of best practices that tooling and ecosystem developers can rely on for the health and integrity of this data This CIP has an explicit section of best practices, brainstormed with attendees of a workshop dedicated to the purpose.\nThis CIP has an explicit section of best practices, brainstormed with attendees of a workshop dedicated to the purpose.\nThe following alternatives were considered, and rejected:\nPlain JSON documents While ultimately flexible and simple, there is a risk that with no way to structure what is officially supported, and the interpretation of each field, tooling authors would have one hand tied behind their back, and would be limited to a minimum common denominator.\nWhile ultimately flexible and simple, there is a risk that with no way to structure what is officially supported, and the interpretation of each field, tooling authors would have one hand tied behind their back, and would be limited to a minimum common denominator.\nCanonicalising the whole document before hashing it Canonicalising requires initially parsing the file as a json, which can by itself cause ambiguities\nCanonicalising requires initially parsing the file as a json, which can by itself cause ambiguities\nA custom JSON format, with reference to CIPs\nAn initial draft of this proposal had an extensions field that operated very similar to @context\nextensions\n@context\nInstead, this CIP chose to go with an industry standard format to leverage the existing tooling and thought that went into JSON-LD\nCBOR or other machine encoding\nThe metadata in question, despite being proliferous, is not expected to to be an undue storage burden; It's not, for example, video data, or storing billions of records.\nIt is more important, then, that the metadata be human readable, so that tooling authors have the option to show this data in its raw format to a user, and for it to be loosely understandable even by non-technical users.\nSee test-vector.md for example.\nThe path for this proposal to be considered active within the community focuses on 4 key stages: Feedback, Implementation, Adoption, and Extension.\nIn order for this standard to be active, the following should be true:\nAt least 1 month of feedback has been solicited, and any relevant changes with broad consensus to the proposal made\nAt least 2 client libraries in existence that support reading an arbitrary JSON file, and returning strongly typed representations of these documents\nAt least 1 widely used producer of governance metadata (such as a wallet, or the cardano-cli)\nAt least 1 widely used consumer of governance metadata (such as a blockchain explorer, governance explorer, voting dashboard, etc)\nAt least 1 CIP in the \"Proposed\" status that outlines additional fields to extend this metadata\nBelow you can find a growing list of community tools which let you sign / verify / canonize / manipulate governance metadata JSON-LD data:\ncardano-signer : A tool to sign with author secret keys, verify signatures, canonize the body content (Linux/Arm/Win/Mac)\ncardano-governance-metadata-lib : A rust library for interacting with Cardano Governance Metadata conforming to CIP-100 (rust)\nThe key stages to get this proposal to active, and the motivation for why each stage is valuable, is outlined below:\nSolicitation of feedback While this proposal represents the input of many prominent community members, it is by no means exhaustive This CIP should receive a moderate, but not egregious, amount of scrutiny and feedback within it's initial goals\nWhile this proposal represents the input of many prominent community members, it is by no means exhaustive\nThis CIP should receive a moderate, but not egregious, amount of scrutiny and feedback within it's initial goals\nImplementation The effectiveness of this standard is greatly amplified if tools and SDKs are built which allow parsing arbitrary data according to this standard Sundae Labs has offered to build Rust and Golang libraries that capture the types outlined above, and implementations in other languages are welcome\nThe effectiveness of this standard is greatly amplified if tools and SDKs are built which allow parsing arbitrary data according to this standard\nSundae Labs has offered to build Rust and Golang libraries that capture the types outlined above, and implementations in other languages are welcome\nAdoption This standard is most effective if it receives widespread adoption Therefore, a path to active includes engaging prominent members of the ecosystem, such as wallets and explorers, to begin producing and consuming documents in accordance with the standard.\nThis standard is most effective if it receives widespread adoption\nTherefore, a path to active includes engaging prominent members of the ecosystem, such as wallets and explorers, to begin producing and consuming documents in accordance with the standard.\nExtension Finally, this standard chooses to fully specify very little of the total surface area of what could be expressed in governance metadata Therefore, to prove the extensibility of the standard, at least one follow-up CIP should be drafted, extending the set of fields beyond \"Minimum Viable Governance\"\nFinally, this standard chooses to fully specify very little of the total surface area of what could be expressed in governance metadata\nTherefore, to prove the extensibility of the standard, at least one follow-up CIP should be drafted, extending the set of fields beyond \"Minimum Viable Governance\"\nI would like to thank those that contributed to this first draft during the online workshop that was held on 2023-04-12.\nCHIL Pool\nAlex Djuric\nCody Butz\nFelix Weber\nLeo Pienasola\nMarkus Gufler\nMichael Madoff\nMohamed Mahmoud\nThomas Upfield\nWilliam Ryan\nSantiago Carmuega\nThe following people helped with the second draft, out of band at at the Edinburgh workshop on 2023-07-12.\nRyan Williams\nMatthias Benkort\nAll Edinburgh Workshop attendees\nThe following people helped with the third draft during the online workshop held on 2023-11-02.\nMike Susko\nThomas Upfield\nLorenzo Bruno\nRyan Williams\nNils Peuser\nSantiago Carmuega\nNick Ulrich\nEp Ep\nThe following people helped with the third draft during the online workshop held on 2023-11-10.\nAdam Dean\nRhys Morgan\nThomas Upfield\nMarcel Baumberg\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0101 | Integration of keccak256 into Plutus\n\nThis CIP proposes an extension of the current Plutus functions to provide support for the keccak256 hashing algorithm, primarily to ensure compatibility with Ethereum's cryptographic infrastructure.\nkeccak256\nThe integration of the ECDSA and Schnorr signatures over the secp256k1 curve into Plutus was a significant step towards interoperability with Ethereum and Bitcoin ecosystems. However, full compatibility is still impossible due to the absence of the keccak256 hashing algorithm in Plutus interpreter, which is a fundamental component of Ethereum's cryptographic framework:\nkeccak256\ndata signing standard EIP-712,\nkeccak256 is the hashing algorithm underlying Ethereum's ECDSA signatures.\nkeccak256\nEVM heavily depends on keccak256 for internal state management\nkeccak256\nAdding keccak256 to Plutus would enhance the potential for cross-chain solutions between Cardano and EVM-based blockchains.\nkeccak256\nA compelling integration that would greatly benefit from keccak256 support on the Cardano blockchain is Hyperlane. Hyperlane is a permissionless interoperability layer that facilitates communication of arbitrary data between smart contracts across multiple blockchains. Hyperlane's interchain security modules rely on the verification of specific cryptographic proofs from one chain to another. These proofs utilize the keccak256 hash to calculate consistent cross-chain message IDs. The multi-signature module verifies that a majority of off-chain validators have signed an ECDSA signature over a keccak256 digest, a common practice in EVM.\nkeccak256\nkeccak256\nkeccak256\nWhile Hyperlane can support different cryptographic primitives for non-EVM chains, doing so could compromise censorship resistance, resulting in only limited support for Cardano in Hyperlane. By implementing this CIP, Cardano could fully integrate with Hyperlane's security modules, enabling Cardano smart contracts to communicate with any blockchain supported by Hyperlane.\nThis proposal aims to introduce a new built-in hash function keccak_256.\nkeccak_256\nThis function will be developed following the keccak256 specification and will utilize the cryptonite implementation. Since cryptonite is already a part of the cardano-base, this simplifies its integration into Plutus. The cost of the keccak_256 operation will scale linearly with the length of the message.\nkeccak256\ncryptonite\ncardano-base\nkeccak_256\nMore specifically, Plutus will gain the following primitive operation:\nkeccak_256 :: ByteString - ByteString\nkeccak_256\nThe input to this function can be a ByteString of arbitrary size, and the output will be a ByteString of 32 bytes. Note that this function aligns with the format of existing hash functions in Plutus, such as blake2b_256\nByteString\nByteString\nWhile the keccak256 function might be implemented in on-chain scripts, doing so would be computationally unfeasible.\nkeccak256\nThe library, cryptonite, is not implemented by and under control of the Plutus team. However,\nIt is a library already used in the Cardano stack to expose SHA3, and can be considered as a trustworthy implementation.\nThe function does not throw any exceptions as hash functions are defined to work with any ByteString input. It does not expect a particular particular structure.\nIt's behaviour is predictable. As mentioned above, the cost of the function is linear with respect to the size of the message provided as input. This is the same behaviour that other hash functions exposed in plutus (blake, sha3) have.\nThis CIP may transition to active status once the Plutus version containing the keccak_256 function is introduced in a node release and becomes available on Mainnet.\nkeccak_256\nA Plutus binding is created for the keccak256 function and included in a new version of Plutus.\nkeccak256\nIntegration tests, similar to those of the existing Plutus hash functions, are added to the testing infrastructure.\nThe function is benchmarked to assess its cost. As for other hash functions available in Plutus (blake2b and sha256), we expect the cost of keccak to be linear with respect to the size of the message. The Plutus team determines the exact costing functions empirically.\nThe ledger is updated to include new protocol parameters to control costing of the new builtins.\nThe Plutus team will develop the binding, integration tests, and benchmarks. The E2E tests will be designed and implemented collaboratively by the testing team, the Plutus team, and community members planning to use this primitive.\nThis CIP is licensed under [Apache-2.0][https://www.apache.org/licenses/LICENSE-2.0].\n2023 Cardano Foundation\n\n---\n\nCIP-0102 | Royalty Datum Metadata\n\nThis proposal makes use of the onchain metadata pattern established in CIP-0068 to provide a way to store royalties with greater assurance and customizability.\nThe inability to create trustless onchain royalty validation with CIP-0027 is a major drawback to Cardano NFTs. The pattern defined in CIP-68 represents an opportunity to upgrade the standard to support onchain validation. This CIP aims to eliminate that drawback and demonstrate better support for developers, NFT creators, and NFT collectors, ultimately attracting dapps & NFT projects that would otherwise have taken their talents to another blockchain.\nIn addition, this standard allows royalties to be split between multiple addresses, another limitation of the CIP-27 royalty schema. Future versions of this standard could also easily support multiple royalty policies defined for a single collection, applied at the level of individual tokens.\nThe following defines the 500 Royalty NFT standard with the registered asset_name_label prefix value\n500\nasset_name_label\nThe royalty NFT is an NFT (non-fungible token).\nroyalty NFT\nThe royalty NFT must have an identical policy id as the collection.\nroyalty NFT\npolicy id\nThe asset name must be 001f4d70526f79616c7479 (hex encoded), it contains the CIP-0067 label 500 followed by the word \"Royalty\".\nasset name\n001f4d70526f79616c7479\n500\nExample: royalty NFT: (500)Royalty reference NFT: (100)Test123\nroyalty NFT\n(500)Royalty\nreference NFT\n(100)Test123\nThe royalty info datum is specified as follows (CDDL):\nbig_int = int / big_uint / big_nint big_uint = #6.2(bounded_bytes) big_nint = #6.3(bounded_bytes) optional_big_int = #6.121([big_int]) / #6.122([]) royalty_recipient = #6.121([ address, ; definition can be derived from: ; https://github.com/input-output-hk/plutus/blob/master/plutus-ledger-api/src/PlutusLedgerApi/V1/Address.hs#L31 int, ; variable fee ( calculation: ⌊1 / (fee / 10)⌋ ); integer division with precision 10 optional_big_int, ; min fee (absolute value in lovelace) optional_big_int, ; max fee (absolute value in lovelace) ]) royalty_recipients = [ * royalty_recipient ] ; version is of type int, we start with version 1 version = 1 ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data royalty_info = #6.121([royalty_recipients, version, extra])\nbig_int = int / big_uint / big_nint big_uint = #6.2(bounded_bytes) big_nint = #6.3(bounded_bytes) optional_big_int = #6.121([big_int]) / #6.122([]) royalty_recipient = #6.121([ address, ; definition can be derived from: ; https://github.com/input-output-hk/plutus/blob/master/plutus-ledger-api/src/PlutusLedgerApi/V1/Address.hs#L31 int, ; variable fee ( calculation: ⌊1 / (fee / 10)⌋ ); integer division with precision 10 optional_big_int, ; min fee (absolute value in lovelace) optional_big_int, ; max fee (absolute value in lovelace) ]) royalty_recipients = [ * royalty_recipient ] ; version is of type int, we start with version 1 version = 1 ; Custom user defined plutus data. ; Setting data is optional, but the field is required ; and needs to be at least Unit/Void: #6.121([]) extra = plutus_data royalty_info = #6.121([royalty_recipients, version, extra])\n; Given a royalty fee of 1.6% (0.016) ; To store this in the royalty datum 1 / (0.016 / 10) => 625 ; To read it back 10 / 625 => 0.016\n; Given a royalty fee of 1.6% (0.016) ; To store this in the royalty datum 1 / (0.016 / 10) => 625 ; To read it back 10 / 625 => 0.016\nBecause the computational complexity of Plutus primitives scales with size, this approach significantly minimizes resource consumption.\nTo prevent abuse, it is recommended that the royalty NFT is stored at the script address of a validator that ensures the specified fees are not arbitrarily changed, such as an always-fails validator.\nroyalty NFT\nIf not specified elsewhere in the token's datums, a malicious user could send transactions to a protocol which do not reference the royalty datum. For full assurances, a new optional flag should be added to the reference datum\nextra = { ... ? royalty_included : big_int }\nextra = { ... ? royalty_included : big_int }\nIf the field is present and 1 the validators must require a royalty input.\nIf the field is present and set to 0 the validators don't need to search for a royalty input.\nIf the field is not present, validators should accept a royalty input, but not require one.\nIn-code examples can be found in the reference implementation.\nA third party has the following NFT d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken and they want to look up the royalties. The steps are\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken\nConstruct royalty NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(500)Royalty Look up royalty NFT and find the output it's locked in. Get the datum from the output and look up metadata by going into the first field of constructor 0. Convert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.\nConstruct royalty NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(500)Royalty\nroyalty NFT\nuser token\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(500)Royalty\nLook up royalty NFT and find the output it's locked in.\nroyalty NFT\nGet the datum from the output and look up metadata by going into the first field of constructor 0.\nConvert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.\nWe want to bring the royalty metadata of the NFT d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken in the Plutus validator context. To do this we\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken\nConstruct royalty NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(500)Royalty (off-chain) Look up royalty NFT and find the output it's locked in. (off-chain) Reference the output in the transaction. (off-chain) Verify validity of datum of the referenced output by checking if policy ID of royalty NFT and user token and their asset names without the asset_name_label prefix match. (on-chain)\nConstruct royalty NFT from user token: d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(500)Royalty (off-chain)\nroyalty NFT\nuser token\nd5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(500)Royalty\nLook up royalty NFT and find the output it's locked in. (off-chain)\nroyalty NFT\nReference the output in the transaction. (off-chain)\nVerify validity of datum of the referenced output by checking if policy ID of royalty NFT and user token and their asset names without the asset_name_label prefix match. (on-chain)\nroyalty NFT\nuser token\nasset_name_label\nThe specification here is made to be as minimal as possible. This is done with expediency in mind and the expectation that additional changes to the specification may be made in the future. The sooner we have a standard established, the sooner we can make use of it. Rather than attempting to anticipate all use cases, we specify with forward-compatibility in mind.\nThis specification is largely based on the royalty specification in Nebula, with a couple key departures:\nThe royalty token is recommended to be locked at a script address, rather than stored in the user's wallet. This encourages projects to guarantee royalties won't change by sending their royalties to an always-fails (or similar) script address, but still allows for creative royalty schemes and minimizes disruption to existing projects.\nThe royalty token is recommended to be locked at a script address, rather than stored in the user's wallet. This encourages projects to guarantee royalties won't change by sending their royalties to an always-fails (or similar) script address, but still allows for creative royalty schemes and minimizes disruption to existing projects.\nThe policyId of the royalty NFT must match that of the reference NFT. This enables lookups based on the user token in the same way as is done for the tokens specified in the original CIP-68 standard.\nThe policyId of the royalty NFT must match that of the reference NFT. This enables lookups based on the user token in the same way as is done for the tokens specified in the original CIP-68 standard.\nIn addition to providing a way to create guaranteed royalties, this has several advantages:\nBackwards Compatibility - Existing royalty implementations will still work, just not have the same assurances.\nMinimal Storage Requirement - An optional boolean has about the smallest memory impact possible. This is especially important because it's attached to the - Reference NFT and will be set for each individual NFT.\nIntra-Collection Utility - This already allows for minting a collection with some NFTs with royalties and some without. A future version of this standard will likely make use of this field to allow for multiple versions of royalties for even more granular control.\nTo keep metadata compatibility with changes coming in the future, we introduce a version field in the datum.\nversion\nSee the CIP-0068 Extension Boilerplate\nThis CIP should receive feedback, criticism, and refinement from: CIP Editors and the community of people involved with NFT projects to review any weaknesses or areas of improvement.\nGuidelines and examples of publication of data as well as discovery and validation should be included as part of of criteria for acceptance.\nMinimal reference implementation making use of Lucid (off-chain), PlutusTx (on-chain): Reference Implementation.\nImplementation and use demonstrated by the community: NFT Projects, Blockchain Explorers, Wallets, Marketplaces.\nPublish open source reference implementation and instructions related to the creation, storage and reading of royalty utxos.\nImplement in open source libraries and tooling such as Lucid, Blockfrost, etc.\nAchieve additional \"buy in\" from existing community actors and implementors such as: blockchain explorers, token marketplaces, minting platforms, wallets.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0103 | Web-Wallet Bridge - Bulk transaction signing\n\nThis CIP extends CIP-30 (Cardano dApp-Wallet Web Bridge) to provide an additional endpoint for dApp to sign multiple transactions in bulk.\nCurrently, there is no way to sign multiple transactions in bulk, and the experience of signing a chain of transactions is suboptimal. We propose the addition of a signTxs endpoint that enable wallets to create an array of interconnected transactions and sign them all at once.\ntype TransactionSignatureRequest = {| cbor: cbor<transaction>, partialSign: bool = false, |};\ntype TransactionSignatureRequest = {| cbor: cbor<transaction>, partialSign: bool = false, |};\nUsed to represent a single transaction awaiting a user's signature. More details on {partialSign} can be found in api.signTx defined in CIP-30.\nSee CIP-30 (Cardano dApp-Wallet Web Bridge) APIError\nSee CIP-30 (Cardano dApp-Wallet Web Bridge) TxSignError\nSee CIP-30 (Cardano dApp-Wallet Web Bridge) TxSendError\napi.cip103.signTxs(txs: TransactionSignatureRequest[]): Promise<cbor<transaction_witness_set>[]>\nErrors: APIError, TxSignError\nAPIError\nTxSignError\nSigns a list of transactions where each transaction can either be independent and/or as a sequence of interconnected transactions where a subsequent transaction depends on a previous one. The returned array of witness sets directly correspond to the elements in the txs parameter, aligning the witness set at index 0 with the transaction at index 0, and so forth.\ntxs\nOn sign error for any transaction in the array, no witnesses are to be returned. Instead a TxSignError is to be thrown. The error message thrown should include a reference to the transaction that caused the sign error, by including the index for failing transaction that map to the input array transaction list.\nTxSignError\nThere are certain things that should be considered by wallets implementing this CIP, namely user visibility of what is signed. Though not explicitly specified in this CIP, as it would be up to the wallet to find a good solution, the wallet should make it clear to the user that multiple transactions are to be signed, and to give a clear overview/summary of what is signed. In addition to visibility, the wallet shall process the input transaction array in the same order as input parameter to allow transactions to be chained by accepting a previous transaction in the array to be used as input in a following transaction.\napi.cip103.submitTxs(txs: cbor<transaction>[]): Promise<hash32[]>\nErrors: APIError, TxSendError\nAPIError\nTxSendError\nExtends CIP-30 (Cardano dApp-Wallet Web Bridge) submitTx with the ability to submit transactions in bulk. Transactions are to be submitted in the same order as provided as inputs. All transactions provided as input should be attempted to be submitted even in case of error. If all transactions are successfully submitted, an array of transaction ids is to be returned. In the case of one or multiple Refused | Failure, a TxSendError | hash32 array is thrown. Each entry in the array represents either a successful submit with hash32 (transaction id), or in the case of Refused | Failure, a TxSendError object. In both cases, the response array directly corresponds to the elements in the txs parameter.\nRefused | Failure\nTxSendError | hash32\nhash32\nRefused | Failure\nTxSendError\ntxs\nOnce approved and final, the proposal must be superseded by another if changes break the specification. Minor adjustments are allowed if needed to improve readability and resolve uncertainties.\nAllowing for bulk signing and submission of transactions can greatly improve the user experience by reducing the amount of steps needed to sign more than one transaction. Allowing multiple transactions to be provided in the same API call also reduces the burden for transaction chaining that was previously mainly possible by keeping track of node mempool to know if input utxo's are available to be spent. Submitting multiple transactions would still be possible without submitTxs addition using the already defined CIP-30 (Cardano dApp-Wallet Web Bridge) submitTx endpoint by calling it multiple times. However, allowing a bulk submit endpoint speeds up submission when many transactions are to be submitted at once as you wouldn't have to await each individual submission.\nsubmitTxs\nIn order for this standard to be active, the following should be true:\nImplemented by at least two wallets.\nAdopted and used by at least one dApp or infrastructure tool to prove usability.\nN/A\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0104 | Web-Wallet Bridge - Account public key\n\nThis CIP extends CIP-30 (Cardano dApp-Wallet Web Bridge) to provide an additional endpoint for dApp to get the extended account public key from a connected wallet.\nNormally it's up to the wallet to handle the logic for utxo selection, derived addresses etc through the established CIP-30 api. Sometimes however, dApp needs greater control due to subpar utxo selection or other specific needs that can only be handled by chain lookup from derived address(es). This moves the control and complexity from wallet to dApp for those dApps that prefer this setup. A dApp has better control and can make a more uniform user experience. By exporting only the account public key, this gives read-only access to the dApp.\nA new endpoint is added namespaced according to this cip extension number that returns the connected account extended public key as cbor T defined in CIP30.\ncbor<T>\napi.cip104.getAccountPub(): Promise<cbor<Bip32PublicKey>>\nErrors: APIError\nReturns hex-encoded string representing cbor of extended account public key. Throws APIError if needed as defined by CIP30.\nWallets implementing this CIP should, but not enforced, request additional access from the user to access this endpoint as it allows for complete read access to account history and derivation paths.\nRaw cbor is returned instead of bech32 encoding to follow specification of other CIP30 endpoints.\nIn order for this standard to be active, the following should be true:\nImplemented by at least two wallets.\nAdopted and used by at least one dApp or infrastructure tool to prove usability.\nCommunication with additional wallets established to widen availability\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0105 | Conway era Key Chains for HD Wallets\n\nThe Conway Ledger era introduces many new features to Cardano, notably features to support community governance via CIP-1694. This includes the introduction of the new first class credentials; drep_credential, committee_cold_credential and committee_hot_credential.\ndrep_credential\ncommittee_cold_credential\ncommittee_hot_credential\nWe propose a HD wallet key derivation paths for registered DReps and constitutional committee members to deterministically derive keys from which credentials can be generated. Such keys are to be known as DRep keys, constitutional committee cold keys and constitutional committee hot keys. Here we define some accompanying tooling standards.\nNote this proposal assumes knowledge of the Conway ledger design (see draft ledger specification) and CIP-1694.\nIn the Conway ledger era, DRep credentials allow registered DReps to be identified on-chain, in DRep registrations, retirements, votes, and in vote delegations from ada holders. Whilst constitutional committee members can be recognized by their cold credentials within update committee governance actions, authorize hot credential certificate and resign cold key certificates. Constitutional committee hot credential can be observed within the authorize hot key certificate and votes.\nCIP-1694 terms these DRep credentials as DRep IDs, which are either generated from blake2b-224 hash digests of Ed25519 public keys owned by the DRep, or are script hash-based. Similarly, both the hot and cold credentials for constitutional committee members can be generated from public key digests or script hashes.\nThis CIP defines a standard way for wallets to derive DRep and constitutional committee keys.\nSince it is best practice to use a single cryptographic key for a single purpose, we opt to keep DRep and committee keys separate from other keys in Cardano.\nBy adding three paths to the CIP-1852 | HD (Hierarchy for Deterministic) Wallets for Cardano, we create an ecosystem standard for wallets to be able to derive DRep and constitutional committee keys. This enables DRep and constitutional committee credential restorability from a wallet seed phrase.\nStakeholders for this proposal are wallets that follow the CIP-1852 standard and tool makers wishing to support DReps and or constitutional committee members. This standard allows DReps and constitutional committee members to use alternative wallets whilst being able to be correctly identified. By defining tooling standards, we enable greater interoperability between governance-focussed tools.\nHere we describe DRep key derivation as it pertains to Cardano wallets that follow the CIP-1852 standard.\nTo differentiate DRep keys from other Cardano keys, we define a new role index of 3:\nrole\n3\nm / 1852' / 1815' / account' / 3 / address_index\nm / 1852' / 1815' / account' / 3 / address_index\nWe strongly recommend that a maximum of one set of DRep keys should be associated with one wallet account, which can be achieved by setting address_index=0.\naddress_index=0\nTools and wallets can generate a DRep ID (drep_credential) from the Ed25519 public DRep key (without chaincode) by creating a blake2b-224 hash digest of the key. As this is key-based credential it should be marked as entry 0 in a credential array. DRep Identifier is further specified in CIP-0129.\ndrep_credential\n0\nHere we describe constitutional committee cold key derivation as it pertains to Cardano wallets that follow the CIP-1852 standard.\nTo differentiate constitutional committee cold keys from other Cardano keys, we define a new role index of 4:\nrole\n4\nm / 1852' / 1815' / account' / 4 / address_index\nm / 1852' / 1815' / account' / 4 / address_index\nWe strongly recommend that a maximum of one set of constitutional committee cold keys should be associated with one wallet account, which can be achieved by setting address_index=0.\naddress_index=0\nTools and wallets can generate a constitutional committee cold credential (committee_cold_credential) from the Ed25519 public constitutional committee cold key (without chaincode) by creating a blake2b-224 hash digest of the key. As this is key-based credential it should be marked as entry 0 in a credential array.\ncommittee_cold_credential\n0\nHere we describe constitutional committee hot key derivation as it pertains to Cardano wallets that follow the CIP-1852 standard.\nTo differentiate constitutional committee hot keys from other Cardano keys, we define a new role index of 5:\nrole\n5\nm / 1852' / 1815' / account' / 5 / address_index\nm / 1852' / 1815' / account' / 5 / address_index\nWe strongly recommend that a maximum of one set of constitutional committee hot keys should be associated with one wallet account, which can be achieved by setting address_index=0.\naddress_index=0\nTools and wallets can generate a constitutional committee hot credential (committee_hot_credential) from the Ed25519 public constitutional committee hot key (without chaincode) by creating a blake2b-224 hash digest of the key. As this is key-based credential it should be marked as entry 0 in a credential array.\ncommittee_hot_credential\n0\nThese are also described in CIP-0005 | Common Bech32 Prefixes, but we include them here for completeness.\nNote we also include the prefixes for script-based credentials in the following subsections, for completeness.\nDRep keys and DRep IDs should be encoded in Bech32 with the following prefixes:\ndrep_sk\ndrep_vk\ndrep_xsk\ndrep_xvk\ndrep\ndrep_vkh\ndrep_script\nConstitutional cold keys and credential should be encoded in Bech32 with the following prefixes:\ncc_cold_sk\ncc_cold_vk\ncc_cold_xsk\ncc_cold_xvk\ncc_cold\ncc_cold_vkh\ncc_cold_script\nConstitutional hot keys and credential should be encoded in Bech32 with the following prefixes:\ncc_hot_sk\ncc_hot_vk\ncc_hot_xsk\ncc_hot_xvk\ncc_hot\ncc_hot_vkh\ncc_hot_script\nSupporting tooling should clearly label these key pairs as \"DRep Keys\".\nExamples of acceptable keyTypes for supporting tools:\nkeyType\nkeyType\nDRepSigningKey_ed25519\nDRepExtendedSigningKey_ed25519_bip32\nDRepVerificationKey_ed25519\nDRepExtendedVerificationKey_ed25519_bip32\nFor hardware implementations:\nkeyType\nDRepHWSigningFile_ed25519\nDRepVerificationKey_ed25519\nSupporting tooling should clearly label these key pairs as \"Constitutional Committee Cold Keys\".\nExamples of acceptable keyTypes for supporting tools:\nkeyType\nkeyType\nConstitutionalCommitteeColdSigningKey_ed25519\nConstitutionalCommitteeColdExtendedSigningKey_ed25519_bip32\nConstitutionalCommitteeColdVerificationKey_ed25519\nConstitutionalCommitteeColdExtendedVerificationKey_ed25519_bip32\nFor hardware implementations:\nkeyType\nConstitutionalCommitteeColdHWSigningFile_ed25519\nConstitutionalCommitteeColdVerificationKey_ed25519\nSupporting tooling should clearly label these key pairs as \"Constitutional Committee Hot Keys\".\nkeyType\nConstitutionalCommitteeHotSigningKey_ed25519\nConstitutionalCommitteeHotExtendedSigningKey_ed25519_bip32\nConstitutionalCommitteeHotVerificationKey_ed25519\nConstitutionalCommitteeHotExtendedVerificationKey_ed25519_bip32\nFor hardware implementations:\nkeyType\nConstitutionalCommitteeHotHWSigningFile_ed25519\nConstitutionalCommitteeHotVerificationKey_ed25519\nThe previous governance key IDs defined by this standard have been superseded by the definitions provided in CIP-0129. Tools implementing this standard are encouraged to consider adopting CIP-0129. Tools that already support CIP-0129 maintain backward compatibility with the legacy formats specified below but should consider fully transitioning to CIP-0129 to standardize key formats across the ecosystem. This will help avoid multiple formats and ensure consistency.\nThis CIP previously also lacked _vkh key definitions, which are now added above possible due to the upgrades defined in CIP-0129. For detailed information on the new specification and the rationale behind the upgrade, please refer to CIP-0129.\n_vkh\ndrep\ndrep_script\ncc_cold\ncc_cold_script\ncc_hot\ncc_hot_script\nThis CIP is not to be versioned using a traditional scheme, rather if any large technical changes are required then a new proposal must replace this one. Small changes can be made if they are completely backwards compatible with implementations, but this should be avoided.\nBy standardizing derivation, naming, and tooling conventions we primarily aim to enable wallet interoperability. By having a standard to generate DRep and constitutional committee credentials from mnemonics, we allow wallets to always be able to discover a user s governance activities.\nThis approach mirrors how stake keys were rolled out, see CIP-0011 | Staking key chain for HD wallets. We deem this necessary since these credentials sit alongside each other in the Conway ledger design.\nThe alternative would be to define a completely different derivation paths, using a different index in the purpose field, similar to the specification outlined within CIP-0036, but this could introduce complications with HW wallet implementations.\nWe believe the overhead that would be introduced by multi-DRep accounts or multi-constitutional-committee is an unjustified expense. Future iterations of this specification may expand on this, but at present this is seen as unnecessary. This avoids the need for DRep, cc hot or cc cold key discovery.\nWe model this on how stake keys are generally handled by wallets. If required, another CIP could, of course, introduce a multi-DRep/CC method.\nFor simplicity, we have omitted network tags within the encoding. This is because we have modeled DRep IDs and CC credentials on stake pool operator IDs, which similarly do not include a network tag.\nThe advantage of including a network tag would be to reduce the likelihood of mislabelling a DRep s network of operation (eg Preview v Cardano mainnet).\nSee Test Vectors File.\nThe DRep derivation path is used by three wallet/tooling implementations. Nufi Lace Yoroi demos wallet\nNufi\nLace\nYoroi\ndemos wallet\nThe constitutional committee derivation paths are used by two implementations. csl-examples cardano-addresses\ncsl-examples\ncardano-addresses\nAuthor to provide an example implementation inside a HD wallet. csl-examples/cip-1852-keys.js\ncsl-examples/cip-1852-keys.js\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0106 | Web-Wallet Bridge - Multisig wallets\n\nThis document describes a CIP-30 extension allowing webpages (i.e. dApps) to interface with Cardano Multisig-wallets. This document is a work in progress and is not yet finalized. It is expected to be updated as the ecosystem evolves.\nIn order to facilitate future dApp development, we will need a way for dApps to communicate with multisig wallets, given the unique complexities of native script based addresses. Special provisions need to be made to make the connector compatible with them.\nSpecifically, apps building transactions need to be able to get the following information from the wallet:\nScript descriptor Any transaction consuming a UTXO from a Plutus-based address must attach the corresponding script.\nAny transaction consuming a UTXO from a Plutus-based address must attach the corresponding script.\nScriptRequirements The TxContext that is required to be able to validate the transaction. It encompasses all the possible combinations of requirements for the transaction to be valid, as such it is represented by an array of ScriptRequirement objects.\nScriptRequirements\nThe TxContext that is required to be able to validate the transaction. It encompasses all the possible combinations of requirements for the transaction to be valid, as such it is represented by an array of ScriptRequirement objects.\nTxContext\nScriptRequirement\nChange Datum The datum that will be used as the change output for the transaction. This is required for wallets based on Plutus V2 and before, as the change output must contain a datum to be valid and spendable.\nThe datum that will be used as the change output for the transaction. This is required for wallets based on Plutus V2 and before, as the change output must contain a datum to be valid and spendable.\nAdditionally, apps need to be able to submit a transaction to the wallet for signing in an asynchronous manner, as gathering of signatures can take a long time and each wallet provider will have its own way of handling this process.\nFinally, the signTx() and signData() endpoints will have to be disabled when using this extension since they are not compatible with native script based addresses.\nA hex-encoded string of the corresponding bytes. This represents the hash of the public key used to sign transactions.\ntype KeyHash = String\ntype KeyHash = String\ntype ScriptRequirementsCode = { Signer: 1, Before: 2, After: 3, } type ScriptRequirement = { code: ScriptRequirementsCode, value: KeyHash|number, }\ntype ScriptRequirementsCode = { Signer: 1, Before: 2, After: 3, } type ScriptRequirement = { code: ScriptRequirementsCode, value: KeyHash|number, }\nCompletedTxErrorCode = { NotFound: 1, NotReady: 2 }\nCompletedTxErrorCode = { NotFound: 1, NotReady: 2 }\nNotFound - The transaction with the given id was not found.\nNotReady - The transaction with the given id is not ready yet.\nFor Plutus V2 and later, partial collateral is supported. This function returns an address that can be used to add collateral to a transaction. The address returned must be owned by one of the signers in the list of signers returned by api.getScriptRequirements().\napi.getScriptRequirements()\ndApp developers can choose to use this address to add collateral to a transaction, or they can choose to use the api.getCollateral() function to get a list of UTXOs that can be used as collateral. If the dApp chooses to use this address, they must ensure that the address is not used for any other purpose, as the wallet may be using it to track collateral, and that the collateral return address is the same one.\napi.getCollateral()\nErrors: APIError\nAPIError\nReturns a list of ScriptRequirements that will be used to validate any transaction sent to the wallet.\nErrors: APIError\nAPIError\nReturns the CBOR-encoded native script that controls this wallet.\nErrors: APIError, TxError\nAPIError\nTxError\nSubmits a transaction to the wallet for signing. The wallet should check that the transaction is valid, gather the required signatures, compose the finalized transaction, and submit the transaction to the network. If the transaction is valid and the wallet is able to sign it, the wallet should return the transaction hash. If the transaction is invalid or the wallet is unable to sign it, the wallet should throw a TxError with the appropriate error code. The wallet should not submit the transaction to the network if it is invalid or the wallet is unable to sign it.\nTxError\nIf the transaction contains hidden metadata, the wallet should not submit the transaction when it is ready, but return it to the dApp when the dApp calls the getCompletedTx function.\ngetCompletedTx\nErrors: APIError, CompletedTxError\nAPIError\nCompletedTxError\nIf the transaction is not ready, the wallet should throw a CompletedTxError with the appropriate error code. If the transaction is ready, the wallet should return the CBOR-encoded transaction and the signatures.\nCompletedTxError\nNative script based addresses cannot provide collateral for transactions. Using this function, dApps can request the wallet to provide collateral for a transaction. The collateral must be a pure ADA UTXO, held by one of the signers in the list of signers returned by api.getScriptRequirements().\napi.getScriptRequirements()\nWhen connecting to a wallet using this extension the following endpoints will be disabled:\napi.signTx(tx: cbor<transaction>, partialSign: bool = false): Promise<cbor<transaction_witness_set>>\napi.signData(addr: Address, payload: Bytes): Promise<DataSignature>\nThese endpoints should return an error if called when using this extension.\nSee justification and explanations provided with each API endpoint.\nThe interface is implemented and supported by multiple wallet providers.\nThe interface is used by multiple dApps to interact with wallet providers.\nProvide some reference implementation of wallet providers leo42/BroClanWallet\nleo42/BroClanWallet\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0107 | URI Scheme - Block and transaction objects\n\nThis extends CIP-13, which describes a Cardano URI scheme, with several more objects that would be useful to be able to assign addresses to, in particular blocks, transactions, transaction metadata, and transaction outputs.\nCIP-13 defined two initial URL authorities, for payment links and delegating to a stakepool.\nHowever, in a number of contexts, it would be useful to canonically link to other Cardano objects, such as:\nProviding links to a transaction, to be opened in a wallet or chain explorer of the users choice\nTo provide richly interconnected metadata, such as in CIP-100\nWithout a canonical standard for how to define these URIs, these objects are either unaddressable, not machine-interpretable, or will suffer from a divergence of convention. Similarly, we seek to fit an existing structure, such as CIP-0013, to reduce the number of different conventions that need be supported by the ecosystem.\nWe extend CIP-13 with 2 new authorities for referencing blocks and transactions.\nExamples:\nweb+cardano://block?hash=c6a8976125193dfae11551c5e80a217403d953c08ebbd69bba904d990854011f web+cardano://block?height=12345678890 web+cardano://transaction/7704a68404facf7126fa356f1b09f0e4c552aeef454cd0daba4208f3a64372e9 web+cardano://transaction/7704a68404facf7126fa356f1b09f0e4c552aeef454cd0daba4208f3a64372e9#1 web+cardano://transaction/7704a68404facf7126fa356f1b09f0e4c552aeef454cd0daba4208f3a64372e9/metadata web+cardano://transaction/d3616b772c91f118346e74863d1722810a4858e4d7cc7663dc2eed345d7bca72/metadata/674 web+cardano://transaction/self/metadata/1694\nweb+cardano://block?hash=c6a8976125193dfae11551c5e80a217403d953c08ebbd69bba904d990854011f web+cardano://block?height=12345678890 web+cardano://transaction/7704a68404facf7126fa356f1b09f0e4c552aeef454cd0daba4208f3a64372e9 web+cardano://transaction/7704a68404facf7126fa356f1b09f0e4c552aeef454cd0daba4208f3a64372e9#1 web+cardano://transaction/7704a68404facf7126fa356f1b09f0e4c552aeef454cd0daba4208f3a64372e9/metadata web+cardano://transaction/d3616b772c91f118346e74863d1722810a4858e4d7cc7663dc2eed345d7bca72/metadata/674 web+cardano://transaction/self/metadata/1694\nWe extend the grammar from CIP-0013 with two new authorities:\nauthorityref = (... | blockref | transactionref)\nauthorityref = (... | blockref | transactionref)\nA link referencing a block by either block hash or height.\nWARNING: Referencing blocks by height is provided in the case of extremely tight space requirements, but referencing a recent block (within the rollback horizon of the chain) is unstable. Due to chain re-orgs, it may refer to different content for different users, or at different times. This should only be used when this is not critical, or for blocks outside of the rollback horizon.\nblockref = \"//block\" query query = ( \"?block=\" block_hash | \"?height=\" block_height) block_hash = 64HEXDIG block_height = *digit\nblockref = \"//block\" query query = ( \"?block=\" block_hash | \"?height=\" block_height) block_hash = 64HEXDIG block_height = *digit\nA link referencing this transaction, another transaction, transaction output, the full transaction metadata, or specific tag within the transaction metadata.\ntransactionref = \"//transaction/\" (tx_id | utxo_id | tx_metadata | tx_metadata_tag) tx_id = \"self\" | 64HEXDIG utxo_id = tx_id \"#\" *digit tx_metadata = tx_id \"/\" metadata tx_metadata_tag = tx_id \"/\" metadata \"/\" *digit\ntransactionref = \"//transaction/\" (tx_id | utxo_id | tx_metadata | tx_metadata_tag) tx_id = \"self\" | 64HEXDIG utxo_id = tx_id \"#\" *digit tx_metadata = tx_id \"/\" metadata tx_metadata_tag = tx_id \"/\" metadata \"/\" *digit\nNOTE: tx_id can be set to \"self\", which is useful for making self-referential metadata, before the transaction ID is known. For example, in CIP-100, you may want to store governance metadata on the transaction which casts the vote. The transaciton hash is not yet known, and so the anchor field cannot link to the transaction by transaction ID.\nanchor\nFor grammar reference, see:\nWikipedia Augmented Backus Naur form\nRFC 2234: Augmented BNF for Syntax Specifications: ABNF\nUnicode in ABNF\nThis CIP defines a canonical format for URIs referencing four new Cardano objects: blocks, transactions, transaction metadata, and specific tags within the transaction metadata. It utilizes existing cardano standards (CIP-0013) and industry standards (URIs), minimizing the number of new concepts that a developer needs to learn. By utilizing URIs, it creates a natural path to integration with existing tools, such as browsers. And finally, it allows a canonical URI for these objects, such as storing CIP-100 metadata on-chain, and referring to it in the anchor field.\nCIP-100 is standardized utilizing these URI schemes for on-chain references.\nAt least one governance tool utilizes these URI schemes\nAt least one explorer or wallet utilizes these URI schemes\nThe current community sentiment towards adopting this in CIP-100 is high (it was the original inspiration).\nAdvocacy and education about this format should be performed by:\nImplementors of CIP-100 and governance metadata tooling\nWallet and Explorer developers\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0108 | Governance Metadata - Governance Actions\n\nThe Conway ledger era ushers in on-chain governance for Cardano via CIP-1694 | A First Step Towards On-Chain Decentralized Governance, with the addition of many new on-chain governance artifacts. Some of these artifacts support the linking off-chain metadata, as a way to provide context.\nThe CIP-100 | Governance Metadata standard provides a base framework for how all off-chain governance metadata should be formed and handled. But this is intentionally limited in scope, so that it can be expanded upon by more specific subsequent CIPs.\nThis proposal aims to provide a specification for off-chain metadata vocabulary that can be used to give context to governance actions. Without a sufficiently detailed standard for governance actions we introduce the possibility to undermine voters ability to adequately assess governance actions. Furthermore a lack of such standards risks preventing interoperability between tools, to the detriment of user experiences.\nFor the many contributors to this proposal, see Acknowledgements.\nBlockchains are poor choices to act as content databases. This is why governance metadata anchors were chosen to provide a way to attach long form metadata content to on-chain events. By only supplying an onchain hash of the off-chain we ensure correctness of data whilst minimizing the amount of data posted to the chain.\nWhen observing from the chain level, tooling can only see the content of the governance action and it's anchor. These on-chain components do not give give any context to the motivation nor off-chain discussion of an governance action. Although this information would likely be desired context for voters. By providing rich contextual metadata we enable voters to make well informed decisions.\nBy standardizing off-chain metadata formats we facilitate interoperability for tooling which creates and/or renders metadata attached to governance actions. This intern promotes a rich user experience between tooling. This is good for all governance participants.\nAlthough there are seven types of governance action defined via CIP-1694, we focus this proposal on defining core properties which must be attached to all types. We leave room for future standards to refine and specialize further to cater more specific for each type of governance action.\nwitness\nHere we extend the potential witnesses, with a witnessAlgorithm that can be set to include support for CIP-08 | Message Signing standard, indicated by CIP-0008. Here we mimic the restrictions as imposed over the CIP-30's implementation in .signData().\nwitnessAlgorithm\nCIP-0008\nThis standard introduces the possibility of using Github markdown text styling within fields.\nThe following properties extend the potential vocabulary of CIP-100's body property.\nbody\ntitle\nA very short freefrom text field. Limited to 80 characters.\n80\nThis SHOULD NOT support markdown text styling.\nAuthors SHOULD use this field to succinctly describe the governance action and its motivation.\nAuthors SHOULD attempt to make this field unique whilst also avoiding hyperbolic language.\ni.e; Increase K protocol parameter to 100,000 to increase decentralization of Cardano.\n100,000\nabstract\nA short freefrom text field. Limited to 2500 characters.\n2500\nThis SHOULD support markdown text styling.\nAuthors SHOULD use this field to expand upon their title by describing the contents of the governance action, its motivation and rationale.\ntitle\nmotivation\nA freeform text field.\nThis SHOULD support markdown text styling.\nThis SHOULD be used by the author to encapsulate all context around the problem that is being solved by the on-chain action.\nThis SHOULD be used to outline the related stakeholders and use cases.\nrationale\nA freeform text field.\nThis SHOULD support markdown text styling.\nThis SHOULD be used by the author to discuss how the content of the governance action addresses the problem outlined within the motivation.\nmotivation\nThis field SHOULD justify the changes being made to Cardano.\ni.e \"by decreasing X parameter by Y we increase Ada earned by SPOs, thus incentivising more people to become SPOs, leading to a more diverse network\"\nThis SHOULD provide evidence of consensus within the community and discuss significant objections or concerns raised during the discussion.\nThis SHOULD include discussion of alternative solutions and related topics/ governance actions.\nThis SHOULD include any recommendations made by relevant organizations or committees.\nreferences\nWe extend CIP-100's references field.\nThis SHOULD NOT support markdown text styling.\nTo be an OPTIONAL set of objects, using the @set property.\n@set\nEach object MUST have a label field to describe the reference, such as; \"blog - Why we must continue to fund Catalyst\".\nlabel\nEach object MUST have a uri field.\nuri\nEach object MAY have a OPTIONAL referenceHash object. Each object MUST have a hashDigest field. Each object MUST have a hashAlgorithm field, which is inherited from CIP-100.\nreferenceHash\nEach object MUST have a hashDigest field.\nhashDigest\nEach object MUST have a hashAlgorithm field, which is inherited from CIP-100.\nhashAlgorithm\nThis should be used by the author to link related or supporting work via the URI, and reference this via the index within their freefrom text fields.\nGovernance action metadata must include all compulsory fields to be considered CIP-0108 compliant. As this is an extension to CIP-100, all CIP-100 fields can be included within CIP-108 compliant metadata.\nSee test-vector.md for examples.\nThis proposal should not be versioned, to update this standard a new CIP should be proposed. Although through the JSON-LD mechanism further CIPs can add to the common governance metadata vocabulary,\nWe intentionally have kept this proposal brief and uncomplicated. This was to reduce the time to develop and deploy this standard. We think it is better to have a base standard which can be improved, rather than meticulously craft a perfect single standard. This way we enable tooling which depends on this standard to start development. Furthermore, it is very difficult to predict future wants/needs now, so by allowing upgrades we build in the ability to improve the standard as new wants/needs arrive.\nThe fields which have been chosen for this standard heavily inspired to those used for CIPs. We did this for two reasons; familiarity and competency. Those who are involved in Cardano are familiar with the CIP format, meaning they will be intuitively understand these fields being reused here. These fields in combination have also been fairly battle tested via the CIPs process and thus act as a good standard to describe problems and their solutions.\nwitness\nWe introduce a new witness type to be able to reuse existing dApp-Wallet infrastructure. The type as described allows for reuse of the CIP-30 signData standard, which means that governance dApps are able to implement this without needing any alterations to existing wallets.\nWe choose to introduce rich text standard here because we see significant value in supporting it. Rich text styling improves the ability for the author to express themselves. Furthermore, most potential voters are use to such standards when reviewing metadata.\nWith this design, we wanted to allow for quick and easy differentiation between governance actions. We achieve this by facilitating users \"layers of investigation\", where some fields are limited in size. This encourages tooling providers to show users the small fields before allowing deep investigation of the larger fields. By allowing this we aim to improve the experience of prospective voters, when sorting though many governance actions.\nThe downside of highlighting some fields over others is that we incentivize hyperbolic and eye catching phrases. Where authors want their governance action to standout in tooling so use overly dramatic phrasing. This creates an environment where there is a race to the bottom on voter's attention. Overall this could decrease the perceived legitimacy of the system. The counter argument is that tooling providers should not use metadata to solely highlight proposals, rather other means such as cryptographically verified submitters.\ntitle\nThis should be used by voters to quickly and easily differentiate between two governance actions which may be having the same or similar on-chain effects. This is why we have chosen a short character limit, as longer titles would reduce the ability for quick reading.\nabstract\nThis gives voters one step more detail beyond the title. This allows for a compact description of the what, why and how without the voter having to read the larger fields.\ntitle\nmotivation\nThe motivation is a chance for the author to fully describe the problem that is being solved by the governance action. This is important as all governance actions are a solution to a problem and thus this is a universal field. By showing relation to stakeholders the author is able to show that they have performed adequate research on the problem. Voters can use this field to determine if the problem is sizable enough to warrant voting on.\nmotivation\nrationale\nThis field gives the author the opportunity to explain how the onchain action is addressing the problem outlined in the motivation. This gives the author a place to discuss any alternative designs or completing governance actions. Voters should be able to use this field to evaluate the applicability of the solution to the problem.\nreferences\nReferences give the author ability to point to supporting research or related work. These should be used by voters to verify the content of supporting research. The inclusion of a hash allows for the supporting documentation to be cryptographically verified.\nShould fields be optional or compulsory? Title, abstract, motivation and rationale should be compulsory as they should be very important to the ability\nTitle, abstract, motivation and rationale should be compulsory as they should be very important to the ability\nHow much vocabulary can be extended to other onchain governance events? It is hard to predict how the scope of future standards before they have been developed.\nIt is hard to predict how the scope of future standards before they have been developed.\nHow to integrate custom set of HTML tags? to allow formatting of longer text fields. Since CIP-100 does not intend to support rich text fields such an inclusion would not fit, so we have included such format here.\nSince CIP-100 does not intend to support rich text fields such an inclusion would not fit, so we have included such format here.\nThis standard is supported by two different tooling providers used to submit governance actions to chain. cardano-signer : A tool to sign with author secret keys, verify signatures, canonize the body content (Linux/Arm/Win/Mac) spo-scripts : A collection of scripts to interact with governance. Create actions, vote on actions incl. signature verification cardano-governance-metadata-lib : A rust library for interacting with Cardano Governance Metadata conforming to CIP-100 (rust)\nThis standard is supported by two different tooling providers used to submit governance actions to chain.\ncardano-signer : A tool to sign with author secret keys, verify signatures, canonize the body content (Linux/Arm/Win/Mac)\nspo-scripts : A collection of scripts to interact with governance. Create actions, vote on actions incl. signature verification\ncardano-governance-metadata-lib : A rust library for interacting with Cardano Governance Metadata conforming to CIP-100 (rust)\nThis standard is supported by two different chain indexing tools, used to read and render metadata. DB-Sync via db-sync-sancho-4.1.0\nThis standard is supported by two different chain indexing tools, used to read and render metadata.\nDB-Sync via db-sync-sancho-4.1.0\nSolicitation of feedback\nRun two online workshops to gather insights from stakeholders.\nSeek community answers on all Open Questions. Implementation\nAuthor to provide example metadata and schema files.\nI would like to thank those that contributed to the Governance Metadata Working Group Workshop #1 hosted by Ryan Williams (see presentation slides with notes).\nThank you to the co-hosts:\nAdam Dean\nThomas Upfield\nThank you to the participants:\nCarlos Lopez de Lara\nIgor Veli kovi\nJohnny Kelly\nKenric Nelson\nKevin Hammond\nLorenzo Bruno\nMike Susko\nRhys Morgan\nEric Alton\nSamuel Leathers\nVladimir Kalnitsky\nI would like to thank those that contributed to the Governance Metadata Working Group Workshop #2 hosted by Ryan Williams (see presentation slides with notes).\nThank you to the co-host:\nAdam Dean\nThank you to the participants:\nMark Byers\nNils Codes\nThank you to the bots that joined also.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0109 | Modular Exponentiation Built-in for Plutus Core\n\nThis CIP proposes an extension of the current plutus functions to provide support for the efficient calculation of modular exponentiation with inverses.\nModular exponentiation is a cornerstone operation in numerous cryptographic protocols. The availability of such a function directly within Plutus will provide a more efficient and reliable means to perform this crucial computation. Therefore, the integration of such a Plutus core built-in is imperative to enhance cryptographic functionalities within the ecosystem.\nMore concretely, the key area where this function would contribute is that of finite field arithmetic, which is a basis for elliptic curves. In this context, a finite field is a set of integers modulo a prime number p. On this set, we have the basic operations of addition, multiplication, additive inversion (negation) and the multiplicative inversion (reciprocal), all reduced modulo the prime number p.\np\np\nWith the current built-in functions, most of these operations can be implemented relatively cheaply, except the reciprocal. This function can be implemented via either the Extended Euclidean algorithm or using Fermat's little theorem. Using the preliminary cost-model for plutus V3, both implementations still consume around ~5 and ~9 of the CPU budget on mainnet. These benchmarks are performed for the scalar field of the BLS12-381 curve, which has a 255 bit prime modulus.\nThe result is that doing such computation in a practical setting on-chain is costly, and other methods should be used to find this multiplicative inverse. A cumbersome method for solving this issue, is calculating the inverse off-chain and bringing it in scope via a redeemer as a claimed inverse of another element. This method works, as one can cheaply check that the claimed inverse is indeed the unique inverse with one multiplication, one modulo reduction and one equality check with the multiplicative identity. In math notation this means that for a field element a, we off-chain compute claimed_inverse_a, such that on-chain we can check: claimed_inverse_a * a = id.\na\nclaimed_inverse_a\nclaimed_inverse_a * a = id\nThe drawback of this technique is that by transferring the calculation off-chain, we incur additional fees due to the increased size of the transaction, as CPU units are cheaper. The creator of the transaction is also burdened by this computation, which often precedes more intricate, non-trivial cryptographic calculations. In the application of zero knowledge, this calculation if often implemented in low-level code, and used via bindings, meaning that extracting these intermediary values breaks existing code and interfaces.\nIn conclusion, integrating modular exponentiation as a core built-in within Plutus is not only essential for enhancing cryptographic capabilities, but also for optimizing on-chain computations. The current approach, which offloads certain calculations like finding multiplicative inverses to off-chain processes, is inefficient and costly in terms of transaction size and computational burden on the transaction creators. Incorporating this function directly into Plutus will streamline these operations, reduce transaction costs, and maintain the integrity of existing tools, thereby significantly advancing the Plutus ecosystem's functionality and user experience.\nA nonexclusive list of cryptographic protocols that use a field and would benefit from having this built-in are:\nThe verification of pairing based zero-knowledge proofs over BLS12-381. This pairing curve has a base field and a scalar field, with primes of size 381 and 255 bits respectively. In, for example, the proof system plonk, a verifier performs one reciprocal for each public input in the 255 bit scalar field. Onchain public key aggregation for Schnorr over SECP256k1: effectively, this aggregation is the point addition of the two keys on the curve, which requires one reciprocal in the SECP256k1 base field (using a 256 bit prime). A more interoperable interface for the BLS-12-381 built-ins: currently, the BLS-12-381 built-ins only expose a compressed version of a point, containing the x coordinate and some marked bits to describe how one can find the corresponding y in the base field. This calculation of finding y requires modular exponentiation in the field.\nThe verification of pairing based zero-knowledge proofs over BLS12-381. This pairing curve has a base field and a scalar field, with primes of size 381 and 255 bits respectively. In, for example, the proof system plonk, a verifier performs one reciprocal for each public input in the 255 bit scalar field.\nOnchain public key aggregation for Schnorr over SECP256k1: effectively, this aggregation is the point addition of the two keys on the curve, which requires one reciprocal in the SECP256k1 base field (using a 256 bit prime).\nA more interoperable interface for the BLS-12-381 built-ins: currently, the BLS-12-381 built-ins only expose a compressed version of a point, containing the x coordinate and some marked bits to describe how one can find the corresponding y in the base field. This calculation of finding y requires modular exponentiation in the field.\nx\ny\ny\nModular exponentiation is mathematically defined as the equivalence relation\nHere we have that the base bbb is an integer, the exponent eee a non-negative integer and the modulus mmm a positive integer. To be more specific, we want eee to be larger or equal to zero, and mmm is larger than zero. This is because what we are effectively doing, is multiplying eee copies of bbb and a modulo reduction by mmm. In this context, multiplying a negative number of copies of bbb has no definition.\nThat said, in the context of multiplicative inversion, a negative exponent can be interpreted as taking the exponent of the inverse of the base number. That is\nWe propose to also include this extension to the plutus built-in, for optimized inversion when this is possible. This is inversion is not guaranteed to exist for all numbers bbb, only when the modulus mmm is a prime number this is guaranteed. We also propose that the built-in fails if this inverse does not exist, and if a modulus is provided that is smaller than one.\nWith the above, we define a new Plutus built-in function with the following type signature\nmodularExponentiation :: Integer -> Integer -> Integer -> Integer\nmodularExponentiation :: Integer -> Integer -> Integer -> Integer\nhere the first argument is the base, the second the exponent and the third the modulus. As mentioned above, the behavior of this function is that it fails if the modulus is not a positive integer, or if the inverse of the base does not exist for a negative exponent. For the lower level implementation, we propose the usage of the integerPowMod function in the ghc-bignum packages. This function has the desired functionality, is optimized, and is easy to integrate in the plutus stack.\nintegerPowMod\nghc-bignum\nThe computational impact of modular exponentiation is complexified by it having three arguments. That said, observe that the integers used can always be bound by the modulus. Preliminary benchmarks on the time consumption of this integerPowMod function show that it can be costed constant in the size of its first argument (the base) and linear in the other two.\nintegerPowMod\nIntegrating this function directly into Plutus will streamline cryptographic operations, reduce transaction costs, and uphold the integrity of existing cryptographic interfaces. It addresses current inefficiencies and enhances the cryptographic capabilities of the Plutus platform.\nFor completeness and an historic perspective, the above functionality can also be attained by a new built-in function that performs normal exponentiation, after which one can reduce with the already present built-in function ModInteger. In the creation of this CIP, this possibility was discussed but put aside. This method has the flaw that the intermediate value of these integers is not bound. Meaning that memory consumption is not efficient for practical use in this setting.\nModInteger\nWe consider the following criteria to be essential for acceptance:\nThe PR for this functionality is merged in the Plutus repository.\nThis PR must include tests, demonstrating that it behaves as the specification requires in this CIP.\nA benchmarked use case is implemented in the Plutus repository, demonstrating that realistic use of this primitive does, in fact, provide major cost savings.\nIOG Plutus team consulted and accept the proposal.\nAuthor to provide preliminary benchmarks of time consumption. https://github.com/perturbing/expFast-bench/tree/cca69b842050de9523493d52c20384bc50c80b22\nhttps://github.com/perturbing/expFast-bench/tree/cca69b842050de9523493d52c20384bc50c80b22\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0110 | Plutus v1 Script References\n\nDespite making up less than half the transactions on Cardano, Plutus v1 scripts occupy around 40 of the total block space since chain inception, and sometimes higher during periods of peak activity. Increasing the space available to blocks is risky, as it impacts the block propagation time. This proposal puts forth a simple way to reduce this strain.\nPlutus v2 introduced a way to publish scripts on-chain, and reference those scripts to satisfy the witness requirement. However, because this was done via a new field on the transaction (i.e. \"Reference Inputs\"), which shows up in the script context, this feature is not backwards compatible with Plutus v1.\nHowever, of the 151gb it takes to represent the 6 year history of the chain, roughly 60 gb of that (nearly 40 ) can be attributed to the wasted space from repeating the same scripts in the last 2 years. This analysis was further confirmed by IOG in this issue.\nIt would be one thing if this was an issue mostly felt before or shortly after the Babbage hard-fork. It was assumed at the time that Plutus v2 would become a dominant player, and that usage of Plutus v1 contracts would die off. However, looking at recent trends in late 2023, it's clear this isn't happening.\nLooking at recent trends through late 2023, considering all scripts included in transactions, Plutus v1 hovered at or slightly below 50 of all scripts in transactions. However, comparing the size of transactions which execute scripts scripts, Plutus v1 scripts make up 90 of that space.\nLooking at periods of saturation can also help us understand where the limits of the chain are. Periods where activity is low and the chain is underutilized can skew our view of the problem: it largely doesn't matter to end users what percentage of space is occupied by different script versions, because the user experience is largely unimpacted and transactions are able to fit into the next block regardless. However, in periods of high activity, when blocks are nearly full, we get a better picture of where user activity is allocating that space, and where the cost savings would be most beneficial. For example, epoch 455 saw nearly 100 block usage for a full epoch, of which an average of 48 (and sometimes as high as 75 ) of the space was occupied by scripts, presumably much of that Plutus v1 scripts.\nThis problem isn't going away: while protocols may migrate to new Plutus v2 or v3 scripts, these old protocols will exist forever. Liquidity locked in these scripts, sometimes permanently, will mean that there is always an arbitrage opportunity that incentivizes a large portion of the block to be occupied by continually republishing these v1 scripts.\nAdditionally, raising the block size is considered incredibly sensitive, as it impacts block propagation times.\nA simple, backwards compatible mechanism for plutus v1 protocols to satisfy the script witness requirement, without changing the script context and causing breaking changes for Plutus v1 scripts, would alleviate quite literally millions of dollars worth of storage requirements, user pain, and developer frustration.\nWe propose relaxing the ledger rule that fails Plutus v1 scripts in transactions that have reference inputs, and to construct a script context that excludes reference inputs.\nThe ledger rule shouldn't change in other ways: for example, Plutus v1 scripts should still fail in the presence of inline datums or reference scripts on spent transaction inputs.\nThe main concern with this relates to backwards compatibility. The ledger makes very strong commitments regarding the behavior of scripts: any observable change represents a risk that there is some script out there that will either be unspendable when it should be, or spendable when it should not be.\nBecause of this, any such change which violates this must satisfy a burden of proof with regards to both the benefits and the risks. This was in fact considered in the original CIP, and at the time, it was decided that the justification would likely not meet that high bar.\nThus, as a rationale for this CIP, we repeat that analysis, hopefully with a different conclusion.\nIn terms of benefit, this approach would immediately allow all major plutus v1 dApps to reduce their transaction sizes dramatically. Some napkin math for both Sundae and Minswap shows that this would cut around 85 of the transaction size for each transaction; Considering the portion of block space currently taken up by Plutus v1 scripts, this represents a significant savings.\nIt is hard to overstate the long-term positive impact that this change could have for real users of the Cardano blockchain.\nIn terms of the risks, there are four main risks to consider:\nFunds that should be spendable are suddenly not spendable; In this case, a user could simply continue to use the existing witness mechanism to provide the scripts, and those funds become spendable again.\nFunds that should be spendable are suddenly not spendable;\nIn this case, a user could simply continue to use the existing witness mechanism to provide the scripts, and those funds become spendable again.\nFunds that should not be spendable are suddenly spendable; In this case, it is very hard to imagine a scenario where this would be true that isn't crafted intentionally. It would have to be some script that was dependent on the transaction fee being above a certain threshold, which is already a dangerous assumption to make given the updatable protocol parameters. In other instances (such as the change to how the minimum UTXO output is calculated) this kind of risk hasn't been an obstacle.\nFunds that should not be spendable are suddenly spendable;\nIn this case, it is very hard to imagine a scenario where this would be true that isn't crafted intentionally. It would have to be some script that was dependent on the transaction fee being above a certain threshold, which is already a dangerous assumption to make given the updatable protocol parameters. In other instances (such as the change to how the minimum UTXO output is calculated) this kind of risk hasn't been an obstacle.\nThe execution units change, without changing the outcome, resulting in a different cost for the user; In this case, the cost would only go down, and it is again hard to imagine a scenario where this is at material risk of violating some protocols integrity in a way that is not already compromised.\nThe execution units change, without changing the outcome, resulting in a different cost for the user;\nIn this case, the cost would only go down, and it is again hard to imagine a scenario where this is at material risk of violating some protocols integrity in a way that is not already compromised.\nGiven the parallel plans to include reference scripts in the cost of the transaction, outlined here, further mitigates these concerns.\nReview of this proposal by the relevant subject matter experts\nImplement the change in the cardano-ledger and cardano-node repositories\nInclude this change in a relevant hard fork Included within the Chang #1 hardfork\nIncluded within the Chang #1 hardfork\nUpdate the formal Agda specification\nImplement [minFeeRefScriptCoinsPerByte] or similar approach, as described here\nUpdate the implementation here\nUpdate property based tests to cover these scenarios\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0112 | Observe Script Type\n\nWe propose to introduce a new Plutus scripts type Observe in addition to those currently available (spending, certifying, rewarding, minting, drep). The purpose of this script type is to allow arbitrary validation logic to be decoupled from any ledger action. Since observe validators are decoupled from actions, you can run them in a transaction without needing to perform any associated action (ie you don't need to consume a script input, or mint a token, or withdraw from a staking script just to execute this validator). Additionally, we propose to introduce a new assertion to native scripts that they can use to check that a particular script hash is in required_observers (which in turn enforces that the script must be executed successfully in the transaction). This addresses a number of technical issues discussed in other CIPs and CPS such as the redundant execution of spending scripts, and the inability to effectively use native scripts in conjunction with Plutus scripts.\nObserve\nrequired_observers\nOften in a plutus validator you want to check \"a particular (different) Plutus script checked this transaction\", but it's annoying (and wasteful) to have to have to lock an output in a script and then check if that output is consumed, or mint a token, or whatever else just to trigger script validation.\nCurrently the main design pattern used to achieve this is a very obscure trick involving staking validators and the fact that you can withdraw 0 from a staking validator to trigger the script validation. A summary of the trick is: Implement all the intended validation logic in a Plutus staking validator, we will call this validator s_v. To check that this validator was executed in the transaction you check if the credential of s_v (StakingCredential) is present in txInfoWdrl, this guarantees that s_v was checked in validation. This relies on the fact that unlike in txInfoMint the ledger does not filter out 0 amount entries in txInfoWdrl. This means that you are allowed to build transactions that withdraw zero from a staking credential which in-turn triggers the staking script associated with that credential to execute in the transaction, which makes it available in txInfoWdrl. This is a enables a very efficient design pattern for checking logic that is shared across multiple scripts.\ns_v\ns_v\nStakingCredential\ntxInfoWdrl\ns_v\ntxInfoMint\ntxInfoWdrl\ntxInfoWdrl\nFor instance, a common design pattern is a token based forwarding validator in which the validator defers its logic to another validator by checking that a state token is present in one of the transaction inputs:\nforwardNFTValidator :: AssetClass -> BuiltinData -> BuiltinData -> ScriptContext -> () forwardNFTValidator stateToken _ _ ctx = assetClassValueOf stateToken (valueSpent (txInfo ctx)) == 1\nforwardNFTValidator :: AssetClass -> BuiltinData -> BuiltinData -> ScriptContext -> () forwardNFTValidator stateToken _ _ ctx = assetClassValueOf stateToken (valueSpent (txInfo ctx)) == 1\nThis pattern is common in protocols that use the batcher architecture. Some protocols improve on the pattern by including the index of the input with the state token in the redeemer:\nforwardNFTValidator :: AssetClass -> BuiltinData -> Integer -> ScriptContext -> () forwardNFTValidator stateToken _ tkIdx ctx = assetClassValueOf stateToken (txInInfoResolved (elemAt tkIdx (txInfoInputs (txInfo ctx)))) == 1 forwardMintPolicy:: AssetClass -> Integer -> ScriptContext -> () forwardMintPolicy stateToken tkIdx ctx = assetClassValueOf stateToken (txInInfoResolved (elemAt tkIdx (txInfoInputs (txInfo ctx)))) == 1\nforwardNFTValidator :: AssetClass -> BuiltinData -> Integer -> ScriptContext -> () forwardNFTValidator stateToken _ tkIdx ctx = assetClassValueOf stateToken (txInInfoResolved (elemAt tkIdx (txInfoInputs (txInfo ctx)))) == 1 forwardMintPolicy:: AssetClass -> Integer -> ScriptContext -> () forwardMintPolicy stateToken tkIdx ctx = assetClassValueOf stateToken (txInInfoResolved (elemAt tkIdx (txInfoInputs (txInfo ctx)))) == 1\nThe time complexity of this validator is O(n) where n is the number of tx inputs. This logic is executed once per input being unlocked / currency symbol being minted. The redundant execution of searching the inputs for a token is the largest throughput bottleneck for these DApps; it is O(n*m) where n is the number of inputs and m is the number of forwardValidator inputs + forwardValidator minting policies. Using the stake validator trick, the time complexity of the forwarding logic is improved to O(1). The forwardValidator logic becomes:\nforwardValidator\nforwardValidator\nforwardWithStakeTrick:: StakingCredential -> BuiltinData -> BuiltinData -> ScriptContext -> () forwardWithStakeTrick obsScriptCred tkIdx ctx = fst (head stakeCertPairs) == obsScriptCred where info = txInfo ctx stakeCertPairs = AssocMap.toList (txInfoWdrl info) stakeValidatorWithSharedLogic :: AssetClass -> BuiltinData -> ScriptContext -> () stakeValidatorWithSharedLogic stateToken _rdmr ctx = assetClassValueOf stateToken (valueSpent (txInfo ctx)) == 1\nforwardWithStakeTrick:: StakingCredential -> BuiltinData -> BuiltinData -> ScriptContext -> () forwardWithStakeTrick obsScriptCred tkIdx ctx = fst (head stakeCertPairs) == obsScriptCred where info = txInfo ctx stakeCertPairs = AssocMap.toList (txInfoWdrl info) stakeValidatorWithSharedLogic :: AssetClass -> BuiltinData -> ScriptContext -> () stakeValidatorWithSharedLogic stateToken _rdmr ctx = assetClassValueOf stateToken (valueSpent (txInfo ctx)) == 1\nFor the staking validator trick (demonstrated above), we are simply checking that the StakingCredential of the the staking validator containing the shared validation logic is in the first pair in txInfoWdrl. If the StakingCredential is present in txInfoWdrl, that means the staking validator (with our shared validation logic) successfully executed in the transaction. This script is O(1) in the case where you limit it to one shared logic validator (staking validator), or if you don't want to break composability with other staking validator, then it becomes O(obs_N) where obs_N is the number of Observe validators that are executed in the transaction as you have to verify that the StakingCredential is present in txInfoWdrl.\ntxInfoWdrl\ntxInfoWdrl\nobs_N\ntxInfoWdrl\nThe proposed changes in this CIP enable this design pattern to exist indepedently from implementation details of stake validators and withdrawals, and improve efficiency and readability for validators that implement it. Furthermore, with the proposed extension to native scripts, we are able to completely get rid of the redundant spending script executions like so:\nobservationValidator :: AssetClass -> BuiltinData -> ScriptContext -> () observationValidator stateToken _redeemer ctx = assetClassValueOf stateToken (valueSpent (txInfo ctx)) == 1\nobservationValidator :: AssetClass -> BuiltinData -> ScriptContext -> () observationValidator stateToken _redeemer ctx = assetClassValueOf stateToken (valueSpent (txInfo ctx)) == 1\nWe simply include the script hash of the above observationValidator in the required_observers field in the transaction body and we lock all the UTxOs that we would like to share the same spending condition into the following native script:\nobservationValidator\nrequired_observers\n{ \"type\": \"observer\", \"keyHash\": \"OUR_OBSERVATION_SCRIPT_HASH\" }\n{ \"type\": \"observer\", \"keyHash\": \"OUR_OBSERVATION_SCRIPT_HASH\" }\nThe above solution (enabled by this CIP) is more clear, concise, flexible and efficient than the alternatives discussed above.\nThe type signature of this script type will be consistent with the type signature of minting and staking validators, namely:\nRedeemer -> ScriptContext -> ()\nRedeemer -> ScriptContext -> ()\nThe type signature of the newly introduced Purpose will be:\nPurpose\nObserve Integer -- ^ where integer is the index into the observations list.\nObserve Integer -- ^ where integer is the index into the observations list.\nScripts are passed information about transactions via the script context. We propose to augment the script context to include information about the observation scripts that are executed in the transaction.\nChanging the script context will require a new Plutus language version in the ledger to support the new interface. The change is: a new field is added to the script context which represents the list of observers that must be present in the transaction.\nThe interface for old versions of the language will not be changed. Scripts with old versions cannot be spent in transactions that include observation scripts, attempting to do so will be a phase 1 transaction validation failure.\nA new field will be introduced into the script context:\n-- | TxInfo for PlutusV3 data TxInfo = TxInfo { txInfoInputs :: [V2.TxInInfo] , txInfoReferenceInputs :: [V2.TxInInfo] , txInfoOutputs :: [V2.TxOut] , txInfoFee :: V2.Value , txInfoMint :: V2.Value , txInfoTxCerts :: [TxCert] , txInfoWdrl :: Map V2.Credential Haskell.Integer , txInfoValidRange :: V2.POSIXTimeRange , txInfoSignatories :: [V2.PubKeyHash] , txInfoRedeemers :: Map ScriptPurpose V2.Redeemer , txInfoData :: Map V2.DatumHash V2.Datum , txInfoId :: V2.TxId , txInfoVotingProcedures :: Map Voter (Map GovernanceActionId VotingProcedure) , txInfoProposalProcedures :: [ProposalProcedure] , txInfoCurrentTreasuryAmount :: Haskell.Maybe V2.Value , txInfoTreasuryDonation :: Haskell.Maybe V2.Value , txInfoObservations :: [V2.Credential] -- ^ newly introduced list of observation scripts that executed in this tx. }\n-- | TxInfo for PlutusV3 data TxInfo = TxInfo { txInfoInputs :: [V2.TxInInfo] , txInfoReferenceInputs :: [V2.TxInInfo] , txInfoOutputs :: [V2.TxOut] , txInfoFee :: V2.Value , txInfoMint :: V2.Value , txInfoTxCerts :: [TxCert] , txInfoWdrl :: Map V2.Credential Haskell.Integer , txInfoValidRange :: V2.POSIXTimeRange , txInfoSignatories :: [V2.PubKeyHash] , txInfoRedeemers :: Map ScriptPurpose V2.Redeemer , txInfoData :: Map V2.DatumHash V2.Datum , txInfoId :: V2.TxId , txInfoVotingProcedures :: Map Voter (Map GovernanceActionId VotingProcedure) , txInfoProposalProcedures :: [ProposalProcedure] , txInfoCurrentTreasuryAmount :: Haskell.Maybe V2.Value , txInfoTreasuryDonation :: Haskell.Maybe V2.Value , txInfoObservations :: [V2.Credential] -- ^ newly introduced list of observation scripts that executed in this tx. }\nThe CDDL for transaction body will change as follows to reflect the new field.\ntransaction_body = { 0 : set<transaction_input> ; inputs , 1 : [* transaction_output] , 2 : coin ; fee , ? 3 : uint ; time to live , ? 4 : certificates , ? 5 : withdrawals , ? 7 : auxiliary_data_hash , ? 8 : uint ; validity interval start , ? 9 : mint , ? 11 : script_data_hash , ? 13 : nonempty_set<transaction_input> ; collateral inputs , ? 14 : required_observers ; Upgraded `required_signers` , ? 15 : network_id , ? 16 : transaction_output ; collateral return , ? 17 : coin ; total collateral , ? 18 : nonempty_set<transaction_input> ; reference inputs , ? 19 : voting_procedures ; Voting procedures , ? 20 : proposal_procedures ; Proposal procedures , ? 21 : coin ; current treasury value , ? 22 : positive_coin ; donation } ; addr_keyhash variant is included for backwards compatibility and will be ; deprecated in the future era, because `credential` already contains `addr_keyhash`. required_observers = nonempty_set<credential / addr_keyhash>\ntransaction_body = { 0 : set<transaction_input> ; inputs , 1 : [* transaction_output] , 2 : coin ; fee , ? 3 : uint ; time to live , ? 4 : certificates , ? 5 : withdrawals , ? 7 : auxiliary_data_hash , ? 8 : uint ; validity interval start , ? 9 : mint , ? 11 : script_data_hash , ? 13 : nonempty_set<transaction_input> ; collateral inputs , ? 14 : required_observers ; Upgraded `required_signers` , ? 15 : network_id , ? 16 : transaction_output ; collateral return , ? 17 : coin ; total collateral , ? 18 : nonempty_set<transaction_input> ; reference inputs , ? 19 : voting_procedures ; Voting procedures , ? 20 : proposal_procedures ; Proposal procedures , ? 21 : coin ; current treasury value , ? 22 : positive_coin ; donation } ; addr_keyhash variant is included for backwards compatibility and will be ; deprecated in the future era, because `credential` already contains `addr_keyhash`. required_observers = nonempty_set<credential / addr_keyhash>\nWe rename the required_signers field to required_observers, promoting it from a list of public key hashes to a list of credentials (i.e. either a KeyHash or ScriptHash). This is consistent with other parts of the transaction that are unlocked by a script or a key witness. required_observers (field 14) is a set of credentials that must be satisfied by the transaction. For public key credentials, if the corresponding signature is not in the witness set, the transaction will fail in phase 1. For script credentials, if the associated scripts is not present in the witness set or as a reference script and executed in the transaction, the transaction will fail in phase 1 validation. This way Plutus scripts can check the script context to know which observation scripts were executed in the transaction. Similarly, since native script conditions use the required_observers field, it is natural that they are now able to require that other scripts observed the transaction (an extension of the ability to check for the presence of key signatures).\nrequired_signers\nrequired_observers\nrequired_observers\nrequired_observers\nThe BNF notation for the abstract syntax of native scripts change as follows to reflect the new field.\n<native_script> ::= <RequireSignature> <vkeyhash> | <RequireObserver> <scripthash> | <RequireTimeBefore> <slotno> | <RequireTimeAfter> <slotno> | <RequireAllOf> <native_script>* | <RequireAnyOf> <native_script>* | <RequireMOf> <num> <native_script>*\n<native_script> ::= <RequireSignature> <vkeyhash> | <RequireObserver> <scripthash> | <RequireTimeBefore> <slotno> | <RequireTimeAfter> <slotno> | <RequireAllOf> <native_script>* | <RequireAnyOf> <native_script>* | <RequireMOf> <num> <native_script>*\nNative scripts are typically represented in JSON syntax. We propose the following JSON representation for the RequireObserver constructor:\nRequireObserver\n{ \"type\": \"observer\", \"keyHash\": \"OBSERVATION_SCRIPT_HASH\" }\n{ \"type\": \"observer\", \"keyHash\": \"OBSERVATION_SCRIPT_HASH\" }\nCurrently Plutus scripts (and native scripts) in a transaction will only execute when the transaction performs the associated ledger action (ie. a Plutus minting policy will only execute if the transaction mints or burns tokens with matching currency symbol). The only exception is the withdraw zero trick which relies on an obscure mechanic where zero amount withdrawals are not filtered by the ledger. Now using required_observers we can specify a list of scripts (supports both native and Plutus scripts) to be executed in the transaction independent of any ledger actions. The newly introduced txInfoObservations field in the script context provides a straightforward way for scripts to check that \"a particular script validated this transaction\".\nrequired_observers\ntxInfoObservations\nThis change is not backwards-compatible and will need to go into a new Plutus language version.\nWe could decide to accept the withdraw-zero staking script trick as an adequate solution, and just preserve the nonsensical withdraw zero case in future language versions.\nThe staking script trick could be abstracted away from the developer by smart contract languages that compile to UPLC. This can be dangerous since by distancing the developer from what is actually happening you open up the door for the developer to act on misguided assumptions.\nThis can be dangerous since by distancing the developer from what is actually happening you open up the door for the developer to act on misguided assumptions.\nThese rules included within a official Plutus version, and released via a major hard fork.\nPasses all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency and usability.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0114 | CBOR Tags Registry\n\nCardano typically uses CBOR to encode structured data. For example, the Block data and individual transactions are all CBOR encoded.\nCIPs define many of the individual data structures used by both the Ledger and Metadata attached to transactions. This CIP defines a set of policies for the creation and maintenance of new CBOR tags for Cardano Data structures. It also collates a list of all currently defined Tags for easy reference. Finally, it recommends best practice for registering those Tags with IANA.\nThis CIP is motivated by the problems outlined in CPS-0014.\nCBOR defines a schemaless method of encoding data. Part of that specification (3.4) defines how generic data can be tagged to assist in the proper interpretation of the encoded data. Currently all Cardano data structures in both the Ledger and metadata do not widely use Tags. Nor is there any standardized way to create a new tag for a Cardano data structure.\nIANA maintains a registry of all CBOR Tags. Anyone can submit a request to register a tag in the range of 32768-18446744073709551615. These are known as First Come First Served tags. To register a tag, an applicant must demonstrate where the Tag is defined, and also the format of the data. If the data structure is defined by a CIP, the natural place to define its Tag is also in a CIP.\nTags can be defined for any data structure defined by a CIP or commonly used by Cardano. Examples of commonly used data structures could be those encoded inside the Cardano block or transaction. Tags should be defined by the following process:\nIf a CBOR tag is desired for an application, first the applicant MUST check that one does not already exist. If it does, then the pre-existing TAG must be used, as well as the defined canonical encoding. This is to prevent redundant and wasteful re-tagging of the same data structures. Otherwise the application may define a CBOR Tag for the necessary data structures as follows: The CIP which defines the data structure can also define a Tag and its encoding in CBOR. (Preferred) Alternatively, if the data structure is already defined by a CIP, then there are two possibilities. The original CIP is edited to add Tags and their canonical representation in CBOR. A supplementary CIP can be made that simply defines the TAG and canonical representation. Such a CIP would reference the original CIP to define the data structure itself. CIP authors would select an unused CBOR Tag in the IANA Registry, and assign it to their data structure. The CIP should add to its \"Path to Active\" that the selected Tag/s have been successfully registered with IANA. This allows the CIP to be edited with the final Tag once IANA have accepted and published the registration. CIP Authors would include in their PR an addition to the Cardano CBOR Tag Registry.\nIf a CBOR tag is desired for an application, first the applicant MUST check that one does not already exist. If it does, then the pre-existing TAG must be used, as well as the defined canonical encoding. This is to prevent redundant and wasteful re-tagging of the same data structures.\nOtherwise the application may define a CBOR Tag for the necessary data structures as follows: The CIP which defines the data structure can also define a Tag and its encoding in CBOR. (Preferred) Alternatively, if the data structure is already defined by a CIP, then there are two possibilities. The original CIP is edited to add Tags and their canonical representation in CBOR. A supplementary CIP can be made that simply defines the TAG and canonical representation. Such a CIP would reference the original CIP to define the data structure itself. CIP authors would select an unused CBOR Tag in the IANA Registry, and assign it to their data structure. The CIP should add to its \"Path to Active\" that the selected Tag/s have been successfully registered with IANA. This allows the CIP to be edited with the final Tag once IANA have accepted and published the registration. CIP Authors would include in their PR an addition to the Cardano CBOR Tag Registry.\nThe CIP which defines the data structure can also define a Tag and its encoding in CBOR. (Preferred) Alternatively, if the data structure is already defined by a CIP, then there are two possibilities. The original CIP is edited to add Tags and their canonical representation in CBOR. A supplementary CIP can be made that simply defines the TAG and canonical representation. Such a CIP would reference the original CIP to define the data structure itself. CIP authors would select an unused CBOR Tag in the IANA Registry, and assign it to their data structure. The CIP should add to its \"Path to Active\" that the selected Tag/s have been successfully registered with IANA. This allows the CIP to be edited with the final Tag once IANA have accepted and published the registration. CIP Authors would include in their PR an addition to the Cardano CBOR Tag Registry.\nThe CIP which defines the data structure can also define a Tag and its encoding in CBOR. (Preferred)\nAlternatively, if the data structure is already defined by a CIP, then there are two possibilities. The original CIP is edited to add Tags and their canonical representation in CBOR. A supplementary CIP can be made that simply defines the TAG and canonical representation. Such a CIP would reference the original CIP to define the data structure itself.\nThe original CIP is edited to add Tags and their canonical representation in CBOR. A supplementary CIP can be made that simply defines the TAG and canonical representation. Such a CIP would reference the original CIP to define the data structure itself.\nThe original CIP is edited to add Tags and their canonical representation in CBOR.\nA supplementary CIP can be made that simply defines the TAG and canonical representation. Such a CIP would reference the original CIP to define the data structure itself.\nCIP authors would select an unused CBOR Tag in the IANA Registry, and assign it to their data structure. The CIP should add to its \"Path to Active\" that the selected Tag/s have been successfully registered with IANA. This allows the CIP to be edited with the final Tag once IANA have accepted and published the registration.\nThe CIP should add to its \"Path to Active\" that the selected Tag/s have been successfully registered with IANA. This allows the CIP to be edited with the final Tag once IANA have accepted and published the registration.\nThe CIP should add to its \"Path to Active\" that the selected Tag/s have been successfully registered with IANA. This allows the CIP to be edited with the final Tag once IANA have accepted and published the registration.\nCIP Authors would include in their PR an addition to the Cardano CBOR Tag Registry.\nRegistration of the Tag with IANA is the sole responsibility of the CIP author. CIPs should not proceed to Active if they define unregistered Tags on data structures. This is to prevent abuse of the Tag number space.\nNOTE: Tags defined by any CIP or marked as not registered with IANA in the registry.json MUST NOT be used outside of testing. Tags not registered with IANA are subject to change during the IANA registration process.\nregistry.json\nCIPs, such as metadata CIPs can freely define when and how tags are used with any CBOR data structure. For example, a CIP may require that a particular field is always tagged. They may also define that a field need not be tagged if it is the typical data structure. Only less common or unusual data structures would be tagged in this case. A CIP may also define that Tags are not used, and only the canonical encoding.\nWhen a Tag is defined, its Canonical encoding must also be defined. This is to ensure that all data that is tagged is encoded in a uniform manor. Even if a CIP does NOT use a tag, it should preferably use the canonical encoding for the data structure. This is to prevent fragmentation and confusion amongst compliant encoders and decoders of the various data structures.\nIf a pre-existing data structure is being tagged, then its most common current encoding should be used as the Canonical encoding. It should not be re-defined.\nFor example ED25519-BIP32 public keys are commonly encoded as a byte array. The array is 32 bytes long with the most significant byte of the key appearing first in the array. If a Tag is defined for this public key, it should simply define the Tag. Its Canonical encoding MUST follow this common encoding scheme.\nED25519-BIP32\nIf a CIP does not refer to a Tag, nor the Canonical encoding specification for the data structure, AND it does not define an alternative encoding. Then the application implementing the encoding should assume it is encoded canonically. This helps ensure backward compatibility with pre-existing CIPs where Tags are not used.\nSimilar to [CIP-0010] this CIP defines a registry of all known tags. The format of the registry is defined by the json schema: CIP Tag Registry Schema. New entries MUST be added to the CIP Tag Registry in a PR for a CIP that first defines a new CBOR Tag. They MUST be updated when the Tag is accepted or rejected by IANA. The registry clearly notes if the tag is currently known to be registered or not. If a Tag is not yet registered then any implementor must be aware that its possible the Tag number could change and is not final. Unregistered tags MUST not be used in any main net on-chain metadata or data structures. They should only be used for testing purposes until registration is complete.\nCreating a registry for CIP Tag values has the following benefit:\nCIP Tag\nIt makes it easy for developers to know which CIP Tags to use, and if they have been registered or not. It makes it easy to avoid collisions with other standards that use CIP Tags. It helps CIP authors to find appropriate CIP tags for their use case, or to define new tags.\nIt makes it easy for developers to know which CIP Tags to use, and if they have been registered or not.\nCIP Tags\nIt makes it easy to avoid collisions with other standards that use CIP Tags.\nIt helps CIP authors to find appropriate CIP tags for their use case, or to define new tags.\nThe process for defining and registering Tags should help provide clarity about how a CIP tag can be defined. It also provides clarity on the responsibility CIP authors have to register the tags they create. If a CIP author is not prepared to take on that responsibility they should not create a Tag.\nCPS-0014 is accepted.\nAt least 1 CIPs are accepted into the Tag registry for historical tags.\nAt least 1 CIPs are accepted into the Tag registry for new tags.\nAt least 3 CIPs of any kind are accepted into the Tag registry.\nAuthor to write the first CBOR tag CIP.\nCPS-0014 - Register of CBOR Tags for Cardano Data structures\nRFC8949 - Concise Binary Object Representation (CBOR)\nIANA CBOR Tag Registry\nThis CIP is licensed under CC-BY-4.0\n2023 Cardano Foundation\n\n---\n\nCIP-0115 | CBOR tag definition - ED25519-BIP32 Keys\n\nCIP-0003 defines ED25519-BIP32 Keys, Key derivation and a signature scheme. This CIP defines CBOR Tags and formalizes the Canonical encoding of data structures for CIP-0003. The intention is to have these tags registered in the IANA CBOR Tag Registry.\nProject Catalyst is in the process of defining new CBOR data structures. We needed a way to reliably disambiguate different 32 byte strings. Rather than making a non-standard encoding scheme specific to our structures we would like to use standard CBOR Tags.\nThis CIP is informed by CPS-0014 and CIP-0114.\nWithout this Tag definition, a metadata CIP which uses ED25519-BIP32 public keys:\nIs likely to just encode public keys as a byte string of 32 bytes; and\nNeeds to redundantly define how the keys are encoded in the byte string.\nMay encode these keys differently to another CIP, which can lead to confusion and potential error.\nBIP32 also defines secp256k1 keys which are also 32 bytes long. This CIP would help disambiguate between these keys and inform the decoder which key is being utilized.\nsecp256k1\nNOTE: These tags are preliminary and subject to change until IANA registration is complete. They MUST not be used outside of testing purposes. They MUST not be used in any data intended to be posted to main-net.\nThis key is defined in ED25519-BIP32.\nThis is encoded as a byte string of size 32 bytes.\ned25519_private_key = #6.32771(bstr .size 32)\ned25519_private_key = #6.32771(bstr .size 32)\nData for the key inside the byte string is encoded in network byte order.\nThis key is defined in ED25519-BIP32.\nThis is encoded as a byte string of size 64 bytes.\ned25519_extended_private_key = #6.32772(bstr .size 64)\ned25519_extended_private_key = #6.32772(bstr .size 64)\nData for the key inside the byte string is encoded in network byte order.\nThis key is defined in ED25519-BIP32.\nThis is encoded as a byte string of size 32 bytes.\ned25519_public_key = #6.32773(bstr .size 32)\ned25519_public_key = #6.32773(bstr .size 32)\nData for the key inside the byte string is encoded in network byte order.\nED25519-BIP32 defines how signatures can be generated on data from private keys. These signatures are defined to be 64 bytes long.\nSignatures are encoded as a byte string of size 64 bytes.\ned25519_bip32_signature = #6.32774(bstr .size 64)\ned25519_bip32_signature = #6.32774(bstr .size 64)\nData for the signature inside the byte string is encoded in network byte order.\nBy defining concrete CBOR tags, it is possible for metadata to unambiguously mark the kind of data encoded. This is conformant with the intent of Tags in CBOR, and aligns with [CIP-CBOR-TAGS].\nAn official published spec is required to register these Tags with IANA. This document also serves that purpose.\nThese tags to be included in [CIP-CBOR-TAGS].\nOne downstream CIP uses at least one of the tags defined in this CIP.\nIANA register all the tags as defined herein.\nTags are to be used by Project Catalyst for CBOR data structure definitions.\nProject Catalyst will also make the application to IANA to register the Tags.\nCPS-0014 - Register of CBOR Tags for Cardano Data structures\nCIP-0003 - Wallet Key Generation\nCIP-0114 - CBOR Tags Registry\nRFC8949 - Concise Binary Object Representation (CBOR)\nRFC8610 - Concise Data Definition Language (CDDL)\nRFC1700 Data Notations (Network Byte Order)\nBIP32 - Hierarchical Deterministic Wallets\nED25519-BIP32 - Hierarchical Deterministic Keys over a Non-linear Keyspace\nIANA CBOR Tag Registry\nThis CIP is licensed under CC-BY-4.0\nCode samples and reference material are licensed under Apache 2.0\n2023 Cardano Foundation\n\n---\n\nCIP-0116 | Standard JSON encoding for Domain Types\n\nCanonical JSON encoding for Cardano domain types lets the ecosystem converge on a single way of serializing data to JSON, thus freeing the developers from repeating roughly the same, but slightly different encoding/decoding logic over and over.\nCardano domain types have canonical CDDL definitions (for every era), but when it comes to use in web apps, where JSON is the universally accepted format, there is no definite standard. This CIP aims to change that.\nThe full motivation text is provided in CPS-11 | Universal JSON Encoding for Domain Types.\nThis CIP is expected to contain multiple json-schema definitions for Cardano Eras and breaking intra-era hardforks starting from Babbage.\ncip-0116-tests repo contains utility functions and a test suite for the schema. In particular, there's a mkValidatorForType function that builds a validator function for any type defined in the schema.\ncip-0116-tests\nmkValidatorForType\nThe schemas should cover Block type and all of its structural components, which corresponds to the scope of CDDL files located in the ledger repo.\nBlock\nBelow you can find some principles outlining the process of schema creation / modification. They are intended to be applied when there is a need to create a schema for a new Cardano era.\nEvery transaction (i.e. CBOR-encoded binary) must have exactly one valid JSON encoding, up to entry ordering in mappings (that are represented as key-value pairs).\nFor a single JSON fixture, however, there are multiple variants of encoding it as CBOR.\nTo simplify transitions of dApps between eras, the scope of changes introduced to the schemas SHOULD be limited to the scope of CDDL changes.\nThese conventions help to keep the schema uniform in style.\nBinary data MUST be encoded as lower-case hexademical strings. Restricting the character set to lower-case letters (a-f) allows for comparisons and equality checks without the need to normalize the values to a uniform case.\na-f\nMap-like container types MUST be encoded as arrays of key-value pairs.\nMap\n\"Map\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"key\": ..., \"value\": ... }, \"required\": [ \"key\", \"value\" ], \"additionalProperties\": false } }\n\"Map\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"key\": ..., \"value\": ... }, \"required\": [ \"key\", \"value\" ], \"additionalProperties\": false } }\nUniqueness of \"key\" objects in a map MUST be preserved (but this property is not expressible via a schema).\n\"key\"\nImplementations MUST consider mappings with conflicting keys invalid.\nSome mapping-like types, specifically Mint, allow for duplicate keys. Types like these should not be encoded as maps, instead, key and value properties should be named differently.\nMint\nkey\nvalue\nEncoding types with variable payloads MUST be done with the use of oneOf and an explicit discriminator property: tag:\noneOf\ntag\n{ \"Credential\": { \"type\": \"object\", \"discriminator\": { \"propertyName\": \"tag\" }, \"oneOf\": [ { \"type\": \"object\", \"properties\": { \"tag\": { \"enum\": [ \"pubkey_hash\" ] }, \"value\": { \"$ref\": \"cardano-babbage.json#/definitions/Ed25519KeyHash\" } }, \"required\": [\"tag\", \"value\"], \"additionalProperties\": false }, { \"type\": \"object\", \"properties\": { \"tag\": { \"enum\": [ \"script_hash\" ] }, \"value\": { \"$ref\": \"cardano-babbage.json#/definitions/ScriptHash\" } }, \"required\": [\"tag\", \"value\"], \"additionalProperties\": false } ] } }\n{ \"Credential\": { \"type\": \"object\", \"discriminator\": { \"propertyName\": \"tag\" }, \"oneOf\": [ { \"type\": \"object\", \"properties\": { \"tag\": { \"enum\": [ \"pubkey_hash\" ] }, \"value\": { \"$ref\": \"cardano-babbage.json#/definitions/Ed25519KeyHash\" } }, \"required\": [\"tag\", \"value\"], \"additionalProperties\": false }, { \"type\": \"object\", \"properties\": { \"tag\": { \"enum\": [ \"script_hash\" ] }, \"value\": { \"$ref\": \"cardano-babbage.json#/definitions/ScriptHash\" } }, \"required\": [\"tag\", \"value\"], \"additionalProperties\": false } ] } }\nOther properties of a tagged object MUST be specified in lower-case snake-case.\nEnums are a special kind of variant types that carry no payloads. These MUST be encoded as string enums.\nenum\nLowercase snake case identifiers MUST be used for the options, e.g.:\n{ \"Language\": { \"title\": \"Language\", \"type\": \"string\", \"enum\": [ \"plutus_v1\", \"plutus_v2\" ] } }\n{ \"Language\": { \"title\": \"Language\", \"type\": \"string\", \"enum\": [ \"plutus_v1\", \"plutus_v2\" ] } }\nAll record types MUST be encoded as objects with explicit list of required properties, and additionalProperties set to false (see \"absence of extensibility\" chapter for the motivation behind this suggestion).\nrequired\nadditionalProperties\nfalse\nSome of the types have identical representations, differing only by nominal name. For example, Slot domain type is expressed as uint in CDDL.\nSlot\nuint\nFor these types, their nominal name SHOULD NOT have a separate definition in the json-schema, and the \"representation type\" should be used via a $ref instead. The domain type name SHOULD be included as title string at the point of usage.\n$ref\ntitle\nSome non-standard format types are used:\nformat\nhex - lower-case hex-encoded byte string\nhex\nbech32 - bech32 string\nbech32\nbase58 - base58 string\nbase58\nuint64 - 64-bit unsigned integer\nuint64\nint128 - 128-bit signed integer\nint128\nstring64 - a unicode string that must not exceed 64 bytes when utf8-encoded.\nstring64\nposint64 - a positive (0 excluded) 64-bit integer. 1 .. 2 64-1\nposint64\n1 .. 2^64-1\nJSON-schema does not allow to express certain properties of some of the types.\nSee the chapter on encoding of mapping types.\nValidity of values of these types can't be expressed as a regular expression, so the implementations MAY validate them separately.\nBech32 strings are not always valid addresses: even if the prefixes are correct, the binary layout of the payload must also be valid.\nThe implementations MAY validate it separately.\nIn CDDL, the length of a tstr value gives the number of bytes, but in json-schema there is no way to specify restrictions on byte lengths. So, maxLength is not the correct way of specifying the limits, but it is still useful, because no string longer than 64 characters satisfies the 64-byte limit.\ntstr\njson-schema\nmaxLength\nauxiliary_data CDDL type is handled specially.\nauxiliary_data\nauxiliary_data = metadata ; Shelley / [ transaction_metadata: metadata ; Shelley-ma , auxiliary_scripts: [ * native_script ] ] / #6.259({ ? 0 => metadata ; Alonzo and beyond , ? 1 => [ * native_script ] , ? 2 => [ * plutus_v1_script ] , ? 3 => [ * plutus_v2_script ] })\nauxiliary_data = metadata ; Shelley / [ transaction_metadata: metadata ; Shelley-ma , auxiliary_scripts: [ * native_script ] ] / #6.259({ ? 0 => metadata ; Alonzo and beyond , ? 1 => [ * native_script ] , ? 2 => [ * plutus_v1_script ] , ? 3 => [ * plutus_v2_script ] })\nInstead of providing all three variants of encoding, we base the schema on the one that is the most general (the last one):\n{ \"AuxiliaryData\": { \"properties\": { \"metadata\": { \"$ref\": \"cardano-babbage.json#/definitions/TransactionMetadata\" }, \"native_scripts\": { \"type\": \"array\", \"items\": { \"$ref\": \"cardano-babbage.json#/definitions/NativeScript\" } }, \"plutus_scripts\": { \"type\": \"array\", \"items\": { \"$ref\": \"cardano-babbage.json#/definitions/PlutusScript\" } } }, } }\n{ \"AuxiliaryData\": { \"properties\": { \"metadata\": { \"$ref\": \"cardano-babbage.json#/definitions/TransactionMetadata\" }, \"native_scripts\": { \"type\": \"array\", \"items\": { \"$ref\": \"cardano-babbage.json#/definitions/NativeScript\" } }, \"plutus_scripts\": { \"type\": \"array\", \"items\": { \"$ref\": \"cardano-babbage.json#/definitions/PlutusScript\" } } }, } }\nIt is up to implementors to decide how to serialize the values into CBOR. The property we want to maintain is preserved regardless of the choice: for every block binary there is exactly one JSON encoding.\nThis CIP should not follow a conventional versioning scheme, rather it should be altered via pull request before a hardforks to add new a JSON schema to align with new ledger ers. Each schema must be standalone and not reuse definitions between eras. Authors MUST follow the Schema Scope, Schema Design Principles and Schema Conventions.\nFurthermore, for each subsequent schema, the changelog must be updated. Authors must clearly articulate the deltas between schemas.\nWe keep the scope of this standard to the data types within Cardano blocks. The rationale for this is that block data is by far the most useful for the majority of Cardano actors. There is also one nice benefit that the definitions can map directly from the provided CDDL file from ledger team.\nThis CIP lays out strong conventions that future schema authors must follow, along with a large set of design principles. The aim is to minimize the potential for unavoidable deltas between schemas.\nBy setting sometimes arbitrary conventions we hope to create a single possible interpretation from CBOR to JSON, alleviating any ambiguity.\nThe schemas MUST NOT be extensible with additional properties. This may sound counter-intuitive and against the spirit of json-schema, but there are some motivations behind that:\nMore safety from typos: object fields that are optional may be specified with slightly incorrect names in dApps' code, leading to inability of the decoders to pick up the values, which may go unnoticed.\nClear delineation between Cardano domain types and user dApp domain types: forcing the developers to store their dApp domain data separately from Cardano data, or close to it (as opposed to mixing these together in a single object) will indirectly motivate better structured dApp code.\nJSON was chosen as there is no viable alternative. The majority of Cardano's web tooling is built with Javascript where JSON is the primary object representation format.\nFurthermore, even across non-Javascript based stacks, JSON enjoys wide tooling support, this improves the potential for builders to adopt this standard.\nWe choose to use Bech32 as the representation for Cardano addresses. When compared to the alternative of hexademical encoding, Bech32 gives the advantages of an included checksum and a human readable prefix.\nOne future ledger era schema is added\nThis standard is implemented within three separate tools, libraries, etc.\nComplete the specification for the current Babbage era\nProvide a test suite validating JSON fixtures for all the types against the schema\nProvide an implementation of validating functions that uses this json-schema mlabs-haskell/cip-0116-tests\nmlabs-haskell/cip-0116-tests\nCollect a list of cardano domain types implementations and negotiate transition to the specified formats with maintainers (if it makes sense and is possible)\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0117 | Explicit script return values\n\nToday, Plutus Core scripts signal success or failure exclusively by whether the script terminates normally or abnormally (with an error). This leads to some false positives, where a script terminates normally, but this was not intended by the author. We propose to additionally look at what the script evaluates to when checking for success or failure.\nConsider the following Plutus scripts, intended to be used as minting policy scripts:\n\\redeemer -> \\datum -> \\script-context -> (builtin error)\n\\redeemer -> \\datum -> \\script-context -> (builtin error)\n\\redeemer -> \\script-context -> (con false)\n\\redeemer -> \\script-context -> (con false)\nToday, both of these will unconditionally succeed!\nMinting policies only receive two arguments, but this script expects three before it does any work. It therefore evaluates successfully to a lambda. This script evaluates successfully to (con false), but the return value is irrelevant since it terminates successfully.\nMinting policies only receive two arguments, but this script expects three before it does any work. It therefore evaluates successfully to a lambda.\nThis script evaluates successfully to (con false), but the return value is irrelevant since it terminates successfully.\n(con false)\nIn both cases the user has made a mistake, but this result is that the script fails open, that is, anyone can spend such an output. A variant of the first mistake is for a script to expect too few arguments, but this will almost always result in an error and so fail closed.1\nHistorically, Plutus Core was going to be a typed language, and at least the first kind of error would have been caught by the typechecker. However, today there is little stopping people from making such mistakes.\nWhile these mistakes are relatively easily avoidable (any good smart contract toolkit should prevent them), it is nonetheless a potential landmine for users.\nThe specification for checking whether a Plutus Core script accepts a transaction changes as follows (the new part is in brackets):\nA Plutus Core script S with arguments A1...An accepts a transaction if 'eval(S A1 ... An)' succeeds [and evaluates to the builtin constant 'unit'].\nThis change is not backwards-compatible and will need to go into a new Plutus ledger language.\nSince the return value of a script will now be significant, a script will only succeed if the whole thing evaluates to 'unit'. This is very unlikely to happen by accident: mistakes in the number of arguments or in what to return will result in failure.\nThe status quo is not terrible, and we could simply accept it.\nThe return value could be a boolean constant, with 'true' indicating success and 'false' indicating failure. This is slightly more complicated, and technically we only need a designated success value, since \"anything else\" indicates failure. We don't need to distinguish between \"normal exit indicating rejection of the transaction\" and \"abnormal exit\".\nThis is slightly more complicated, and technically we only need a designated success value, since \"anything else\" indicates failure. We don't need to distinguish between \"normal exit indicating rejection of the transaction\" and \"abnormal exit\".\nWe could specifically detect when a script returns a lambda, and say that that is a failure. This is patching up one particular hole, whereas the proposal here has much more coverage by failing everything that doesn't quite specifically return 'true'.\nThis is patching up one particular hole, whereas the proposal here has much more coverage by failing everything that doesn't quite specifically return 'true'.\nThe proposal is implemented in the Ledger.\nThe ledger changes are released on Mainnnet.\nThe Plutus team will implement the changes to the Ledger.\nThis CIP is licensed under CC-BY-4.0.\nIt is not universally clear whether it is good to fail open or closed, but generally for systems like this we tend to fail closed, and it is also easier to detect such failures during testing.\nIt is not universally clear whether it is good to fail open or closed, but generally for systems like this we tend to fail closed, and it is also easier to detect such failures during testing.\nIt is not universally clear whether it is good to fail open or closed, but generally for systems like this we tend to fail closed, and it is also easier to detect such failures during testing.\n2023 Cardano Foundation\n\n---\n\nCIP-0119 | Governance metadata - DReps\n\nThe Conway ledger era ushers in on-chain governance for Cardano via CIP-1694 | A First Step Towards On-Chain Decentralized Governance, with the addition of many new on-chain governance artefacts. Some of these artefacts support linking to off-chain metadata as a way to provide context.\nThe CIP-100 | Governance Metadata standard provides a base framework for how all off-chain governance metadata should be formed and handled. But this is intentionally limited in scope, so that it can be expanded upon by more specific subsequent CIPs.\nThis proposal aims to provide a specification for off-chain metadata vocabulary that can be used to give context to CIP-100 for DRep registration and updates. Without a sufficiently detailed standard for DRep metadata we introduce the possibility to undermine the ability of DReps to explain their motivations and therefore people looking for someone to represent them with the best possible information available to make that choice. Furthermore a lack of such standards risks preventing interoperability between tools, to the detriment of user experiences.\nThank you to everyone who participated in the CIP workshops, and to @ryun1 for creating the JSON-LD schemas for this CIP and for his excellent technical support and invaluble advice. Thank you also to the other CIP editors and attendees of the CIP Editors' Meetings where this CIP was refined, most notably @rphair and @Crypto2099.\nCIP-1694 has set forth a model of a blockchain controlled by its community, and in doing so has challenged providers to build apps and tools that will allow users easy access to the governance features currently being built into Cardano. Minimum viable tools must be ready at the time these governance features are launched.\nThe motivation for this CIP therefore is to provide these toolmakers with a simple and easy to accommodate standard which, once adopted, will allow them to read and display the metadata created by anyone who follows this standard when creating their DRep registration or update metadata. Tooling designed for DReps so that they can easily create metadata will also be made possible, because toolmakers will not need to individually innovate over the contents or structure of the metadata that their tool creates.\nMetadata is needed because blockchains are poor choices to act as content databases. This is why governance metadata anchors were chosen to provide a way to attach long form metadata content to on-chain events. By only supplying a url to the off-chain metadata, and a hash of that metadata to the blockchain we ensure correctness of data whilst minimising the amount of data posted on-chain.\nI believed that this CIP would provide a benefit to:\nWhen observing from the chain level, tooling can only see the content and history of DRep registration and update certificates and any associated anchors. These on-chain components do not give any context to the motivation of a DRep, even though this information would likely be the desired context for people who might delegate their voting power. By providing rich contextual metadata we enable people choosing a DRep to delegate their voting power to make well informed decisions.\nDReps will be able to use tools that create metadata in a standard format. This in turn will allow their metadata to be read by apps that will render their words on screen for potential delegating Ada Holders to enjoy, this may lead to greater levels of delegation.\nBy standardising off-chain metadata formats for any tooling which creates and/or renders metadata that is referenced in DRep registration and update transactions we facilitate interoperability. This in turn promotes a rich user experience between tooling. This is good for all governance participants.\nThis CIP explains the structure of any metadata referenced in a metadata anchor optionally included in any DRep registration or update transaction.\nThis CIP has been written for individuals acting in the capacity of DReps, and not for teams of people collaborating as a single DRep, although this does not preclude teams from using metadata in the structure explained by this CIP.\nDRep Metadata will not follow the CIP-100 specification related to signing the metadata, the authors property can be left blank without the need for tooling providers to warn their users that the author has not been validated. Instead the author can be derived from the DRep ID associated with the registration or update. The need for an authors field will also be discarded in favour of including givenNames and Identity inside of the body field. For the avoidance of doubt this CIP recommends that the entire authors property be left blank, and that tooling ignore it.\nauthors\nauthors\ngivenName\nIdentity\nbody\nLike CIP-108, this CIP also extends the potential vocabulary of CIP-100's body property.\nbody\nFurthermore we extend the Schema.org definition of a Person. Any property of Person maybe included within the body.\nbody\nReminder for tooling providers/builders DRep metadata is user generated content.\nThe following are a list of properties tooling should expect to encounter:\npaymentAddress\nOptional\nBech32 encoded payment address, for the same network as the DRep registration is to be submitted to.\nDReps may want to receive tokens for a variety of reasons such as:\ndonations\nexpenses\nany incentive program\nTherefore there MAY be a paymentAddress field in the metadata where such payments could be sent. This makes such an address public and payments to DReps transparent.\npaymentAddress\nThis SHOULD NOT be confused with the address property of a Person, address in the context of a DRep refers to their location and NOT their payment address.\naddress\naddress\ngivenName\nCompulsory\nThis is a property inherited from Person\nIt is the only compulsory property\nA very short freeform text field. Limited to 80 characters.\nThis MUST NOT support markdown text styling.\nIt is intended that authors will use this field for their profile name/ username.\nimage\nOptional\nThis is a property inherited from Person\nThis SHOULD be treated as the profile picture of the individual\nThis MUST contain a fully described imageObject property\nimageObject\nimageObject\nThis is to be included in a metadata file as a property of the image property, only if the image property is included.\nimage\nimage\nIt explains the image to those (inc. tools) who are viewing it.\nimageObject MUST take one of the following forms: base64 encoded image URL of image\nimageObject\nbase64 encoded image URL of image\nbase64 encoded image\nURL of image\nimageObject contains a base64 encoded image in its contentUrl property in a dataURI format:\nimageObject\ncontentUrl\ni.e. data:content/type;base64, (AND NOT data:domain.tld)\ne.g. contentURL:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg== (AND NOT contentURL:https://avatars.githubusercontent.com/u/113025685?v=4)\nIf the imageObject DOES NOT contain a base64 encoded image, the contentUrl MUST contain the URL where the image can be found and the sha256 property MUST be populated with the SHA256 hash of the image file contents found at the contentUrl. The SHA256 hash is needed in order for readers to verify that the image has not been altered since the metadata anchor was submitted on-chain.\nimageObject\ncontentUrl\nsha256\ncontentUrl\nobjectives\nOptional\nA freeform text field with a maximum of 1000 characters\nA short description of what the person believes and what they want to achieve as a DRep\nmotivations\nOptional\nA freeform text field with a maximum of 1000 characters\nA short description of why they want to be a DRep, what personal and professional experiences have they had that have driven them to register\nqualifications\nOptional\nA freeform text field with a maximum of 1000 characters\nA space where the registrant can to list the qualifications they hold that are relevant to their role as a DRep\nThis is distinct from the properties of a Person listed as knows and knowsAbout because it encompasses things that the DRep has done as well as things that they know that qualify them for this position.\nknows\nknowsAbout\nreferences\nOptional\nThis CIP extends the references property from CIP-100\nreferences\nreferences contain the following sub-properties @type, label, and uri\nreferences\n@type\nlabel\nuri\nThis CIP adds two @type identifiers \"Identity\" and \"Link\"\n@type\n@type\nOptional\nIt is expected that these links will be the addresses of the social media/ websites associated with the DRep in order to give a person reviewing this information a fulsome overview of the DRep's online presence.\nThe creator of the metadata SHOULD add a label, this label SHOULD describe the source of the url, e.g. if it is a link to the DRep's X account then the label SHOULD say \"X\". If it is the only personal website provided by the DRep the label should say \"Personal Website\" rather than domain_name.com.\nlabel\nlabel\nlabel\nlabel\nThe label of each Link SHOULD NOT be left blank\nlabel\nLink\nEach Link MUST have exactly one uri (as specified in CIP-100) which SHOULD not be blank.\nLink\nuri\n@type\nOptional\nThe uri of a reference with the @type \"Identity\" is a way for DReps to prove that they are who they say they are\nuri\n@type\nIt is expected that the \"Identity\" of a DRep will be the addresses of their most active social media account twitter, linkedin etc. or personal website.\nThe DRep must reference their DRep ID in a prominent place at the location that is specified in the uri property of that reference. This will be used by people reviewing this DRep to prove and verify that the person described in the metadata is the same as the person who set up the social media profile.\nuri\ndoNotList\nOptional\nIs a boolean expression that can be given a single value of either true or false.\ntrue\nfalse\nIf not included then the value is assumed to be false.\nfalse\nA true value means that the DRep does not want to campaign for delegation via tooling.\ntrue\nA false value means that the DRep does want to campaign for delegation via tooling and thus be shown via that tooling.\nfalse\ne.g. a DRep who does not want to appear in GovTool s DRep Directory feature creates metadata with doNotList=true.\ndoNotList=true\nOnly the givenName property is listed above as compulsory, DRep metadata must include it to be considered CIP-119 compliant. As this is an extension to CIP-100, all CIP-100 fields can be included within CIP-119 compliant metadata.\ngivenName\nSee test-vector.md for examples.\nThis proposal should not be versioned, to update this standard a new CIP should be proposed. Although through the JSON-LD mechanism further CIPs can add to the common governance metadata vocabulary.\nWe intentionally have kept this proposal brief and uncomplicated. This was to reduce the time to develop and deploy this standard. This way we enable tooling which depends on this standard to start development.\nThe compulsory nature of this field was controversial because the givenNames cannot be made unique and therefore are open to abuse (by e.g. copycats). However this is not a reason to not include a givenName, it a reason for people reviewing a DRep's profile to properly check their whole profile before delegating to them. A givenName MUST be included because a person must always have a name. Iit is a human readable identifier, it is the property that people reviewing DReps will most likely identify a given DRep by, even in the presence of copycats.\ngivenName\ngivenName\ngivenName\nIt has been suggested that the objectives, motivations, and qualifications properties (or at least the latter two) could be one freeform property instead of 3. The rationale for having 3 separate properties is to provide structure to DReps so that they have a useful set of prompts about what they can and should write about. The author noticed in research that a single bio field in a form typically resulted in lower quality, often single line, responses from respondents than when this bio field was split into smaller fields with more highly specified purposes.\nobjectives\nmotivations\nqualifications\nbio\nbio\nIt has also been suggested that the format of the input into these three properties could be more tightly specified for example qualifications could require a list of qualifications. Whilst this will probably be needed I have left this up to a future CIP to specify what these specifications should be because at this stage (MVP) I have no concrete examples of how people will end up using these fields and I want to leave it up to the community to experiment with this.\nqualifications\nPKI cryptographic solutions were also considered in the place of the current solution however they were generally considered too complex to implement for minimum viable tooling. There were also other issues with cryptographic forms of identification for MVPs:\nsolutions that involve a public/private key setup still require people verifying the identity of a DRep to know what their authentic public key is. specifying the use of a verification service such as Keybase would lead to centralisation and therefore reliance on a 3rd party for the identity validation of DReps.\nsolutions that involve a public/private key setup still require people verifying the identity of a DRep to know what their authentic public key is.\nspecifying the use of a verification service such as Keybase would lead to centralisation and therefore reliance on a 3rd party for the identity validation of DReps.\nThis CIP is not written to specifically cover the metadata created to describe teams of people collaborating to register as a DRep, but it is written to cover the metadata created by individuals to describe themselves in their capacity as a DRep. Therefore DReps are people.\nPeople who want to extend the use of the DRep metadata can now do so in a way that allows tooling providers to use off the peg solutions. Furthermore there may be SEO benefits to using schema.org templates.\nimageObject\nAccording to schema.org The image property inherited from Person can either be a URL to a separate location where an image is stored, or it can be an imageObject.\nimage\nimageObject\nFor the following reasons it was originally intended that this CIP would specify the use of an imageObject with a b64 encoded image only, because:\nimageObject\nThe data at the location specified by a URL could be subject to change without the hash in the metadata anchor needing to be changed Choosing just one way to write and read image data would to limit the amount of tooling options that need to be created to cater to those wishing to create DRep metadata.\nThe data at the location specified by a URL could be subject to change without the hash in the metadata anchor needing to be changed\nChoosing just one way to write and read image data would to limit the amount of tooling options that need to be created to cater to those wishing to create DRep metadata.\nHowever it was pointed out that this may quickly lead to relatively massive (multi-megabyte) metadata files that are more difficult to fetch and store without providing substantial value. Even IPFS would take a relatively long time to serve these files, and if there was a need to index them by some chain indexer (such as DB-Sync) then this could massively increase the storage space needed to run the indexer.\nIt is also the case that CIP-100 allows for metadata to be saved within a governance transaction, and including b64 encoded images directly within transactions would be troublesome due to their size. This would not be an issue with including an image file URL.\nTherefore it was decided to allow a provision for people to submit imageObject's with a URL only if a hash was included OR with a base64 encoded image, and allow them to make the decision as to which was most appropriate for their use case.\nimageObject\ndoNotList\nThis field was intended for DReps who wish to identify themselves via rich metadata but are not seeking to campaign for delegations. By not being listed via \"DRep aggregation/campaign\" tools the idea is that these DReps are less likely to attract unwanted delegation from ada holders. These DReps could be organizations that want to use their ada to vote in a transparent way on-chain but do not wish to vote on the behalf of others.\nIt is expected that tooling such as block explorers will list DReps using doNotList=true. Tooling built specifically for DRep campaign and delegation should respect the intent of this field.\ndoNotList=true\nThis proposal cannot force tooling to respect this desire from DReps. DReps must be aware that any information anchored on-chain can be found via tooling and may result in delegation.\nCIP-1694 allows for DReps to be registered using a native or Plutus script credential, this implies that individuals could organise to form a team that would have a broad range of expertise, and would perhaps therefore be more attractive to some delegating Ada Holders.\nParticipants at the workshop held to debut the first draft of this CIP strongly advocated in favour of including features that would assist a DRep comprised of a team of individuals to properly describe their endeavour.\nThis CIP has not included these features, the decision not to include these features was made in order to simplify this CIP so that it became suitable for the minimum viable level of tooling, with the expectation that further CIPs will build on it.\nbio\npaymentAddress\ntype\nPublish JSON-LD schemas & test-vector.md\nAdoption by at least one community tool\nThere have been 3 lively public workshops on this subject, and I would like to thank the following people\nAbhik Nag\nAdam Dean\nAdam Rusch\nDigital Fortress (Rick McCracken)\nEduardo Silka\nJose Miguel De Gamboa\nLeonardo Silka\nLorenzo Bruno\nMike Susko\nNicolas Cerny\nPeter Wolcott\nRyan Williams\nSheldon Hunt\nTyler Wales\nUpstream SPO\nValeria Devaux\nA recording of this meeting can be found here\nAdam Rusch\nAleksandar Djuricic (Aleks)\nChristopher Hockaday\nDigital Fortress (Rick McCracken)\nEduardo Silka\nHD5000\nIgor Velickovic\nInput Endorsers\nKonstantinos Dermentzis\nLeonardo Silka\nMichael Madoff\nMicha Sza owski\nMike Hornan\nPeter Wolcott\nRyan\nRyan (Cerkoryn)\nRyan Williams\nSteve Lockhart\nA recording of this meeting can be found here\nAndreas,\nLinh P,\nMark Byers,\nMike Hornan,\nPedro Lucas,\nPhil Lewis,\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0120 | Constitution specification\n\nCardano's minimum viable governance model as described within CIP-1694 | A First Step Towards On-Chain Decentralized Governance introduces the concept of a Cardano constitution. Although CIP-1694 gives no definition to the constitution's content or form.\nThis proposal aims to describe a standardized technical form for the Cardano Constitution to enhance the accessibility and safety of the document.\nNote: This proposal only covers the technical form of the constitution, this standard is agnostic to the content of the constitution.\nCIP-1694 defines the on-chain anchor mechanism used to link the off-chain Constitution document to on-chain actions. This mechanism was chosen due to its simplicity and cost effectiveness, moving the potentially large Cardano constitution off-chain, leaving only a hash digest and URI on-chain. This is the extent to which CIP-1694 outlines the Cardano Constitution: CIP-1694 does not provide suggestions around hashing algorithm, off-chain storage best practices or use of rich text styling.\nBy formalizing the form of the constitution and its iterations, we aim to promote its longevity and accessibility. This is essential to ensure the effectiveness of the CIP-1694 governance model.\nThis standard will impact how Ada holders read the Constitution but the main stakeholders for this are the tool makers who wish to read, render and write a constitution.\nWithout describing best practices for the form and handling of the constitution, we risk the constitution document being stored in an insecure manner. By storing the constitution on a decentralized platform, we can ensure its immutability and permissionless access. This is a step to improve the longevity and accessibility of each constitution iteration.\nBy defining a file extension and formatting rules for the constitution we ensure that tooling reading and writing the constitution will be interoperable. Furthermore we aim to make the role of constitution iteration comparison tools easier, by minimizing formatting and style changes between iterations. This will reduce compatibility issues between tools, promoting the accessibility of the constitution.\nRich text formatting greatly enhances the readability of text, especially in large complex documents. Without the ability to format text, it could easily become cumbersome to read, negatively effecting the accessability of the Cardano constitution.\nThe following specification SHOULD be applied to all constitution iterations. This standard could be augmented in the future via a separate CIP which aims to replace this one.\nTo avoid unnecessary edits, and therefore checksum changes, constitution authors MUST follow the following standard English capitalisation rules unless a translation language indicates otherwise:\nConstitution\nconstitution\nThe Constitution in effect either the \"initial\" one or any new constitution is unique and therefore capitalised (\"Constitution\") as a proper noun.\nDraft or proposed constitutions are not unique & therefore are not capitalised (\"constitution\") as a common noun.\n\"Cardano Constitution\" is a very specific proper noun phrase (also a title) and so each word is capitalised.\nThe phrase \"the Constitution\", unless used non-specifically (e.g. \"the constitution that voters prefer\"), would generally be assumed to be the Constitution and capitalised as a proper noun.\nAda\nada\nADA\nAda is the currency, while ada indicates units of that currency (e.g. \"Ada holders can accumulate more ada to increase their influence.\")\nAda\nada\nADA is the trading symbol (e.g. \"Fluctuations in ADA might influence decisions about the Treasury.\")\nADA\nThe constitution text MUST only contain printable text characters in UTF-8 encoding.\nEach line in the constitution text MUST contain at maximum 80 characters, including spaces and punctuation.\nWhile 80 characters is a limit, authors don't have to try and always hit 80. Legibility of the raw (unrendered) document SHOULD be kept in mind.\nThe constitution text MUST only contain a maximum of one sentence per line, with each sentence followed by a newline. Each new sentence SHOULD start on its own line with a capitalized letter.\nLong sentences can be split multiple lines, when writing the author SHOULD try to split long sentences along natural breaks.\nExample:\nThis is a short sentence on one line. This is a long sentence and I have valid reasons for it being so long, such as being an example of a long sentence. When this sentence is rendered it SHOULD be shown to directly follow the sentence above. This sentence is the start of a new paragraph.\nThis is a short sentence on one line. This is a long sentence and I have valid reasons for it being so long, such as being an example of a long sentence. When this sentence is rendered it SHOULD be shown to directly follow the sentence above. This sentence is the start of a new paragraph.\nWhen rendered, these newlines between sentences SHOULD NOT be shown as newlines. Instead they SHOULD be rendered as a space character between sentences.\nParagraphs are shown by leaving a blank line between text.\nConstitution files MUST be .txt files.\n.txt\nConstitution files SHOULD be named in sequential whole numbers. Following the pattern cardano-constitution-{i}.txt where {i} is the iteration number.\ncardano-constitution-{i}.txt\n{i}\nStarting from an interim constitution named cardano-constitution-0.txt, the next constitution SHOULD be named cardano-constitution-1.txt.\ncardano-constitution-0.txt\ncardano-constitution-1.txt\nTo prevent misalignment, constitutions governing networks other than Cardano's mainnet, CAN be prefixed with the network name. For example, on the preview network the constitution file COULD be named preview-cardano-constitution-0.txt.\npreview-cardano-constitution-0.txt\nWhen supplying a constitution hash digest to chain, the algorithm used MUST be Blake2b-256. Before creating a hash digest the constitution plain text MUST be in its raw text, including any Rich Text Formatting related characters.\nThe each ratified constitution MUST be stored, immutably on a distributed storage mechanism where backups can be easily made in a permissionless manner by interested parties. This storage platform SHOULD be easily accessible, with strong tooling support.\nWhen generating a URI for the document, authors SHOULD NOT include centralized gateways.\nWe propose using the InterPlanetary File System (IPFS).\nThe constitution text MAY include a strict subset of rich text styling as defined in this specification. Tooling rendering the constitution SHOULD recognize these and render them faithfully.\nTo create paragraphs, use a blank line to separate one or more lines of text.\nExamples:\nHere's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*.\nHere's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*.\nHeaders are denoted via a hashtag character # followed by a space . There are six levels of headers, with each lower level set via an added #. Headers are ended via a line break. Headers SHOULD be followed below by a blank line. Headers SHOULD not be preceded by whitespace.\n#\n#\nThe lower the number of # the larger order the text SHOULD be rendered.\n#\nExample:\n# H1 ## H2 Some non-heading text. ### H3 #### H4 ##### H5 ###### H6\n# H1 ## H2 Some non-heading text. ### H3 #### H4 ##### H5 ###### H6\nIf text is in a header no other formatting can be applied.\nAn empty line SHOULD be left above and below each heading\nEmphasis is applied to text between single or double asterisks, without space between asterisks and text. Italicized emphasis is shown via single asterisk (*). Bold emphasis is shown via double asterisks (**).\n*\n**\nEmphasis cannot span multiple lines.\nExamples:\nEmphasis, aka italics, with single *asterisks*. Strong emphasis, aka bold, with double **asterisks**.\nEmphasis, aka italics, with single *asterisks*. Strong emphasis, aka bold, with double **asterisks**.\nBoth italicized and bold cannot be applied to the same text.\nTexted can be highlighted as code, when encased without spaces by backticks. This MUST not contain line breaks.\nExample:\nInline `code` has `back-ticks around` it.\nInline `code` has `back-ticks around` it.\nThe text contained within headings or emphasis cannot be highlighted as code.\nTo create an ordered list, add line items with numbers followed by one period and then one space. Each line item is separated by an empty line. The numbers MUST be in numerical order, but the list SHOULD start with the number one.\nOrdered lists MUST NOT have indented items. Ordered lists MUST NOT include headings.\n1. This is the first item in my ordered list 2. this is the second item in my list 3. the third item\n1. This is the first item in my ordered list 2. this is the second item in my list 3. the third item\nTo create an unordered list, add dashes (-) and one space, in front of line items. Each line item is separated by an empty line.\n-\nUnordered lists MUST NOT have indented items. Unordered lists MUST NOT start with a number followed by a period.\n- this is my list - I like unordered lists\n- this is my list - I like unordered lists\nUnordered lists MUST NOT include headings.\nWhen rendering the raw document tooling COULD use standard markdown rendering tools.\nWhen submitting an update constitution governance action, tooling SHOULD verify the document hash digest and matches the document.\nTooling reading constitution anchors from chain SHOULD always perform a correctness check using the hash digest on-chain. If the hash provided on-chain does not match the hash produced from the off-chain document, then the tooling SHOULD highlight this in a very obvious way to users.\nTools writing constitutions SHOULD strive to follow this specification. If tooling discovering and rendering constitution documents discovers that the document does not follow the \"MUST\"s in this specification then a small warning SHOULD be given to users.\nAuthors SHOULD aim to keep the document as clean and consistent as possible.\nText SHOULD try to be left aligned, without using unneeded whitespace leading or trailing lines.\nSpaces SHOULD be used over tab characters.\nThe last line in the document SHOULD be empty.\nSee Test vector file.\nWe choose to restrict the maximum number of characters per line in aims of improving readability of the document in plain text and within diff views.\nIt SHOULD also be considered that the 80-character limit also helps one find run-on sentences. If your sentence is much longer than 80, it might need breaking down. And you SHOULD never want to see a sentence that's over two lines long in normal text.\nBy limiting documents to one sentence per line we hope to improve the experience when comparing documents and commenting on specific sentences. Conventional document comparison tools such as git diff views, compare documents on a by line basis. By spreading text across lines, it greatly improves tooling's ability to differentiate between documents.\nFurthermore, isolating one sentence per line, allows users to more easily isolate specific lines to comment upon. This gives each sentence an unambiguous reference point, which can be very useful for sharing and discussing.\nWe chose to only allow replacement of this document rather than including a more conventional versioning scheme. This was done for simplicity to minimize the amount of effort required to create tooling which reads and writes constitutions.\nThe alternative was to add some details of version to the constitution document. This would make changing hashing algorithm, rich text formatting, etc. much easier. But this makes the standard and subsequent, more complex than necessary. We do not believe the added complexity is justified, for the expected number of future replacement CIPs to this one.\nThe text file was chosen, due to its ubiquity across platforms. By choosing a common format, we improve the accessibility of the raw document.\nWe choose to add sequential numbering to constitution document iterations to improve differentiation between documents.\nBlake2b-256 was chosen for its common use across the Cardano ecosystem. This means that a lot of Cardano tooling already has this algorithm implemented. This lowers the bar to entry for existing tool makers to add constitutional support.\nFor simplicity, we decide not to include an easy mechanism for changing of hashing algorithms between constitutions.\nEnsuring the Cardano Constitution and its iterations can be accessed in a permissionless manor is paramount. Permissionless networks such as IPFS reduce the ability for parties to censor the content. With each interested party able to make copies of constitutions, this improves the resilience of the documents from deletion.\nThe primary competing idea to platforms such as IPFS is to store the constitution text on Cardano itself. This would philosophically be superior to storing the document off-chain, keeping the Cardano Constitution on Cardano. The counter point to this is that, Cardano is not a general data storage system, rather it is a ledger. Storing data on Cardano is expensive and difficult, without strong tooling support.\nRich text styling will greatly improve the readability of the constitution documents, when rendered.\nMarkdown styling was chosen due to its ubiquity, with strong tooling support. Furthermore, markdown has a benefit in that the unrendered documents are still human readable. This is in contrast to other solutions such as HTML.\nWe chose a strict subset of markdown text styling for two reasons. Firstly, markdown contains a very large and varied syntax, reducing the scope making implementation easier for all tooling. Secondly, some features of markdown may not want to be used in a formal constitution document. Embedded HTML or videos are likely things to be avoided.\nHow can we support multi-languages? The Cardano constitution will be in English, but we will add best practice guidelines via Best Practices.\nThe Cardano constitution will be in English, but we will add best practice guidelines via Best Practices.\nShould we specify any standardization for the proposal policy? Due to lack of interest in this, we will leave it out of this standard.\nDue to lack of interest in this, we will leave it out of this standard.\nHow can we add page breaks? We wont, instead we will prioritize a minimum set of rich text formatting. We can provide some guidance via Best Practices.\nWe wont, instead we will prioritize a minimum set of rich text formatting. We can provide some guidance via Best Practices.\nDo we want a mechanism for specifying authors? (similar to CIP-100) No, as CIP-100 compliant metadata can be supplied at time of constitution update.\nNo, as CIP-100 compliant metadata can be supplied at time of constitution update.\nWhat SHOULD we name the constitution file? we could embed some nice naming or metadata. Naming the file cardano-constitution seems specific enough, adding iteration numbers is a nice addition too.\nNaming the file cardano-constitution seems specific enough, adding iteration numbers is a nice addition too.\ncardano-constitution\nThis standard is followed for the interim Cardano Constitution\nThis standard is utilized by two tools reading constitution data from chain constitution.gov.tools\nconstitution.gov.tools\nAnswer all Open Questions\nReview from the Civics Committee\nAuthor to provide a test vector file with examples.\nWe would like to thank those who reviewed the first draft of this proposal;\nDanielle Stanko\nKevin Hammond\nSteven Johnson\nWe would like to thank Robert Phair (@rphair) for his expert contributions to this proposal.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0121 | Integer-ByteString conversions\n\nPlutus Core primitive operations to convert BuiltinInteger to BuiltinByteString, and vice-versa. Furthermore, the BuiltinInteger conversion allows different endianness for the encoding (most-significant-first and most-significant-last), as well as padding with zeroes based on a requested length if required.\nBuiltinInteger\nBuiltinByteString\nBuiltinInteger\nPlutus Core creates a strong abstraction boundary between the concepts of 'number' (represented by BuiltinInteger) and 'blob of bytes' (represented by BuiltinByteString), defining different sets of (largely non-overlapping) operations for each. This is, in principle, a good practice, as these concepts are distinct in (most of) the operations that make sense on them. However, sometimes, being able to 'move between' these two 'worlds' is important: namely, the ability to represent a given BuiltinInteger as a BuiltinByteString, as well as to convert between this representation and the BuiltinInteger it represents. Currently, no such capability exists: while CIP-0058 proposed such a capability (among others), to date, this has not been implemented into Plutus Core.\nBuiltinInteger\nBuiltinByteString\nBuiltinInteger\nBuiltinByteString\nBuiltinInteger\nTo see why such a capability would be beneficial, we give two motivating use cases.\nConsider the following code snippet:\nvalidBidTerms :: AuctionTerms -> CurrencySymbol -> BidTerms -> Bool validBidTerms AuctionTerms {..} auctionID BidTerms {..} | BidderInfo {..} <- bt'Bidder = validBidderInfo bt'Bidder && -- The bidder pubkey hash corresponds to the bidder verification key. verifyEd25519Signature at'SellerVK (sellerSignatureMessage auctionID bi'BidderVK) bt'SellerSignature && -- The seller authorized the bidder to participate verifyEd25519Signature bi'BidderVK (bidderSignatureMessage auctionID bt'BidPrice bi'bidderPKH) bt'BidderSignature -- The bidder authorized the bid bidderSignatureMessage :: CurrencySymbol -> Integer -> PubKeyHash -> BuiltinByteString bidderSignatureMessage auctionID bidPrice bidderPKH = toByteString auctionID <> toByteString bidPrice <> toByteString bidderPKH sellerSignatureMessage :: CurrencySymbol -> BuiltinByteString -> BuiltinByteString sellerSignatureMessage auctionID bidderVK = toByteString auctionID <> bidderVK\nvalidBidTerms :: AuctionTerms -> CurrencySymbol -> BidTerms -> Bool validBidTerms AuctionTerms {..} auctionID BidTerms {..} | BidderInfo {..} <- bt'Bidder = validBidderInfo bt'Bidder && -- The bidder pubkey hash corresponds to the bidder verification key. verifyEd25519Signature at'SellerVK (sellerSignatureMessage auctionID bi'BidderVK) bt'SellerSignature && -- The seller authorized the bidder to participate verifyEd25519Signature bi'BidderVK (bidderSignatureMessage auctionID bt'BidPrice bi'bidderPKH) bt'BidderSignature -- The bidder authorized the bid bidderSignatureMessage :: CurrencySymbol -> Integer -> PubKeyHash -> BuiltinByteString bidderSignatureMessage auctionID bidPrice bidderPKH = toByteString auctionID <> toByteString bidPrice <> toByteString bidderPKH sellerSignatureMessage :: CurrencySymbol -> BuiltinByteString -> BuiltinByteString sellerSignatureMessage auctionID bidderVK = toByteString auctionID <> bidderVK\nHere, we attempt to verify (using the Curve25519) that a bid at an auction was signed by a particular bidder. The message to verify must include the bid placed, represented using Integer here (which translates to BuiltinInteger onchain). However, the verifyEd25519Signature primitive can only accept BuiltinByteStrings as messages to verify. Thus, we have a problem: how to include the placed bid into the bid message to be verified?\nInteger\nBuiltinInteger\nverifyEd25519Signature\nBuiltinByteString\nMore generally, constructing messages to sign usually consists of concatenating together some primitives represented as BuiltinByteStrings. We currently have a way to do this for (some) strings using encodeUtf8, but no way to do this for BuiltinIntegers.\nBuiltinByteString\nencodeUtf8\nBuiltinInteger\nFinite fields, also known as Galois fields, are a common algebraic structure in cryptographic constructions. Many, if not most, common constructions in cryptography use finite fields as their basis, including Curve25519, Curve448 and the Pasta curves, to name but a few. Elements in a finite field are naturally representable as BuiltinIntegers of bounded size onchain, but for applications like the constructions specified above (and indeed, anything built atop such constructions), we need to be able to perform the following tasks efficiently:\nBuiltinInteger\nVerify that a particular value belongs to the field; and\nPerform bitwise (that is, non-numerical) operations on such values, possibly together with numerical ones.\nFurthermore, Case 2 presents two further challenges: endianness and padding. Due to many cryptographic algorithms being designed for use over the network, their specifications assume a big-endian byte ordering in their implementations. Likewise, due to the finiteness of a finite field's elements, they can be encoded in a fixed-length form, which implementations make use of, both for convenience and efficiency.\nWhile it is not outright impossible to perform conversions from BuiltinInteger to BuiltinByteString currently, it is unreasonably difficult and resource-intensive: BuiltinInteger to BuiltinByteString involves a repeated combination of division-with-remainder in a loop, while BuiltinByteString to BuiltinInteger involves repeated multiplications by large constants and accumulations. Aside from these both requiring looping (with the overheads this imposes), both of these are effectively quadratic operations with current primitives: the only means we have to accumulate BuiltinByteStrings is by consing or appending (which are both quadratic due to BuiltinByteString being a counted array), and any BuiltinInteger operation is linear in the size of its arguments. This makes even Case 1 far more effort, both for the developer and the node, than it should be, and Case 2 ranges from difficult to impossible once we factor in the limited available primitive operations and the endianness and padding problems.\nBuiltinInteger\nBuiltinByteString\nBuiltinInteger\nBuiltinByteString\nBuiltinByteString\nBuiltinInteger\nBuiltinByteString\nBuiltinByteString\nBuiltinInteger\nWe propose that two primitives be added to Plutus Core: one for converting BuiltinIntegers to BuiltinByteStrings, the other for converting BuiltinByteStrings to BuiltinIntegers. The first of these primitives would allow for specifying an endianness for the result, as well as to perform padding to a required length if necessary; the second primitive is able to operate on padded or unpadded encodings, in either endianness.\nBuiltinInteger\nBuiltinByteString\nBuiltinByteString\nBuiltinInteger\nAdditionally, we state the following goals that any implementation of such primitives must have.\nThe representation produced by the BuiltinInteger to BuiltinByteString conversion should be 'minimal', representing only the number being given to it, and no other information besides. It would be tempting to, for example, encode the endianness requested into the BuiltinByteString, but ultimately, this information could be added later by users if they want it, while removing it would be trickier. Additionally, metadata-related concerns would complicate both the specification and implementation of the primitives, for arguably marginal benefit.\nBuiltinInteger\nBuiltinByteString\nBuiltinByteString\nUsers of these primitives should not need to know how exactly BuiltinIntegers are represented to use them successfully. This is beneficial to both users (as they now don't have to concern themselves with platform-specific implementation issues) and Plutus Core maintainers (as changes in the representation of BuiltinInteger aren't going to affect these primitives).\nBuiltinInteger\nBuiltinInteger\nWhile for fixed-size numbers, two's-complement is the default choice for negative number representations, for arbitrary-size numbers, there is no agreed-upon choice. Furthermore, indicating the 'negativity' of a number would require making representations larger or more complex regardless of which representation we chose, while also complicating both the primitives we want to define, and any user-defined operations on such representations, possibly in ways that users do not want. Lastly, for our cases, negative values are not really needed, and if the ability to encode negative numbers was necessary, users could still define whichever one(s) they needed themselves, with little effort or computational cost.\nThis CIP partially supercedes CIP-0058: specifically, the specifications here replace the integerToByteString and byteStringToInteger primitives specified in CIP-0058, as improved, and more general, solutions.\nintegerToByteString\nbyteStringToInteger\nWe describe the specification of two Plutus Core primitives, which will have the following signatures:\nbuiltinIntegerToByteString :: BuiltinBool - BuiltinInteger - BuiltinInteger - BuiltinByteString\nbuiltinIntegerToByteString :: BuiltinBool -> BuiltinInteger -> BuiltinInteger -> BuiltinByteString\nbuiltinByteStringToInteger :: BuiltinBool - BuiltinByteString - BuiltinInteger\nbuiltinByteStringToInteger :: BuiltinBool -> BuiltinByteString -> BuiltinInteger\nTo describe the semantics of these primitives, we first specify how we represent a BuiltinInteger as a BuiltinByteString; after that, we describe the two primitives, as well as giving some properties they must follow.\nBuiltinInteger\nBuiltinByteString\nOur BuiltinByteString representations of non-negative BuiltinIntegers treat the BuiltinInteger being represented as a sequence of digits in base-256. Thus, any byte in the BuiltinByteString representation corresponds to a single base-256 digit, whose digit value is equal to its value as an 8-bit unsigned integer. For example, the byte 0x80 would have digit value 128, while the byte 0x03 would have digit value 3.\nBuiltinByteString\nBuiltinInteger\nBuiltinInteger\nBuiltinByteString\n0x80\n0x03\nTo determine place value, we define two possible arrangements of digits in such a representation: most-significant-first, and most-significant-last. In the most-significant-first representation, the first digit (that is, the byte at index 0) has the highest place value; in the most-significant-last representation, the first digit instead has the lowest place value. These correspond to the notions of big-endian and little-endian respectively.\nFor any positive BuiltinInteger i, let\nBuiltinInteger\ni\ni0 2560+i1 2561+ +ik 256ki_0 \\times 256 0 + i_1 \\times 256 1 + \\ldots + i_k \\times 256 ki0 2560+i1 2561+ +ik 256k\nbe its base-256 form. Then, for the most-significant-first representation, the BuiltinByteString encoding for i is the BuiltinByteString b such that indexByteString bs j=ik j\\texttt{indexByteString bs j} = i_{k - j}indexByteString bs j=ik j . For the most-significant-last encoding, we instead have indexByteString bs j=ij\\texttt{indexByteString bs j} = i_jindexByteString bs j=ij .\nBuiltinByteString\ni\nBuiltinByteString\nb\nFor example, consider the number 123_456. Its base-256 form is\n123_456\n64 * 256 ^ 0 + 226 * 256 ^ 1 + 1 * 256 ^ 2\n64 * 256 ^ 0 + 226 * 256 ^ 1 + 1 * 256 ^ 2\nTherefore, its most-significant-first representation would be\n[ 0x01, 0xE2, 0x40 ]\n[ 0x01, 0xE2, 0x40 ]\nwhile its most-significant-last representation would be\n[ 0x40, 0xE2, 0x01 ]\n[ 0x40, 0xE2, 0x01 ]\nFor 0, in line with the above definition, both its most-significant-first and most-significant-last representation is [] (that is, the empty BuiltinByteString).\n0\n[]\nBuiltinByteString\nTo represent any given non-negative BuiltinInteger i as above, we require a minimum number of base-256 digits. For positive i, this is max 1, log 256(i) \\max \\\\{1, \\lceil \\log_{256}(\\texttt{i}) \\rceil \\\\}max1, log256 (i) ; for i = 0, we define this to be 000. We can choose to represent i with more digits than this minimum, by the use of padding. Let kkk be the minimum number of digits to represent i, and let jjj be a positive number: to represent i using k+jk + jk+j digits in the most-significant-first encoding, we set the first jjj bytes of the encoding as 0x0; for the most-significant-last encoding, we set the last jjj bytes of the encoding as 0x0 instead.\nBuiltinInteger\ni\ni\ni = 0\ni\ni\ni\n0x0\n0x0\nTo extend our previous example, a five-digit, most-significant-first representation of 123_456 is\n123_456\n[ 0x00, 0x00, 0x01, 0xC2, 0x80 ]\n[ 0x00, 0x00, 0x01, 0xC2, 0x80 ]\nwhile the most-significant-last representation would be\n[ 0x80, 0xC2, 0x01, 0x00, 0x00 ]\n[ 0x80, 0xC2, 0x01, 0x00, 0x00 ]\nWe observe that these extra digits do not change what exact BuiltinInteger is represented, as any zero digit has zero place value.\nBuiltinInteger\nbuiltinIntegerToByteString\nWe can now describe the semantics of the builtinIntegerToByteString primitive. The builtinIntegerToByteString function takes three arguments; we specify (and name) them below:\nbuiltinIntegerToByteString\nbuiltinIntegerToByteString\nWhether the most-significant-first encoding should be used. This is the endianness argument, which has type BuiltinBool. The number of bytes required in the output (if such a requirement exists). This is the length argument, which has type BuiltinInteger. The BuiltinInteger to convert. This is the input.\nWhether the most-significant-first encoding should be used. This is the endianness argument, which has type BuiltinBool.\nBuiltinBool\nThe number of bytes required in the output (if such a requirement exists). This is the length argument, which has type BuiltinInteger.\nBuiltinInteger\nThe BuiltinInteger to convert. This is the input.\nBuiltinInteger\nIf the input is negative, builtinIntegerToByteString fails. In this case, the resulting error message must specify at least the following information:\nbuiltinIntegerToByteString\nThat builtinIntegerToByteString failed due to a negative conversion attempt; and\nbuiltinIntegerToByteString\nWhat negative BuiltinInteger was passed as the input.\nBuiltinInteger\nIf the length argument is outside the closed interval (0,229 1)(0, 2 {29} - 1)(0,229 1), builtinIntegerToByteString fails. In this case, the resulting error message must specify at least the following information:\nbuiltinIntegerToByteString\nThat builtinIntegerToByteString failed due to an invalid length argument; and\nbuiltinIntegerToByteString\nWhat BuiltinInteger was passed as the length argument.\nBuiltinInteger\nIf the input is 0, builtinIntegerToByteString returns the BuiltinByteString consisting of a number of zero bytes equal to the length argument.\n0\nbuiltinIntegerToByteString\nBuiltinByteString\nIf the input is positive, and the length argument is also positive, let d be the minimum number of digits required to represent the input (as per the representation described above). If d is greater than the length argument, builtinIntegerToByteString fails. In this case, the resulting error message must specify at least the following information:\nd\nd\nbuiltinIntegerToByteString\nThat builtinIntegerToByteString failed due to the requested length being insufficient for the input;\nbuiltinIntegerToByteString\nWhat BuiltinInteger was passed as the length argument; and\nBuiltinInteger\nWhat BuiltinInteger was passed as the input.\nBuiltinInteger\nIf d is equal to, or greater, than the length argument, builtinIntegerToByteString returns the BuiltinByteString encoding the input. This will be the most-significant-first encoding if the endianness argument is True, or the most-significant-last encoding if the endianness argument is False. The resulting BuiltinByteString will be padded to the length specified by the padding argument if necessary.\nd\nbuiltinIntegerToByteString\nBuiltinByteString\nTrue\nFalse\nBuiltinByteString\nIf the input is positive, and the length argument is zero, builtinIntegerToByteString returns the BuiltinByteString encoding the input. Its length will be minimal (that is, no padding will be done). If the endianness argument is True, the result will use the most-significant-first encoding, and if the endianness argument is False, the result will use the most-significant-last encoding.\nbuiltinIntegerToByteString\nBuiltinByteString\nTrue\nFalse\nWe give some examples of the intended behaviour of builtinIntegerToByteString below:\nbuiltinIntegerToByteString\n-- fails due to negative input builtinIntegerToByteString False 0 (-1) -- => ERROR -- endianness argument doesn't affect this case builtinIntegerToByteString True 0 (-1) -- => ERROR -- length argument doesn't affect this case builtinIntegerToByteString False 100 (-1) -- => ERROR -- zero case, no padding builtinIntegerToByteString False 0 0 -- => [] -- endianness argument doesn't affect this case builtinIntegerToByteString True 0 0 -- => [] -- length argument adds more zeroes, but endianness doesn't matter builtinIntegerToByteString False 5 0 -- => [ 0x00, 0x00, 0x00, 0x00, 0x00 ] builtinIntegerToByteString True 5 0 -- => [ 0x00, 0x00, 0x00, 0x00, 0x00 ] -- length argument too large (2^29) builtinIntegerToByteString False 536870912 0 -- => ERROR -- endianness doesn't affect this case builtinIntegerToByteString True 536870912 0 -- => ERROR -- fails due to insufficient digits (404 needs 2) builtinIntegerToByteString False 1 404 -- => ERROR -- endianness argument doesn't affect this case builtinIntegerToByteString True 1 404 -- => ERROR -- zero length argument is exactly the same as requesting exactly the right -- digit count builtinIntegerToByteString False 2 404 -- => [ 0x94, 0x01 ] builtinIntegerToByteString False 0 404 -- => [ 0x94, 0x01 ] -- switching endianness argument reverses the result builtinIntegerToByteString True 2 404 -- => [ 0x01, 0x94 ] builtinIntegerToByteString True 0 404 -- => [ 0x01, 0x94 ] -- padding for most-significant-last goes at the end builtinIntegerToByteString False 5 404 -- => [ 0x94, 0x01, 0x00, 0x00, 0x00 ] -- padding for most-significant-first goes at the start builtinIntegerToByteString True 5 404 -- => [ 0x00, 0x00, 0x00, 0x01, 0x94 ]\n-- fails due to negative input builtinIntegerToByteString False 0 (-1) -- => ERROR -- endianness argument doesn't affect this case builtinIntegerToByteString True 0 (-1) -- => ERROR -- length argument doesn't affect this case builtinIntegerToByteString False 100 (-1) -- => ERROR -- zero case, no padding builtinIntegerToByteString False 0 0 -- => [] -- endianness argument doesn't affect this case builtinIntegerToByteString True 0 0 -- => [] -- length argument adds more zeroes, but endianness doesn't matter builtinIntegerToByteString False 5 0 -- => [ 0x00, 0x00, 0x00, 0x00, 0x00 ] builtinIntegerToByteString True 5 0 -- => [ 0x00, 0x00, 0x00, 0x00, 0x00 ] -- length argument too large (2^29) builtinIntegerToByteString False 536870912 0 -- => ERROR -- endianness doesn't affect this case builtinIntegerToByteString True 536870912 0 -- => ERROR -- fails due to insufficient digits (404 needs 2) builtinIntegerToByteString False 1 404 -- => ERROR -- endianness argument doesn't affect this case builtinIntegerToByteString True 1 404 -- => ERROR -- zero length argument is exactly the same as requesting exactly the right -- digit count builtinIntegerToByteString False 2 404 -- => [ 0x94, 0x01 ] builtinIntegerToByteString False 0 404 -- => [ 0x94, 0x01 ] -- switching endianness argument reverses the result builtinIntegerToByteString True 2 404 -- => [ 0x01, 0x94 ] builtinIntegerToByteString True 0 404 -- => [ 0x01, 0x94 ] -- padding for most-significant-last goes at the end builtinIntegerToByteString False 5 404 -- => [ 0x94, 0x01, 0x00, 0x00, 0x00 ] -- padding for most-significant-first goes at the start builtinIntegerToByteString True 5 404 -- => [ 0x00, 0x00, 0x00, 0x01, 0x94 ]\nWe also describe properties that any implementation of builtinIntegerToByteString must have. Throughout, q is not negative, p is positive, d is in the closed interval (0,229 1)(0, 2 {29} - 1)(0,229 1), k is in the closed interval (1,229 1)(1, 2 {29} - 1)(1,229 1), 0 = j k, and 1 = r = 255. We also define singleton x = consByteString x emptyByteString.\nbuiltinIntegerToByteString\nq\np\nd\nk\n0 <= j < k\n1 <= r <= 255\nsingleton x = consByteString x emptyByteString\nlengthOfByteString (builtinIntegerToByteString e d 0) = d indexByteString (builtinIntegerToByteString e k 0) j = 0 lengthOfByteString (builtinIntegerToByteString e 0 p) 0 builtinIntegerToByteString False 0 (multiplyInteger p 256) = consByteString 0 (builtinIntegerToByteString False 0 p) builtinIntegerToByteString True 0 (multiplyInteger p 256) = appendByteString (builtinIntegerToByteString True 0 p) (singleton 0) builtinIntegerToByteString False 0 (plusInteger (multiplyInteger q 256) r) = appendByteString (builtinIntegerToByteString False 0 r) (builtinIntegerToByteString False 0 q)` builtinIntegerToByteString True 0 (plusInteger (multiplyInteger q 256) r) = appendByteString (builtinIntegerToByteString False 0 q) (builtinIntegerToByteString False 0 r)\nlengthOfByteString (builtinIntegerToByteString e d 0) = d\nlengthOfByteString (builtinIntegerToByteString e d 0) = d\nindexByteString (builtinIntegerToByteString e k 0) j = 0\nindexByteString (builtinIntegerToByteString e k 0) j = 0\nlengthOfByteString (builtinIntegerToByteString e 0 p) 0\nlengthOfByteString (builtinIntegerToByteString e 0 p) > 0\nbuiltinIntegerToByteString False 0 (multiplyInteger p 256) = consByteString 0 (builtinIntegerToByteString False 0 p)\nbuiltinIntegerToByteString False 0 (multiplyInteger p 256) = consByteString 0 (builtinIntegerToByteString False 0 p)\nbuiltinIntegerToByteString True 0 (multiplyInteger p 256) = appendByteString (builtinIntegerToByteString True 0 p) (singleton 0)\nbuiltinIntegerToByteString True 0 (multiplyInteger p 256) = appendByteString (builtinIntegerToByteString True 0 p) (singleton 0)\nbuiltinIntegerToByteString False 0 (plusInteger (multiplyInteger q 256) r) = appendByteString (builtinIntegerToByteString False 0 r) (builtinIntegerToByteString False 0 q)`\nbuiltinIntegerToByteString False 0 (plusInteger (multiplyInteger q 256) r) = appendByteString (builtinIntegerToByteString False 0 r)\nbuiltinIntegerToByteString True 0 (plusInteger (multiplyInteger q 256) r) = appendByteString (builtinIntegerToByteString False 0 q) (builtinIntegerToByteString False 0 r)\nbuiltinIntegerToByteString True 0 (plusInteger (multiplyInteger q 256) r) = appendByteString (builtinIntegerToByteString False 0 q) (builtinIntegerToByteString False 0 r)\nbuiltinByteStringToInteger\nThe builtinByteStringToInteger primitive takes two arguments. We specify, and name, these below:\nbuiltinByteStringToInteger\nWhether the input uses the most-significant-first encoding. This is the stated endianness argument, which has type BuiltinBool. The BuiltinByteString to convert. This is the input.\nWhether the input uses the most-significant-first encoding. This is the stated endianness argument, which has type BuiltinBool.\nBuiltinBool\nThe BuiltinByteString to convert. This is the input.\nBuiltinByteString\nIf the input is the empty BuiltinByteString, builtinByteStringToInteger returns 0. If the input is non-empty, builtinByteStringToInteger produces the BuiltinInteger encoded by the input. The encoding is treated as most-significant-first if the stated endianness argument is True, and most-significant-last if the stated endianness argument is False. The input encoding may be padded or not.\nBuiltinByteString\nbuiltinByteStringToInteger\nbuiltinByteStringToInteger\nBuiltinInteger\nTrue\nFalse\nWe give some examples of the intended behaviour of builtinByteStringToInteger below:\nbuiltinByteStringToInteger\n-- empty input gives zero builtinByteStringToInteger False emptyByteString => 0 -- stated endianness argument doesn't affect this case builtinByteStringToInteger True emptyByteString => 0 -- if all the bytes are the same, stated endianness argument doesn't matter builtinByteStringToInteger False (consByteString 0x01 (consByteString 0x01 emptyByteString) -- => 257 builtinByteStringToInteger True (consByteString 0x01 (consByteString 0x01 emptyByteString) -- => 257 -- most-significant-first padding is at the start builtinByteStringToInteger True (consByteString 0x00 (consByteString 0x01 (consByteString 0x01 emptyByteString))) -- => 257 builtinByteStringToInteger False (consByteString 0x00 (consByteString 0x01 (consByteString 0x01 emptyByteString))) -- => 65792 -- most-significant-last padding is at the end builtinByteStringToInteger False (consByteString 0x01 (consByteString 0x01 (consByteString 0x00 emptyByteString) -- => 257 builtinByteStringToInteger True (consByteString 0x01 (consByteString 0x01 (consByteString 0x00 emptyByteString) -- => 65792\n-- empty input gives zero builtinByteStringToInteger False emptyByteString => 0 -- stated endianness argument doesn't affect this case builtinByteStringToInteger True emptyByteString => 0 -- if all the bytes are the same, stated endianness argument doesn't matter builtinByteStringToInteger False (consByteString 0x01 (consByteString 0x01 emptyByteString) -- => 257 builtinByteStringToInteger True (consByteString 0x01 (consByteString 0x01 emptyByteString) -- => 257 -- most-significant-first padding is at the start builtinByteStringToInteger True (consByteString 0x00 (consByteString 0x01 (consByteString 0x01 emptyByteString))) -- => 257 builtinByteStringToInteger False (consByteString 0x00 (consByteString 0x01 (consByteString 0x01 emptyByteString))) -- => 65792 -- most-significant-last padding is at the end builtinByteStringToInteger False (consByteString 0x01 (consByteString 0x01 (consByteString 0x00 emptyByteString) -- => 257 builtinByteStringToInteger True (consByteString 0x01 (consByteString 0x01 (consByteString 0x00 emptyByteString) -- => 65792\nWe also describe properties that any builtinByteStringToInteger implementation must have. Throughout, q is not negative and 0 = w8 = 255.\nbuiltinByteStringToInteger\nq\n0 <= w8 <= 255\nbuiltinByteStringToInteger b (builtinIntegerToByteString b 0 q) = q builtinByteStringToInteger b (consByteString w8 emptyByteString) = w8 builtinIntegerToByteString b (lengthOfByteString bs) (builtinByteStringToInteger b bs) = bs\nbuiltinByteStringToInteger b (builtinIntegerToByteString b 0 q) = q\nbuiltinByteStringToInteger b (builtinIntegerToByteString b 0 q) = q\nbuiltinByteStringToInteger b (consByteString w8 emptyByteString) = w8\nbuiltinByteStringToInteger b (consByteString w8 emptyByteString) = w8\nbuiltinIntegerToByteString b (lengthOfByteString bs) (builtinByteStringToInteger b bs) = bs\nbuiltinIntegerToByteString b (lengthOfByteString bs) (builtinByteStringToInteger b bs) = bs\nWe believe that these operations address both of the described cases well, while also meeting the goals stated at the start of this CIP. Our specified primitives address both the problems of endianness and padding specified in Case 2, while also ensuring that use cases like Case 1 (where bounding length isn't important) are not made more difficult than necessary. The representation we have chosen is metadata-free, doesn't depend on any representation choices (current or future) of BuiltinInteger, while also being flexible enough to satisfy both cases where endianness and padding matter, and when they don't.\nBuiltinInteger\nAs part of this proposal, we considered two alternative possibilities:\nUse the CIP-0058 versions of these operations; and Have a uniform treatment of the length argument for builtinIntegerToByteString (always minimum or always maximum).\nUse the CIP-0058 versions of these operations; and\nHave a uniform treatment of the length argument for builtinIntegerToByteString (always minimum or always maximum).\nbuiltinIntegerToByteString\nCIP-0058 defines a sizeable collection of bitwise primitive operations for Plutus Core, mostly for use over BuiltinByteStrings. As part of these, it also defines conversion functions similar to builtinIntegerToByteString and builtinByteStringToInteger, which are named integerToByteString and byteStringToInteger respectively. Unlike the operations specified in this CIP, the CIP-0058 operations do not address the problems of either padding or endianness: more precisely, the representations constructed are always minimally-sized, and use a big-endian encoding. While in the context they are being presented in, these choices are defensible, they do not adequately address Case 2, and in particular, many cryptographic constructions used with finite fields. Users of the CIP-0058 primitives who needed to ensure a minimum length of a converted BuiltinInteger would have to pad manually, which CIP-0058 gives no additional support for; additionally, if a little-endian representation was required, the BuiltinByteString result would have to be reversed, which has quadratic cost if using only Plutus Core primitives. Thus, we consider these implementations to be a good attempt, but not suited to even their intended use, much less more general applications.\nBuiltinByteString\nbuiltinIntegerToByteString\nbuiltinByteStringToInteger\nintegerToByteString\nbyteStringToInteger\nBuiltinInteger\nBuiltinByteString\nAn alternative possibility for the length argument of builtinIntegerToByteString would be to treat the argument as either a minimum, or a maximum, rather than our more hybrid approach. Specifically, for any input i, let d be the minimum number of digits required to represent i as per the description of our representation, and let k be the length argument. Then:\nbuiltinIntegerToByteString\ni\nd\ni\nk\nThe minimum length argument approach would produce a result of size min d,k\\min \\\\{ \\texttt{d}, \\texttt{k} \\\\}mind,k; that is, if the length argument is smaller than the minimum required digits, the minimum would be used instead.\nThe maximum length argument approach would produce a result of size k, and would error if d k\\texttt{d} \\texttt{k}d k.\nk\nBoth the minimum length argument, and the maximum length argument, approaches have merits. The maximum length argument approach in particular is useful for Case 2: in such a setting, we already know the maximum size of any element's representation, and if we somehow ended up with a larger representation than this, it would be a mistake, which the maximum length argument would catch immediately. For the minimum length argument approach, the advantage would be more for Case 1: where the length of the representation is not known (and the user isn't particularly concerned anyway). In such a situation, the user could pass any argument and know that the conversion would still work.\nHowever, both of these approaches have disadvantages as well. The minimum length argument approach would be more tedious to use with Case 2, as each conversion would require a size check of the resulting BuiltinByteString. While this is not expensive, it is annoying, and given the complexity of the constructions that would be built atop of any finite field implementations, it feels unreasonable to require this from users. Likewise, the maximum length argument approach is unreasonable for situations like Case 1: the only way to establish how many digits would be required involves performing an integer logarithm in base 256, which is inefficient and error-prone. Our hybrid approach gives the benefits of both the minimum length argument and maximum length argument approaches, without the downsides of either: we observe that, for situations like Case 1, the length argument would be 0 in practically all cases, which is a value that would not be useful in any situation where the maximum length argument approach would be used. This observation allows our approach to work equally well for both Case 1 and 2, with minimal friction.\nBuiltinByteString\n0\nWe consider the following criteria to be essential for acceptance:\nA proof-of-concept implementation of the operation specified here must exist, outside of the Plutus source tree. The implementation must be in Haskell.\nThe proof-of-concept implementation must have tests, demonstrating that it behaves as the specification requires, and that the representations it produces match the described representation in this CIP.\nThe proof-of-concept implementation must demonstrate that it will successfully build, and pass its tests, using all GHC versions currently usable to build Plutus (8.10, 9.2 and 9.6 at the time of writing), across all Tier 1 platforms.\nIdeally, the implementation should also demonstrate its performance characteristics by well-designed benchmarks.\nMLabs have completed the implementation of the proof of concept as required (located here. This implementation has been merged into Plutus Core, and will be released in the upcoming V3 release.\nThis CIP is licensed under the Apache-2.0 license.\n2023 Cardano Foundation\n\n---\n\nCIP-0122 | Logical operations over BuiltinByteString\n\nWe describe the semantics of a set of logical operations for Plutus BuiltinByteStrings. Specifically, we provide descriptions for:\nBuiltinByteString\nBitwise logical AND, OR, XOR and complement;\nReading a bit value at a given index;\nSetting bits value at given indices; and\nReplicating a byte a given number of times.\nAs part of this, we also describe the bit ordering within a BuiltinByteString, and provide some laws these operations should obey.\nBuiltinByteString\nBitwise operations, both over fixed-width and variable-width blocks of bits, have a range of uses, including data structures (especially succinct ones) and cryptography. Currently, operations on individual bits in Plutus Core are difficult, or outright impossible, while also keeping within the tight constraints required onchain. While it is possible to some degree to work with individual bytes over BuiltinByteStrings, this isn't sufficient, or efficient, when bit maniputations are required.\nBuiltinByteString\nTo demonstrate where bitwise operations would allow onchain possibilities that are currently either impractical or impossible, we give the following use cases.\nAn integer set (also known as a bit set, bitmap, or bitvector) is a succinct data structure for representing a set of numbers in a pre-defined range [0,n)[0, n)[0,n) for some n Nn \\in \\mathbb{N}n N. The structure supports the following operations:\nConstruction given a fixed number of elements, as well as the bound nnn.\nConstruction of the empty set (contains no elements) and the universe (contains all elements).\nSet union, intersection, complement and difference (symmetric and asymmetric).\nMembership testing for a specific element.\nInserting or removing elements.\nThese structures have a range of uses. In addition to being used as sets of bounded natural numbers, an integer set could also represent an array of Boolean values. These have a range of applications, mostly as 'backends' for other, more complex structures. Furthermore, by using some index arithmetic, integer sets can also be used to represent binary matrices (in any number of dimensions), which have an even wider range of uses:\nRepresentations of graphs in adjacency-matrix form\nChecking the rules for a game of Go\nFSM representation\nRepresentation of an arbitrary binary relation between finite sets\nThe succinctness of the integer set (and the other succinct data structures it enables) is particularly valuable on-chain, due to the limited transaction size and memory available.\nTypically, such a structure would be represented as a packed array of bytes (similar to the Haskell ByteString). Essentially, given a bound nnn, the packed array has a length in bytes large enough to contain at least nnn bits, with a bit at position iii corresponding to the value i Ni \\in \\mathbb{N}i N. This representation ensures the succinctness of the structure (at most 7 bits of overhead are required if n=8k+1n = 8k + 1n=8k+1 for some k Nk \\in \\mathbb{N}k N), and also allows all the above operations to be implemented efficiently:\nByteString\nConstruction given a fixed number of elements and the bound nnn involves allocating the packed array, then modifying some bits to be set.\nConstruction of the empty set is a packed array where every byte is 0x00, while the universe is a packed array where every byte is 0xFF.\n0x00\n0xFF\nSet union is bitwise OR over both arguments.\nSet intersection is bitwise AND over both arguments.\nSet complement is bitwise complement over the entire packed array.\nSymmetric set difference is bitwise XOR over both arguments; asymmetric set difference can be defined using a combination of bitwise complement and bitwise OR.\nMembership testing is checking whether a bit is set.\nInserting an element is setting the corresponding bit.\nRemoving an element is clearing the corresponding bit.\nGiven that this is a packed representation, these operations can be implemented very efficiently by relying on the cache-friendly properties of packed array traversals, as well as making use of optimized routines available in many languages. Thus, this structure can be used to efficiently represent sets of numbers in any bounded range (as ranges not starting from 000 can be represented by storing an offset), while also being minimal in space usage.\nCurrently, such a structure cannot be easily implemented in Plutus Core while preserving the properties described above. The two options using existing primitives are either to use [BuiltinInteger], or to mimic the above operations over BuiltinByteString. The first of these is not space or time-efficient: each BuiltinInteger takes up multiple machine words of space, and the list overheads introduced are linear in the number of items stored, destroying succinctness; membership testing, insertion and removal require either maintaining an ordered list or forcing linear scans for at least some operations, which are inefficient over lists; and 'bulk' operations like union, intersection and complement become very difficult and time-consuming. The second is not much better: while we preserve succinctness, there is no easy way to access individual bits, only bytes, which would require a division-remainder loop for each such operation, with all the overheads this imposes; intersection, union and symmetric difference would have to be simulated byte-by-byte, requiring large lookup tables or complex conditional logic; and construction would require immense amounts of copying and tricky byte construction logic. While it is not outright impossible to make such a structure using current primitives, it would be so impractical that it could never see real use.\n[BuiltinInteger]\nBuiltinByteString\nBuiltinInteger\nFurthermore, for sparse (or dense) integer sets (that is, where either most elements in the range are absent or present respectively), a range of compression techniques have been developed. All of these rely on bitwise operations to achieve their goals, and can potentially yield significant space savings in many cases. Given the limitations onchain that we have to work within, having such techniques available to implementers would be a huge potential advantage.\nHashing, that is, computing a fixed-length 'fingerprint' or 'digest' of a variable-length input (typically viewed as binary) is a common task required in a range of applications. Most notably, hashing is a key tool in cryptographic protocols and applications, either in its own right, or as part of a larger task. The value of such functionality is such that Plutus Core already contains primitives for certain hash functions, specifically two variants of SHA256 and BLAKE2b. At the same time, hash functions choices are often determined by protocol or use case, and providing individual primitives for every possible hash function is not a scalable choice. It is much preferrable to give necessary tools to implement such functionality to users of Plutus (Core), allowing them to use whichever hash function(s) their applications require.\nAs an example, we consider the Argon2 family of hash functions. In order to implement any variant of this family requires the following operations:\nConversion of numbers to bytes Bytestring concatenation BLAKE2b hashing Floor division Indexing bytes in a bytestring Logical XOR\nConversion of numbers to bytes\nBytestring concatenation\nBLAKE2b hashing\nFloor division\nIndexing bytes in a bytestring\nLogical XOR\nOperations 1 to 5 are already provided by Plutus Core (with 1 being included in CIP-121); however, without logical XOR, no function in the Argon2 family could be implemented. While in theory, it could be simulated with what operations already exist, much as with Case 1, this would be impractical at best, and outright impossible at worst, due to the severe limits imposed on-chain. This is particularly the case here, as all Argon2 variants call logical XOR in a loop, whose step count is defined by multiple user-specified (or protocol-specified) parameters.\nWe observe that this requirement for logical XOR is not unique to the Argon2 family of hash functions. Indeed, logical XOR is widely used for a variety of cryptographic applications, as it is a low-cost mixing function that happens to be self-inverting, as well as preserving randomness (that is, a random bit XORed with a non-random bit will give a random bit).\nWe describe the proposed operations in several stages. First, we specify a scheme for indexing individual bits (rather than whole bytes) in a BuiltinByteString. We then specify the semantics of each operation, as well as giving costing expectations and some examples. Lastly, we provide some laws that any implementation of these operations is expected to obey.\nBuiltinByteString\nWe begin by observing that a BuiltinByteString is a packed array of bytes (that is, BuiltinIntegers in the range [0,255][0, 255][0,255]) according to the API provided by existing Plutus Core primitives. In particular, we have the ability to access individual bytes by index as a primitive operation. Thus, we can view a BuiltinByteString as an indexed collection of bytes; for any BuiltinByteString bbb of length nnn, and any i 0,1, ,n 1i \\in 0, 1, \\ldots, n - 1i 0,1, ,n 1, we define bib\\\\{i\\\\}bi as the byte at index iii in bbb, as defined by the indexByteString primitive. In essence, for any BuiltinByteString of length n, we have byte indexes as follows:\nBuiltinByteString\nBuiltinInteger\nBuiltinByteString\nBuiltinByteString\nindexByteString\nBuiltinByteString\nn\n| Index | 0 | 1 | ... | n - 1 | |-------|----|----| ... |----------| | Byte | w0 | w1 | ... | w(n - 1) |\n| Index | 0 | 1 | ... | n - 1 | |-------|----|----| ... |----------| | Byte | w0 | w1 | ... | w(n - 1) |\nTo view a BuiltinByteString as an indexed collection of bits, we must first consider the bit ordering within a byte. Suppose i 0,1, ,7i \\in 0, 1, \\ldots, 7i 0,1, ,7 is an index into a byte www. We say that the bit at iii in www is set when\nBuiltinByteString\nOtherwise, the bit at iii in www is clear. We define w[i]w[i]w[i] to be 111 when the bit at iii in www is set, and 000 otherwise; this is the value at index iii in www.\nFor example, consider the byte represented by the BuiltinInteger 42. By the above scheme, we have the following:\nBuiltinInteger\nPut another way, we can view w[i]=1w[i] = 1w[i]=1 to mean that the (i+1)(i + 1)(i+1) th least significant digit in www's binary representation is 111, and likewise, w[i]=0w[i] = 0w[i]=0 would mean that the iiith least significant digit in www's binary representation is 000. Continuing with the above example, 424242 is represented in binary as 00101010; we can see that the second-least-significant, fourth-least-significant, and sixth-least-significant digits are 1, and all the others are zero. This description mirrors the way bytes are represented on machine architectures.\n00101010\n1\nWe now extend the above scheme to BuiltinByteStrings. Let bbb be a BuiltinByteString whose length is nnn, and let i 0,1, ,8 n 1i \\in 0, 1, \\ldots, 8 \\cdot n - 1i 0,1, ,8 n 1. For any j 0,1, ,n 1j \\in 0, 1, \\ldots, n - 1j 0,1, ,n 1, let j =n j 1j {\\prime} = n - j - 1j =n j 1. We say that the bit at iii in bbb is set if\nBuiltinByteString\nBuiltinByteString\nWe define the bit at iii in bbb being clear analogously. Similarly to bits in a byte, we define b[i]b[i]b[i] to be 111 when the bit at iii in bbb is set, and 000 otherwise; similarly to bytes, we term this the value at index iii in bbb.\nAs an example, consider the BuiltinByteString [42, 57, 133]: that is, the BuiltinByteString bbb such that b0=42b\\\\{0\\\\} = 42b0=42, b1=57b\\\\{1\\\\} = 57b1=57 and b2=133b\\\\{2\\\\} = 133b2=133. We observe that the range of 'valid' bit indexes iii into bbb is in [0,3 8 1=23][0, 3 \\cdot 8 - 1 = 23][0,3 8 1=23]. Consider i=4i = 4i=4; by the definition above, this corresponds to the byte index 2, as 48 =0\\left\\lfloor\\frac{4}{8}\\right\\rfloor = 0 84 =0, and 3 0 1=23 - 0 - 1 = 23 0 1=2 (as bbb has length 333). Within the byte 133133133, this means we have 13324 mod 2 0\\left\\lfloor\\frac{133}{2 4}\\right\\rfloor \\mod 2 \\equiv 0 24133 mod2 0. Thus, b[4]=0b[4] = 0b[4]=0. Consider instead the index i=19i = 19i=19; by the definition above, this corresponds to the byte index 0, as 198 =2\\left\\lfloor\\frac{19}{8}\\right\\rfloor = 2 819 =2, and 3 2 1=03 - 2 - 1 = 03 2 1=0. Within the byte 424242, this means we have 4223 mod 2 1\\left\\lfloor\\frac{42}{2 3}\\right\\rfloor\\mod 2 \\equiv 1 2342 mod2 1. Thus, b[19]=1b[19] = 1b[19]=1.\nBuiltinByteString\n[42, 57, 133]\nBuiltinByteString\nPut another way, our byte indexes run 'the opposite way' to our bit indexes. Thus, for any BuiltinByteString of length nnn, we have bit indexes relative byte indexes as follows:\nBuiltinByteString\n| Byte index | 0 | 1 | ... | n - 1 | |------------|--------------------------------|----| ... |-------------------------------| | Byte | w0 | w1 | ... | w(n - 1) | |------------|--------------------------------|----| ... |-------------------------------| | Bit index | 8n - 1 | 8n - 2 | ... | 8n - 8 | ... | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\n| Byte index | 0 | 1 | ... | n - 1 | |------------|--------------------------------|----| ... |-------------------------------| | Byte | w0 | w1 | ... | w(n - 1) | |------------|--------------------------------|----| ... |-------------------------------| | Bit index | 8n - 1 | 8n - 2 | ... | 8n - 8 | ... | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\nWe describe precisely the operations we intend to implement, and their semantics. These operations will have the following signatures:\nbitwiseLogicalAnd :: BuiltinBool - BuiltinByteString - BuiltinByteString - BuiltinByteString\nbitwiseLogicalAnd :: BuiltinBool -> BuiltinByteString -> BuiltinByteString -> BuiltinByteString\nbitwiseLogicalOr :: BuiltinBool - BuiltinByteString - BuiltinByteString - BuiltinByteString\nbitwiseLogicalOr :: BuiltinBool -> BuiltinByteString -> BuiltinByteString -> BuiltinByteString\nbitwiseLogicalXor :: BuiltinBool - BuiltinByteString - BuiltinByteString - BuiltinByteString\nbitwiseLogicalXor :: BuiltinBool -> BuiltinByteString -> BuiltinByteString -> BuiltinByteString\nbitwiseLogicalComplement :: BuiltinByteString - BuiltinByteString\nbitwiseLogicalComplement :: BuiltinByteString -> BuiltinByteString\nreadBit :: BuiltinByteString - BuiltinInteger - BuiltinBool\nreadBit :: BuiltinByteString -> BuiltinInteger -> BuiltinBool\nwriteBits :: BuiltinByteString - [(BuiltinInteger, BuiltinBool)] - BuiltinByteString\nwriteBits :: BuiltinByteString -> [(BuiltinInteger, BuiltinBool)] -> BuiltinByteString\nreplicateByteString :: BuiltinInteger - BuiltinInteger - BuiltinByteString\nreplicateByteString :: BuiltinInteger -> BuiltinInteger -> BuiltinByteString\nWe assume the following costing, for both memory and execution time:\nbitwiseLogicalAnd\nBuiltinByteString\nbitwiseLogicalOr\nBuiltinByteString\nbitwiseLogicalXor\nBuiltinByteString\nbitwiseLogicalComplement\nBuiltinByteString\nreadBit\nwriteBits\nreplicateByteString\nFor the binary logical operations (that is, bitwiseLogicalAnd, bitwiseLogicalOr and bitwiseLogicalXor), the we have two choices of semantics when handling BuiltinByteString arguments of different lengths. We can either produce a result whose length is the minimum of the two arguments (which we call truncation semantics), or produce a result whose length is the maximum of the two arguments (which we call padding semantics). As these can both be useful depending on context, we allow both, controlled by a BuiltinBool flag, on all the operations listed above.\nbitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalXor\nBuiltinByteString\nBuiltinBool\nIn cases where we have arguments of different lengths, in order to produce a result of the appropriate lengths, one of the arguments needs to be either padded or truncated. Let short and long refer to the BuiltinByteString argument of shorter length, and of longer length, respectively. The following table describes what happens to the arguments before the operation:\nshort\nlong\nBuiltinByteString\nshort\nlong\nWe pad with different bytes depending on operation: for bitwiseLogicalAnd, we pad with 0xFF, while for bitwiseLogicalOr and bitwiseLogicalXor we pad with 0x00 instead. We refer to arguments so changed as semantics-modified arguments.\nbitwiseLogicalAnd\n0xFF\nbitwiseLogicalOr\nbitwiseLogicalXor\n0x00\nFor example, consider the BuiltinByteStrings x = [0x00, 0xF0, 0xFF] and y = [0xFF, 0xF0]. The following table describes what the semantics-modified versions of these arguments would become for each operation and each semantics:\nBuiltinByteString\nx = [0x00, 0xF0, 0xFF]\ny = [0xFF, 0xF0]\nx\ny\nbitwiseLogicalAnd\n[0x00, 0xF0, 0xFF]\n[0xFF, 0xF0, 0xFF]\nbitwiseLogicalAnd\n[0x00, 0xF0]\n[0xFF, 0xF0]\nbitwiseLogicalOr\n[0x00, 0xF0, 0xFF]\n[0xFF, 0xF0, 0x00]\nbitwiseLogicalor\n[0x00, 0xF0]\n[0xFF, 0xF0]\nbitwiseLogicalXor\n[0x00, 0xF0, 0xFF]\n[0xFF, 0xF0, 0x00]\nbitwiseLogicalXor\n[0x00, 0xF0]\n[0xFF, 0xF0]\nBased on the above, we observe that under padding semantics, the result of any of the listed operations would have a byte length of 3, while under truncation semantics, the result would have a byte length of 2 instead.\nbitwiseLogicalAnd\nbitwiseLogicalAnd takes three arguments; we name and describe them below.\nbitwiseLogicalAnd\nWhether padding semantics should be used. If this argument is False, truncation semantics are used instead. This is the padding semantics argument, and has type BuiltinBool. The first input BuiltinByteString. This is the first data argument. The second input BuiltinByteString. This is the second data argument.\nWhether padding semantics should be used. If this argument is False, truncation semantics are used instead. This is the padding semantics argument, and has type BuiltinBool.\nFalse\nBuiltinBool\nThe first input BuiltinByteString. This is the first data argument.\nBuiltinByteString\nThe second input BuiltinByteString. This is the second data argument.\nBuiltinByteString\nLet b1,b2b_1, b_2b1 ,b2 refer to the semantics-modified first data argument and semantics-modified second data argument respectively, and let nnn be either of their lengths in bytes; see the section on padding versus truncation semantics for the exact specification of this. Let the result of bitwiseLogicalAnd, given b1,b2b_1, b_2b1 ,b2 and some padding semantics argument, be brb_rbr , also of length nnn in bytes. We use b1ib_1\\\\{i\\\\}b1 i to refer to the byte at index iii in b1b_1b1 (and analogously for b2b_2b2 , brb_rbr ); see the section on the bit indexing scheme for the exact specification of this.\nbitwiseLogicalAnd\nFor all i 0,1, ,n 1i \\in 0, 1, \\ldots, n - 1i 0,1, ,n 1, we have b_r\\\\{i\\\\} = b_0\\\\{i\\\\} \\text{ }\\\\& \\text{ } b_1\\\\{i\\\\}, where \\\\& refers to a bitwise AND.\nSome examples of the intended behaviour of bitwiseLogicalAnd follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\nbitwiseLogicalAnd\nBuiltinByteString\n-- truncation semantics bitwiseLogicalAnd False [] [0xFF] => [] bitwiseLogicalAnd False [0xFF] [] => [] bitwiseLogicalAnd False [0xFF] [0x00] => [0x00] bitwiseLogicalAnd False [0x00] [0xFF] => [0x00] bitwiseLogicalAnd False [0x4F, 0x00] [0xF4] => [0x44] -- padding semantics bitwiseLogicalAnd True [] [0xFF] => [0xFF] bitwiseLogicalAnd True [0xFF] [] => [0xFF] bitwiseLogicalAnd True [0xFF] [0x00] => [0x00] bitwiseLogicalAnd True [0x00] [0xFF] => [0x00] bitwiseLogicalAnd True [0x4F, 0x00] [0xF4] => [0x44, 0x00]\n-- truncation semantics bitwiseLogicalAnd False [] [0xFF] => [] bitwiseLogicalAnd False [0xFF] [] => [] bitwiseLogicalAnd False [0xFF] [0x00] => [0x00] bitwiseLogicalAnd False [0x00] [0xFF] => [0x00] bitwiseLogicalAnd False [0x4F, 0x00] [0xF4] => [0x44] -- padding semantics bitwiseLogicalAnd True [] [0xFF] => [0xFF] bitwiseLogicalAnd True [0xFF] [] => [0xFF] bitwiseLogicalAnd True [0xFF] [0x00] => [0x00] bitwiseLogicalAnd True [0x00] [0xFF] => [0x00] bitwiseLogicalAnd True [0x4F, 0x00] [0xF4] => [0x44, 0x00]\nbitwiseLogicalOr\nbitwiseLogicalOr takes three arguments; we name and describe them below.\nbitwiseLogicalOr\nWhether padding semantics should be used. If this argument is False, truncation semantics are used instead. This is the padding semantics argument, and has type BuiltinBool. The first input BuiltinByteString. This is the first data argument. The second input BuiltinByteString. This is the second data argument.\nWhether padding semantics should be used. If this argument is False, truncation semantics are used instead. This is the padding semantics argument, and has type BuiltinBool.\nFalse\nBuiltinBool\nThe first input BuiltinByteString. This is the first data argument.\nBuiltinByteString\nThe second input BuiltinByteString. This is the second data argument.\nBuiltinByteString\nLet b1,b2b_1, b_2b1 ,b2 refer to the semantics-modified first data argument and semantics-modified second data argument respectively, and let nnn be either of their lengths in bytes; see the section on padding versus truncation semantics for the exact specification of this. Let the result of bitwiseLogicalOr, given b1,b2b_1, b_2b1 ,b2 and some padding semantics argument, be brb_rbr , also of length nnn in bytes. We use b1ib_1\\\\{i\\\\}b1 i to refer to the byte at index iii in b1b_1b1 (and analogously for b2b_2b2 , brb_rbr ); see the section on the bit indexing scheme for the exact specification of this.\nbitwiseLogicalOr\nFor all i 0,1, ,n 1i \\in 0, 1, \\ldots, n - 1i 0,1, ,n 1, we have bri=b0i b1ib_r\\\\{i\\\\} = b_0\\\\{i\\\\} \\text{ } \\| \\text{ } b_1\\\\{i\\\\}br i=b0 i b1 i, where \\| refers to a bitwise OR.\n-- truncation semantics bitwiseLogicalOr False [] [0xFF] => [] bitwiseLogicalOr False [0xFF] [] => [] bitwiseLogicalOr False [0xFF] [0x00] => [0xFF] bitwiseLogicalOr False [0x00] [0xFF] => [0xFF] bitwiseLogicalOr False [0x4F, 0x00] [0xF4] => [0xFF] -- padding semantics bitwiseLogicalOr True [] [0xFF] => [0xFF] bitwiseLogicalOr True [0xFF] [] => [0xFF] bitwiseLogicalOr True [0xFF] [0x00] => [0xFF] bitwiseLogicalOr True [0x00] [0xFF] => [0xFF] bitwiseLogicalOr True [0x4F, 0x00] [0xF4] => [0xFF, 0x00]\n-- truncation semantics bitwiseLogicalOr False [] [0xFF] => [] bitwiseLogicalOr False [0xFF] [] => [] bitwiseLogicalOr False [0xFF] [0x00] => [0xFF] bitwiseLogicalOr False [0x00] [0xFF] => [0xFF] bitwiseLogicalOr False [0x4F, 0x00] [0xF4] => [0xFF] -- padding semantics bitwiseLogicalOr True [] [0xFF] => [0xFF] bitwiseLogicalOr True [0xFF] [] => [0xFF] bitwiseLogicalOr True [0xFF] [0x00] => [0xFF] bitwiseLogicalOr True [0x00] [0xFF] => [0xFF] bitwiseLogicalOr True [0x4F, 0x00] [0xF4] => [0xFF, 0x00]\nbitwiseLogicalXor\nbitwiseLogicalXor takes three arguments; we name and describe them below.\nbitwiseLogicalXor\nWhether padding semantics should be used. If this argument is False, truncation semantics are used instead. This is the padding semantics argument, and has type BuiltinBool. The first input BuiltinByteString. This is the first data argument. The second input BuiltinByteString. This is the second data argument.\nWhether padding semantics should be used. If this argument is False, truncation semantics are used instead. This is the padding semantics argument, and has type BuiltinBool.\nFalse\nBuiltinBool\nThe first input BuiltinByteString. This is the first data argument.\nBuiltinByteString\nThe second input BuiltinByteString. This is the second data argument.\nBuiltinByteString\nLet b1,b2b_1, b_2b1 ,b2 refer to the semantics-modified first data argument and semantics-modified second data argument respectively, and let nnn be either of their lengths in bytes; see the section on padding versus truncation semantics for the exact specification of this. Let the result of bitwiseLogicalXor, given b1,b2b_1, b_2b1 ,b2 and some padding semantics argument, be brb_rbr , also of length nnn in bytes. We use b1ib_1\\\\{i\\\\}b1 i to refer to the byte at index iii in b1b_1b1 (and analogously for b2b_2b2 , brb_rbr ); see the section on the bit indexing scheme for the exact specification of this.\nbitwiseLogicalXor\nFor all i 0,1, ,n 1i \\in 0, 1, \\ldots, n - 1i 0,1, ,n 1, we have bri=b0i b1ib_r\\\\{i\\\\} = b_0\\\\{i\\\\} \\text{ } \\wedge \\text{ } b_1\\\\{i\\\\}br i=b0 i b1 i, where \\wedge refers to a bitwise XOR.\nSome examples of the intended behaviour of bitwiseLogicalXor follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\nbitwiseLogicalXor\nBuiltinByteString\n-- truncation semantics bitwiseLogicalXor False [] [0xFF] => [] bitwiseLogicalXor False [0xFF] [] => [] bitwiseLogicalXor False [0xFF] [0x00] => [0xFF] bitwiseLogicalXor False [0x00] [0xFF] => [0xFF] bitwiseLogicalXor False [0x4F, 0x00] [0xF4] => [0xBB] -- padding semantics bitwiseLogicalOr True [] [0xFF] => [0xFF] bitwiseLogicalOr True [0xFF] [] => [0xFF] bitwiseLogicalOr True [0xFF] [0x00] => [0xFF] bitwiseLogicalOr True [0x00] [0xFF] => [0xFF] bitwiseLogicalOr True [0x4F, 0x00] [0xF4] => [0xBB, 0x00]\n-- truncation semantics bitwiseLogicalXor False [] [0xFF] => [] bitwiseLogicalXor False [0xFF] [] => [] bitwiseLogicalXor False [0xFF] [0x00] => [0xFF] bitwiseLogicalXor False [0x00] [0xFF] => [0xFF] bitwiseLogicalXor False [0x4F, 0x00] [0xF4] => [0xBB] -- padding semantics bitwiseLogicalOr True [] [0xFF] => [0xFF] bitwiseLogicalOr True [0xFF] [] => [0xFF] bitwiseLogicalOr True [0xFF] [0x00] => [0xFF] bitwiseLogicalOr True [0x00] [0xFF] => [0xFF] bitwiseLogicalOr True [0x4F, 0x00] [0xF4] => [0xBB, 0x00]\nbitwiseLogicalComplement\nbitwiseLogicalComplement takes a single argument, of type BuiltinByteString; let bbb refer to that argument, and nnn its length in bytes. Let brb_rbr be the result of bitwiseLogicalComplement; its length in bytes is also nnn. We use b[i]b[i]b[i] to refer to the value at index iii of bbb (and analogously for brb_rbr ); see the section on the bit indexing scheme for the exact specification of this.\nbitwiseLogicalComplement\nBuiltinByteString\nbitwiseLogicalComplement\nFor all i 0,1, ,8 n 1i \\in 0, 1, \\ldots , 8 \\cdot n - 1i 0,1, ,8 n 1, we have\nSome examples of the intended behaviour of bitwiseLogicalComplement follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\nbitwiseLogicalComplement\nBuiltinByteString\nbitwiseLogicalComplement [] => [] bitwiseLogicalComplement [0x0F] => [0xF0] bitwiseLogicalComplement [0x4F, 0xF4] => [0xB0, 0x0B]\nbitwiseLogicalComplement [] => [] bitwiseLogicalComplement [0x0F] => [0xF0] bitwiseLogicalComplement [0x4F, 0xF4] => [0xB0, 0x0B]\nreadBit\nreadBit takes two arguments; we name and describe them below.\nreadBit\nThe BuiltinByteString in which the bit we want to read can be found. This is the data argument. A bit index into the data argument, of type BuiltinInteger. This is the index argument.\nThe BuiltinByteString in which the bit we want to read can be found. This is the data argument.\nBuiltinByteString\nA bit index into the data argument, of type BuiltinInteger. This is the index argument.\nBuiltinInteger\nLet bbb refer to the data argument, of length nnn in bytes, and let iii refer to the index argument. We use b[i]b[i]b[i] to refer to the value at index iii of bbb; see the section on the bit indexing scheme for the exact specification of this.\nIf i 0i 0i 0 or i 8 ni \\geq 8 \\cdot ni 8 n, then readBit fails. In this case, the resulting error message must specify at least the following information:\nreadBit\nThat readBit failed due to an out-of-bounds index argument; and\nreadBit\nWhat BuiltinInteger was passed as an index argument.\nBuiltinInteger\nOtherwise, if b[i]=0b[i] = 0b[i]=0, readBit returns False, and if b[i]=1b[i] = 1b[i]=1, readBit returns True.\nreadBit\nFalse\nreadBit\nTrue\nSome examples of the intended behaviour of readBit follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\nreadBit\nBuiltinByteString\n-- Indexing an empty BuiltinByteString fails readBit [] 0 => error readBit [] 345 => error -- Negative indexes fail readBit [] (-1) => error readBit [0xFF] (-1) => error -- Indexing reads 'from the end' readBit [0xF4] 0 => False readBit [0xF4] 1 => False readBit [0xF4] 2 => True readBit [0xF4] 3 => False readBit [0xF4] 4 => True readBit [0xF4] 5 => True readBit [0xF4] 6 => True readBit [0xF4] 7 => True -- Out-of-bounds indexes fail readBit [0xF4] 8 => error readBit [0xFF, 0xF4] 16 => error -- Larger indexes read backwards into the bytes from the end readBit [0xF4, 0xFF] 10 => False\n-- Indexing an empty BuiltinByteString fails readBit [] 0 => error readBit [] 345 => error -- Negative indexes fail readBit [] (-1) => error readBit [0xFF] (-1) => error -- Indexing reads 'from the end' readBit [0xF4] 0 => False readBit [0xF4] 1 => False readBit [0xF4] 2 => True readBit [0xF4] 3 => False readBit [0xF4] 4 => True readBit [0xF4] 5 => True readBit [0xF4] 6 => True readBit [0xF4] 7 => True -- Out-of-bounds indexes fail readBit [0xF4] 8 => error readBit [0xFF, 0xF4] 16 => error -- Larger indexes read backwards into the bytes from the end readBit [0xF4, 0xFF] 10 => False\nwriteBits\nwriteBits takes two arguments: we name and describe them below.\nwriteBits\nThe BuiltinByteString in which we want to change some bits. This is the data argument. A list of index-value pairs, indicating which positions in the data argument should be changed to which value. This is the change list argument. Each index has type BuiltinInteger, while each value has type BuiltinBool.\nThe BuiltinByteString in which we want to change some bits. This is the data argument.\nBuiltinByteString\nA list of index-value pairs, indicating which positions in the data argument should be changed to which value. This is the change list argument. Each index has type BuiltinInteger, while each value has type BuiltinBool.\nBuiltinInteger\nBuiltinBool\nLet bbb refer to the data argument of length nnn in bytes. We define writeBits recursively over the structure of the change list argument. Throughout, we use brb_rbr to refer to the result of writeBits, whose length is also nnn. We use b[i]b[i]b[i] to refer to the value at index iii of bbb (and analogously, brb_rbr ); see the section on the bit indexing scheme for the exact specification of this.\nwriteBits\nwriteBits\nIf the change list argument is empty, we return the data argument unchanged. Otherwise, let (i,v)(i, v)(i,v) be the head of the change list argument, and \\ell its tail. If i 0i 0i 0 or i 8 ni \\geq 8 \\cdot ni 8 n, then writeBits fails. In this case, the resulting error message must specify at least the following information:\nwriteBits\nThat writeBits failed due to an out-of-bounds index argument; and\nwriteBits\nWhat BuiltinInteger was passed as iii.\nBuiltinInteger\nOtherwise, for all j 0,1, 8 n 1j \\in 0, 1, \\ldots 8 \\cdot n - 1j 0,1, 8 n 1, we have\nThen, if we did not fail as described above, we repeat the writeBits operation, but with brb_rbr as the data argument and \\ell as the change list argument.\nwriteBits\nSome examples of the intended behaviour of writeBits follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\nwriteBits\nBuiltinByteString\n-- Writing an empty BuiltinByteString fails writeBits [] [(0, False)] => error -- Irrespective of index writeBits [] [(15, False)] => error -- And value writeBits [] [(0, True)] => error -- And multiplicity writeBits [] [(0, False), (1, False)] => error -- Negative indexes fail writeBits [0xFF] [((-1), False)] => error -- Even when mixed with valid ones writeBits [0xFF] [(0, False), ((-1), True)] => error -- In any position writeBits [0xFF] [((-1), True), (0, False)] => error -- Out-of-bounds indexes fail writeBits [0xFF] [(8, False)] => error -- Even when mixed with valid ones writeBits [0xFF] [(1, False), (8, False)] => error -- In any position writeBits [0xFF] [(8, False), (1, False)] => error -- Bits are written 'from the end' writeBits [0xFF] [(0, False)] => [0xFE] writeBits [0xFF] [(1, False)] => [0xFD] writeBits [0xFF] [(2, False)] => [0xFB] writeBits [0xFF] [(3, False)] => [0xF7] writeBits [0xFF] [(4, False)] => [0xEF] writeBits [0xFF] [(5, False)] => [0xDF] writeBits [0xFF] [(6, False)] => [0xBF] writeBits [0xFF] [(7, False)] => [0x7F] -- True value sets the bit writeBits [0x00] [(5, True)] => [0x20] -- False value clears the bit writeBits [0xFF] [(5, False)] => [0xDF] -- Larger indexes write backwards into the bytes from the end writeBits [0xF4, 0xFF] [(10, False)] => [0xF0, 0xFF] -- Multiple items in a change list apply cumulatively writeBits [0xF4, 0xFF] [(10, False), (1, False)] => [0xF0, 0xFD] writeBits (writeBits [0xF4, 0xFF] [(10, False)]) [(1, False)] => [0xF0, 0xFD] -- Order within a change list is unimportant among unique indexes writeBits [0xF4, 0xFF] [(1, False), (10, False)] => [0xF0, 0xFD] -- But _is_ important for identical indexes writeBits [0x00, 0xFF] [(10, True), (10, False)] => [0x00, 0xFF] writeBits [0x00, 0xFF] [(10, False), (10, True)] => [0x04, 0xFF] -- Setting an already set bit does nothing writeBits [0xFF] [(0, True)] => [0xFF] -- Clearing an already clear bit does nothing writeBits [0x00] [(0, False)] => [0x00]\n-- Writing an empty BuiltinByteString fails writeBits [] [(0, False)] => error -- Irrespective of index writeBits [] [(15, False)] => error -- And value writeBits [] [(0, True)] => error -- And multiplicity writeBits [] [(0, False), (1, False)] => error -- Negative indexes fail writeBits [0xFF] [((-1), False)] => error -- Even when mixed with valid ones writeBits [0xFF] [(0, False), ((-1), True)] => error -- In any position writeBits [0xFF] [((-1), True), (0, False)] => error -- Out-of-bounds indexes fail writeBits [0xFF] [(8, False)] => error -- Even when mixed with valid ones writeBits [0xFF] [(1, False), (8, False)] => error -- In any position writeBits [0xFF] [(8, False), (1, False)] => error -- Bits are written 'from the end' writeBits [0xFF] [(0, False)] => [0xFE] writeBits [0xFF] [(1, False)] => [0xFD] writeBits [0xFF] [(2, False)] => [0xFB] writeBits [0xFF] [(3, False)] => [0xF7] writeBits [0xFF] [(4, False)] => [0xEF] writeBits [0xFF] [(5, False)] => [0xDF] writeBits [0xFF] [(6, False)] => [0xBF] writeBits [0xFF] [(7, False)] => [0x7F] -- True value sets the bit writeBits [0x00] [(5, True)] => [0x20] -- False value clears the bit writeBits [0xFF] [(5, False)] => [0xDF] -- Larger indexes write backwards into the bytes from the end writeBits [0xF4, 0xFF] [(10, False)] => [0xF0, 0xFF] -- Multiple items in a change list apply cumulatively writeBits [0xF4, 0xFF] [(10, False), (1, False)] => [0xF0, 0xFD] writeBits (writeBits [0xF4, 0xFF] [(10, False)]) [(1, False)] => [0xF0, 0xFD] -- Order within a change list is unimportant among unique indexes writeBits [0xF4, 0xFF] [(1, False), (10, False)] => [0xF0, 0xFD] -- But _is_ important for identical indexes writeBits [0x00, 0xFF] [(10, True), (10, False)] => [0x00, 0xFF] writeBits [0x00, 0xFF] [(10, False), (10, True)] => [0x04, 0xFF] -- Setting an already set bit does nothing writeBits [0xFF] [(0, True)] => [0xFF] -- Clearing an already clear bit does nothing writeBits [0x00] [(0, False)] => [0x00]\nreplicateByteString\nreplicateByteString takes two arguments; we name and describe them below.\nreplicateByteString\nThe desired result length, of type BuiltinInteger. This is the length argument. The byte to place at each position in the result, represented as a BuiltinInteger (corresponding to the unsigned integer this byte encodes). This is the byte argument.\nThe desired result length, of type BuiltinInteger. This is the length argument.\nBuiltinInteger\nThe byte to place at each position in the result, represented as a BuiltinInteger (corresponding to the unsigned integer this byte encodes). This is the byte argument.\nBuiltinInteger\nLet nnn be the length argument, and www the byte argument. If n 0n 0n 0, then replicateByteString fails. In this case, the resulting error message must specify at least the following information:\nreplicateByteString\nThat replicateByteString failed due to a negative length argument; and\nreplicateByteString\nWhat BuiltinInteger was passed as the length argument.\nBuiltinInteger\nIf n 0n \\geq 0n 0, and w 0w 0w 0 or w 255w 255w 255, then replicateByteString fails. In this case, the resulting error message must specify at least the following information:\nreplicateByteString\nThat replicateByteString failed due to the byte argument not being a valid byte; and\nreplicateByteString\nWhat BuiltinInteger was passed as the byte argument.\nBuiltinInteger\nOtherwise, let bbb be the result of replicateByteString, and let bib\\\\{i\\\\}bi be the byte at position iii of bbb, as per the section describing the bit indexing scheme. We have:\nreplicateByteString\nThe length (in bytes) of bbb is nnn; and\nFor all i 0,1, ,n 1i \\in 0, 1, \\ldots, n - 1i 0,1, ,n 1, bi=wb\\\\{i\\\\} = wbi=w.\nSome examples of the intended behaviour of replicateByteString follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\nreplicateByteString\nBuiltinByteString\n-- Replicating a negative number of times fails replicateByteString (-1) 0 => error -- Irrespective of byte argument replicateByteString (-1) 3 => error -- Out-of-bounds byte arguments fail replicateByteString 1 (-1) => error replicateByteString 1 256 => error -- Irrespective of length argument replicateByteString 4 (-1) => error replicateByteString 4 256 => error -- Length of result matches length argument, and all bytes are the same replicateByteString 0 0xFF => [] replicateByteString 4 0xFF => [0xFF, 0xFF, 0xFF, 0xFF]\n-- Replicating a negative number of times fails replicateByteString (-1) 0 => error -- Irrespective of byte argument replicateByteString (-1) 3 => error -- Out-of-bounds byte arguments fail replicateByteString 1 (-1) => error replicateByteString 1 256 => error -- Irrespective of length argument replicateByteString 4 (-1) => error replicateByteString 4 256 => error -- Length of result matches length argument, and all bytes are the same replicateByteString 0 0xFF => [] replicateByteString 4 0xFF => [0xFF, 0xFF, 0xFF, 0xFF]\nWe describe laws for all three operations that work over two BuiltinByteStrings, that is, bitwiseLogicalAnd, bitwiseLogicalOr and bitwiseLogicalXor, together, as many of them are similar (and related). We describe padding semantics and truncation semantics laws, as they are slightly different.\nBuiltinByteStrings\nbitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalXor\nAll three operations above, under both padding and truncation semantics, are commutative semigroups. Thus, we have:\nbitwiseLogicalAnd s x y = bitwiseLogicalAnd s y x bitwiseLogicalAnd s x (bitwiseLogicalAnd s y z) = bitwiseLogicalAnd s (bitwiseLogicalAnd s x y) z -- and the same for bitwiseLogicalOr and bitwiseLogicalXor\nbitwiseLogicalAnd s x y = bitwiseLogicalAnd s y x bitwiseLogicalAnd s x (bitwiseLogicalAnd s y z) = bitwiseLogicalAnd s (bitwiseLogicalAnd s x y) z -- and the same for bitwiseLogicalOr and bitwiseLogicalXor\nNote that the semantics (designated as s above) must be consistent in order for these laws to hold. Furthermore, under padding semantics, all the above operations are commutative monoids:\ns\nbitwiseLogicalAnd True x \"\" = bitwiseLogicalAnd True \"\" x = x -- and the same for bitwiseLogicalOr and bitwiseLogicalXor\nbitwiseLogicalAnd True x \"\" = bitwiseLogicalAnd True \"\" x = x -- and the same for bitwiseLogicalOr and bitwiseLogicalXor\nUnder truncation semantics, \"\" (that is, the empty BuiltinByteString) acts instead as an absorbing element:\n\"\"\nBuiltinByteString\nbitwiseLogicalAnd False x \"\" = bitwiseLogicalAnd False \"\" x = \"\" -- and the same for bitwiseLogicalOr and bitwiseLogicalXor\nbitwiseLogicalAnd False x \"\" = bitwiseLogicalAnd False \"\" x = \"\" -- and the same for bitwiseLogicalOr and bitwiseLogicalXor\nbitwiseLogicalAnd and bitwiseLogicalOr are also semilattices, due to their idempotence:\nbitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalAnd s x x = x -- and the same for bitwiseLogicalOr\nbitwiseLogicalAnd s x x = x -- and the same for bitwiseLogicalOr\nbitwiseLogicalXor is instead involute:\nbitwiseLogicalXor\nbitwiseLogicalXor s x (bitwiseLogicalXor s x x) = bitwiseLogicalXor s (bitwiseLogicalXor s x x) x = x\nbitwiseLogicalXor s x (bitwiseLogicalXor s x x) = bitwiseLogicalXor s (bitwiseLogicalXor s x x) x = x\nAdditionally, under padding semantics, bitwiseLogicalAnd and bitwiseLogicalOr are self-distributive:\nbitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalAnd True x (bitwiseLogicalAnd True y z) = bitwiseLogicalAnd True (bitwiseLogicalAnd True x y) (bitwiseLogicalAnd True x z) bitwiseLogicalAnd True (bitwiseLogicalAnd True x y) z = bitwiseLogicalAnd True (bitwiseLogicalAnd True x z) (bitwiseLogicalAnd True y z) -- and the same for bitwiseLogicalOr\nbitwiseLogicalAnd True x (bitwiseLogicalAnd True y z) = bitwiseLogicalAnd True (bitwiseLogicalAnd True x y) (bitwiseLogicalAnd True x z) bitwiseLogicalAnd True (bitwiseLogicalAnd True x y) z = bitwiseLogicalAnd True (bitwiseLogicalAnd True x z) (bitwiseLogicalAnd True y z) -- and the same for bitwiseLogicalOr\nUnder truncation semantics, bitwiseLogicalAnd is only left-distributive over itself, bitwiseLogicalOr and bitwiseLogicalXor:\nbitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalXor\nbitwiseLogicalAnd False x (bitwiseLogicalAnd False y z) = bitwiseLogicalAnd False (bitwiseLogicalAnd False x y) (bitwiseLogicalAnd False x z) bitwiseLogicalAnd False x (bitwiseLogicalOr False y z) = bitwiseLogicalOr False (bitwiseLogicalAnd False x y) (bitwiseLogicalAnd False x z) bitwiseLogicalAnd False x (bitwiseLogicalXor False y z) = bitwiseLogicalXor False (bitwiseLogicalAnd False x y) (bitwiseLogicalAnd False x z)\nbitwiseLogicalAnd False x (bitwiseLogicalAnd False y z) = bitwiseLogicalAnd False (bitwiseLogicalAnd False x y) (bitwiseLogicalAnd False x z) bitwiseLogicalAnd False x (bitwiseLogicalOr False y z) = bitwiseLogicalOr False (bitwiseLogicalAnd False x y) (bitwiseLogicalAnd False x z) bitwiseLogicalAnd False x (bitwiseLogicalXor False y z) = bitwiseLogicalXor False (bitwiseLogicalAnd False x y) (bitwiseLogicalAnd False x z)\nbitwiseLogicalOr under truncation semantics is left-distributive over itself and bitwiseLogicalAnd:\nbitwiseLogicalOr\nbitwiseLogicalAnd\nbitwiseLogicalOr False x (bitwiseLogicalOr False y z) = bitwiseLogicalOr False (bitwiseLogicalOr False x y) (bitwiseLogicalOr False x z) bitwiseLogicalOr False x (bitwiseLogicalAnd False y z) = bitwiseLogicalAnd False (bitwiseLogicalOr False x y) (bitwiseLogicalOr False x z)\nbitwiseLogicalOr False x (bitwiseLogicalOr False y z) = bitwiseLogicalOr False (bitwiseLogicalOr False x y) (bitwiseLogicalOr False x z) bitwiseLogicalOr False x (bitwiseLogicalAnd False y z) = bitwiseLogicalAnd False (bitwiseLogicalOr False x y) (bitwiseLogicalOr False x z)\nIf the first and second data arguments to these operations have the same length, these operations satisfy several additional laws. We describe these briefly below, with the added note that, in this case, padding and truncation semantics coincide:\nbitwiseLogicalAnd and bitwiseLogicalOr form a bounded lattice\nbitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalAnd is distributive over itself, bitwiseLogicalOr and bitwiseLogicalXor\nbitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalXor\nbitwiseLogicalOr is distributive over itself and bitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalAnd\nWe do not specify these laws here, as they do not hold in general. At the same time, we expect that any implementation of these operations will be subject to these laws.\nbitwiseLogicalComplement\nThe main law of bitwiseLogicalComplement is involution:\nbitwiseLogicalComplement\nbitwiseLogicalComplement (bitwiseLogicalComplement x) = x\nbitwiseLogicalComplement (bitwiseLogicalComplement x) = x\nIn combination with bitwiseLogicalAnd and bitwiseLogicalOr, bitwiseLogicalComplement gives rise to the famous De Morgan laws, irrespective of semantics:\nbitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalComplement\nbitwiseLogicalComplement (bitwiseLogicalAnd s x y) = bitwiseLogicalOr s (bitwiseLogicalComplement x) (bitwiseLogicalComplement y) bitwiseLogicalComplement (bitwiseLogicalOr s x y) = bitwiseLogicalAnd s (bitwiseLogicalComplement x) (bitwiseLogicalComplement y)\nbitwiseLogicalComplement (bitwiseLogicalAnd s x y) = bitwiseLogicalOr s (bitwiseLogicalComplement x) (bitwiseLogicalComplement y) bitwiseLogicalComplement (bitwiseLogicalOr s x y) = bitwiseLogicalAnd s (bitwiseLogicalComplement x) (bitwiseLogicalComplement y)\nFor bitwiseLogicalXor, we instead have (again, irrespective of semantics):\nbitwiseLogicalXor\nbitwiseLogicalXor s x (bitwiseLogicalComplement x) = x\nbitwiseLogicalXor s x (bitwiseLogicalComplement x) = x\nThroughout, we assume any index arguments to be 'in-bounds'; that is, all the index arguments used in the statements of any law are such that the operation they are applied to wouldn't produce an error.\nThe first law of writeBits is similar to the set-twice law of lenses:\nwriteBits\nwriteBits bs [(i, b1), (i, b2)] = writeBits bs [(i, b2)]\nwriteBits bs [(i, b1), (i, b2)] = writeBits bs [(i, b2)]\nTogether with readBit, we obtain the remaining two analogues to the lens laws:\nreadBit\n-- writing to an index, then reading from that index, gets you what you wrote readBit (writeBits bs [(i, b)]) i = b -- if you read from an index, then write that value to that same index, nothing -- happens writeBits bs [(i, readBit bs i)] = bs\n-- writing to an index, then reading from that index, gets you what you wrote readBit (writeBits bs [(i, b)]) i = b -- if you read from an index, then write that value to that same index, nothing -- happens writeBits bs [(i, readBit bs i)] = bs\nFurthermore, given a fixed data argument, writeBits acts as a monoid homomorphism lists under concatenation to functions:\nwriteBits\nwriteBits bs [] = bs writeBits bs (is <> js) = writeBits (writeBits bs is) js\nwriteBits bs [] = bs writeBits bs (is <> js) = writeBits (writeBits bs is) js\nreplicateByteString\nGiven a fixed byte argument, replicateByteString acts as a monoid homomorphism from natural numbers under addition to BuiltinByteStrings under concatenation:\nreplicateByteString\nBuiltinByteString\nreplicateByteString 0 w = \"\" replicateByteString (n + m) w = replicateByteString n w <> replicateByteString m w\nreplicateByteString 0 w = \"\" replicateByteString (n + m) w = replicateByteString n w <> replicateByteString m w\nAdditionally, for any 'in-bounds' index (that is, any index for which indexByteString won't error) i, we have\nindexByteString\ni\nindexByteString (replicateByteString n w) i = w\nindexByteString (replicateByteString n w) i = w\nLastly, we have\nlengthByteString (replicateByteString n w) = n\nlengthByteString (replicateByteString n w) = n\nThe operations, and semantics, described in this CIP provide a set of well-defined bitwise logical operations, as well as bitwise access and modification, to allow cases similar to Case 1 to be performed efficiently and conveniently. Furthermore, the semantics we describe would be reasonably familiar to users of other programming languages (including Haskell) which have provisions for bitwise logical operations of this kind, as well as some way of extending these operations to operate on packed byte vectors. At the same time, there are several choices we have made that are somewhat unusual, or could potentially have been implemented differently based on existing work: most notably, our choice of bit indexing scheme, the padding-versus-truncation semantics, and the multiplicitous definition of bit modification. Among existing work, a particularly important example is CIP-58, which makes provisions for operations similar to the ones described here, and from which we differ in several important ways. We clarify the reasoning behind our choices, and how they differ from existing work, below.\nAside from the issues we list below, we don't consider other operations controversial. Indeed, bitwiseLogicalComplement has a direct parallel to the implementation in CIP-58, and replicateByteString is a direct wrapper around the replicate function in ByteString. Thus, we do not discuss them further here.\nbitwiseLogicalComplement\nreplicateByteString\nreplicate\nByteString\nOur work relates to both CIP-58 and CIP-121. Essentially, our goal with both this CIP and CIP-121 is to both break CIP-58 into more manageable (and reviewable) parts, and also address some of the design choices in CIP-58 that were not as good (or as clear) as they could have been. In this regard, this CIP is a direct continuation of CIP-121; CIP-121 dealt with conversions between BuiltinByteString and BuiltinInteger, while this CIP handles bit indexing more generally, as well as 'parallel' logical operations that operate on all the bits of a BuiltinByteString in bulk.\nBuiltinByteString\nBuiltinInteger\nBuiltinByteString\nWe describe how our work in this CIP relates to (and in some cases, supercedes) CIP-58, as well as how it follows on from CIP-121, in more detail below.\nThe bit indexing scheme we describe here is designed around two considerations. Firstly, we want operations on these bits, as well as those results, to be as consistent and as predictable as possible: any individual familiar with such operations on variable-length bitvectors from another language shouldn't be surprised by the semantics. Secondly, we want to anticipate future bitwise operation extensions, such as shifts and rotations, and have the indexing scheme support efficient implementations (and predictable semantics) for these.\nWhile prior art for bit access (and modification) exists in almost any programming language, these are typically over types of fixed width (usually bytes, machine words, or something similar); for variable-width types, these typically are either not implemented at all, or if they are implemented, this is done in an external library, with varying support for certain operations. An example of the first is Haskell's ByteString, which has no way to even access, much less modify, individual bits; an example of the second is the CRoaring library for C, which supports all the operations we describe in this CIP, along with multiple others. In the second case, the exact arrangement of bits inside the representation is not something users are exposed to directly: instead, the bitvector type is opaque, and the library only guarantees consistency of API. In our case, this is not a viable choice, as we require bit access and byte access to both work on BuiltinByteString, and thus, some consistency of representation is required.\nByteString\nBuiltinByteString\nThe scheme for indexing bits within a byte that we describe in the relevant section is the same as the one used by the Data.Bits API in Haskell for Word8 bit indexing, and mirrors the decisions of most languages that provide such an API at all, as well as the conventional definition of such operations as (w i) & 1 for access, w | (1 i) for setting, and w & ~(1 i) for clearing. We could choose to 'flip' this indexing, by using a similar operation for 'index flipping' as we currently use for bytes: essentially, instead of\nData.Bits\nWord8\n(w >> i) & 1\nw | (1 << i)\nw & ~(1 << i)\nwe would instead use\nto designate bit iii as set (and analogously for clear). Together with the ability to choose not to flip the byte index, we get four possibilities, which have been described previously. For clarity, we name, and describe, them below. Throughout, we use n as the length of a given BuiltinByteString in bytes.\nn\nBuiltinByteString\nThe first possibility is that we 'flip' neither bit, nor byte, indexes. We call this the no-flip variant:\n| Byte index | 0 | 1 | ... | n - 1 | |------------|-------------------------------|-------------------| ... |--------------------------------| | Byte | w0 | w1 | ... | w(n - 1) | |------------|-------------------------------|-------------------| ... |--------------------------------| | Bit index | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 | 15 | 14 | ... | 8 | ... | 8n - 1 | 8n - 2 | ... | 8n - 8 |\n| Byte index | 0 | 1 | ... | n - 1 | |------------|-------------------------------|-------------------| ... |--------------------------------| | Byte | w0 | w1 | ... | w(n - 1) | |------------|-------------------------------|-------------------| ... |--------------------------------| | Bit index | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 | 15 | 14 | ... | 8 | ... | 8n - 1 | 8n - 2 | ... | 8n - 8 |\nThe second possibility is that we 'flip' both bit and byte indexes. We call this the both-flip variant:\n| Byte index | 0 | ... | n - 2 | n - 1 | |------------|--------------------------------| ... |------------------|-------------------------------| | Byte | w0 | ... | w (n - 2) | w(n - 1) | |------------|--------------------------------| ... |------------------|-------------------------------| | Bit index | 8n - 8 | 8n - 7 | ... | 8n - 1 | ... | 8 | 9 | ... | 15 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n| Byte index | 0 | ... | n - 2 | n - 1 | |------------|--------------------------------| ... |------------------|-------------------------------| | Byte | w0 | ... | w (n - 2) | w(n - 1) | |------------|--------------------------------| ... |------------------|-------------------------------| | Bit index | 8n - 8 | 8n - 7 | ... | 8n - 1 | ... | 8 | 9 | ... | 15 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\nThe third possibility is that we 'flip' bit indexes, but not byte indexes. We call this the bit-flip variant:\n| Byte index | 0 | 1 | ... | n - 1 | |------------|-------------------------------|--------------| ... |--------------------------------| | Byte | w0 | w1 | ... | w(n - 1) | |------------|-------------------------------|--------------| ... |--------------------------------| | Bit index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | ... | 15 | ... | 8n - 8 | 8n - 7 | ... | 8n - 1 |\n| Byte index | 0 | 1 | ... | n - 1 | |------------|-------------------------------|--------------| ... |--------------------------------| | Byte | w0 | w1 | ... | w(n - 1) | |------------|-------------------------------|--------------| ... |--------------------------------| | Bit index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | ... | 15 | ... | 8n - 8 | 8n - 7 | ... | 8n - 1 |\nThe fourth possibility is the one we describe in the bit indexing scheme section, which is also the scheme chosen by CIP-58. We repeat it below for clarity:\n| Byte index | 0 | 1 | ... | n - 1 | |------------|--------------------------------|----| ... |-------------------------------| | Byte | w0 | w1 | ... | w(n - 1) | |------------|--------------------------------|----| ... |-------------------------------| | Bit index | 8n - 1 | 8n - 2 | ... | 8n - 8 | ... | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\n| Byte index | 0 | 1 | ... | n - 1 | |------------|--------------------------------|----| ... |-------------------------------| | Byte | w0 | w1 | ... | w(n - 1) | |------------|--------------------------------|----| ... |-------------------------------| | Bit index | 8n - 1 | 8n - 2 | ... | 8n - 8 | ... | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\nOn the face of it, these schemes appear equivalent: they are all consistent, and all have formal descriptions, and quite similar ones at that. However, we believe that only the one we chose is the correct one. To explain this, we introduce two notions that we consider to be both intuitive and important, then specify why our choice of indexing scheme fits those notions better than any other.\nThe first notion is index locality. Intuitively, this states that if two indexes are 'close' (in that their absolute difference is small), the values at those indexes should be 'close' (in that their positioning in memory should be separated less). We believe this notion to be reasonable, as this is an expectation from array indexing (and indeed, BuiltinByteString indexing), as well as the reason why packed array data is efficient on modern memory hierarchies. Extending this notion to bits, we can observe that the both-flip and no-flip variants of the bit indexing scheme do not preserve index locality: the separation between a bit at index 000 and index 111 is significantly different to the separation between a bit at index 777 and index 888 in both representations, despite their absolute difference being identical. Thus, we believe that these two variants are not viable, as they are not only confusing from the point of view of behaviour, they would also make implementation of future operations (such as shifts or rotations) significantly harder to both do, and also reason about. Thus, only the bit-flip variant, as well as our choice, remain contenders.\nBuiltinByteString\nThe second notion is most-significant-first conversion agreement. This notion refers to the CIP-121 concept of the same name, and ensures that (at least for the most-significant-first arrangement), the following workflow doesn't produce unexpected results:\nConvert a BuiltinInteger to BuiltinByteString using builtinIntegerToByteString with the most-significant-first endianness argument. Manipulate the bits of the result of step 1 using the operations specified here. Convert the result of step 2 back to a BuiltinInteger using builtinByteStringToInteger with the most-significant-first endianness argument.\nConvert a BuiltinInteger to BuiltinByteString using builtinIntegerToByteString with the most-significant-first endianness argument.\nBuiltinInteger\nBuiltinByteString\nbuiltinIntegerToByteString\nManipulate the bits of the result of step 1 using the operations specified here.\nConvert the result of step 2 back to a BuiltinInteger using builtinByteStringToInteger with the most-significant-first endianness argument.\nBuiltinInteger\nbuiltinByteStringToInteger\nThis workflow is directly relevant to Case 2. The Argon2 family of hashes use certain inputs (which happen to be numbers) both as numbers (meaning, for arithmetic operatons) and also as blocks of binary (specifically for XOR). This is not unique to Argon2, or even hashing, as a range of operations (especially in cryptographic applications) use similar approaches, whether for performance, semantics or both. In such cases, users of our primitives (both logical and conversion) must be confident that their changes 'translate' in the way they expect between these two 'views' of the data.\nThe choice of most-significant-first as the arrangement that we must agree with seems somewhat arbitrary at a glance, for two reasons: firstly, it's not clear why we must pick a single arrangement to be consistent with; secondly, the reasoning for the choice of most-significant-first over most-significant-last as the arrangement to agree with isn't immediately apparent. To see why this is the only choice that we consider reasonable, we first observe that, according to the definition of the bit indexing scheme given in the corresponding section, as well as the corresponding definition for the bit-flip variant, we view a BuiltinByteString of length nnn as a binary natural number with exactly 8n8n8n digits, and the value at index iii corresponds to the digits whose place value is either 2i2 i2i (for the bit-flip variant), or 28n i 12 {8n - i - 1}28n i 1 (for our chosen method). Put another way, under the specification for the bit-flip variant, the least significant binary digit is first, whereas in our chosen specification, the least significant binary digit is last. CIP-121's conversion primitives mirror this reasoning: the most-significant-first arrangement corresponds to our chosen method, while the most-significant-last arrangement corresponds to the bit-flip variant instead. The difference is the digit value: for us, the digit value is (effectively) 2, while for CIP-121's conversion primitives, it is 256 instead.\nBuiltinByteString\nWe also observe that, when we index a BuiltinByteString's bytes, we get back a BuiltinInteger, whic has a numerical value as a natural number in the range [0,255][0, 255][0,255]. Putting these two observations together, we consider it sensible that, given a non-empty BuiltinByteString, if we were to get the values at bit indexes 000 through 777, then sum their corresponding place values (treating clear bits as 000 and set bits as the appropriate place value), we should get the same result as indexing whichever byte those bits came from.\nBuiltinByteString\nBuiltinInteger\nBuiltinByteString\nConsider the BuiltinByteString whose only byte is 424242, whose representation is as follows:\nBuiltinByteString\n| Byte index | 0 | |------------|----------| | Byte | 00101010 |\n| Byte index | 0 | |------------|----------| | Byte | 00101010 |\nWe note that, if we index this BuiltinByteString at byte position 000, we get back the answer 424242. Furthermore, if we use builtinByteStringToInteger from CIP-121 with such a BuiltinByteString, we get the result 424242 as well, regardless of the endianness argument we choose.\nBuiltinByteString\nbuiltinByteStringToInteger\nBuiltinByteString\nUnder the bit-flip variant, the bit indexes of this BuiltinByteString would be as follows:\nBuiltinByteString\n| Byte index | 0 | |------------|-------------------------------| | Byte | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | |------------|-------------------------------| | Bit index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n| Byte index | 0 | |------------|-------------------------------| | Byte | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | |------------|-------------------------------| | Bit index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\nHowever, we immediately see a problem: under this indexing scheme, the 22=42 2 = 422=4 place value is 111, which would suggest that in the binary representation of 424242, the corresponding digit is also 111. However, this is not the case. Under our scheme of choice however, we get the correct answer:\n| Byte index | 0 | |------------|-------------------------------| | Byte | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | |------------|-------------------------------| | Bit index | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\n| Byte index | 0 | |------------|-------------------------------| | Byte | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | |------------|-------------------------------| | Bit index | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\nHere, the 444 place value is correctly 000. This demonstrates that of the two indexing scheme possibilities that preserve index locality, only one can be consistent with any choice of byte arrangement, whether most-significant-first or most-significant-last: the one we chose. This implies that we cannot be consistent with both arrangements while also preserving index locality.\nLet us now consider a larger example BuiltinByteString:\nBuiltinByteString\n| Byte index | 0 | 1 | |------------|----------|----------| | Byte | 00101010 | 11011011 |\n| Byte index | 0 | 1 | |------------|----------|----------| | Byte | 00101010 | 11011011 |\nThis would produce two different results when converted with builtinByteStringToInteger, depending on the choice of endianness argument:\nbuiltinByteStringToInteger\nFor the most-significant-first arrangement, the result is 42 256+223=1097542 * 256 + 223 = 1097542 256+223=10975.\nFor the most-significant-last arrangement, the result is 223 256+42=57130223 * 256 + 42 = 57130223 256+42=57130.\nThese have the following 'breakdowns' in base-2:\n10975=8096+2048+512+256+32+16+8+4+2+1=213+211+29+28+25+24+23+22+21+2010975 = 8096 + 2048 + 512 + 256 + 32 + 16 + 8 + 4 + 2 + 1 = 2 13 + 2 11 + 2 9 + 2 8 + 2 5 + 2 4 + 2 3 + 2 2 + 2 1 + 2 010975=8096+2048+512+256+32+16+8+4+2+1=213+211+29+28+25+24+23+22+21+20\n57130=32768+16386+4096+2048+1024+512+256+32+8+2=215+214+212+211+210+29+28+25+23+2157130 = 32768 + 16386 + 4096 + 2048 + 1024 + 512 + 256 + 32 + 8 + 2 = 2 15 + 2 14 + 2 12 + 2 11 + 2 10 + 2 9 + 2 8 + 2 5 + 2 3 + 2 157130=32768+16386+4096+2048+1024+512+256+32+8+2=215+214+212+211+210+29+28+25+23+21\nUnder the bit-flip variant, the bit indexes of this BuiltinByteString would be as follows:\nBuiltinByteString\n| Byte index | 0 | 1 | |------------|-------------------------------|-------------------------------------| | Byte | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | |------------|-------------------------------|-------------------------------------| | Bit index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n| Byte index | 0 | 1 | |------------|-------------------------------|-------------------------------------| | Byte | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | |------------|-------------------------------|-------------------------------------| | Bit index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\nWe immediately see a problem, as in this representation, it suggests that the 21=22 1 = 221=2 place value has zero digit value. This is true of neither 109751097510975 nor 571305713057130's base-2 forms, which have the 222 place value with a 111 digit value. This suggests that the bit-flip variant cannot agree with either choice of arrangement in general.\nHowever, if we view the bit indexes using our chosen scheme:\n| Byte index | 0 | 1 | |------------|-------------------------------------|-------------------------------| | Byte | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | |------------|-------------------------------------|-------------------------------| | Bit index | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\n| Byte index | 0 | 1 | |------------|-------------------------------------|-------------------------------| | Byte | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | |------------|-------------------------------------|-------------------------------| | Bit index | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\nthe 222 place value is correctly shown as having a digit value of 1.\nCombining these observations, we note that, assuming we value index locality, choosing our scheme gives us consistency with the most-significant-first arrangement, as well as consistency with byte indexing digit values, but choosing the bit-flip variant gives us neither. As we need both index locality and consistency with at least one arrangement, our choice is the correct one. The fact that we also get byte indexing digit values being consistent is another reason for our choice.\nFor the operations defined in this CIP taking two BuiltinByteString arguments (that is, bitwiseLogicalAnd, bitwiseLogicalOr, and bitwiseLogicalXor), when the two arguments have identical lengths, the semantics are natural, mirroring the corresponding operations on the Boolean algebra 28n\\textbf{2} {8n}28n, where nnn is the length of either argument in bytes. When the arguments do not have matching lengths, however, the situation becomes more complex, as there are several ways in which we could define these operations. The most natural possibilities are as follows; we repeat some of the definitions used in the corresponding section.\nBuiltinByteString\nbitwiseLogicalAnd\nbitwiseLogicalOr\nbitwiseLogicalXor\nExtend the shorter argument with the identity element (all-1s for bitwiseLogicalAnd, all-0s otherwise) to match the length of the longer argument, then perform the operation as if on matching-length arguments. We call this padding semantics.\nbitwiseLogicalAnd\nIgnore the bytes of the longer argument whose indexes would not be valid for the shorter argument, then perform the operation as if on matching-length arguments. We call this truncation semantics.\nFail with an error whenever argument lengths don't match. We call this match semantics.\nFurthermore, for both padding and truncation semantics, we can choose to pad (or truncate) low index bytes or high index bytes. To illustrate the difference, consider the two BuiltinByteStrings (written as arrays of bytes for simplicity) [0xFF, 0x0F, 0x00] and [0x8F, 0x99]. Under padding semantics, padding low index bytes would give us [0x00, 0x8F, 0x99] (or [0xFF, 0x8F, 0x99] depending on operation), while padding high index bytes would give us [0x8F, 0x99, 0x00] (or [0x8F, 0x99, 0xFF] depending on operation). Under truncation semantics, truncating low index bytes would give us [0x0F, 0x00], while truncating high index bytes would give us [0xFF, 0x0F].\nBuiltinByteString\n[0xFF, 0x0F, 0x00]\n[0x8F, 0x99]\n[0x00, 0x8F, 0x99]\n[0xFF, 0x8F, 0x99]\n[0x8F, 0x99, 0x00]\n[0x8F, 0x99, 0xFF]\n[0x0F, 0x00]\n[0xFF, 0x0F]\nIt is not a priori clear which of these we should choose: they are subject to different laws (as evidenced by the corresponding section), none of which are strict supersets of each other (at least not for all inputs possible). While CIP-58 chose match semantics, we believe this was not the correct decision: we use Case 1 to justify the benefit of having other semantics described above available.\nConsider the following operation: given a bound kkk, a 'direction' (larger or smaller), and an integer set, remove all elements indicates by the direction and kkk (that is, either smaller than kkk or larger than kkk, as indicated by the direction). This could be done using a bitwiseLogicalAnd and a mask. However, under match semantics, this mask would have to have a length equal to the integer set representation; under padding semantics, the mask would potentially only need (k)\\Theta(k) (k) length, depending on direction. This is noteworthy, as padding the mask would require an additional copy operation, only to produce a value that would be discarded immediately.\nbitwiseLogicalAnd\nConsider instead the following operation: given two integer sets with different (upper) bounds, take their intersection, producing an integer set whose size is the minimum of the two. This can once again be done using bitwiseLogicalAnd, but under match semantics (or padding semantics for that matter), we would first have to slice the longer argument, while under truncation semantics, we wouldn't need to.\nbitwiseLogicalAnd\nMatch semantics can be useful for Case 1 as well. Consider the case that a representation of an integer set is supplied as an input datum (in its Data encoding). In order to deserialize it, we need to verify at least whether it has the right length in bytes to represent an integer set with a given bound. Under padding or truncation semantics, we would have to check this at deserialization time; under exact match semantics, provided we were sure that at least one argument is of a known size, we could simply perform the necessary operations and let the match semantics error if given something inappropriate.\nData\nIt is also worth noting that truncation semantics are well-established in the Haskell ecosystem. Viewed another way, all of the operations under discussion in this sections are specialized versions of the zipWith operation; Haskell libraries provide this type of operation for a range of linear collections, including lists, Vectors, and mostly notably, ByteStrings. In all of these cases, truncation semantics are what is implemented; it would be surprising to developers coming from Haskell to find that they have to do additional work to replicate them in Plutus. While we don't anticipate direct use of Plutus Core primitives by developers (although this is not an unheard-of case), we should enable library authors to build familiar APIs on top of Plutus Core primitives, which suggests truncation semantics should be available, at least as an option.\nzipWith\nVector\nByteString\nAll the above suggests that no single choice of semantics will satisfy all reasonable needs, if only from the point of view of efficiency. This suggests, much as for CIP-121 primitives and endianness issues, that the primitive should allow a choice in what semantics get used for any given call. Ideally, we would allow a choice of any of the three options described above (along with a choice of low or high index padding or truncation); however, this is awkward to do in Plutus Core. While the choice between two options is straightforward (pass a BuiltinBool), the choice between more than this number would require something like a BuiltinInteger argument with 'designated values' ('0 means match', '1 means low-index padding', etc). This is not ideal, as they involve additional checks, argument redundancy, or both. In light of this, we made the following decisions:\nBuiltinBool\nBuiltinInteger\nWe would choose only two of the three semantics, and have this choice controlled for any given call be controlled by a BuiltinBool flag; and For padding or truncation semantics, we would always use either low or high index padding (or truncation).\nWe would choose only two of the three semantics, and have this choice controlled for any given call be controlled by a BuiltinBool flag; and\nBuiltinBool\nFor padding or truncation semantics, we would always use either low or high index padding (or truncation).\nThis leads naturally to two questions: which of the three semantics above we can afford to exclude, and whether low or high index padding should be chosen. We believe that the correct choices are to exclude match semantics, and to use high index padding and truncation, for several reasons.\nFirstly, we can simulate match semantics with either padding or truncation semantics, together with a length check. While we could also simulate padding semantics via match semantics similarly, the amount of effort (both developer and computational) required is significantly more in that case: a length check is a constant-time operation, while manually padding is linear at best (and even then, it requires operations only this CIP provides, as it would be quadratic otherwise), and on top of that, manual padding is much fiddlier and easier to get wrong.\nSecondly, truncation semantics are common enough in Haskell that we believe excluding them as an option is both surprising and wrong. Any developer familiar with Haskell has interacted with various zipWith operations, and having our primitives behave differently to this at minimum creates friction for implementers of higher-level abstractions atop the primitives in this CIP. While Haskellers are not exclusive users of Plutus primitives (directly or not), there are definitely enough of them that not having truncation semantics available would create a lot of unnecessary friction.\nzipWith\nThirdly, outside of error checking, match semantics give few benefits, performance or otherwise. The examples above demonstrate cases where padding and truncation semantics lead to better performance, less fiddly implementations, or both: finding such a case for match semantics outside of error checking is difficult at best.\nThis combination of reasoning leads us to consider padding and truncation as the two semantics we should retain, and this guided our implementation choices accordingly. With regard to padding (or truncating) low or high indexes, given that we pad (or truncate) whole bytes by necessity, it makes the corresponding operations (effectively) operate over bytes, or rather, they view BuiltinByteStrings as linear collections of bytes, rather than bits. When viewed this way, the zipWith analogy with Haskell suggests that truncating high is the correct choice: truncating low would be quite surprising to a Haskeller familiar with how zipWith-style operations behave. Furthermore, as having padding low and truncating high would be confusing (and arguably quite strange), padding high seems like the correct choice. Thus, we decided to both pad and truncate high in light of this.\nBuiltinByteString\nzipWith\nzipWith\nwriteBits in our description takes a change list argument, allowing changing multiple bits at once. This is an added complexity, and an argument can be made that something similar to the following operation would be sufficient:\nwriteBits\nwriteBit :: BuiltinByteString -> BuiltinInteger -> BuiltinBool -> BuiltinByteString\nwriteBit :: BuiltinByteString -> BuiltinInteger -> BuiltinBool -> BuiltinByteString\nEssentially, writeBit bs i v would be equivalent to writeBits bs [(i, v)] as currently defined. This was the choice made by CIP-58, with the consideration of simplicity in mind.\nwriteBit bs i v\nwriteBits bs [(i, v)]\nAt the same time, due to the immutability semantics of Plutus Core, each time writeBit would be called, we would have to copy its BuiltinByteString argument. Thus, a sequence of kkk setBit calls in a fold over a BuiltinByteString of length nnn would require (nk)\\Theta(nk) (nk) time and (nk)\\Theta(nk) (nk) space. Meanwhile, if we instead used writeBits, the time drops to (n+k)\\Theta(n + k) (n+k) and the space to (n)\\Theta(n) (n), which is a non-trivial improvement. While we cannot avoid the worst-case copying behaviour of setBit (if we have a critical path of read-write dependencies of length kkk, for example), and 'list packing' carries some cost, we have benchmarks that show not only that this 'packing cost' is essentially zero, but that for BuiltinByteStrings of 30 bytes or fewer, copying completely overwhelms the work required to modify the bits specified in the change list argument. This alone is good evidence for having writeBits instead; indeed, there is prior art for doing this in the vector library, for the exact reasons we give here.\nwriteBit\nBuiltinByteString\nsetBit\nBuiltinByteString\nwriteBits\nsetBit\nBuiltinByteString\nwriteBits\nvector\nThe argument could also be made whether this design should be extended to other primitive operations in this CIP which both take BuiltinByteString arguments and also produce BuiltinByteString results. We believe that this is not as justified as in the writeBits case, for several reasons. Firstly, for bitwiseLogicalComplement, it's not clear what benefit this would have at all: the only possible signature such an operation would have is [BuiltinByteString] - [BuiltinByteString], which in effect would be a specialized form of mapping. While an argument could be made for a general form of mapping as a Plutus Core primitive, it wouldn't be reasonable for an operation like this to be considered for such.\nBuiltinByteString\nBuiltinByteString\nwriteBits\nbitwiseLogicalComplement\n[BuiltinByteString] -> [BuiltinByteString]\nSecondly, the performance benefits of such an operation aren't nearly as significant in theory, and likely wouldn't be in practice either. Consider this hypothetical operation (with fold semantics):\nbitwiseLogicalXors :: BuiltinBool -> [BuiltinByteString] -> BuiltinByteString\nbitwiseLogicalXors :: BuiltinBool -> [BuiltinByteString] -> BuiltinByteString\nSimulating this operation as a fold using bitwiseLogicalXor, in the worst case, irrespective of padding or truncation semantics, requires (nk)\\Theta(nk) (nk) time and space, where nnn is the size of each BuiltinByteString in the argument list, and kkk is the length of the argument list itself. Using bitwiseLogicalXors instead would reduce the space required to (n)\\Theta(n) (n), but would not affect the time complexity at all.\nbitwiseLogicalXor\nBuiltinByteString\nbitwiseLogicalXors\nLastly, it is questionable whether 'bulk' operations like bitwiseLogicalXors above would see as much use as writeBits. In the context of Case 1, bitwiseLogicalXors corresponds to taking the symmetric difference of multiple integer sets; it seems unlikely that the number of sets we'd want to do this with would frequently be higher than 2. However, in the same context, writeBits corresponds to constructing an integer set given a list of members (or, for that matter, non-members): this is an operation that is both required by the case description, and also much more likely to be used often.\nbitwiseLogicalXors\nwriteBits\nbitwiseLogicalXors\nwriteBits\nOn the basis of the above, we believe that choosing to implement writeBits as a 'bulk' operation, but to leave others as 'singular' is the right choice.\nwriteBits\nWe consider the following criteria to be essential for acceptance:\nA proof-of-concept implementation of the operations specified in this document, outside of the Plutus source tree. The implementation must be in GHC Haskell, without relying on the FFI.\nThe proof-of-concept implementation must have tests, demonstrating that it behaves as the specification requires.\nThe proof-of-concept implementation must demonstrate that it will successfully build, and pass its tests, using all GHC versions currently usable to build Plutus (8.10, 9.2 and 9.6 at the time of writing), across all Tier 1 platforms.\nIdeally, the implementation should also demonstrate its performance characteristics by well-designed benchmarks.\nMLabs has begun the implementation of the proof-of-concept as required in the acceptance criteria. Upon completion, we will send a pull request to Plutus with the implementation of the primitives for Plutus Core, mirroring the proof-of-concept.\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0123 | Bitwise operations over BuiltinByteString\n\nWe describe the semantics of a set of bitwise operations for Plutus BuiltinByteStrings. Specifically, we provide descriptions for:\nBuiltinByteString\nBit shifts and rotations\nCounting the number of set bits (popcount)\npopcount\nFinding the first set bit\nWe base our work on similar operations described in CIP-58, but use the bit indexing scheme from the logical operations cip for the semantics. This is intended as follow-on work from both of these.\nBitwise operations, both over fixed-width and variable-width blocks of bits, have a range of uses. Indeed, we have already proposed CIP-122, with some example cases, and a range of primitive operations on BuiltinByteStrings designed to allow bitwise operations in the service of those example cases, as well as many others. These operations form a core of functionality, which is important and necessary, but not complete. We believe that the operations we describe in this CIP form a useful 'completion' of the work in CIP-122, based on similar work done in the earlier CIP-58.\nBuiltinByteString\nTo demonstrate why our proposed operations are useful, we re-use the cases provided in the CIP-122, and show why the operations we describe would be beneficial.\nFor integer sets, the previous description lacks two important, and useful, operations:\nGiven an integer set, return its cardinality; and\nGiven an integer set, return its minimal member (or specify it is empty).\nThese operations have a range of uses. The first corresponds to the notion of Hamming weight, which can be used for operations ranging from representing boards in chess games to exponentiation by squaring to succinct data structures. Together with bitwise XOR, it can also compute the Hamming distance. The second operation also has a range of uses, ranging from succinct priority queues to integer normalization. It is also useful for rank-select dictionaries, a succinct structure that can act as the basis of a range of others, such as dictionaries, multisets and trees of different arity.\nIn all of the above, these operations need to be implemented efficiently to be useful. While we could use only bit reading to perform all of these, it is extremely inefficient: given an input of length nnn, assuming that any bit distribution is equally probable, we need (8 n)\\Theta(8 \\cdot n) (8 n) time in the average case. While it is impossible to do both of these operations in sub-linear time in general, the large constant factors this imposes (as well as the overhead of looping over bit indexes) is a cost we can ill afford on-chain, especially if the goal is to use these operations as 'building blocks' for something like a data structure.\nIn our previously-described case, we stated what operations we would need for the Argon2 family of hashes specifically. However, Argon2 has a specific advantage in that the number of operations it requires are both relatively few, and the most complex of which (BLAKE2b hashing) already exists in Plutus Core as a primitive. However, other hash functions (and indeed, many other cryptographic primitives) rely on two other important instructions: bit shifts and bit rotations. As an example, consider SHA512, which is an important component in several cryptographic protocols (including Ed25519 signature verification): its implementation requires both shifts and rotations to work.\nLike with Case 1, we can theoretically simulate both rotations and shifts using a combination of bit reads and bit writes to an empty BuiltinByteString. However, the cost of this is extreme: we would need to produce a list of index-value pairs of length equal to the Hamming weight of the input, only to then immediately discard it! To put this into some perspective, for an 8-byte input, performing a rotation involves allocating an expected 32 index-value pairs, using significantly more memory than the result. On-chain, we can't really afford this cost, especially in an operation intended to be used as part of larger constructions (as would be necessary here).\nBuiltinByteString\nWe describe the proposed operations in several stages. First, we give an overview of the proposed operations' signatures and costings; second, we describe the semantics of each proposed operation in detail, as well as some examples. Lastly, we provide laws that any implementation of the proposed operations should obey.\nThroughout, we make use of the bit indexing scheme described in a CIP-122. We also re-use the notation x[i]x[i]x[i] to refer to the value of at bit index iii of xxx, and the notation xix\\\\{i\\\\}xi to refer to the byte at byte index iii of xxx: both are specified in CIP-122.\nOur proposed operations will have the following signatures:\nbitwiseShift :: BuiltinByteString - BuiltinInteger - BuiltinByteString\nbitwiseShift :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString\nbitwiseRotate :: BuiltinByteString - BuiltinInteger - BuiltinByteString\nbitwiseRotate :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString\ncountSetBits :: BuiltinByteString - BuiltinInteger\ncountSetBits :: BuiltinByteString -> BuiltinInteger\nfindFirstSetBit :: BuiltinByteString - BuiltinInteger\nfindFirstSetBit :: BuiltinByteString -> BuiltinInteger\nWe assume the following costing, for both memory and execution time:\nbitwiseShift\nBuiltinByteString\nbitwiseRotate\nBuiltinByteString\ncountSetBits\nfindFirstSetBit\nbitwiseShift\nbitwiseShift takes two arguments; we name and describe them below.\nbitwiseShift\nThe BuiltinByteString to be shifted. This is the data argument. The shift, whose sign indicates direction and whose magnitude indicates the size of the shift. This is the shift argument, and has type BuiltinInteger.\nThe BuiltinByteString to be shifted. This is the data argument.\nBuiltinByteString\nThe shift, whose sign indicates direction and whose magnitude indicates the size of the shift. This is the shift argument, and has type BuiltinInteger.\nBuiltinInteger\nLet bbb refer to the data argument, whose length in bytes is nnn, and let iii refer to the shift argument. Let the result of bitwiseShift called with bbb and iii be brb_rbr , also of length nnn.\nbitwiseShift\nFor all j 0,1, 8 n 1j \\in 0, 1, \\ldots 8 \\cdot n - 1j 0,1, 8 n 1, we have\nSome examples of the intended behaviour of bitwiseShift follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\nbitwiseShift\nBuiltinByteString\n-- Shifting the empty bytestring does nothing bitwiseShift [] 3 => [] -- Regardless of direction bitwiseShift [] (-3) => [] -- Positive shifts move bits to higher indexes, cutting off high indexes and -- filling low ones with zeroes bitwiseShift [0xEB, 0xFC] 5 => [0x7F, 0x80] -- Negative shifts move bits to lower indexes, cutting off low indexes and -- filling high ones with zeroes bitwiseShift [0xEB, 0xFC] (-5) => [0x07, 0x5F] -- Shifting by the total number of bits or more clears all bytes bitwiseShift [0xEB, 0xFC] 16 => [0x00, 0x00] -- Regardless of direction bitwiseShift [0xEB, 0xFC] (-16) => [0x00, 0x00]\n-- Shifting the empty bytestring does nothing bitwiseShift [] 3 => [] -- Regardless of direction bitwiseShift [] (-3) => [] -- Positive shifts move bits to higher indexes, cutting off high indexes and -- filling low ones with zeroes bitwiseShift [0xEB, 0xFC] 5 => [0x7F, 0x80] -- Negative shifts move bits to lower indexes, cutting off low indexes and -- filling high ones with zeroes bitwiseShift [0xEB, 0xFC] (-5) => [0x07, 0x5F] -- Shifting by the total number of bits or more clears all bytes bitwiseShift [0xEB, 0xFC] 16 => [0x00, 0x00] -- Regardless of direction bitwiseShift [0xEB, 0xFC] (-16) => [0x00, 0x00]\nbitwiseRotate\nbitwiseRotate takes two arguments; we name and describe them below.\nbitwiseRotate\nThe BuiltinByteString to be rotated. This is the data argument. The rotation, whose sign indicates direction and whose magnitude indicates the size of the rotation. This is the rotation argument, and has type BuiltinInteger.\nThe BuiltinByteString to be rotated. This is the data argument.\nBuiltinByteString\nThe rotation, whose sign indicates direction and whose magnitude indicates the size of the rotation. This is the rotation argument, and has type BuiltinInteger.\nBuiltinInteger\nLet bbb refer to the data argument, whose length in bytes is nnn, and let iii refer to the rotation argument. Let the result of bitwiseRotate called with bbb and iii be brb_rbr , also of length nnn.\nbitwiseRotate\nFor all j 0,1, 8 n 1j \\in 0, 1, \\ldots 8 \\cdot n - 1j 0,1, 8 n 1, we have br=b[(j i) mod (8 n)]b_r = b[(j - i) \\text{ } \\mathrm{mod} \\text { } (8 \\cdot n)]br =b[(j i) mod (8 n)].\nSome examples of the intended behaviour of bitwiseRotate follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\nbitwiseRotate\nBuiltinByteString\n-- Rotating the empty bytestring does nothing bitwiseRotate [] 3 => [] -- Regardless of direction bitwiseRotate [] (-1) => [] -- Positive rotations move bits to higher indexes, 'wrapping around' for high -- indexes into low indexes bitwiseRotate [0xEB, 0xFC] 5 => [0x7F, 0x9D] -- Negative rotations move bits to lower indexes, 'wrapping around' for low -- indexes into high indexes bitwiseRotate [0xEB, 0xFC] (-5) => [0xE7, 0x5F] -- Rotation by the total number of bits does nothing bitwiseRotate [0xEB, 0xFC] 16 => [0xEB, 0xFC] -- Regardless of direction bitwiseRotate [0xEB, 0xFC] (-16) => [0xEB, 0xFC] -- Rotation by more than the total number of bits is the same as the remainder -- after division by number of bits bitwiseRotate [0xEB, 0xFC] 21 =>[0x7F, 0x9D] -- Regardless of direction, preserving sign bitwiseRotate [0xEB, 0xFC] (-21) => [0xE7, 0x5F]\n-- Rotating the empty bytestring does nothing bitwiseRotate [] 3 => [] -- Regardless of direction bitwiseRotate [] (-1) => [] -- Positive rotations move bits to higher indexes, 'wrapping around' for high -- indexes into low indexes bitwiseRotate [0xEB, 0xFC] 5 => [0x7F, 0x9D] -- Negative rotations move bits to lower indexes, 'wrapping around' for low -- indexes into high indexes bitwiseRotate [0xEB, 0xFC] (-5) => [0xE7, 0x5F] -- Rotation by the total number of bits does nothing bitwiseRotate [0xEB, 0xFC] 16 => [0xEB, 0xFC] -- Regardless of direction bitwiseRotate [0xEB, 0xFC] (-16) => [0xEB, 0xFC] -- Rotation by more than the total number of bits is the same as the remainder -- after division by number of bits bitwiseRotate [0xEB, 0xFC] 21 =>[0x7F, 0x9D] -- Regardless of direction, preserving sign bitwiseRotate [0xEB, 0xFC] (-21) => [0xE7, 0x5F]\ncountSetBits\nLet bbb refer to countSetBits' only argument, whose length in bytes is nnn, and let rrr be the result of calling countSetBits on bbb. Then we have\ncountSetBits\ncountSetBits\nSome examples of the intended behaviour of countSetBits follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\ncountSetBits\nBuiltinByteString\n-- The empty bytestring has no set bits countSetBits [] => 0 -- Bytestrings with only zero bytes have no set bits countSetBits [0x00, 0x00] => 0 -- Set bits are counted regardless of where they are countSetBits [0x01, 0x00] => 1 countSetBits [0x00, 0x01] => 1\n-- The empty bytestring has no set bits countSetBits [] => 0 -- Bytestrings with only zero bytes have no set bits countSetBits [0x00, 0x00] => 0 -- Set bits are counted regardless of where they are countSetBits [0x01, 0x00] => 1 countSetBits [0x00, 0x01] => 1\nfindFirstSetBit\nLet bbb refer to findFirstSetBit's only argument, whose length in bytes is nnn, and let rrr be the result of calling findFirstSetBit on bbb. Then we have the following:\nfindFirstSetBit\nfindFirstSetBit\nr 1,0,1, ,8 n 1r \\in -1, 0, 1, \\ldots, 8 \\cdot n - 1r 1,0,1, ,8 n 1 If for all i 0,1, n 1i \\in 0, 1, \\ldots n - 1i 0,1, n 1, bi=0x00b\\\\{i\\\\} = \\texttt{0x00}bi=0x00, then r= 1r = -1r= 1; otherwise, r 1r -1r 1. If r 1r -1r 1, then b[r]=1b[r] = 1b[r]=1, and for all i 0,1, ,r 1i \\in 0, 1, \\ldots, r - 1i 0,1, ,r 1, b[i]=0b[i] = 0b[i]=0.\nr 1,0,1, ,8 n 1r \\in -1, 0, 1, \\ldots, 8 \\cdot n - 1r 1,0,1, ,8 n 1\nIf for all i 0,1, n 1i \\in 0, 1, \\ldots n - 1i 0,1, n 1, bi=0x00b\\\\{i\\\\} = \\texttt{0x00}bi=0x00, then r= 1r = -1r= 1; otherwise, r 1r -1r 1.\nIf r 1r -1r 1, then b[r]=1b[r] = 1b[r]=1, and for all i 0,1, ,r 1i \\in 0, 1, \\ldots, r - 1i 0,1, ,r 1, b[i]=0b[i] = 0b[i]=0.\nSome examples of the intended behaviour of findFirstSetBit follow. For brevity, we write BuiltinByteString literals as lists of hexadecimal values.\nfindFirstSetBit\nBuiltinByteString\n-- The empty bytestring has no first set bit findFirstSetBit [] => -1 -- Bytestrings with only zero bytes have no first set bit findFirstSetBit [0x00, 0x00] => -1 -- Only the first set bit matters, regardless what comes after it findFirstSetBit [0x00, 0x02] => 1 findFirstSetBit [0xFF, 0xF2] => 1\n-- The empty bytestring has no first set bit findFirstSetBit [] => -1 -- Bytestrings with only zero bytes have no first set bit findFirstSetBit [0x00, 0x00] => -1 -- Only the first set bit matters, regardless what comes after it findFirstSetBit [0x00, 0x02] => 1 findFirstSetBit [0xFF, 0xF2] => 1\nThroughout, we use bitLen bs to indicate the number of bits in bs; that is, sizeOfByteString bs * 8. We also make reference to logical operations from a previous CIP as part of specifying these laws.\nbitLen bs\nbs\nsizeOfByteString bs * 8\nWe describe the laws for bitwiseShift and bitwiseRotate together, as they are similar. Firstly, we observe that bitwiseShift and bitwiseRotate both form a monoid homomorphism between natural number addition and function composition:\nbitwiseShift\nbitwiseRotate\nbitwiseShift\nbitwiseRotate\nbitwiseShift bs 0 = bitwiseRotate bs 0 = bs bitwiseShift bs (i + j) = bitwiseShift (bitwiseShift bs i) j bitwiseRotate bs (i + j) = bitwiseRotate (bitwiseRotate bs i) j\nbitwiseShift bs 0 = bitwiseRotate bs 0 = bs bitwiseShift bs (i + j) = bitwiseShift (bitwiseShift bs i) j bitwiseRotate bs (i + j) = bitwiseRotate (bitwiseRotate bs i) j\nHowever, bitwiseRotate's homomorphism is between integer addition and function composition: namely, i and j in the above law are allowed to have different signs. bitwiseShift's composition law only holds if i and j don't have opposite signs: that is, if they're either both non-negative or both non-positive.\nbitwiseRotate\ni\nj\nbitwiseShift\ni\nj\nShifts by more than the number of bits in the data argument produce an empty BuiltinByteString:\nBuiltinByteString\n-- n is non-negative bitwiseShift bs (bitLen bs + n) = bitwiseShift bs (- (bitLen bs + n)) = replicateByteString (sizeOfByteString bs) 0x00\n-- n is non-negative bitwiseShift bs (bitLen bs + n) = bitwiseShift bs (- (bitLen bs + n)) = replicateByteString (sizeOfByteString bs) 0x00\nRotations, on the other hand, exhibit 'modular roll-over':\n-- n is non-negative bitwiseRotate bs (binLen bs + n) = bitwiseRotate bs n bitwiseRotate bs (- (bitLen bs + n)) = bitwiseRotate bs (- n)\n-- n is non-negative bitwiseRotate bs (binLen bs + n) = bitwiseRotate bs n bitwiseRotate bs (- (bitLen bs + n)) = bitwiseRotate bs (- n)\nShifts clear bits at low indexes if the shift argument is positive, and at high indexes if the shift argument is negative:\n-- 0 < n < bitLen bs, and 0 <= i < n readBit (bitwiseShift bs n) i = False readBit (bitwiseShift bs (- n)) (bitLen bs - i - 1) = False\n-- 0 < n < bitLen bs, and 0 <= i < n readBit (bitwiseShift bs n) i = False readBit (bitwiseShift bs (- n)) (bitLen bs - i - 1) = False\nRotations instead preserve all set and clear bits, but move them around:\n-- 0 <= i < bitLen bs readBit bs i = readBit (bitwiseRotate bs j) (modInteger (i + j) (bitLen bs))\n-- 0 <= i < bitLen bs readBit bs i = readBit (bitwiseRotate bs j) (modInteger (i + j) (bitLen bs))\ncountSetBits\ncountSetBits forms a monoid homomorphism between BuiltinByteString concatenation and natural number addition:\ncountSetBits\nBuiltinByteString\ncountSetBits \"\" = 0 countSetBits (x <> y) = countSetBits x + countSetBits y\ncountSetBits \"\" = 0 countSetBits (x <> y) = countSetBits x + countSetBits y\ncountSetBits also demonstrates that bitwiseRotate indeed preserves the number of set (and thus clear) bits:\ncountSetBits\nbitwiseRotate\ncountSetBits bs = countSetBits (bitwiseRotate bs i)\ncountSetBits bs = countSetBits (bitwiseRotate bs i)\nThere is also a relationship between the result of countSetBits on a given argument and its complement:\ncountSetBits\ncountSetBits bs = bitLen bs - countSetBits (bitwiseLogicalComplement bs)\ncountSetBits bs = bitLen bs - countSetBits (bitwiseLogicalComplement bs)\nFurthermore, countSetBits exhibits (or more precisely, gives evidence for) the inclusion-exclusion principle from combinatorics, but only under truncation semantics:\ncountSetBits\ncountSetBits (bitwiseLogicalXor False x y) = countSetBits (bitwiseLogicalOr False x y) - countSetBits (bitwiseLogicalAnd False x y)\ncountSetBits (bitwiseLogicalXor False x y) = countSetBits (bitwiseLogicalOr False x y) - countSetBits (bitwiseLogicalAnd False x y)\nLastly, countSetBits has a relationship to bitwise XOR, regardless of semantics:\ncountSetBits\ncountSetBits (bitwiseLogicalXor semantics x x) = 0\ncountSetBits (bitwiseLogicalXor semantics x x) = 0\nfindFirstSetBit\nBuiltinByteStrings where every byte is the same (provided they are not empty) have the same first bit as a singleton of that same byte:\nBuiltinByteString\n-- 0 <= w8 <= 255, n >= 1 findFirstSetBit (replicateByteString n w8) = findFirstSetBit (replicateByteString 1 w8)\n-- 0 <= w8 <= 255, n >= 1 findFirstSetBit (replicateByteString n w8) = findFirstSetBit (replicateByteString 1 w8)\nAdditionally, findFirstSet has a relationship to bitwise XOR, regardless of semantics:\nfindFirstSet\nfindFirstSetBit (bitwiseLogicalXor semantics x x) = -1\nfindFirstSetBit (bitwiseLogicalXor semantics x x) = -1\nAny result of a findFirstSetBit operation that isn't -1 gives a valid bit index to a set bit, but any non-negative BuiltinInteger less than this will give an index to a clear bit:\nfindFirstSetBit\n-1\nBuiltinInteger\n-- bs is not all zero bytes or empty readBit bs (findFirstSetBit bs) = True -- 0 <= i < findFirstSet bs readBit bs i = False\n-- bs is not all zero bytes or empty readBit bs (findFirstSetBit bs) = True -- 0 <= i < findFirstSet bs readBit bs i = False\nOur four operations, along with their semantics, fulfil the requirements of both our cases, and are law-abiding, familiar, consistent and straightforward to implement. Furthermore, they relate directly to operations provided by CIP-122, as well as being identical to the equivalent operations in CIP-58. At the same time, some alternative choices could have been made:\nNot implementing these operations at all, instead requiring higher-level APIs to implement them atop CIP-122 primitives;\nProviding only some of these four operations;\nHaving findFirstSetBit return the bit length for an all-zero argument, instead of -1;\nfindFirstSetBit\n-1\nIncluding a way of finding the last set bit as well.\nWe discuss our choices with regards to all the above, and specify why we made the choices that we did.\nWhile we chose to provide all of these operations as primitives, we could instead have required higher-level APIs to provide these on the basis of CIP-122 bit reads and writes. This is indeed possible:\nbitwiseShift and bitwiseRotate is a loop over all bits in the data argument, which is used to construct a change list that sets appropriate bits (with the right offset), which then gets applied to a BuiltinByteString of the same length as the data argument, but full of zero bytes.\nbitwiseShift\nbitwiseRotate\nBuiltinByteString\ncountSetBits is a fold over all bit indexes, incrementing an accumulator every time a set bit is found.\ncountSetBits\nfindFirstSetBit is a fold over all bit indexes, returning the first index with a set bit, or (-1) if none can be found.\nfindFirstSetBit\nHowever, especially for bitwiseShift and bitwiseRotate, this comes at significant cost. For shifts and rotations, we have to construct a change list argument whose length is proportional to the Hamming weight of the data argument. Assuming that all inputs are equally likely, this is proportional to the bit length of the data argument: such a change list is likely significantly larger than the result of the shift or rotation, making the memory cost of such operations far higher than it needs to be. Given the constraints on memory and execution time on-chain, at minimum, these two operations would have to be primitives to make them viable at all: Case 2-style implementations would have intolerably large memory costs otherwise, as such algorithms often make heavy use of both shifts and rotations.\nbitwiseShift\nbitwiseRotate\nThe case for countSetBits and findFirstSetBit is less clear here, as they wouldn't require nearly as much of a memory cost if implemented atop CIP-122 primitives using folds. However, the cost would still be significant: countSetBits requires looping over every bit index in the argument, and findFirstSetBit (again, assuming any bit distribution in an argument is equally probable) requires looping over about half this many. This would make these operations unreasonable even over smaller inputs, which would make applications like rank-select dictionaries (which expect these operations to be fast and low-cost) unworkable. As succinct data structures are foremost in our minds when considering the current primitives, we believe it is important that countSetBits and findFirstSetBit are as efficient as they could be, hence their inclusion.\ncountSetBits\nfindFirstSetBit\ncountSetBits\nfindFirstSetBit\ncountSetBits\nfindFirstSetBit\nWhen designing our operations, we tried to keep them familiar to Haskellers, namely by having them behave like the similar operations from the Bits and FiniteBits type classes. In particular, bitwiseShift and bitwiseRotate act similarly given the same shift (or rotation) argument, as positive values shift left and negative ones shift right. The only exception is the choice for findFirstSetBit to return -1 when no set bits are found, which runs counter to the way countTrailingZeros from FiniteBits works, which instead returns the bit length. This is also different to how this operations works in hardware. Indeed, having findFirstSetBit work this way would not only be more familiar, it would also provide additional laws:\nBits\nFiniteBits\nbitwiseShift\nbitwiseRotate\nfindFirstSetBit\n-1\ncountTrailingZeros\nFiniteBits\nfindFirstSetBit\n-- Not possible under our current definition -- bitLen is the bit length of the argument 0 <= popcount bs <= bitLen bs - findFirstSet bs\n-- Not possible under our current definition -- bitLen is the bit length of the argument 0 <= popcount bs <= bitLen bs - findFirstSet bs\nUnder both definitions, the intent is the same: if the argument doesn't contain any set bits, produce an invalid index. The difference is that we choose to produce a negative invalid index, whereas the consensus is to produce a non-negative invalid index instead. However, one task that is likely to come up frequently when using findFirstSetBit is checking whether the index we were given was valid or not (essentially, whether the argument has any nonzero bytes in it). Under our definition, all that would be required is\nfindFirstSetBit\nlet found = findFirstSetBit bs in if found < 0 then weMissed else validIndex\nlet found = findFirstSetBit bs in if found < 0 then weMissed else validIndex\nThis is a cheap operation in Plutus Core, requiring only comparing against a constant. However, if we used the more widely-used definition, we would instead have to do this:\nlet found = findFirstSetBit bs bitLen = 8 * sizeOfByteString bs in if found >= bitLen then weMissed else validIndex\nlet found = findFirstSetBit bs bitLen = 8 * sizeOfByteString bs in if found >= bitLen then weMissed else validIndex\nThis requires us to do considerably more work (finding the length of the argument, multiplying by 8, then compare against that result), and is also much more prone to error: users have to remember to use a = comparison, as well as to multiply the argument length by 8. This is less of an issue with implementations of this operation in other languages, as their equivalent operations are designed for fixed-width arguments (indeed, FiniteBits requires this), which makes their bit length a constant. In our case, this isn't as simple, as BuiltinByteStrings have variable length, which would make the cost described above unavoidable. Our solution is both more efficient and less error-prone: all a user needs to remember is that invalid indexes from findFirstSetBit are negative. On this basis, we decided to vary from conventional approaches.\n>=\nFiniteBits\nBuiltinByteString\nfindFirstSetBit\nOne notable omission from our operators is the equivalent of counting leading zeroes: namely, an operation that would find the last set bit. Typically, both a count of leading and trailing zeroes is provided in both hardware and software: this is the case for FiniteBits, as well as most hardware implementations. To relate this to our cases, specifically Case 1, this would allow us to efficiently find the largest element in an integer set. The reason we omit this operation is because, when compared with counting trailing zeroes, it is far less useful: while it can be used for computing fast integer square roots, it lacks many other uses. Counting trailing zeroes, on the other hand, is essential for rank-select dictionaries (specifically for the select operation), and also enables a range of other uses, which we have already mentioned. In order to limit the number of new primitives, we decided that counting leading zeroes can be omitted for now. However, our design doesn't preclude such an operation from being added later if a use case for it is found to be useful.\nFiniteBits\nselect\nWe consider the following criteria to be essential for acceptance:\nA proof-of-concept implementation of the operations specified in this document, outside of the Plutus source tree. The implementation must be in GHC Haskell, without relying on the FFI.\nThe proof-of-concept implementation must have tests, demonstrating that it behaves as the specification requires.\nThe proof-of-concept implementation must demonstrate that it will successfully build, and pass its tests, using all GHC versions currently usable to build Plutus (8.10, 9.2 and 9.6 at the time of writing), across all Tier 1 platforms.\nIdeally, the implementation should also demonstrate its performance characteristics by well-designed benchmarks.\nMLabs has begun the implementation of the proof-of-concept as required in the acceptance criteria. Upon completion, we will send a pull request to Plutus with the implementation of the primitives for Plutus Core, mirroring the proof-of-concept.\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0124 | Extend token metadata for translations\n\nThis proposal defines an additional property to the metadata standard for tokens (NFTs and FTs) to support text localization.\nCurrent token metadata only supports a single hardcoded language (mostly English), which limits the accessibility to a certain culture. To get closer to mass adoption, we need to bring down language barriers by extending the current standard to support translations. This is especially relevant for games, metaverse solutions, and RealFi use cases of NFTs.\nThis proposal follows the same specifications as CIP-0025 (all versions).\nThe name of a culture consists of its [ISO-639] language code with small letters and its [ISO-3166] country/region code with capital letter separated by a dash \"-\". For instance, this proposal was written in \"en-US\": English with the US culture.\nThis convention is compatible with most operative systems (Linux and Windows) and widely used translation software.\nThe extended JSON metadata standard (CIP-25) allows flexible off- and on-chain string tranlations:\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array>, \"mediaType\": image/<mime_sub_type>, \"description\": <string | array>, \"files\": [{ \"name\": <string>, \"mediaType\": <mime_type>, \"src\": \"<uri | array>\" }], <other properties> \"strings\": { \"de-CH\": { \"name\": <string in Swiss German>, \"image\": <localized uri for Swiss German | array>, \"description\": <string in Swiss German | array> <other localized properties> }, \"it-IT\": { \"name\": <string in Italian>, \"image\": <localized uri for Italian | array>, \"description\": <string in Italian | array> <other localized properties> }, <other languages and cultures> } }, }, \"version\": <version_id>, <information about collection> \"strings\": { \"de-CH\": { <localized information about collection in de-CH> }, \"it-IT\": { <localized information about collection in it-IT> }, <other languages and cultures> } } }\n{ \"721\": { \"<policy_id>\": { \"<asset_name>\": { \"name\": <string>, \"image\": <uri | array>, \"mediaType\": image/<mime_sub_type>, \"description\": <string | array>, \"files\": [{ \"name\": <string>, \"mediaType\": <mime_type>, \"src\": \"<uri | array>\" }], <other properties> \"strings\": { \"de-CH\": { \"name\": <string in Swiss German>, \"image\": <localized uri for Swiss German | array>, \"description\": <string in Swiss German | array> <other localized properties> }, \"it-IT\": { \"name\": <string in Italian>, \"image\": <localized uri for Italian | array>, \"description\": <string in Italian | array> <other localized properties> }, <other languages and cultures> } }, }, \"version\": <version_id>, <information about collection> \"strings\": { \"de-CH\": { <localized information about collection in de-CH> }, \"it-IT\": { <localized information about collection in it-IT> }, <other languages and cultures> } } }\nExtended versions of CIP-25\nVersion 1\nVersion 2\nThe new strings properties are optional, but if included, they must be valid JSON objects. The JSON object's keys and structure should match the same keys defined on the policy_id level for collection-specific information, or on the asset_name level for asset-specific properties, depending which attributes have translations. By doing so, the developer experience to access the localized strings significantly improves.\nstrings\nThe JSON object's keys and structure should match the same keys defined on the policy_id level for collection-specific information, or on the asset_name level for asset-specific properties, depending which attributes have translations. By doing so, the developer experience to access the localized strings significantly improves.\npolicy_id\nasset_name\nAfter fetching the policy's metadata, the procedure stays the same when looking up CIP-25 properties. However, to find culture-based translations, developers have to access the strings property located at the same level of the wanted information.\nstrings\nIn case of the policy_id level (collections):\nLookup the 721 key Lookup the Policy Id of the token Lookup the strings property Lookup for the culture You end up with the translated metadata for the policy\nLookup the 721 key\nLookup the Policy Id of the token\nLookup the strings property\nLookup for the culture\nYou end up with the translated metadata for the policy\nIn case of the asset_name level (specific assets):\nLookup the 721 key Lookup the Policy Id of the token Lookup the Asset name of the token Lookup the strings property Lookup for the culture You end up with the translated metadata for the specific asset\nLookup the 721 key\nLookup the Policy Id of the token\nLookup the Asset name of the token\nLookup the strings property\nLookup for the culture\nYou end up with the translated metadata for the specific asset\nNote The metadata's size is a constraint that should be considered as it could eventually push the boundaries of Cardano's transaction limits and scalability in terms of memory resources. The translations under the \"strings\" keys can be stored off-chain on an IPFS server using the proposed JSON structure. Then, the localized texts will be accessible through an URL, similarly to the \"image\" property.\nTo access the localized strings from the fetched metadata for a native asset, we can simply access the JSON properties from the front end by using the user's selected culture:\nconst response = await fetch(`${<BASE_URL>}/policyId/${<policyId>}`); const metadata = response.json(); const policy = metadata[\"721\"][<policy_id>]; // This check determines if the translations are stored off-chain on an IPFS url function isValidURL(url) { try { new URL(url); return true; } catch (e) { return false; } } function fetchTranslationStrings(url) { const response = await fetch(url); const translations = await response.json(); return translations || null; } function getPolicyString(policy, key, culture=\"en\") { // translations are stored off-chain if(isValidURL(policy[\"strings\"])) { const translations = fetchTranslationStrings(policy[\"strings\"]); if(translations) { return translations[culture][key] || policy[key]; // default value (not localized) } } // translations are stored on-chain return policy[\"strings\"][culture][key] ? policy[\"strings\"][culture][key] : policy[key]; // default value (not localized) } function getAssetString(policy, asset, key, culture=\"en\") { // translations are stored off-chain if(isValidURL(policy[asset][\"strings\"])) { const translations = fetchTranslationStrings(policy[asset][\"strings\"]); if(translations) { return translations[culture][key] || policy[asset][key]; // default value (not localized) } } // translations are stored on-chain return policy[asset][\"strings\"][culture][key] ? policy[asset][\"strings\"][culture][key] : policy[asset][key]; // default value (not localized) } console.log(`Default policy property: ${getPolicyString(policy, <policy_property>)}`); console.log(`Localized policy property: ${getPolicyString(policy, <policy_property>, \"it-IT\")}`); console.log(`Default asset name: ${getAssetString(policy, <asset_name>, \"name\")}`); console.log(`Localized asset name: ${getAssetString(policy, <asset_name>, \"name\", \"de-CH\")}`);\nconst response = await fetch(`${<BASE_URL>}/policyId/${<policyId>}`); const metadata = response.json(); const policy = metadata[\"721\"][<policy_id>]; // This check determines if the translations are stored off-chain on an IPFS url function isValidURL(url) { try { new URL(url); return true; } catch (e) { return false; } } function fetchTranslationStrings(url) { const response = await fetch(url); const translations = await response.json(); return translations || null; } function getPolicyString(policy, key, culture=\"en\") { // translations are stored off-chain if(isValidURL(policy[\"strings\"])) { const translations = fetchTranslationStrings(policy[\"strings\"]); if(translations) { return translations[culture][key] || policy[key]; // default value (not localized) } } // translations are stored on-chain return policy[\"strings\"][culture][key] ? policy[\"strings\"][culture][key] : policy[key]; // default value (not localized) } function getAssetString(policy, asset, key, culture=\"en\") { // translations are stored off-chain if(isValidURL(policy[asset][\"strings\"])) { const translations = fetchTranslationStrings(policy[asset][\"strings\"]); if(translations) { return translations[culture][key] || policy[asset][key]; // default value (not localized) } } // translations are stored on-chain return policy[asset][\"strings\"][culture][key] ? policy[asset][\"strings\"][culture][key] : policy[asset][key]; // default value (not localized) } console.log(`Default policy property: ${getPolicyString(policy, <policy_property>)}`); console.log(`Localized policy property: ${getPolicyString(policy, <policy_property>, \"it-IT\")}`); console.log(`Default asset name: ${getAssetString(policy, <asset_name>, \"name\")}`); console.log(`Localized asset name: ${getAssetString(policy, <asset_name>, \"name\", \"de-CH\")}`);\nFollowing the specifications stated on CIP-25, the strings can be only changed if the policy allows it.\nThis metadata standard extension is backward compatible and it doesn't affect applications using the current standard. Dapps implementing the proposed extended standard can also default on the legacy values if the localized strings are not available on an asset.\nImplementation is compliant with JSON conventions.\nImplementation is compliant with the [ISO-639] standard for language code, and the [ISO-3166] standard for country/region code.\nImplementations and peer review verify that: NFT metadata standard extension covers all existing localization needs and use cases on web2. Access to localized strings is easy and logical from a coding perspective.\nNFT metadata standard extension covers all existing localization needs and use cases on web2.\nAccess to localized strings is easy and logical from a coding perspective.\nPropose this method in documentation and references for web3 developers.\nNMKR has supported this CIP with peer feedback.\nNMKR has provided a pilot implementation of this localization method.\nCIP about Media Token Metadata Standard CIP-0025.\n[ISO-639] language code.\n[ISO-3166] country/region code.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0127 | ripemd-160 hashing in Plutus Core\n\nThis CIP follows closely the (CIP-0101)1 and proposes an extension of the current Plutus functions with another hashing primitive RIPEMD-160. Primary goal is to introduce compatibility with Bitcoin's cryptographic infrastructure.\nRIPEMD-160\nThe integration of the ECDSA and Schnorr signatures over the secp256k1 curve into Plutus was a significant step towards interoperability with Ethereum and Bitcoin ecosystems. However, full compatibility is still impossible due to the absence of the RIPEMD-160 hashing algorithm in Plutus interpreter, which is a fundamental component of Bitcoin's cryptographic framework. Most of common addresses in Bitcoin are derived from double hashing procedure involving SHA-256 followed by RIPEMD-160 function:\nRIPEMD-160\nSHA-256\nRIPEMD-160\nP2KH\nP2WPKH\nP2SH\nP2WSH\nAdding RIPEMD-160 to Plutus would enhance the potential for cross-chain solutions between Cardano and Bitcoin blockchains and complements the set of primitives which we already have in that regard. It would allow for the verification of Bitcoin addresses and transactions on-chain. This addition enables also the verification of signed messages that identify the signer by the public key hash, which has not yet been witnessed on the Bitcoin blockchain.\nRIPEMD-160\nThe RIPEMD-160 is not only relevant to Bitcoin - other chains like Cosmos or BNB also use it for address generation.\nThis proposal aims to introduce a new built-in hash function RIPEMD-160.\nRIPEMD-160\nThis function will be developed following the RIP-160 specification and will utilize the cryptonite\nRIP-160\ncryptonite\nSince cardano-base already relies on cryptonite in the context of keccak-256 we would like to expose RIPEMD-160 through the same library, to facilitate its integration into Plutus.\ncardano-base\ncryptonite\nkeccak-256\nRIPEMD-160\nMore specifically, Plutus will gain the following primitive operation:\nripemd_160 :: ByteString - ByteString\nripemd_160\nThe input to this function can be a ByteString of arbitrary size, and the output will be a ByteString of 20 bytes. Note that this function aligns with the format of existing hash functions in Plutus, such as blake2b_256\nByteString\nByteString\nWhile the RIPEMD-160 function might be implemented in on-chain scripts, doing so would be computationally unfeasible.\nRIPEMD-160\nThe library, cryptonite, is not implemented by and under control of the Plutus team. However,\nIt is a library already used in the Plutus stack to expose KECCAK-256, and can be considered as a trustworthy implementation.\nIts behaviour is predictable and computationally efficient. The cost of the function is linear with respect to the size of the message provided as input. This is the same behaviour that other hash functions exposed in plutus (blake, sha3, keccak-256) have.\nA cardano-base binding is created for the ripemd-160 function and included in a new version of the library.\ncardano-base\nripemd-160\nA Plutus binding is created for the ripemd_160 function and included in a new version of Plutus.\nripemd_160\nIntegration tests, similar to those of the existing Plutus hash functions, are added to the testing infrastructure.\nThe function is benchmarked to assess its cost. As for other hash functions available in Plutus (blake2b, sha256 and keccak_256), we expect the cost of ripemd_160 to be linear with respect to the size of the message. The Plutus team determines the exact costing functions empirically.\nripemd_160\nThe ledger is updated to include new protocol parameters to control costing of the new builtins.\nThis CIP may transition to active status once the Plutus version containing the ripemd_160 function is introduced in a node release and becomes available on Mainnet.\nripemd_160\nThe Plutus team will develop the binding, integration tests, and benchmarks. The E2E tests will be designed and implemented collaboratively by the testing team, the Plutus team, and community members planning to use this primitive.\nThis CIP is licensed under [Apache-2.0][https://www.apache.org/licenses/LICENSE-2.0].\nI did not hesitate to reuse parts of the original text of (CIP-0101)[../CIP-0101/README.md) without explicit quotations. This approach was approved by the original authors.\nI did not hesitate to reuse parts of the original text of (CIP-0101)[../CIP-0101/README.md) without explicit quotations. This approach was approved by the original authors.\nI did not hesitate to reuse parts of the original text of (CIP-0101)[../CIP-0101/README.md) without explicit quotations. This approach was approved by the original authors.\n2023 Cardano Foundation\n\n---\n\nCIP-0128 | Preserving Order of Transaction Inputs\n\nWe propose the introduction of a new structure for transaction inputs aimed at enhancing the execution efficiency of Plutus contracts.\nThis CIP facilitates the preservation of input ordering from a submitted transaction, diverging from the current state where the ledger reorders transaction inputs lexicographically. Preserving the input ordering from a submitted transaction enables a validator to efficiently identify the required inputs in accordance with its validation logic, thus eliminating the need to use unnecessary computation by traversing all transaction inputs or performing sorting within the validator itself.\nFurthermore, this implementation offers the potential to enhance existing design patterns already used in production, specifically by improving the ability to group and match inputs and outputs located at the specific index positions.\nAccording to the Babbage CDDL transaction_body , the inputs and reference inputs of a transaction body are represented as a set, which indicates the non-duplicate inputs. However, the ledger not only process the transaction_inputs as a set , but also orders them lexicographically, first by transaction_id and then by index.\ntransaction_id\nindex\nThe primary issue with the ledger ordering inputs lexicographically is that validators must traverse all the inputs or sort them within the validator to find the required inputs for the validation logic. This process can lead to inefficiencies and increases the risk of exceeding the execution budget specially when processing a large set of inputs.\nFor instance, consider a spending validator that needs to access its own input and output. Each function described below necessitates traversing all inputs and outputs to fulfill such validation:\nfindOwnInput :: ScriptContext -> Maybe TxInInfo findOwnInput ScriptContext{scriptContextTxInfo=TxInfo{txInfoInputs}, scriptContextPurpose=Spending txOutRef} = find (\\TxInInfo{txInInfoOutRef} -> txInInfoOutRef == txOutRef) txInfoInputs findOwnInput _ = Nothing\nfindOwnInput :: ScriptContext -> Maybe TxInInfo findOwnInput ScriptContext{scriptContextTxInfo=TxInfo{txInfoInputs}, scriptContextPurpose=Spending txOutRef} = find (\\TxInInfo{txInInfoOutRef} -> txInInfoOutRef == txOutRef) txInfoInputs findOwnInput _ = Nothing\ngetContinuingOutputs :: ScriptContext -> [TxOut] getContinuingOutputs ctx | Just TxInInfo{txInInfoResolved=TxOut{txOutAddress}} <- findOwnInput ctx = filter (f txOutAddress) (txInfoOutputs $ scriptContextTxInfo ctx) where f addr TxOut{txOutAddress=otherAddress} = addr == otherAddress getContinuingOutputs _ = traceError \"Lf\" -- \"Can't get any continuing outputs\"\ngetContinuingOutputs :: ScriptContext -> [TxOut] getContinuingOutputs ctx | Just TxInInfo{txInInfoResolved=TxOut{txOutAddress}} <- findOwnInput ctx = filter (f txOutAddress) (txInfoOutputs $ scriptContextTxInfo ctx) where f addr TxOut{txOutAddress=otherAddress} = addr == otherAddress getContinuingOutputs _ = traceError \"Lf\" -- \"Can't get any continuing outputs\"\nvalidatorA :: BuiltinData -> BuiltinData -> ScriptContext -> Bool validatorA _datum _redeemer context = validateWithInputOutput input output where Just (TxInInfo {txInInfoResolved = input}) = findOwnInput context [output] = getContinuingOutputs context validateWithInputOutput _ _ = True\nvalidatorA :: BuiltinData -> BuiltinData -> ScriptContext -> Bool validatorA _datum _redeemer context = validateWithInputOutput input output where Just (TxInInfo {txInInfoResolved = input}) = findOwnInput context [output] = getContinuingOutputs context validateWithInputOutput _ _ = True\nFurthermore, preserving the order of inputs from the submitted transaction will facilitate the utilization of index redeemer patterns that many projects are transitioning to, as illustrated below.\nvalidatorIndex :: BuiltinData -> Integer -> ScriptContext -> Bool validatorIndex _datum index ScriptContext {scriptContextTxInfo = txInfo, scriptContextPurpose = Spending txOutRef} = validateInputOutput input output && txOutRef == intxOutRef where inputs = txInfoInputs txInfo outputs = txInfoOutputs txInfo TxInInfo {txInInfoOutRef = intxOutRef, txInInfoResolved = input} = inputs P.!! index output = outputs P.!! index validateWithInputOutput _ _ = True\nvalidatorIndex :: BuiltinData -> Integer -> ScriptContext -> Bool validatorIndex _datum index ScriptContext {scriptContextTxInfo = txInfo, scriptContextPurpose = Spending txOutRef} = validateInputOutput input output && txOutRef == intxOutRef where inputs = txInfoInputs txInfo outputs = txInfoOutputs txInfo TxInInfo {txInInfoOutRef = intxOutRef, txInInfoResolved = input} = inputs P.!! index output = outputs P.!! index validateWithInputOutput _ _ = True\nGiven the deterministic nature of the extended UTXO model, the determination of inputs and outputs can already be achieved through the off-chain code, eliminating the necessity to find or traverse inputs and outputs within the validator.\nAs per protocol specifications, the transaction body is structured as follows:\ntransaction_body = { 0 : set<transaction_input> ; inputs , 1 : [* transaction_output] , 2 : coin ; fee , ? 3 : uint ; time to live , ? 4 : [* certificate] , ? 5 : withdrawals , ? 6 : update , ? 7 : auxiliary_data_hash , ? 8 : uint ; validity interval start , ? 9 : mint , ? 11 : script_data_hash , ? 13 : set<transaction_input> ; collateral inputs , ? 14 : required_signers , ? 15 : network_id , ? 16 : transaction_output ; collateral return; New , ? 17 : coin ; total collateral; New , ? 18 : set<transaction_input> ; reference inputs; New }\ntransaction_body = { 0 : set<transaction_input> ; inputs , 1 : [* transaction_output] , 2 : coin ; fee , ? 3 : uint ; time to live , ? 4 : [* certificate] , ? 5 : withdrawals , ? 6 : update , ? 7 : auxiliary_data_hash , ? 8 : uint ; validity interval start , ? 9 : mint , ? 11 : script_data_hash , ? 13 : set<transaction_input> ; collateral inputs , ? 14 : required_signers , ? 15 : network_id , ? 16 : transaction_output ; collateral return; New , ? 17 : coin ; total collateral; New , ? 18 : set<transaction_input> ; reference inputs; New }\nSpecifically, the inputs and reference inputs are currently represented as a set:\n0 : set<transaction_input> ; inputs\n0 : set<transaction_input> ; inputs\n18 : set<transaction_input> ; reference inputs; New\n18 : set<transaction_input> ; reference inputs; New\nThe proposed solution suggests modifying the inputs representation to an oset type:\noset\nAn oset type behaves much like a set but remembers the order in which the elements were originally inserted.\noset\nset\n0 : oset<transaction_input> ; inputs\n0 : oset<transaction_input> ; inputs\n18 : oset<transaction_input> ; reference inputs; New\n18 : oset<transaction_input> ; reference inputs; New\nThe motivation behind this CIP stems from observed limitations and inefficiencies associated with the current lexicographical ordering of transaction inputs.\nThe strict lexicographical ordering mandated by the ledger requires traversing inputs to locate the appropriate inputs for the validation logic, which can lead to execution inefficiencies.\nTo address these issues, the proposed solution suggests transitioning from an set based representation of transaction inputs and reference inputs to an oset type, which preserves the order of elements.\nset\noset\nThis CIP tries to revive the original draft CIP-0051\nThis approach involves maintaining the current set-based representation of transaction inputs and reference inputs.\nIncluded within a Plutus version within a Cardano mainnet hardfork.\nPasses all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0129 | Governance Identifiers\n\nThis Cardano Improvement Proposal (CIP) defines a standardized structure for encoding and representing various governance and credential identifiers, specifically designed for DRep, Constitutional Committee (CC) keys, and Governance Actions within the Conway era. This specification introduces a single-byte header that encapsulates metadata related to the key type and credential type, allowing identifiers to retain critical metadata even when stored as byte arrays. By encoding this metadata directly into the bech32 format, we enhance both usability and interoperability across Cardano infrastructure and tools.\nThe Conway era on Cardano introduces new governance features, requiring unique and identifiable credentials for roles such as DReps, Constitutional Committee members, and distinct governance actions. Existing infrastructure and tools that process bech32 identifiers often decode and store the raw byte data for efficiency, unintentionally stripping away the metadata embedded in the bech32 prefix. This CIP addresses that limitation by embedding metadata into a structured single-byte header, allowing credentials to be stored in byte form without losing essential metadata. This standardization facilitates seamless linkage, sharing, and compatibility of governance identifiers across the ecosystem, supporting a robust and interoperable governance framework in Cardano.\nWe define a bytes representation for the various credentials and identifiers along with the their bech32 prefixes in this CIP. Taking inspiration from the Cardano addresses bytes format, we define an 8 bit header and a payload to define the key, which look similar to the reward address byte format but with a new specification and using the governance credentials.\nIn this CIP, We also define a simple bech32 prefix for gov actions, which does not have a credential. Gov actions only contain transaction ID bytes and an index, defined in the Gov Action Section below. The chosen prefixes for each identifier align with Cardano's established naming convention used in ledger specification, ensuring easy recognition and minimizing confusion within the ecosystem.\nIn the header-byte, bits [7;4] indicate the type of gov key being used. The remaining four bits [3;0] are used to define the credential type. There are currently 3 types of credentials defined in the Cardano Conway era, this specification will allow us to define a maximum of 16 different types of keys in the future.\n1 byte variable length <------> <-------------------> ┌────────┬─────────────────────┐ │ header │ key │ └────────┴─────────────────────┘ 🔎 ╎ 7 6 5 4 3 2 1 0 ╎ ┌─┬─┬─┬─┬─┬─┬─┬─┐ ╰╌╌╌╌╌╌╌╌ |t│t│t│t│c│c│c│c│ └─┴─┴─┴─┴─┴─┴─┴─┘\n1 byte variable length <------> <-------------------> ┌────────┬─────────────────────┐ │ header │ key │ └────────┴─────────────────────┘ 🔎 ╎ 7 6 5 4 3 2 1 0 ╎ ┌─┬─┬─┬─┬─┬─┬─┬─┐ ╰╌╌╌╌╌╌╌╌ |t│t│t│t│c│c│c│c│ └─┴─┴─┴─┴─┴─┴─┴─┘\nThere are currently 3 types of governance keys, and it is defined using the first half (bits [7;4]) of the header. The different types are summarized below,\nt t t t . . . .\n0000....\n0001....\n0010....\nThe second half of the header (bits [3;0]) refers to the credential type which can have the values and types as summarized in the table below,\nWe reserve the values 0 and 1 to prevent accidental conflicts with Cardano Address Network tags, ensuring that governance identifiers remain distinct and are not inadvertently processed as addresses.\n. . . . c c c c\n....0010\n....0011\nCardano Conway era introduces proposal procedures to submit gov actions. Gov actions are vote on chain by different kinds of keys, and as such it is necessary to be able to share gov actions across communication channels. The gov action is currently defined as combination of a transaction ID it was submitted in and an index within the transaction.\nWe define a byte format to combine the tx ID and the index to form a single valid byte string, as such it can be converted into a hex format and have its own bech32 prefix.\nTransaction ID is always a 32 byte length, hence we can append the index bytes to the tx id, please see below\nExample 1 TX ID in Hex - 0000000000000000000000000000000000000000000000000000000000000000 Gov Action index in HEX - 11 (number 17) gov action ID - 000000000000000000000000000000000000000000000000000000000000000011 bech32 - gov_action1qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqpzklpgpf\n0000000000000000000000000000000000000000000000000000000000000000\n11\n000000000000000000000000000000000000000000000000000000000000000011\ngov_action1qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqpzklpgpf\nExample 2 TX ID in Hex - 1111111111111111111111111111111111111111111111111111111111111111 Gov Action index in HEX - 00 (number 0) gov action ID - 111111111111111111111111111111111111111111111111111111111111111100 bech32 - gov_action1zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zygsq6dmejn\n1111111111111111111111111111111111111111111111111111111111111111\n00\n111111111111111111111111111111111111111111111111111111111111111100\ngov_action1zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zygsq6dmejn\ndrep\ncc_hot\ncc_cold\ngov_action\nWe can define a complete identifier as per the spec above by combining the header and the key, see below\nKey - 00000000000000000000000000000000000000000000000000000000 Type - Key Hash\n00000000000000000000000000000000000000000000000000000000\nKey Hash\nIdentifier - 0200000000000000000000000000000000000000000000000000000000\n0200000000000000000000000000000000000000000000000000000000\nBech32 - cc_hot1qgqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqvcdjk7\ncc_hot1qgqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqvcdjk7\nKey - 00000000000000000000000000000000000000000000000000000000 Type - Script Hash\n00000000000000000000000000000000000000000000000000000000\nScript Hash\nIdentifier - 1300000000000000000000000000000000000000000000000000000000\n1300000000000000000000000000000000000000000000000000000000\nBech32 - cc_cold1zvqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq6kflvs\ncc_cold1zvqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq6kflvs\nKey - 00000000000000000000000000000000000000000000000000000000 Type - Key Hash\n00000000000000000000000000000000000000000000000000000000\nKey Hash\nIdentifier - 2200000000000000000000000000000000000000000000000000000000\n2200000000000000000000000000000000000000000000000000000000\nBech32 - drep1ygqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq7vlc9n\ndrep1ygqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq7vlc9n\nThis CIP achieves its objectives by introducing a unified header and payload structure for governance-related keys, allowing for metadata to be directly embedded within the byte-level representation of each identifier. By defining a single-byte header that includes both key type and credential type, the proposal provides a consistent, compact format that retains crucial metadata even when stored or transmitted as raw byte arrays. This specification is designed to be forward-compatible, with a capacity to support up to 16 key types, allowing it to evolve with Cardano s governance and credential requirements.\nThis approach aligns with existing Cardano address encoding practices while adding specificity for governance keys in the Conway era. By also defining distinct bech32 prefixes for each identifier type, the CIP enhances user-friendliness and makes it easier for tooling and infrastructure to recognize, validate, and link these identifiers within the ecosystem. This design ensures governance identifiers are not only interoperable across platforms but also intuitive and accessible, paving the way for streamlined governance interactions within Cardano s tooling and community.\nTools, Wallets, and Explorers to utilize the identifiers and bech32 prefixes defined in this CIP for communication and view purposes.\nRequires updating Ledger nano app, and Trezor. The changes can be proposed once the CIP is merged.\nTooling CNTools SPO Scripts typhonjs Gov Tools cardano-signer\nCNTools\nSPO Scripts\ntyphonjs\nGov Tools\ncardano-signer\nAPIs Koios Cardanoscan API Blockfrost\nKoios\nCardanoscan API\nBlockfrost\nExplorers Cardanoscan.io AdaStat.net\nCardanoscan.io\nAdaStat.net\nWallets Eternl Typhon Wallet Lace\nEternl\nTyphon Wallet\nLace\nThis CIP uses some bech32 prefixes which are already defined by CIP105 and ref by CIP005, This PR includes updates to both the CIPs with updated vector spec. The suggested changes align with the design of the original CIP005.\nThis CIP uses some bech32 prefixes which are already defined by CIP105 and ref by CIP005, This PR includes updates to both the CIPs with updated vector spec. The suggested changes align with the design of the original CIP005.\nFor key generation tools - To move faster to this CIP, current tools which implement CIP105 based bech32 prefixes do not have to implement this CIP specification. They can change the prefix according to the updates suggested in this CIP, updating bech32 prefix will be a find and replace sort of an updated and it will instantly become compatible with this CIP.\nFor key generation tools - To move faster to this CIP, current tools which implement CIP105 based bech32 prefixes do not have to implement this CIP specification. They can change the prefix according to the updates suggested in this CIP, updating bech32 prefix will be a find and replace sort of an updated and it will instantly become compatible with this CIP.\nTools like explorers and wallets - These tools can potentially support both formats to start with for the purpose of allowing users to search for drep, cold keys, hot keys as these 3 are impacted by this CIP upgrade. And can continue to show only this CIP specification, having an easy backward compatibility as well moving to this CIP standard.\nTools like explorers and wallets - These tools can potentially support both formats to start with for the purpose of allowing users to search for drep, cold keys, hot keys as these 3 are impacted by this CIP upgrade. And can continue to show only this CIP specification, having an easy backward compatibility as well moving to this CIP standard.\nThis CIP does not require a hard fork for implementation, the goal is to use the identifies specified in this CIP for the UI, and as a medium of communication for sharing such keys and IDs.\nThis CIP does not require a hard fork for implementation, the goal is to use the identifies specified in this CIP for the UI, and as a medium of communication for sharing such keys and IDs.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0133 | Plutus support for Multi-Scalar Multiplication over BLS12-381\n\nThe CIP proposes an extension of the current Plutus functions to provide support for the efficient computation of the multi-scalar multiplication over the BLS12-381 curve. This operation is crucial in a number of cryptographic protocols that can enhance the capabilities of the Cardano blockchain.\nMulti-scalar multiplication (MSM) is an algebraic group operation of the following form. Let GGG be a group of prime order ppp. Let g0,g1,...,gN 1g_0, g_1, ..., g_{N-1}g0 ,g1 ,...,gN 1 be elements of GGG and let e0,e1,...,eN 1e_0, e_1, ..., e_{N-1}e0 ,e1 ,...,eN 1 be elements of ZpZ_pZp . Then, the multi-scalar multiplication MMM is calculated as M= i=0N 1ei giM=\\sum_{i=0} {N-1} e_i \\cdot g_iM= i=0N 1 ei gi .\nThis operation appears in many cryptographic protocols. Its naive implementation requires NNN scalar multiplications and NNN group additions. However, the performance can be significantly improved by employing advanced algorithms, such as the Pippenger Approach. Moreover, it can be further optimized for a particular group type (e.g., for elliptic curve groups [BH22, L23]).\nMulti-scalar multiplication appears in various cryptographic protocols, such as cryptographic signatures, zero-knowledge proofs, SNARK systems, and others. It is especially important in elliptic-curve-based SNARK proof systems, where large-scale MSMs can become a bottleneck in both proving and verification algorithms.\nThe recent Chang upgrade in Cardano included CIP-0381, which introduced built-in support for operations over the BLS12-381 elliptic curve (the implementation uses blst library). It made it feasible to implement various SNARK systems on-chain in Cardano. However, MSM still remains a bottleneck for many use cases. Implementing MSM naively, even using built-in functions for BLS12-381, consumes a significant portion of the computational budget of a transaction. It hinders implementation of such SNARK systems as KZG-based PLONK or Groth16, which require computations of MSM.\nOur benchmarks show that MSM of 10 G1G1G1 points over BLS12-381 curve consumes 7.74 of the computational budget of a transaction, while MSM of size more than 129 cannot fit into a single transaction at all. It impedes verification of complex circuits which might require much larger MSM.\nWe also did preliminary benchmarks to compare an optimized blst implementation of MSM with naive implementation using just blst group operations. We used Rust bindings to do these benchmarks (the underlying bindings are the same as used in cardano-base for bslt, which means we can expect similar behavior for the Haskell stack). The results for G1 group are the following (more results in the repository):\n* the sudden time improvement after 32 points is attributed to the inner workings of the blst library, which may operate differently for the MSM of size less than 32 (this should be carefully analyzed when establishing costing function for MSM built-in).\nAs it can be seen the performance improvement rises quickly with the size of the MSM. Note that the current threshold for Plutus naive implementation is 129 points per transaction. Our blst benchmarks show that naive MSM of size 129 takes approximately the same time as optimized MSM for more than 4000 points, which gives a hint to what improvements we can expect with a Plutus built-in for MSM. Moreover, these benchmarks do not account for the Plutus overhead to call many built-in BLS12 functions while implementing the naive MSM, so the final improvement may be even larger. On the other hand, it is important to mention that if those points are brought into the script as input, their number would be constrained by the size of the script and by the computational complexity of points decompression. The benchmarks of points decompression in CIP-0381 shows that up to 300 G1 points can be passed as input to a 16kb script. However, in real cryptographic protocols typically only a part of the points involved in MSM is passed as input, while another part is computed during the execution (e.g., in PLONK-based SNARKs).\nThe availability of MSM built-in function in Plutus language will provide more efficient and reliable means to perform this important computation. Implementing an optimized MSM manually in Plutus deemed to be infeasible because of its complexity and because it operates at the level of basic curve operations.\nA nonexclusive list of cryptographic protocols that would benefit from having MSM built-in for BLS12-381 curve:\nThe verification of pairing-based SNARKs over BLS12-381. For instance, the size of the MSM in Groth16 verifier depends on the number of public inputs. The size of the MSM in KZG-based PLONK verifier depends on the arithmetization structure. Public key aggregation in BLS multi-signature aggregation scheme. It is a popular scheme that allows to aggregate many signatures into a common message, so that verifying the short multi-signature is fast. A cryptographic accumulator. It is a cryptographic primitive that allows a prover to succinctly commit to a set of values while being able to provide proofs of (non-)membership. Accumulators have found numerous applications including signature schemes, anonymous credentials, zero-knowledge proof systems, and others.\nThe verification of pairing-based SNARKs over BLS12-381. For instance, the size of the MSM in Groth16 verifier depends on the number of public inputs. The size of the MSM in KZG-based PLONK verifier depends on the arithmetization structure.\nPublic key aggregation in BLS multi-signature aggregation scheme. It is a popular scheme that allows to aggregate many signatures into a common message, so that verifying the short multi-signature is fast.\nA cryptographic accumulator. It is a cryptographic primitive that allows a prover to succinctly commit to a set of values while being able to provide proofs of (non-)membership. Accumulators have found numerous applications including signature schemes, anonymous credentials, zero-knowledge proof systems, and others.\nThe above mentioned cryptographic protocols are used in many Cardano products, for instance:\nPartner chains - a crucial component for the scalability of Cardano. Its interoperability with Cardano relies on the ability to construct a secure bridge for message passing. A reliable trustless bridge requires SNARK proofs for efficient proving of the partner chain state.\nHydra - a prominent layer-2 solution for scalability of Cardano. Hydra relies on a multisignature scheme, where all participants of the side channel need to agree on the new state. Moreover, Hydra tails could benefit from SNARKs for proving correct spending of a set of transactions.\nMithril - a protocol for helping to scale the adoption of Cardano and its accessibility for users. It creates certified snapshots of the Cardano blockchain allowing to obtain a verified version of the current state without having to download and verify the full history of the blockchain. Mithril utilizes a stake-based threshold multisignature scheme based on elliptic curve pairings. Even though at the moment most use cases of Mithril relies on off-chain computations, eventually the Mithril certificates might also be verified in Plutus smart contracts.\nAtala Prism - a decentralized identification mechanism. One of the properties it can provide is anonymity: users can selectively disclose attributes of their certificate or prove statements without disclosing their identity. Up to date, the most efficient solutions for doing that use pairing-based zero-knowledge protocols.\nIn conclusion, integrating multi-scalar multiplication as a core built-in within Plutus is not only essential for enhancing cryptographic capabilities, but also for optimizing on-chain computations. The current approach of naive manual implementation in Plutus is inefficient for large-scale MSMs. Incorporating this function directly will streamline these operations, reduce transaction costs, and maintain the integrity of existing tools, thereby significantly advancing the Plutus ecosystem's functionality and user experience.\nThe MSM for BLS12-381 is implemented in blst library, which is already a dependency for the cardano-base. This library provides efficient and formally verified implementation of operations over the BLS12-381 elliptic curve. It has been used for implementing CIP-0381. Basically, we would like to expose several additional functions from this library in Plutus API.\nWe propose to define two new Plutus functions bls12_381_G1_multiScalarMul and bls12_381_G2_multiScalarMul as follows:\nbls12_381_G1_multiScalarMul :: [Integer] -> [bls12_381_G1_element] -> bls12_381_G1_element bls12_381_G2_multiScalarMul :: [Integer] -> [bls12_381_G2_element] -> bls12_381_G2_element\nbls12_381_G1_multiScalarMul :: [Integer] -> [bls12_381_G1_element] -> bls12_381_G1_element bls12_381_G2_multiScalarMul :: [Integer] -> [bls12_381_G2_element] -> bls12_381_G2_element\nThe types bls12_381_G1_element and bls12_381_G2_element are already introduced by CIP-0381. Given two arrays of scalars and group elements the functions compute multi-scalar multiplication for the corresponding subgroup. The arrays of scalars and group elements must be non-empty and of equal size. If the input arrays are empty or not equal, the functions must fail. These new functions naturally extend a set of operations over BLS12-381 defined by CIP-0381.\nThe computational impact of multi-scalar multiplication is complicated by it having dynamic-size arguments. Preliminary benchmarks show that the computational complexity grows linearly with the size of the MSM. This should be reflected in the costing function. It should also be taken into account that the efficiency of the MSM algorithm may vary depending on the blst setup.\nThere may be an extra complication in the costing procedure because all scalars have to be reduced modulo the order of the group before being passed to the blst functions (this happens here for the existing BLS12-381 scalar multiplication function in cardano-crypto-class). Presumably this is almost zero-cost for scalars already in the correct range, but if we pass in a very long list of very large scalars, the aggregated reduction time might be quite significant, and this must be taken into account in the costing function to guard against the possibility of a large amount of computation being done too cheaply.\ncardano-crypto-class\nIntegrating these functions directly into Plutus will streamline cryptographic operations, reduce transaction costs, and uphold the integrity of existing cryptographic interfaces. It addresses current inefficiencies and enhances the cryptographic capabilities of the Plutus platform.\nIt will allow the implementation of complex cryptographic protocols on-chain in Plutus smart contracts, significantly expanding the capabilities of the Cardano blockchain.\nWe consider the following criteria to be essential for acceptance:\nThe PR for this functionality is merged in the Plutus repository.\nThis PR must include tests, demonstrating that it behaves as the specification requires in this CIP.\nA benchmarked use case is implemented in the Plutus repository, demonstrating that realistic use of this primitive does, in fact, provide major cost savings.\nIOG Plutus team consulted and accept the proposal.\nAuthors to provide preliminary benchmarks of naive MSM implementation in Plutus and blst MSM. https://github.com/input-output-hk/plutus-msm-bench https://github.com/dkaidalov/bench-blst-msm/\nhttps://github.com/input-output-hk/plutus-msm-bench\nhttps://github.com/dkaidalov/bench-blst-msm/\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0134 | Cardano URIs - Address Representation\n\nThis CIP proposes an extension to CIP-0013 to allow easy and unambiguous encoding of CIP-0019/CIP-0105 Addresses into URL's.\nCIP-0013 defines encoding for payment addresses and stake pool, however CIP-0019 and CIP-0105 define numerous other address types.\nThese addresses cannot currently be encoded into URIs unambiguously. This extension proposes a simple and forward compatible method of encoding such addresses into the URI scheme defined by CIP-0013.\nx509 certificates can contain name or alternative name information related to either the issuer of the certificate or its subject. It is desirable to distinguish an Issuer or Subject of a certificate by one or more on-chain keys. This, for example, can facilitate the ability to link off-chain actions authorized with a x509 certificate, with an on-chain identity.\nCurrently, there is no standard way to embed a Cardano address, such as a stake address, or a dRep address as distinguishing name within a x509 certificate.\nA URI is one of the possible names that can be associated with the Issuer or Subject of a certificate. This URI can be referenced in the alternative name for both the Issuer and Subject of a certificate by using the uniformResourceIdentifier in the general name. CIP-0013 does not define a method for stake or dRep addresses, etc., to be encoded in the URI scheme it defines.\nAllowing these addresses to be easily encoded as URIs allows them to be 100 interoperable with existing public key infrastructure and certificate creation tools.\nWe extend CIP-0013 with a single new authority for referencing Cardano addresses in CIP-0019 format.\nWe extend the grammar from CIP-0013 with the new authority:\nauthorityref = (... | addr ) addr = \"//addr/\" cip19-addr cip19-addr = *cip19-char cip19-char = ALPHA / DIGIT / \"_\"\nauthorityref = (... | addr ) addr = \"//addr/\" cip19-addr cip19-addr = *cip19-char cip19-char = ALPHA / DIGIT / \"_\"\nEffectively, any address string specified by CIP-0019, CIP-0105 or future extension to either of these specifications can be embedded directly within the URI.\nweb+cardano://addr/addr1qx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer3n0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgse35a3x web+cardano://addr/addr_test1gz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer5pnz75xxcrdw5vky web+cardano://addr/stake1uyehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gh6ffgw web+cardano://addr/stake_test1uqehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gssrtvn\nweb+cardano://addr/addr1qx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer3n0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgse35a3x web+cardano://addr/addr_test1gz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer5pnz75xxcrdw5vky web+cardano://addr/stake1uyehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gh6ffgw web+cardano://addr/stake_test1uqehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gssrtvn\nweb+cardano://addr/drep_vk17axh4sc9zwkpsft3tlgpjemfwc0u5mnld80r85zw7zdqcst6w54sdv4a4e web+cardano://addr/drep15k6929drl7xt0spvudgcxndryn4kmlzpk4meed0xhqe25nle07s web+cardano://addr/drep_script16pjhzfkm7rqntfezfkgu5p50t0mkntmdruwlp089zu8v29l95rg web+cardano://addr/cc_cold_vk149up407pvp9p36lldlp4qckqqzn6vm7u5yerwy8d8rqalse3t04q7qsvwl web+cardano://addr/cc_cold1lmaet9hdvu9d9jvh34u0un4ndw3yewaq5ch6fnwsctw02xxwylj web+cardano://addr/cc_cold_script14ehj5f64f40xju0086fnunctulkh46mq7munm7upe4hpcwpcat web+cardano://addr/cc_cold_script14ehj5f64f40xju0086fnunctulkh46mq7munm7upe4hpcwpcatv web+cardano://addr/cc_hot_vk10y48lq72hypxraew74lwjjn9e2dscuwphckglh2nrrpkgweqk5hschnzv5 web+cardano://addr/cc_hot17mffcrm3vnfhvyxt7ea3y65e804jfgrk6pjn78aqd9vg7xpq8dv web+cardano://addr/cc_hot_script16fayy2wf9myfvxmtl5e2suuqmnhy5zx80vxkezen7xqwskncf40\nweb+cardano://addr/drep_vk17axh4sc9zwkpsft3tlgpjemfwc0u5mnld80r85zw7zdqcst6w54sdv4a4e web+cardano://addr/drep15k6929drl7xt0spvudgcxndryn4kmlzpk4meed0xhqe25nle07s web+cardano://addr/drep_script16pjhzfkm7rqntfezfkgu5p50t0mkntmdruwlp089zu8v29l95rg web+cardano://addr/cc_cold_vk149up407pvp9p36lldlp4qckqqzn6vm7u5yerwy8d8rqalse3t04q7qsvwl web+cardano://addr/cc_cold1lmaet9hdvu9d9jvh34u0un4ndw3yewaq5ch6fnwsctw02xxwylj web+cardano://addr/cc_cold_script14ehj5f64f40xju0086fnunctulkh46mq7munm7upe4hpcwpcat web+cardano://addr/cc_cold_script14ehj5f64f40xju0086fnunctulkh46mq7munm7upe4hpcwpcatv web+cardano://addr/cc_hot_vk10y48lq72hypxraew74lwjjn9e2dscuwphckglh2nrrpkgweqk5hschnzv5 web+cardano://addr/cc_hot17mffcrm3vnfhvyxt7ea3y65e804jfgrk6pjn78aqd9vg7xpq8dv web+cardano://addr/cc_hot_script16fayy2wf9myfvxmtl5e2suuqmnhy5zx80vxkezen7xqwskncf40\nBy extending CIP-0013 to allow a CIP-0019 encoded address to be simply embedded in the URI scheme, we enable existing certificate creation tools and public key infrastructure to be used to easily create certificates that reference Cardano addresses.\nIt is envisioned that this extension could have use cases beyond the one presented here.\nCommunity Feedback and Review Integrated.\nDemonstration of Cardano addresses being embedded in x509 certificates using existing tools.\nAt least one project utilizing this standard.\nProject Catalyst intends to use this standard to facilitate linking of on-chain and off-chain identity with x509 certificates. This specification does not deal with the processes or proofs required, simply the URI scheme that is required to embed a Cardano address in a x509 certificate.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0135 | Disaster Recovery Plan for Cardano networks\n\nWhile the Cardano mainnet and other networks have proven to be highly resilient, it is necessary to proactively consider the possible recovery mechanisms and procedures that may be required in the unlikely event of a major failure where the network is unable to recover itself.\nThis CIP considers three representative scenarios and addresses specific considerations relevant in each case:\nScenario 1 - Long-Lived Network Partition Scenario 2 - Failure to Make Blocks for an Extended Period of Time Scenario 3 - Bad Blocks Minted on Chain\nTo ensure successful recovery in the event of a chain failure, it's crucial to establish effective communication channels and exercise recovery procedures in advance to familiarize the community and stake pool operators (SPOs) with the process.\nThis CIP is based on an earlier IOHK technical report that is referenced below, supplemented by internal documentation and discussions that have not been publicly released. It should be considered to be a living document that is reviewed and revised on a regular basis.\nNote that although the focus of disaster recovery is on Cardano mainnet, since this is the greatest risk of loss of funds, the recovery procedures are generic and apply to other Cardano networks, including SanchoNet, Preview, PreProd or private networks. Appropriate adjustments may need to be made to reflect differences in timing or other concerns.\nThis CIP is needed to familiarize stakeholders with the processes and procedures that should be followed in the unlikely event that the Cardano mainnet, or another Cardano network, encounters a situation where the built-in on-chain recovery mechanisms fail.\nWhile the exact recovery process will depend on the unique nature of the failure, there are three main scenarios we can consider.\nOuroboros Praos is designed to cope with real-world networking conditions, in which some nodes may temporarily be disconnected from the network. In this case, the network will continue to make blocks, perhaps at some lower chain density (reflecting the temporary loss of stake to the network as a whole). As nodes rejoin the network, they will then participate in normal block production once again. In this way, the network remains resilient to changes in connectivity.\nIf many nodes become disconnected, the network could divide into two or more completely disconnected parts. Each part of the network could then form its own chain, backed by the stake that is participating in its own partition. Under normal conditions, Praos will also deal with this situation. When the partitioned group of nodes reconnects, the longest chain will dominate, and the shorter chain will be discarded. The nodes on the shorter chain will automatically rollback to the point where the fork occurred, and then rejoin the main chain. This is perfectly normal. Such forks will typically last only a few blocks.\nHowever, in an extreme situation, the partition may persist beyond the Praos rollback limit of k blocks (currently 2,160 blocks on mainnet). In this case, the nodes will not be able to rollback to rejoin the main chain, since this would violate the required Praos guarantees.\nDisconnected nodes must be reconnected to the main chain by their operators. This can be done by truncating the local block database to a point before the chain fork and then resyncing against the main network, using the db-truncator tool, for example.\ndb-truncator\nFull node wallets can also be recovered in the same way, though this may require technical skills that the end users do not possess. It may be easier, if slower, for them to simply resynchronize their nodes from the start of the chain (i.e. from the genesis block).\nOuroboros Genesis provides additional resilience when recovering from long lived network partitions. In Praos nodes resyncing from a point before the chain fork could still in some cases follow the alternative chain (if it is the first one seen) and extra mechanisms may be needed to avoid this possibility. In Praos, for example, this may require that all participants on the alternative chain truncate the local block database prior to the partition being resolved. In Ouroboros Genesis when resyncing from a point before the chain fork, the chain selection rules will ensure selection of the correct path for the main chain assuming the partition has been resolved.\nAlternative methods to resynchronise the node to the main chain might include the use of Mithril or other signed snapshots. These would allow faster recovery. However, in this case, care needs to be taken to achieve the correct balance of trust against speed of recovery.\nAlthough block producing nodes will rejoin the main network following the remediation described above, the blocks that they have minted while they were disconnected will not be included in the main chain. This may have real world effects that will not be automatically remedied when the nodes rejoin the main chain. For example, transactions may have been processed that have significant real world value, or assumptions may have been made about chains of evidence/validity, or the timing of transactions. End users should be aware of the possibility and include provisions in their contracts to cover this eventuality. It may be necessary to resubmit some or all of the transactions that were processed on the minority chain onto the main chain. To avoid unexpected effects, this should be done by the end users/applications, and not by block producers acting on their behalf.\nIf they are not observant, stake pools, full node wallets and other node users (e.g. explorers) could continue indefinitely on the minority chain. Such users should take care to be aware of this situation and take steps to rejoin the main chain as quickly as possible. A reliable and trusted public warning system should be considered that can alert users and advise them on how to rejoin the main chain.\nOn Cardano mainnet, partitions of less than 2,160 blocks will automatically rejoin the main chain. With current Cardano mainnet settings, this represents a period of up to 12 hours during which automatic rollback will occur. If the partition exceeds 2,160 blocks, then the procedure described above will be necessary to allow nodes to rejoin the main chain. Other Cardano networks may have different timing characteristics.\nOuroboros Praos requires at least one block to be produced every 3k/f slots. With the current Cardano mainnet settings, that is a 36 hour period. Such an event is extremely unlikely, but if it were to happen then the network would be unable to make any further blocks.\nIt is recommended to monitor the chain for block production. If a low density period is observed, then block producers should be notified, and efforts made to mint new blocks prior to the expiry of the 3k/f window. If this is not possible then the remediation procedures should be followed.\nIdentify a small group of block producing nodes that will be used to recover the chain. For Cardano mainnet, this group should have sufficient delegated stake to be capable of generating at least 9 blocks in a 36 hour window. It should be isolated from the rest of the network. The chain can then be recovered by resetting the wall clocks on the group of block producing nodes, restarting them from the last good block on the Cardano network, playing forward the chain production at high speed (10x usual speed is recommended), while inserting new empty blocks at the slots which are allocated to the block producers. The recovery nodes can then be restarted with normal settings, including connections to the network. Ouroboros Genesis then allows other nodes in the network to rapidly resynchronize with the newly restored chain. This would leave one or more gaps in the chain, interspersed with empty blocks.\nIn order to avoid allegations of unfair behaviour, block producing nodes that are used to recover the network should donate any rewards that they receive during recovery to the treasury.\nUnlike Scenario 1, no transactions will be submitted that need to be resubmitted on the chain. Users will, however, experience an extended period during which the chain is unavailable. Cardano applications and contracts should be designed with this possibility in mind. Full node wallets and other node users should recover quickly once the network is restarted but there may be a period of instability while network connections are re-established and the Ouroboros Genesis snapshot is distributed across all nodes.\nThe chain will tolerate a gap of up to 3k/f slots (36 hours with current Cardano mainnet settings). A period of low chain density could have security implications that affect dynamic availability and leave open the possibility for future long range attacks. This may be particularly relevant should chain recovery be performed as described above (using less stake than is required for an honest majority). To mitigate the presence of an extended period of low chain density we may need to make use of the lightweight checkpointing mechanism in Ouroborus Genesis. Alternatively, Mithril could also be used to provide certified snapshots to stake pools as a means to verify the correct state of the ledger.\nThe adoption of Mithril for fast bootstrapping by light clients and edge nodes should help to mitigate risks for the types of users on the network that do not participate in consensus.\nAs described below, Ouroboros Genesis snapshots may also be useful as part of the recovery process.\nIn the event that a bad block was to be minted on-chain, then some or all validators might be unable to process the block. They would therefore stop, and be unable to restart. Wallet and other nodes might be unable to synchronise beyond the point of the bad block.\nDepending on the cause of the issue and its severity, alternative remediations might be possible.\nScenario 3.1: if some existing node versions were able to process the block, but others were not, then the chain would continue to grow at a lower chain density. SPOs would need to be persuaded to upgrade (or downgrade) to a suitable node version that would allow the chain to continue. The chain density would then gradually recover to its normal level. Other users would need to upgrade (or downgrade) to a version of the node that could follow the full chain.\nScenario 3.2: if no node version was able to process the block and a gap of less than 3k/f slots existed, then the chain could be rolled back immediately before the bad block was created, and nodes restarted from this point. The chain would then grow as normal, with a small gap around the bad block. In this case, care would need to be taken that the rogue transaction was not accidentally reinserted into the chain. This might involve clearing node mempools, applying filters on the transaction, or developing and deploying a new node version that rejected the bad block.\nScenario 3.3: an alternative to rolling back would be to develop and deploy a \"hot-fix\" node that could accept the bad block, either as an exception, or as new acceptable behaviour. Nodes would then be able to incorporate the bad block as part of the chain, minting new blocks as usual, or following the chain. In this case, the bad block would persist on-chain indefinitely and future nodes would also need to accept the bad block. Such an approach is best used when the rejected block has behaviour that was unanticipated, but which is benign in nature. This will leave no abnormal gaps in the chain.\nScenario 3.4: if more than 3k/f slots have passed since the bad block was minted, then it will be necessary to roll back the chain immediately prior to the bad block as in Scenario 3.2, and then proceed as described for Scenario 2. As with Scenario 2, this will leave a series of gaps in the chain that are interspersed with empty blocks.\nIf more than 3k/f slots have passed since the bad block was minted on-chain (36 hours with current Cardano mainnet settings), then a mix of recovery techniques will be needed, as described in Scenario 3.4. When deciding on the correct recovery technique for Scenarios 3.1-3.3, consideration should be given as to whether the recovery can be successfully completed before 3k/f slots have elapsed. In case of doubt, the procedure for Scenario 3.4 should be followed.\nAny of the above conditions may result in a period of lower chain density. The updated consensus mechanism introduced in Ouroboros Genesis relies on making chain density comparisons to assist a node when catching up with the network, in order to reduce the reliance on having trusted peers when syncing. As such, low-density periods pose a potential security risk for the future; they are periods where a motivated adversary could perform a long-range attack by building a higher density chain.\nIn order to mitigate this, Genesis introduces the concepts of lightweight checkpoints. A lightweight checkpoint is effectively a block point - a combination of block number and hash - which can be distributed along with the node. Unlike Mithril Snapshots (see below), Genesis lightweight snapshots are not assured by any committee - rather, they form part of the trusted codebase distributed with the node, or by other parties.\nWhen syncing, a Genesis node will refuse to validate past the block number of any lightweight checkpoint if the chain does not contain the correct block at that point.\nGenesis snapshots play two potential roles in disaster recovery:\nIn scenarios where the network is split, a lightweight snapshot could guide a node from the abandoned partition in connecting to the main partition. In general this should not be needed, however, since the main partition should win out in any Genesis density comparisons. This usage also falls closer to scenario 2, in that it relies on an external source imposing a chain selection, which must then be trusted by all parties. Following a disaster recovery procedure, a sufficient number of blocks covering the low density period should be added to the list of lightweight checkpoints. These would serve the purpose of preventing a subsequent long-range attack.\nIn scenarios where the network is split, a lightweight snapshot could guide a node from the abandoned partition in connecting to the main partition. In general this should not be needed, however, since the main partition should win out in any Genesis density comparisons. This usage also falls closer to scenario 2, in that it relies on an external source imposing a chain selection, which must then be trusted by all parties.\nFollowing a disaster recovery procedure, a sufficient number of blocks covering the low density period should be added to the list of lightweight checkpoints. These would serve the purpose of preventing a subsequent long-range attack.\nNote that, in this second scenario, concerns about the legitimacy of the checkpoint are much less salient. The checkpoint can be issued post disaster recovery, at such a time where the points it contains are in the past, and are both agreed upon and easy to verify for all honest parties.\nMithril is a stake-based threshold multi-signatures scheme. One of the applications of this protocol in Cardano is to create certified snapshots of the Cardano blockchain. Mithril snapshots allow nodes or applications to obtain a verified copy of the current state of the blockchain without having to download and verify the full history.\nSPOs that participate in the Mithril network provide signed snapshots to a Mithril aggregator that is responsible for collecting individual signatures from Mithril signers and aggregating them into a multi-signature. Using this capability, the Mithril aggregator can then provide certified snapshots of the Cardano blockchain that can potentially be used as a trusted source for recovery purposes.\nProvided that it gains sufficient adoption on the Cardano network and that snapshots continue to be signed by an honest majority of stake pools following a chain recovery event, Mithril may therefore provide an alternative solution to Ouroboros Genesis checkpoints as a way to verify the correct state of the ledger\nMonitor Cardano mainnet for periods of low density and take early action if an extended period is observed. Identify a collection of block producer nodes that has sufficient stake to mint at least 9 blocks in any 36 hour window. Set up emergency communication channels with stake pool operators and other community members. Practice disaster recovery procedures on a regular basis. Provide signed Mithril snapshots and a way for full node wallet users and others to recover from this snapshot. Determine how to employ Ouroboros Genesis snapshots as part of the disaster recovery process\nMonitor Cardano mainnet for periods of low density and take early action if an extended period is observed.\nIdentify a collection of block producer nodes that has sufficient stake to mint at least 9 blocks in any 36 hour window.\nSet up emergency communication channels with stake pool operators and other community members.\nPractice disaster recovery procedures on a regular basis.\nProvide signed Mithril snapshots and a way for full node wallet users and others to recover from this snapshot.\nDetermine how to employ Ouroboros Genesis snapshots as part of the disaster recovery process\nOne of the key requirements for successful disaster recovery will be proper engagement with the community.\nIdentify stake pool operators (SPOs) who can assist with disaster recovery Discuss disaster recovery requirements with Intersect's Technical Working Groups and Security Council Identify and establish the right communications channels with the community, including Intersect Set up regular disaster recovery practice sessions\nIdentify stake pool operators (SPOs) who can assist with disaster recovery\nDiscuss disaster recovery requirements with Intersect's Technical Working Groups and Security Council\nIdentify and establish the right communications channels with the community, including Intersect\nSet up regular disaster recovery practice sessions\nThis CIP outlines key disaster recovery scenarios that the Cardano community should understand to mitigate potential network outages. As a living document, it will be regularly reviewed and updated to inform stakeholders and encourage more detailed contingency planning. The CIP aims to facilitate discussions, establish recovery procedures, and encourage regular recovery practice exercises to ensure preparedness and validation of recovery actions in the event of an outage.\nThe proposal has been reviewed by the community and sufficiently advertised on various channels. Intersect Technical Groups Intersect Discord Channels Cardano Forum\nThe proposal has been reviewed by the community and sufficiently advertised on various channels.\nIntersect Technical Groups\nIntersect Discord Channels\nCardano Forum\nAll major concerns or feedback have been addressed.\nAll major concerns or feedback have been addressed.\nN/A\nCardano Disaster Recovery Plan (May 2021)\nCardano Incident Reports\nJanuary 2023 Block Production Temporary Outage\nDB Truncator Tool\nDB Synthesizer Tool\nOuroboros Genesis\nMithril\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0136 | Governance metadata - Constitutional Committee votes\n\nThe Conway ledger era ushers in on-chain governance for Cardano via CIP-1694 | A First Step Towards On-Chain Decentralized Governance, with the addition of many new on-chain governance artifacts. Some of these artifacts support the linking of off-chain metadata, as a way to provide context to on-chain actions.\nThe CIP-100 | Governance Metadata standard provides a base framework for how all off-chain governance metadata can be formed and handled. This standard was intentionally limited in scope, so that it can be expanded upon by more specific subsequent CIPs.\nThis proposal aims to provide a specification for the off-chain metadata vocabulary that can be used to give context to Constitutional Committee (CC) votes.\nThe high-level motivation for this proposal is to provide a standard which improves legitimacy of Cardano's governance system.\nGovernance action authors are likely to have dedicated a significant amount of time to making their action meaningful and effective (as well as locking a significant deposit). If this action is not able to be ratified by the CC, it is fair for the author to expect a reasonable explanation from the CC.\nWithout reasonable context being provided by the CC votes, authors may struggle to iterate upon their actions, until they are deemed constitutional. This situation could decrease perceived legitimacy in Cardano's governance.\nBy producing a standard we hope to encourage all CC members to attach rich contextual metadata to their votes. This context should show CC member's decision making is fair and reasonable.\nThis context allows the other voting bodies to adequately check the power of the CC.\nThe CC and their votes are fundamentally very different from the other voting bodies. This makes reusing standards from these voting bodies problematic.\nCardano's Interim Constitution Article VI Section 4 states:\nConstitutional Committee processes shall be transparent. The Constitutional Committee shall publish each decision. When voting no on a proposal, the Committee shall set forth the basis for its decision with reference to specific Articles of this Constitution that are in conflict with a given proposal.\nConstitutional Committee processes shall be transparent. The Constitutional Committee shall publish each decision. When voting no on a proposal, the Committee shall set forth the basis for its decision with reference to specific Articles of this Constitution that are in conflict with a given proposal.\nThis section suggests that the CC shall provide rationale documents. Specifying a standard structure and common vocabulary for these documents aids the creation of supporting tooling.\nBy creating and implementing these metadata standards we facilitate the creation of tooling that can read and write this data. Such tooling greatly expands the reach and effectiveness of rationales as it allows for rich user interfaces to be created. i.e. translation tools, rationale comparison tools.\nWe define a specification for fields which can be added to CC votes.\nThe following properties extend the potential vocabulary of CIP-100's body property.\nbody\nsummary\nA short text field. Limited to 300 characters.\n300\nAuthors SHOULD use this field to clearly state their stance on the issue.\nAuthors SHOULD use this field to succinctly describe their rationale.\nAuthors SHOULD give a brief overview of the main arguments will support your position.\nThis SHOULD NOT support markdown text styling.\nCompulsory.\nrationaleStatement\nA long text field.\nAuthors SHOULD use this field to fully describe their rationale.\nAuthors SHOULD discuss their arguments in full detail.\nThis field SHOULD support markdown text styling.\nCompulsory.\nprecedentDiscussion\nA long text field.\nThe author SHOULD use this field to discuss what they feel is relevant precedent.\nThis field SHOULD support markdown text styling.\nOptional.\ncounterargumentDiscussion\nA long text field.\nThe author SHOULD use this field to discuss significant counter arguments to the position taken.\nThis field SHOULD support markdown text styling.\nOptional.\nconclusion\nA long text field.\nThe author SHOULD use this field to conclude their rationale.\nThis SHOULD NOT support markdown text styling.\nOptional.\ninternalVote\nA custom object field.\nThis field SHOULD be used to reflect any internal voting decisions within CC member.\nThis field SHOULD be used by members who are constructed from organizations or consortiums.\nOptional.\nconstitutional\nA positive integer.\nThe author SHOULD use this field to represent a number of internal votes for the constitutionality of the action.\nOptional.\nunconstitutional\nA positive integer.\nThe author SHOULD use this field to represent a number of internal votes against the constitutionality of the action.\nOptional.\nabstain\nA positive integer.\nThe author SHOULD use this field to represent a number of internal abstain votes for the action.\nOptional.\ndidNotVote\nA positive integer.\nThe author SHOULD use this field to represent a number of unused internal votes.\nOptional.\nagainstVote\nA positive integer.\nThe author SHOULD use this field to represent a number of internal votes to not vote on the action.\nOptional.\nreferences\nHere we extend CIP-100's references field.\nreferences\nRelevantArticles\nWe add to CIP-100's @types, with a type of RelevantArticles.\n@type\nRelevantArticles\nAuthors SHOULD use this field to list the relevant constitution articles to their argument.\nCC must include all compulsory fields to be considered CIP-136 compliant. As this is an extension to CIP-100, all CIP-100 fields can be included within CIP-136 compliant metadata.\nSee test-vector.md for examples.\nThis proposal should not be versioned, to update this standard a new CIP should be proposed. Although through the JSON-LD mechanism further CIPs can add to the common governance metadata vocabulary.\nBy providing a peer reviewed structure for CC vote rationale, we encourage detailed voting rationales increasing the legitimacy of CC votes within the governance system.\nsummary\nWe include compulsory summary with limited size to allow for the creation of tooling which layers of inspection to vote rationale. This allows readers to get a summary of a rationale at a high level before reading all the details.\nrationaleStatement\nThis field allows for a very long-form discussion of their rationale. This is compulsory because it forms the core of their rationale.\nBy setting some fields to compulsory we ensure a minimum amount of data for downstream tools to expect to render.\nprecedentDiscussion\nThis is a dedicated field to be able to discuss specific precedent of votes. By separating this from rationaleStatement we encourage specific discussion of precedence as well as clear separation in tooling.\nrationaleStatement\ncounterargumentDiscussion\nThis is a dedicated field to be able to discuss counterarguments from those proposed in the other fields. By separating this from rationaleStatement we encourage specific discussion of counterarguments as well as clear separation in tooling.\nrationaleStatement\ninternalVote\nThis field, gives the ability for CC members who are operated by multiple individuals to share insights on specific voting choice of the individuals. This could add additional context to the workings and opinions of the individuals who operate the CC member.\nrelevantArticles\nBy providing a new type to CIP-100 References we encourage tooling to differentiate clearly References to the constitution from other types of Reference.\nReferences\nReferences\nReference\nThis standard is supported by two separate tools, which create and submit CC votes.\nThis standard is supported by two different chain indexing tools, used to read and render metadata.\nSeek feedback from individuals who are members of current Interim Constitutional Committee.\nAuthor to provide test vectors, examples, and schema files.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0137 | Decentralized Message Queue\n\nWe propose to create a decentralized message diffusion protocol leveraging the Cardano network layer. This protocol allows to follow a topic based diffusion of messages from publishers to subscribers in a decentralized way.\nThe messages can be sent and received by nodes running in a non intrusive way side by side to the Cardano node in order to enable inter-nodes communications.\nIn this way, we can significantly reduce the cost and effort required to build a decentralised network for message diffusion by using Cardano's established infrastructure, with limited impact on the performance and no impact on the security of the Cardano network.\nMany protocols in the Cardano ecosystem need the capability to diffuse messages in a decentralized manner. However, it is not possible to diffuse any type of message from Cardano block producers to a limited subset of subscribed peers. Nonetheless, the Cardano network has a proven efficient, reliable and secure infrastructure which is used to diffuse a transaction from one peer to all the other peers in the network. This infrastructure can be leveraged to achieve the goal of diffusing other types of messages with the guarantees offered by the Cardano network and a reduced development overhead.\nMithril is a protocol based on a Stake-based Threshold Multi-signature scheme which leverages the Cardano SPOs to certify data from the Cardano chain in a trustless way. Mithril is currently used in the Cardano ecosystem in order to enable fast bootstrapping of full nodes and enabling secure light wallets. The Mithril protocol coordinates the collection of individual signatures originating from the signers (run by SPOs) by the aggregators which combine them into Mithril multi-signatures and certificates. In order to be fully decentralized, the protocol needs to rely on a decentralized peer to peer network which, if built from the ground up, would require significant efforts and investment. Furthermore, the majority of SPO's, as the representatives of Cardano's active stake, will have to adopt and operate Mithril nodes alongside their Cardano node. Thus a natural solution is to use the Cardano network layer to significantly facilitate the development of the Mithril protocol without a significant impact on the Cardano network or high maintenance efforts for the SPOs. Mithril will be a fundamental first user of the proposed Decentralized Message Queue and it will be used as an illustrative example throughout this document.\nOther protocols in the Cardano ecosystem, such as Leios and Peras (and probably other protocols in the future), also need the capability to diffuse messages originating from block producers in a decentralized fashion. However, in the Leios and Peras cases, the Cardano node itself is a producer and consumer of these messages. We have taken into consideration this need for a generic solution in the design proposed.\nThe proposed solution is described in detail below.\nThis specification proposes to create 3 new mini-protocols in the Cardano network layer:\n3\nnode-2-node: Message Submission mini-protocol: Diffusion of the messages on the Cardano network.\nnode-2-node\nMessage Submission mini-protocol: Diffusion of the messages on the Cardano network.\nnode-2-client: Local Message Submission mini-protocol: Local submission of a message to be diffused by the Cardano network. Local Message Notification mini-protocol: Local notification of a message received from the Cardano network.\nnode-2-client\nLocal Message Submission mini-protocol: Local submission of a message to be diffused by the Cardano network.\nLocal Message Notification mini-protocol: Local notification of a message received from the Cardano network.\n[!NOTE] The terms Message producer, Message consumer and Network node may represent different entities depending on the concrete implementation made for a specific protocol:\nthe Network node could be either the Cardano node itself or a Decentralized Message Queue node or DMQ node implementing the mini-protocols described in this document. Opting in for one of these possibilities will depend on a careful analysis of the impact on the security of the Cardano node, impact on the load of the Cardano network, the specific network topology needed by the protocol and the needed level of coupling with the Cardano node itself (access to ledger, consensus, ...). It's worth mentioning that each protocol will implement its own version of the Network node by leveraging a common implementation of the mini-protocols.\nthe message Message producer and Message consumer could be either the Cardano node itself or another node able to interact with the Network node through the node-to-client mini-protocols detailed in this document.\nHere is a summary of the meanings of these terms depending on the protocol:\nThe node to node message submission protocol is used to transfer messages between full nodes. It follows a pull-based strategy where the inbound side asks for new messages and the outbound side returns them back. This protocol is designed to guard both sides against resource consumption attacks from the other side in a trustless setting.\n[!NOTE] There exists a local message submission protocol which is used when the server trusts a local client as described in the following section.\nstateDiagram-v2 classDef White fill:white,stroke:white classDef Black fill:white,stroke:black classDef Blue fill:white,stroke:blue classDef Green fill:white,stroke:green start:::White --> StInit:::Green StInit:::Green --> StIdle:::Blue : MsgInit StIdle:::Blue --> StMessageIdsBlocking:::Green : MsgRequestMessageIdsBlocking StMessageIdsBlocking:::Green --> StIdle:::Blue : MsgReplyMessageIds StIdle:::Blue --> StMessageIdsNonBlocking:::Green : MsgRequestMessageIdsNonBlocking StMessageIdsNonBlocking:::Green --> StIdle:::Blue : MsgReplyMessageIds StMessageIdsBlocking:::Green --> StDone:::Black : MsgDone StIdle:::Blue --> StMessages:::Green : MsgRequestMessages StMessages:::Green --> StIdle:::Blue : MsgReplyMessages\nstateDiagram-v2 classDef White fill:white,stroke:white classDef Black fill:white,stroke:black classDef Blue fill:white,stroke:blue classDef Green fill:white,stroke:green start:::White --> StInit:::Green StInit:::Green --> StIdle:::Blue : MsgInit StIdle:::Blue --> StMessageIdsBlocking:::Green : MsgRequestMessageIdsBlocking StMessageIdsBlocking:::Green --> StIdle:::Blue : MsgReplyMessageIds StIdle:::Blue --> StMessageIdsNonBlocking:::Green : MsgRequestMessageIdsNonBlocking StMessageIdsNonBlocking:::Green --> StIdle:::Blue : MsgReplyMessageIds StMessageIdsBlocking:::Green --> StDone:::Black : MsgDone StIdle:::Blue --> StMessages:::Green : MsgRequestMessages StMessages:::Green --> StIdle:::Blue : MsgReplyMessages\nMsgInit: Initial message of the protocol.\nMsgRequestMessageIdsNonBlocking(ack,req): The inbound side asks for new message ids and acknowledges old ids. The outbound side immediately replies (possible with an empty list).\nMsgRequestMessageIdsBlocking(ack,req): The inbound side asks for new messages ids and acknowledges old ids. The outbound side will block until new messages are available.\nMsgReplyMessageIds([(id,size)]): The outbound side replies with a list of available messages. The list contains pairs of message ids and the corresponding size of the message in bytes. In the blocking case the reply is guaranteed to contain at least one message. In the non-blocking case, the reply may contain an empty list.\nMsgRequestMessages([id]): The inbound side requests messages by sending a list of message-ids.\nMsgReplyMessages([messages]): The outbound side replies with a list messages.\nMsgClientDone: The outbound side terminates the mini-protocol.\n[!NOTE] The StInit state is needed as it allows to start all outbound sides on the same side of the connection, which is needed as the information flows in the opposite direction with this special message submission mini-protocol. This is also the case with the tx-submission mini-protocol because information flows in the other direction than for headers with chain-sync mini-protocol or blocks with block-fetch mini-protocol.\nStInit\n1 2 messageSubmissionMessage 3 = msgInit 4 / msgRequestMessageIds 5 / msgReplyMessageIds 6 / msgRequestMessages 7 / msgReplyMessages 8 / msgDone 9 10 msgInit = [0] 11 msgRequestMessageIds = [1, isBlocking, messageCount, messageCount] 12 msgReplyMessageIds = [2, [ *messageIdAndSize ] ] 13 msgRequestMessages = [3, messageIdList ] 14 msgReplyMessages = [4, ] 15 msgDone = [5, ] 16 17 isBlocking = false / true 18 messageCount = word16 19 messageId = bstr 20 messageBody = bstr 21 messageIdAndSize = [ messageId, messageSizeInBytes ] 22 messageIdList = [ * messageId ] 23 messageList = [ * message ] 24 messageSizeInBytes = word32 25 kesSignature = bstr 26 operationalCertificate = bstr 27 blockNumber = word32 28 29 message = [ 30 messageId, 31 messageBody, 32 blockNumber, 33 kesSignature, 34 operationalCertificate 35 ] 30\n1 2 messageSubmissionMessage 3 = msgInit 4 / msgRequestMessageIds 5 / msgReplyMessageIds 6 / msgRequestMessages 7 / msgReplyMessages 8 / msgDone 9 10 msgInit = [0] 11 msgRequestMessageIds = [1, isBlocking, messageCount, messageCount] 12 msgReplyMessageIds = [2, [ *messageIdAndSize ] ] 13 msgRequestMessages = [3, messageIdList ] 14 msgReplyMessages = [4, ] 15 msgDone = [5, ] 16 17 isBlocking = false / true 18 messageCount = word16 19 messageId = bstr 20 messageBody = bstr 21 messageIdAndSize = [ messageId, messageSizeInBytes ] 22 messageIdList = [ * messageId ] 23 messageList = [ * message ] 24 messageSizeInBytes = word32 25 kesSignature = bstr 26 operationalCertificate = bstr 27 blockNumber = word32 28 29 message = [ 30 messageId, 31 messageBody, 32 blockNumber, 33 kesSignature, 34 operationalCertificate 35 ] 30\nThis mini-protocol is designed with two goals in mind:\ndiffuse messages with high efficiency\nprotect from asymmetric resource attacks from the message consumer against the message provider\nThe mini-protocol is based on two pull-based operations:\nthe message consumer asks for message ids,\nand uses these ids to request a batch of messages (which it has not received yet)\nThe outbound side is responsible for initiating the mini-protocol with a peer node, but the inbound side (i.e. the other node) is the one who asks for information.\nThe outbound side maintains a limited size FIFO queue of outstanding messages for each of the inbound sides it is connected to, so does the inbound side with a mirror FIFO queue of message ids:\nthe inbound side asks for the next message ids and acknowledges for the previous message ids received (and removed from its queue).\nthe outbound side removes the acknowledged ids from the FIFO queue it maintains for the inbound side.\nthe inbound side can download the content of the messages by giving an unordered list of ids to the outbound side.\nthe outbound side reply omits any message that may have become invalid in the meantime.\nThe protocol supports blocking and non-blocking requests:\nthe outbound side must reply immediately to a non-blocking request.\nthe inbound side must wait until the outbound side has at least one message available.\nif the current queue of the inbound side is empty, it must use a blocking request and a non-blocking request otherwise.\nblocking request must be done if and only if the buffer of unacknowledged ids is empty (this also means that one cannot do a non-blocking request if the unacknowledged ids buffer is empty).\none cannot request 0 ids either through a blocking or a non-blocking request.\n0\nit is a protocol error to send a message which id wasn't requested.\nIn order to bound the resource requirements needed to store the messages in a network node, their lifetime should be limited. A time to live can be set as a protocol parameter for each topic, and once the timespan has elapsed the message is discarded in the internal state of the network node. The time to live can be based on the timestamp of reception of the message on the network node or on the block number embedded in the message.\n[!NOTE] Computations are based on the assumption of a 30 minutes TTL for messages and are assuming that the messages are stored once in the memory of the network node (i.e. the aforementioned FIFO queues store reference to the messages).\nFor a total of 3,100 Cardano SPOs on the mainnet, on an average 50 of them will be eligible to send signatures (i.e. will win at least one lottery in the Mithril protocol). This means that if the full Cardano stake distribution is involved in the Mithril protocol, only 1,550 signers will send signatures at each round:\nmainnet\nthe maximum number of valid messages stored by a node at any given time is:\nthe maximum extra memory for the valid messages stored by a node at any given time is:\nThe message body is signed with the KES key of the SPO. This signature and the operational certificate of the SPO are appended to the message which is diffused.\nBefore being diffused to other peers, an incoming message must be verified by the receiving node. This is done with the following steps:\nVerify that the operational certificate is valid by checking that the KES verification key is signed by cold secret key.\nVerify the KES signature of the message body with the KES verification key from the operational certificate.\nCompute the SPO pool id by hashing the cold verification key from the operational certificate. Make sure that this pool id is part of the stake distribution (the network layer will need to have access to this information).\nVerify that the announced id of the message is verified upon reception.\nIf any of these step fails, the message is considered as invalid, which is a protocol violation.\n[!WARNING] We also probably need to make sure that the KES key used to sign is from the latest rotation:\neither the last seen opcert number in the block headers of the chain.\nor the last seen opcert number from a previous message diffused.\nor the last opcert number recorded in the Mithril signer registration.\nIf the opcert number received is strictly lower than the previous one which has been seen, it should be considered as a protocol violation.\n[!NOTE] Computations are based on the assumption of a 2 ms KES signature verification time on a virtual CPU, which may vary depending on the infrastructure.\nFor a total of 3,100 Cardano SPOs on the mainnet, on an average 50 of them will be eligible to send signatures (i.e. will win at least one lottery in the Mithril protocol). This means that if the full Cardano stake distribution is involved in the Mithril protocol, only 1,550 signers will send signatures at each round:\nmainnet\nthe number of messages received by a node which need to be verified is:\nthe extra CPU time for the verification of messages based on the aforementioned volume of messages received is:\n[!NOTE] The below computations of the network throughput and volume apply a multiplicative factor of 2 to the number of messages transmitted to reflect the redundancy of the diffusion mechanism.\n[!WARNING] Some compression can be applied to the Mithril signatures which allows them to always be on the lower bound size, but it is not implemented yet.\nThe following tables gather figures about expected network load in the case of Mithril using the mini-protocol to diffuse the individual signatures:\nFor a total of 3,100 Cardano SPOs on the mainnet, on an average 50 of them will be eligible to send signatures (i.e. will win at least one lottery in the Mithril protocol). This means that if the full Cardano stake distribution is involved in the Mithril protocol, only 1,550 signers will send signatures at each round:\nmainnet\nthe network outbound throughput of a peer is:\nthe network outbound volume of a peer is:\n[!NOTE]\nThese data apply to cloud providers which bill the traffic on the volume, not the bandwidth.\nSome cloud providers offer a free tier for the first 100GB of traffic which is not taken into consideration here for simplicity.\nFor a total of 3,000 SPOs sending messages, the extra networking cost incurred for a Cardano full node is:\n3,000\nIn this attack, a malicious sender would attempt to create multiple identities impersonating SPOs. This attack is completely mitigated by the Message authentication mechanism as only active SPO on the Cardano chain can be authenticated and send messages. This would be considered as a protocol violation and the malicous peer would be disconnected.\nIn this attack, a malicious SPO would send different messages to different peers. This attack needs to be handled by the receiver of the message as the network layer does not verify the content of the message body by design.\nIn the specific case of Mithril, the individual signature is unique so there will be two cases:\nthe message embeds a valid signature and it will be accepted by the receiving Mithril aggregator.\nthe message embeds an invalid signature and it will be rejected by the receiving Mithril aggregator.\nIn this attack, a malicous SPO would try to flood the network by sending many messages at once. In that case, the network layer could detect that the throughput of messages originating from a SPO is above a threshold and consider it as a protocol violation, thus disconnecting the malicous peer. If a peer asks for N messages and receives more than N messages, then it would also be considered as a protocol violation. Also, the way mini-protocols are implemented allows to set a maximum message size.\nA standalone network node will use its own handshake. It can introduce its own protocol parameters, but quite likely it will start with NodeToNodeVersionData:\nhandshake\nNodeToNodeVersionData\ndata NodeToNodeVersionData = NodeToNodeVersionData { networkMagic :: !NetworkMagic , diffusionMode :: !DiffusionMode , peerSharing :: !PeerSharing , query :: !Bool } deriving (Show, Typeable, Eq)\ndata NodeToNodeVersionData = NodeToNodeVersionData { networkMagic :: !NetworkMagic , diffusionMode :: !DiffusionMode , peerSharing :: !PeerSharing , query :: !Bool } deriving (Show, Typeable, Eq)\nnetworkMagic: this is used for debugging purpose and to make sure the network node runs on the right network (it should be different than the existing networkMagics).\nnetworkMagic\nnetworkMagic\ndiffusionMode: this would be useful if there are initiator-only nodes, e.g. a network node running next to an edge node (a wallet).\ndiffusionMode\npeerSharing: this will be useful to implement peer sharing in the side network if this is needed.\npeerSharing\nquery: this is useful for tools like cardano-cli ping.\nquery\ncardano-cli ping\nThe local message submission mini-protocol is used by local clients to submit message to a local network node. This mini-protocol is not used to diffuse messages from a network node to another.\nThe protocol follows a simple request-response pattern:\nThe client sends a request with a single message. The server either accepts the message (and returns a confirmation) or rejects it (and returns the reason)\nThe client sends a request with a single message.\nThe server either accepts the message (and returns a confirmation) or rejects it (and returns the reason)\nstateDiagram-v2 classDef White fill:white,stroke:white classDef Black fill:white,stroke:black classDef Blue fill:white,stroke:blue classDef Green fill:white,stroke:green start:::White --> StIdle:::Blue; StIdle:::Blue --> StBusy:::Green : MsgSubmitMessage StBusy:::Green --> StIdle:::Blue : MsgAcceptMessage StBusy:::Green --> StIdle:::Blue : MsgRejectMessage StIdle:::Blue --> StDone:::Black : MsgDone\nstateDiagram-v2 classDef White fill:white,stroke:white classDef Black fill:white,stroke:black classDef Blue fill:white,stroke:blue classDef Green fill:white,stroke:green start:::White --> StIdle:::Blue; StIdle:::Blue --> StBusy:::Green : MsgSubmitMessage StBusy:::Green --> StIdle:::Blue : MsgAcceptMessage StBusy:::Green --> StIdle:::Blue : MsgRejectMessage StIdle:::Blue --> StDone:::Black : MsgDone\nMsgSubmitMessage(message): The client submits a message.\nMsgAcceptMessage: The server accepts the message.\nMsgRejectMessage(reason): The server rejects the message and replies with a reason.\nMsgDone: The client terminates the mini-protocol.\nThe local message notification mini-protocol is used by local clients to be notified about new message received by the network node.\nThe protocol follows a simple request-response pattern:\nThe client sends a request with a single message. The server either accepts the message (and returns a confirmation) or rejects it (and returns the reason)\nThe client sends a request with a single message.\nThe server either accepts the message (and returns a confirmation) or rejects it (and returns the reason)\nstateDiagram-v2 classDef White fill:white,stroke:white classDef Black fill:white,stroke:black classDef Blue fill:white,stroke:blue classDef Green fill:white,stroke:green start:::White --> StIdle:::Blue; StIdle:::Blue --> StBusy:::Green : MsgNextMessage StBusy:::Green --> StIdle:::Blue : MsgHasMessage StBusy:::Green --> StIdle:::Blue : MsgTimeoutMessage StIdle:::Blue --> StDone:::Black : MsgDone\nstateDiagram-v2 classDef White fill:white,stroke:white classDef Black fill:white,stroke:black classDef Blue fill:white,stroke:blue classDef Green fill:white,stroke:green start:::White --> StIdle:::Blue; StIdle:::Blue --> StBusy:::Green : MsgNextMessage StBusy:::Green --> StIdle:::Blue : MsgHasMessage StBusy:::Green --> StIdle:::Blue : MsgTimeoutMessage StIdle:::Blue --> StDone:::Black : MsgDone\nMsgNextMessage: The client asks for the next message.\nMsgHasMessage(message): The server has received a message.\nMsgTimeoutMessage: The server has not received a message and times out.\nMsgDone: The client terminates the mini-protocol.\nMithril requires strong network foundations to support interactions between its various nodes: Mithril needs to exist in a decentralized context where multiple aggregators can operate seamlessly and independently. Mithril needs participation of all or nearly all of the Cardano network SPOs to provide maximal security to the multi-signatures embedded in the certificates. Creating a separate network would entail significant costs and efforts (there are more than 3,000 SPOs which would need to be connected with resilient and secure network, and much more passive nodes). Cardano SPOs need to be vigilant about what other applications run on their block producers and relay nodes. While a separate p2p network could be created, incoming connections must be treated carefully and all the same DoS considerations as with Cardano would need to apply. By standardizing the message diffusion of Mithril in the same way as the Cardano protocol stack, the additional risk of operating Mithril is greatly reduced. The Cardano network is very efficient for diffusion (e.g. broadcasting) which is precisely what is needed for Mithril. Mithril signer node needs to run on the same machine as the Cardano block producing node (to access the KES keys). It makes sense to use the same network layer, which will also facilitate a high level of participation.\nMithril requires strong network foundations to support interactions between its various nodes:\nMithril needs to exist in a decentralized context where multiple aggregators can operate seamlessly and independently.\nMithril needs participation of all or nearly all of the Cardano network SPOs to provide maximal security to the multi-signatures embedded in the certificates.\nCreating a separate network would entail significant costs and efforts (there are more than 3,000 SPOs which would need to be connected with resilient and secure network, and much more passive nodes).\nCardano SPOs need to be vigilant about what other applications run on their block producers and relay nodes. While a separate p2p network could be created, incoming connections must be treated carefully and all the same DoS considerations as with Cardano would need to apply. By standardizing the message diffusion of Mithril in the same way as the Cardano protocol stack, the additional risk of operating Mithril is greatly reduced.\nThe Cardano network is very efficient for diffusion (e.g. broadcasting) which is precisely what is needed for Mithril.\nMithril signer node needs to run on the same machine as the Cardano block producing node (to access the KES keys). It makes sense to use the same network layer, which will also facilitate a high level of participation.\nWhy would it be great for Cardano to support a decentralized message queue with its network? This is a required feature to make the Cardano ecosystem scalable. The design is versatile enough to support present and future use cases.\nWhy would it be great for Cardano to support a decentralized message queue with its network?\nThis is a required feature to make the Cardano ecosystem scalable.\nThe design is versatile enough to support present and future use cases.\nIn this section, we propose an architecture for Cardano and Mithril. Note, in this section, Mithril is used as a placeholder for a possible Mithril application: we've seen many ideas about how Mithril can be used in Cardano and all of them can follow this design.\nCardano\nMithril\nMithril\nMithril\nMithril\nCardano\nAny software included in cardano-node needs to undergo a rigorous development and review process to avoid catastrophic events. We think that merging Cardano, a mature software, with new technologies, should be a process, and thus we propose first to develop Decentralized Message Queue (DMQ) nodes as standalone processes which communicate with cardano-node via its local interface (node-to-client protocol). As we will see, this approach has advantages for the Mithril network and SPOs.\ncardano-node\nCardano\nDecentralized Message Queue (DMQ)\ncardano-node\nnode-to-client\nMithril\nOuroboros-Network (ON) is, to a large extent, an agnostic network stack, which can be adapted to be used by both Cardano and DMQ nodes. To construct an overlay network, the stack needs to access stake pool information registered on chain. This can be done via the node-to-client protocol over a Unix socket. Since DMQ nodes will have their own end-points, we'll either need to propose a modification to the SPO registration process, which includes Mithril instantiated DMQ nodes, or we could pass incoming Mithril connections from cardano-node to the DMQ node via CMSG_DATA.\nOuroboros-Network\nCardano\nDMQ\nnode-to-client\nDMQ\nMithril\nDMQ\nMithril\ncardano-node\nDMQ\nOuroboros-Network outbound governor component, which is responsible for creating the overlay network has a built-in mechanism which churns connections based on a defined metric. By developing a standalone Mithril network node, we can design metrics specific for the purpose. This way, the Mithril network will optimise for its benefits rather than being stuck in a suboptimal solution from its perspective - if Mithril and Cardano were tied more strongly (e.g. as part cardano-node), then we wouldn't have that opportunity because Cardano must meet its deadlines, or the security of the system as a whole must be at stake.\nOuroboros-Network\nMithril network\nMithril\nMithril\nCardano\ncardano-node\nCardano\nEventually, there will be many Mithril applications, and all of them might have different network requirements and thus might require slightly different configurations. We SHOULD aim to build something that can be used as a scaffolding for such applications. This may also open future avenues for delivering new functionalities that fit together well with existing infrastructure while still mitigating the risk of catastrophic events at the core of the system.\nMithril\nPlease also note that any protocols and their instances that will be built as part of the standalone DMQ node could be reused in future for Peras and Leios. It will give us even more confidence that the future core system will be built from components that are used in production.\nDMQ\nPeras\nLeios\nCardano and DMQ nodes, as separate executables, can still be packaged together, lowering the barrier of participation. When run separately, the SPO is also in better control of the resources dedicated to each process - this is important for the health of both systems.\nCardano\nDMQ\nAnother benefit of such a design is that DMQ node can be developed on its own pace, not affected by significant changes in other parts of the system like ledger - the Cardano Core team restrains itself to not publishing new cardano-node versions with significant changes across many of its subsystems - just for the sake of clarity of where to look if a bug is found.\nDMQ\ncardano-node\nIn this design, a DMQ node runs alongside a cardano-node, which is connected to it via a UNIX socket (node-to-client) protocol. This means an SPO will run a DMQ node instantiated for Mithril per cardano-node. Future protocols will run their custom instantiated new DMQ node instance. The DMQ node connected to BP can also have access to necessary keys for signing purposes. DMQ node might also use the KES agent, as cardano-node will in the near future, to securely access the KES key.\nDMQ\ncardano-node\nnode-to-client\nDMQ\nMithril\ncardano-node\nDMQ\nDMQ\nDMQ\ncardano-node\nA Cardano node implementing the previously described mini-protocols is released for production. A message producer node (e.g. a Mithril signer) publishing messages to the local Cardano node through mini-protocols is released. A message subscriber node (e.g. a Mithril aggregator) receiving messages from the local Cardano node is released.\nA Cardano node implementing the previously described mini-protocols is released for production.\nA message producer node (e.g. a Mithril signer) publishing messages to the local Cardano node through mini-protocols is released.\nA message subscriber node (e.g. a Mithril aggregator) receiving messages from the local Cardano node is released.\n[!WARNING] A hard-fork of the Cardano chain may be required if some information, like peer ports for an overlay network, are to be registered by the SPOs on-chain.\nWrite a \"formal\" specification of the protocols along with vectors/conformance checker for protocol's structure and state machine logic.\nWrite an architecture document extending this CIP with more technical details about the implementation.\nValidate protocol behaviour with all relevant parties (Network and Node teams).\nValidate performance profile and impact on Cardano network.\nImplement the Cardano n2n and n2c mini-protocols in the Cardano node.\nImplement the Cardano n2c mini-protocols in Mithril signer and aggregator nodes.\nThe Shelley Networking Protocol: https://ouroboros-network.cardano.intersectmbo.org/pdfs/network-spec/network-spec.pdf\nIntroduction to the design of the Data Diffusion and Networking for Cardano Shelley: https://ouroboros-network.cardano.intersectmbo.org/pdfs/network-design/network-design.pdf\nMithril: Stake-based Threshold Multisignatures: https://iohk.io/en/research/library/papers/mithril-stake-based-threshold-multisignatures/\nMithril Network Architecture: https://mithril.network/doc/mithril/mithril-network/architecture\nMithril Protocol in depth: https://mithril.network/doc/mithril/mithril-protocol/protocol\nMithril Certificate Chain in depth: https://mithril.network/doc/mithril/mithril-protocol/certificates\nFast Bootstrap a Cardano node: https://mithril.network/doc/manual/getting-started/bootstrap-cardano-node\nRun a Mithril Signer node (SPO): https://mithril.network/doc/manual/getting-started/run-signer-node/\nMithril Threat Model: https://mithril.network/doc/mithril/threat-model\nThis CIP is licensed under Apache-2.0\n2023 Cardano Foundation\n\n---\n\nCIP-0138 | Plutus Core Builtin Type - `Array`\n\nWe propose an array builtin type for Plutus Core. This type will have constant-time lookup, which is a useful feature that is otherwise not possible to achieve.\nThe first part of CPS-0013 outlines in great detail the motivation for introducing this new builtin type.\nTo summarize, it is currently not possible to write a data structure with constant-time lookup in Plutus Core. We propose to solve this problem by introducing an array type into Plutus Core's builtin language, as it is the standard example of a data structure with this property, and it is a key component of many classical algorithms and data structures.\nAccess to an array type would provide significant performance improvement opportunities to users of Plutus Core, since currently they must rely on suboptimal data structures such as lists for looking up elements inside a collection.\nWe add the following builtin type: Array of kind Type - Type representing a one-dimensional array type with indices of type Integer. Elements are indexed consecutively starting from 0.\nArray\nType -> Type\nInteger\n0\nNote: Here Type is the universe of all builtin types, since we do not consider types formed out of applying builtin types to arbitrary types to be inhabited.\nType\nThe Array builtin type should be implemented with a fixed-size immutable array structure that has constant-time lookup.\nArray\nWe add the following builtin functions:\nindexArray of type forall a . Integer - Array a - a. It returns the element at the given index in the array, or fails with an error if the index is outside the bounds of the array. It uses constant time and constant memory. lengthOfArray of type forall a . Array a - Integer It returns the length of the array. It uses constant time and constant memory. listToArray of type forall a . List a - Array a It converts the argument builtin list into a builtin array. It uses linear time and linear memory.\nindexArray of type forall a . Integer - Array a - a. It returns the element at the given index in the array, or fails with an error if the index is outside the bounds of the array. It uses constant time and constant memory.\nindexArray\nforall a . Integer -> Array a -> a\nIt returns the element at the given index in the array, or fails with an error if the index is outside the bounds of the array.\nIt uses constant time and constant memory.\nlengthOfArray of type forall a . Array a - Integer It returns the length of the array. It uses constant time and constant memory.\nlengthOfArray\nforall a . Array a -> Integer\nIt returns the length of the array.\nIt uses constant time and constant memory.\nlistToArray of type forall a . List a - Array a It converts the argument builtin list into a builtin array. It uses linear time and linear memory.\nlistToArray\nforall a . List a -> Array a\nIt converts the argument builtin list into a builtin array.\nIt uses linear time and linear memory.\nAs with all Plutus Core builtin types, arrays must have a fixed binary representation.\nFor arrays we have chosen the following representation, based on the one currently implemented in the flat encoding for the Haskell Array type.\nArray\nWe define the result of encoding a Plutus Core array of type Array a and with length n, as follows:\nArray a\nn\nLet: Ea be an encoding function for a, i.e. it is of type a - ByteString. Ei be an encoding function for Integer ++ be bytestring concatenation bin(i) be the binary representation for any number i.\nEa be an encoding function for a, i.e. it is of type a - ByteString.\nEa\na\na -> ByteString\nEi be an encoding function for Integer\nEi\nInteger\n++ be bytestring concatenation\n++\nbin(i) be the binary representation for any number i.\nbin(i)\ni\nThe first bits in the resulting sequence will be Ei(0) ++ Ei(n-1), that is the encoding of the first index followed by the encoding of the last.\nEi(0) ++ Ei(n-1)\nAs binary, arrays are split into blocks of 255 elements. For each block, the first bits in the block encoding represent the number of elements in the block. This can be decided as follows: there will be n div 255 blocks where the next bits are bin(255) and, if n mod 255 0, one final block where the next bits are bin(n mod 255).\n255\nthere will be n div 255 blocks where the next bits are bin(255)\nn div 255\nbin(255)\nand, if n mod 255 0, one final block where the next bits are bin(n mod 255).\nn mod 255 > 0\nbin(n mod 255)\nInside each block, the next bits come from encoding all the elements pertaining to the block: we will refer to each block as b0 ... bm where m+1 is the number of blocks; indexArray is a function with the same behaviour as the proposed indexArray builtin, since it is fixed, we omit the array argument for brevity; for the block bi, the bit sequence will be: Ea(indexArray(i*255)) ++ Ea(indexArray(i*255 + 1)) ++ ... ++ Ea(indexArray(i*255 + m)).\nwe will refer to each block as b0 ... bm where m+1 is the number of blocks;\nb0 ... bm\nm+1\nindexArray is a function with the same behaviour as the proposed indexArray builtin, since it is fixed, we omit the array argument for brevity;\nindexArray\nindexArray\nfor the block bi, the bit sequence will be: Ea(indexArray(i*255)) ++ Ea(indexArray(i*255 + 1)) ++ ... ++ Ea(indexArray(i*255 + m)).\nbi\nEa(indexArray(i*255)) ++ Ea(indexArray(i*255 + 1)) ++ ... ++ Ea(indexArray(i*255 + m))\nAfter all blocks have been encoded, the resulting binary sequence is appended with exactly 8 bits of value 0.\n8\n0\nRemarks:\nEmpty arrays are represented as: Ei(0) ++ Ei(-1) ++ 00000000. This follows from the encoding described above, as n is 0 and there are no elements to encode.\nEi(0) ++ Ei(-1) ++ 00000000\nn\n0\nThe decoding of arrays is, of course, the inverse of the encoding function.\n------------ -- Example 1 >>> encode [1] as an array: 00000000 00000000 00000001 00000010 00000000 -- the first and last indices are 0, the number of elements is 1 -- 00000010 is the encoding of 1 of type Integer -- the bytestring ends with 8 bits of value 0 ------------ -- Example 2 >>> encode [1, 1, 1] as an array: 00000000 00000100 00000011 00000010 00000010 00000010 00000000 -- ^ 0 ^ 2 ^ 3 ^ 1 ^ 1 ^ 1 ------------ -- Example 3 >>> encode [11, 22, 33, 44] as an array: 00000000 00000110 00000100 00010110 00101100 01000010 01011000 00000000 -- ^ 0 ^ 3 ^ 4 ^ 11 ^ 22 ^ 33 ^ 44 ------------ -- Example 4 >>> encode [True, True, True] as an array: 00000000 00000100 00000011 11100000 000 -- True is encoded as a single bit of value 1 -- notice how the eight final zeros are split between multiple bytes ------------ -- Example 5 >>> encode [True ... True] (255 times) 00000000 11111100 00000011 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111110 0000000 -- 11111100 00000011 is the representation of 254, the last index -- 11111111 is the total number of elements in the block, 255 -- True is encoded to 1, so the bytes get filled except the last one, since 255 does not divide by 8 -- there are again 8 final zeros split between the two last bytes ------------ -- Example 6 >>> encode [True ... True] (256 times) 00000000 11111110 00000011 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111110 00000011 00000000 -- again, since True does not encode to full bytes, the above example can be tricky to decode, but is nevertheless correct -- 0 0000001 is the beginning of the second block, it represents the number of elements in the block which is 1 -- immediately after, there is 1 bit which is the encoding of the single element of this block -- and finally, the last 8 zeros\n------------ -- Example 1 >>> encode [1] as an array: 00000000 00000000 00000001 00000010 00000000 -- the first and last indices are 0, the number of elements is 1 -- 00000010 is the encoding of 1 of type Integer -- the bytestring ends with 8 bits of value 0 ------------ -- Example 2 >>> encode [1, 1, 1] as an array: 00000000 00000100 00000011 00000010 00000010 00000010 00000000 -- ^ 0 ^ 2 ^ 3 ^ 1 ^ 1 ^ 1 ------------ -- Example 3 >>> encode [11, 22, 33, 44] as an array: 00000000 00000110 00000100 00010110 00101100 01000010 01011000 00000000 -- ^ 0 ^ 3 ^ 4 ^ 11 ^ 22 ^ 33 ^ 44 ------------ -- Example 4 >>> encode [True, True, True] as an array: 00000000 00000100 00000011 11100000 000 -- True is encoded as a single bit of value 1 -- notice how the eight final zeros are split between multiple bytes ------------ -- Example 5 >>> encode [True ... True] (255 times) 00000000 11111100 00000011 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111110 0000000 -- 11111100 00000011 is the representation of 254, the last index -- 11111111 is the total number of elements in the block, 255 -- True is encoded to 1, so the bytes get filled except the last one, since 255 does not divide by 8 -- there are again 8 final zeros split between the two last bytes ------------ -- Example 6 >>> encode [True ... True] (256 times) 00000000 11111110 00000011 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111111 11111110 00000011 00000000 -- again, since True does not encode to full bytes, the above example can be tricky to decode, but is nevertheless correct -- 0 0000001 is the beginning of the second block, it represents the number of elements in the block which is 1 -- immediately after, there is 1 bit which is the encoding of the single element of this block -- and finally, the last 8 zeros\nThe following section presents the reasoning behind the above specification of a Plutus core builtin array type.\nIt is important to mention that we based our decisions on the desire to keep the builtin language as small as possible, i.e. to not introduce types or functions which are not essential for definitional purposes or essential for providing a practical interface to users.\nIt also discusses some alternatives or additions which should be considered as part of the preliminary investigation.\nThe indexArray builtin function is necessary, since access to constant-time lookup is the main requirement outlined in this CIP. However, there remains the question of how users should deal with the function's partiality. We considered the following options:\nindexArray\nIntroduce another new builtin type, one which implements Maybe semantics. The type signature for indexArray would become forall a . Integer - Array a - Maybe a and it would return Nothing for out-of-bounds lookups. The obvious disadvantage is the necessity of adding another new builtin type (there is no Maybe builtin type in Plutus Core), which would further increase the complexity of the builtin language. Another disadvantage would be that this solution is the most costly: users will incur additional costs in deconstructing the returned Maybe. Failed lookups return a default value provided by the caller: indexArray :: forall a . Integer - a - Array a - a. This solution is problematic whenever there is no sensible default value and the user wants the function to fail. Since Plutus Core is strict, it is not possible to pass error as the default value without it getting evaluated before the call and terminating execution immediately. As of the time of writing, builtin functions cannot be higher-order. However, that is subject to change in the near future when pattern matching builtins will be supported by Plutus Core. This feature would allow a safe version of indexArray with type forall a . Integer - (() - a) - Array a - a to be expressible in the builtins language. Include lengthOfArray and require the user to perform appropriate length guards before calling indexArray. This is a familiar option from other languages. By definition arrays are fixed-size and their length is available in constant time. It also allows users to omit bounds checks when they know a priori that the index is in bounds.\nIntroduce another new builtin type, one which implements Maybe semantics. The type signature for indexArray would become forall a . Integer - Array a - Maybe a and it would return Nothing for out-of-bounds lookups. The obvious disadvantage is the necessity of adding another new builtin type (there is no Maybe builtin type in Plutus Core), which would further increase the complexity of the builtin language. Another disadvantage would be that this solution is the most costly: users will incur additional costs in deconstructing the returned Maybe.\nIntroduce another new builtin type, one which implements Maybe semantics. The type signature for indexArray would become forall a . Integer - Array a - Maybe a and it would return Nothing for out-of-bounds lookups.\nMaybe\nindexArray\nforall a . Integer -> Array a -> Maybe a\nNothing\nThe obvious disadvantage is the necessity of adding another new builtin type (there is no Maybe builtin type in Plutus Core), which would further increase the complexity of the builtin language.\nMaybe\nAnother disadvantage would be that this solution is the most costly: users will incur additional costs in deconstructing the returned Maybe.\nMaybe\nFailed lookups return a default value provided by the caller: indexArray :: forall a . Integer - a - Array a - a. This solution is problematic whenever there is no sensible default value and the user wants the function to fail. Since Plutus Core is strict, it is not possible to pass error as the default value without it getting evaluated before the call and terminating execution immediately. As of the time of writing, builtin functions cannot be higher-order. However, that is subject to change in the near future when pattern matching builtins will be supported by Plutus Core. This feature would allow a safe version of indexArray with type forall a . Integer - (() - a) - Array a - a to be expressible in the builtins language.\nFailed lookups return a default value provided by the caller: indexArray :: forall a . Integer - a - Array a - a.\nindexArray :: forall a . Integer -> a -> Array a -> a\nThis solution is problematic whenever there is no sensible default value and the user wants the function to fail. Since Plutus Core is strict, it is not possible to pass error as the default value without it getting evaluated before the call and terminating execution immediately.\nerror\nAs of the time of writing, builtin functions cannot be higher-order. However, that is subject to change in the near future when pattern matching builtins will be supported by Plutus Core. This feature would allow a safe version of indexArray with type forall a . Integer - (() - a) - Array a - a to be expressible in the builtins language.\nindexArray\nforall a . Integer -> (() -> a) -> Array a -> a\nInclude lengthOfArray and require the user to perform appropriate length guards before calling indexArray. This is a familiar option from other languages. By definition arrays are fixed-size and their length is available in constant time. It also allows users to omit bounds checks when they know a priori that the index is in bounds.\nInclude lengthOfArray and require the user to perform appropriate length guards before calling indexArray.\nlengthOfArray\nindexArray\nThis is a familiar option from other languages.\nBy definition arrays are fixed-size and their length is available in constant time.\nIt also allows users to omit bounds checks when they know a priori that the index is in bounds.\nAfter considering these three options, we concluded that the addition of the lengthOfArray builtin function is both necessary and sufficient for introducing a well-defined array type into the builtin language.\nlengthOfArray\nThe last required functionality for having a practical interface is the ability to construct arrays.\nDue to our decision of providing immutable arrays, it is difficult (or very expensive) to build up arrays incrementally. A naive approach would require repeated copying and potentially the usage of quadratic space and time.\nA more appropriate approach would be to construct the array in bulk, however that would require an intermediate representation of a collection of elements. Fortunately this already exists in the builtin language in the form of builtin lists. We can then naturally introduce a function which transforms lists into arrays: listToArray.\nlistToArray\nMany array-like data structures support cheap slices, e.g. a function with the following type signature: slice :: Integer - Integer - Array a - Array a, which produces a view of the original array between the two indices (similarly, the same can be achieved using an indexArray and a lengthOfArray).\nslice :: Integer -> Integer -> Array a -> Array a\nindexArray\nlengthOfArray\nWe do not propose to add slice to our builtin set, since it is very easy to build a data structure that supports slices on top of array, simply by tracking some additional integers to track the subset of the array that is in view.\nslice\narray\nData\nIn CPS-0013 the idea of representing the arguments to the Data constructor Constr as an Array in Plutus Core was presented as being more appropriate than the current list representation.\nData\nConstr\nArray\nA significant requirement in implementing this modification is maintaining backwards compatibility. Therefore, we cannot simply modify the internal representation of Data.\nData\nOne idea would be to add a new builtin such as the following: unConstrDataArray :: Data - (Integer, Array Data). However, this builtin will inevitably have linear time complexity since it is based on a list traversal. So it does not actually solve the original problem, unless it can be shown experimentally that, in practice, these lists are usually small enough for the transformation to be negligible.\nunConstrDataArray :: Data -> (Integer, Array Data)\nThe feature is implemented according to the implementation plan and merged into the master branch of the plutus repository.\ncardano-ledger is updated to include new protocol parameters to control costing of the new builtins.\nThe feature is integrated into cardano-node and released as part of a hard fork.\nThe implementation of this CIP should not proceed without an empirical assessment of the effectiveness of the new primitives, as per the following plan:\nImplement the new primitives according to the specification, including the experimental versions discussed in the CIP. Assign a preliminary cost to the new builtin functions. Consider similar operations and their current costs. Create variants of the existing benchmarks and potentially add some more. From the total set of newly implemented builtins, find a minimal but practical set of primitives which are indeed significantly faster in both real-time performance and modelled costs. If such a set does not exist, find out why. This means that the preliminary investigation was not successful. If it does, revise the specification to include the final set of primitives.\nImplement the new primitives according to the specification, including the experimental versions discussed in the CIP.\nAssign a preliminary cost to the new builtin functions. Consider similar operations and their current costs.\nCreate variants of the existing benchmarks and potentially add some more.\nFrom the total set of newly implemented builtins, find a minimal but practical set of primitives which are indeed significantly faster in both real-time performance and modelled costs.\nIf such a set does not exist, find out why. This means that the preliminary investigation was not successful. If it does, revise the specification to include the final set of primitives.\nIf the preliminary performance investigation was not successful, this CIP should be revised according to the findings of the experiment. Otherwise, the implementation can proceed:\nDetermine the most appropriate costing functions for modelling the builtin's performance and assign costs accordingly. Add the new builtin type and functions to the appropriate sections in the Plutus Core Specification. Formalize the new builtin type and functions in the plutus-metatheory. The final version of the feature is ready to be merged into plutus and accepted by the Plutus Core team.\nDetermine the most appropriate costing functions for modelling the builtin's performance and assign costs accordingly.\nAdd the new builtin type and functions to the appropriate sections in the Plutus Core Specification.\nFormalize the new builtin type and functions in the plutus-metatheory.\nThe final version of the feature is ready to be merged into plutus and accepted by the Plutus Core team.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-0381 | Plutus support for Pairings over BLS12-381\n\nThis CIP proposes an extension of the current plutus functions to provide support for basic operations over BLS12-381 curve to the plutus language. We expose a candidate implementation, and describe clearly the benefits that this would bring. In a nutshell, pairing friendly curves will enable a large number of cryptographic primitives that will be essential for the scalability of Cardano.\nPairing Friendly Curves are a type of curves that provide the functionality of computing pairings. A pairing is a binary function that maps two points from two groups to a third element in a third target group. For a more in-depth introduction to pairings, we recommend reading Pairings for Beginners or Pairings for Cryptographers. For level of detail required in this document, it is sufficient to understand that a pairing is a map: e:G1 X G2 - GT, which satisfies the following properties:\nBilinearity: for all a,b in F *_q, for all P in G1, Q in G2: e(aP,bQ)=e(P,Q) (ab)\nNon-degeneracy: e != 1\nComputability: There exists an efficient algorithm to compute e\nwhere G1, G2 and GT are three distinct groups of order a prime q. Pairing computation is an expensive operation. However, they enable a range of interesting cryptographic primitives which can be used for scaling Cardano and many other use cases. We now provide a list of use cases of pairings as well as an estimated operation count to understand the number of 'expensive' operations that need to be computed for each of them (a preliminary benchmark can be found in Section 'Costing of function').\nSidechains are a crucial component for the scalability of Cardano, and its interoperability with other chains/tokens/smart contracts. However, sidechains need to periodically commit their state to the Cardano mainnet to provide the same security guarantees as the latter. This periodical commitment is performed through a threshold signature by the dynamic committee of the Sidechain. The most interesting construction for medium sized committees is ATMS, presented in the paper Proof of Stake Sidechains, in Section 5.3, which requires pairings. We have yet not found an efficient solution that does not require pairings. ATMS is a k-of-t threshold signature scheme (meaning that we need at least k signers to participate).\nZero Knowledge Proofs are an incredibly powerful tool. There exist different types of zero knowledge proofs, but the most succinct (and cheaper to verify) rely on pairings for verification. Zero knowledge proofs can be used to make Mithril certificates, or sidechain checkpoints even more succinct, or to create layer 2 solutions to provide scalability (by the means of ZK-Rollups, which are used to scale Ethereum, e.g. Loopring, zkSync or Aztec among others). Plonk verification is quite complex, and differs depending on the number of custom gates. Implementations may differ, and adding custom gates or blinders will affect these estimates. Pairing evaluations would not be affected, but scalar multiplications and additions over G1 will increase linearly with respect to the additional gates and blinders. In general, it is not expected that the number of custom gates is larger than a few dozens. In this section we expose the verifier complexity as described here (version last checked April 2022). We count every challenge computation as a single hash evaluation. We omit the scalar X scalar multiplications and additions.We assume that point validation is performed by the external C library, and therefore do not count that in this estimate. The computation is the following: 6 hash computations 1 vanishing polynomial (1 scalar exponentiation) Quotient polynomial evaluation (1 scalar inverse) First part of batch poly commitment (9 scalar mults and 9 additions in G1) Fully batched poly commitment (5 scalar mults and 5 additions over G1) Group encoded batch verification (1 scalar mult over G1) Batch-validate all evaluations (3 scalar mults and 4 additions over G1, plus two Miller loops + final verify)\nscalar X scalar\n6 hash computations\n1 vanishing polynomial (1 scalar exponentiation)\nQuotient polynomial evaluation (1 scalar inverse)\nFirst part of batch poly commitment (9 scalar mults and 9 additions in G1)\nFully batched poly commitment (5 scalar mults and 5 additions over G1)\nGroup encoded batch verification (1 scalar mult over G1)\nBatch-validate all evaluations (3 scalar mults and 4 additions over G1, plus two Miller loops + final verify)\nHydra is another crucial component for scalability of Cardano (there is a series of blog posts available, with a good summary of the solution here). Hydra relies on a multisignature scheme, where all participants of the side channel need to agree on the new state. This can be achieved with non-pairing friendly curves (as it is currently designed), but pairing based signature schemes provide much more elegant constructions that reduce interaction among signers. Moreover, Hydra tails could benefit from SNARKs for proving correct spending of a set of transactions. For costing multisignatures we use BLS, whose verification is relatively simple. One only needs to compute 2 Miller loops and a final verify operation.Some applications might be interested in a smart contract aggregating signatures or keys. For this, we require n group additions over G1 and G2 respectively, for n the number of submitted signatures.\nMithril currently does not require plutus support. However, Mithril, as a technology, allows for signature generation representing all stakeholders of Cardano (or any proof of stake system). These types of certificates might eventually be used for certifying sidechains, and plutus support will be crucial. Again, Mithril relies on pairing based signatures. Let k be the number of submitted signatures (as above, k is a security parameter determining the number of required signatures). However, in mithril k N where N is the total number of eligible signers. Mithril verification goes as follows: Check the k is large enough k G1 additions k G2 additions k (log_2(N) hash computations + equality checks) 2 Miller loops + final verify\nCheck the k is large enough\nk G1 additions\nk G2 additions\nk (log_2(N) hash computations + equality checks)\n2 Miller loops + final verify\nATALA is a decentralized identification mechanism. One of the properties they want to provide is anonymity: users can selectively disclose attributes of their certificate or prove statements regarding them without disclosing their identity. Up to date, the most recent, efficient and interesting solutions to provide these are pairing based (Hyperledger Fabric/Idemix standardisation effort, coconut credentials used by Nym, among others). We use Coconut certs as an example. There is no decision on what type of construction we will eventually use. Coconut credentials (and other types of credentials we are looking at) are built in a way that it is efficient to prove statements about attributes. To this end, it is required to build, and be able to verify, relations over discrete logarithm values. However, for sake of simplicity in this computation, we consider the simplest form of anonymous credentials, which contains no attributes. We note that proving statements regarding attributes does not require further pairing evaluations. Proof verification reduces to a relation involving elements in G1 and G2. This verification does not require the pairing evaluation, but does require G1 and G2 additions and multiplications. 2 Miller loops + final verify\nProof verification reduces to a relation involving elements in G1 and G2. This verification does not require the pairing evaluation, but does require G1 and G2 additions and multiplications.\n2 Miller loops + final verify\nWe now provide the technical specification.\nThe added types will be the following, all of which can be represented as a byte array. Even if these types are equivalent to byte arrays of a given size, it makes sense to include these types, to enforce deserialization, and therefore some checks on the data used by the smart contract. In particular, bls12_381_G1_element and bls12_381_G2_element can only be generated with byte arrays that represent a point which is part of the prime order subgroup. On the other hand, bls12_381_MlResult can only be generated as a result of the bls12_381_millerLoop computatio.\nbls12_381_G1_element\nbls12_381_G2_element\nbls12_381_MlResult\nbls12_381_millerLoop\nbls12_381_G1_element\nbls12_381_G1_element\nbls12_381_G2_element\nbls12_381_G2_element\nbls12_381_MlResult\nbls12_381_MlResult\nWe need to support the binary operation of G1 and G2 (which are additive groups), as well as the binary operation over MlResult (which is represented as a multiplicative group). We also want to enable hashing to G1 and G2. In particular, we expose the hash to curve (which we denote with hashToGroup) algorithm as described in hash to curve draft, using BLS12381G1_XMD:SHA-256_SSWU_RO_ and BLS12381G2_XMD:SHA-256_SSWU_RO_ for G1 and G2 respectively. We do not include the type of the Scalar, nor operations related to it. These type of operations can already be performed with the Integer type. In particular, we need the following functions:\nhashToGroup\nBLS12381G1_XMD:SHA-256_SSWU_RO_\nBLS12381G2_XMD:SHA-256_SSWU_RO_\nInteger\nGroup operations: bls12_381_G1_add :: bls12_381_G1_element - bls12_381_G1_element - bls12_381_G1_element bls12_381_G1_scalarMul :: Integer - bls12_381_G1_element - bls12_381_G1_element bls12_381_G1_neg :: bls12_381_G1_element - bls12_381_G1_element bls12_381_G2_add :: bls12_381_G2_element - bls12_381_G2_element - bls12_381_G2_element bls12_381_G2_scalarMul :: Integer - bls12_381_G2_element - bls12_381_G2_element bls12_381_G2_neg :: bls12_381_G2_element - bls12_381_G2_element bls12_381_mulMlResult :: bls12_381_MlResult - bls12_381_MlResult - bls12_381_MlResult\nbls12_381_G1_add :: bls12_381_G1_element - bls12_381_G1_element - bls12_381_G1_element\nbls12_381_G1_add :: bls12_381_G1_element -> bls12_381_G1_element -> bls12_381_G1_element\nbls12_381_G1_scalarMul :: Integer - bls12_381_G1_element - bls12_381_G1_element\nbls12_381_G1_scalarMul :: Integer -> bls12_381_G1_element -> bls12_381_G1_element\nbls12_381_G1_neg :: bls12_381_G1_element - bls12_381_G1_element\nbls12_381_G1_neg :: bls12_381_G1_element -> bls12_381_G1_element\nbls12_381_G2_add :: bls12_381_G2_element - bls12_381_G2_element - bls12_381_G2_element\nbls12_381_G2_add :: bls12_381_G2_element -> bls12_381_G2_element -> bls12_381_G2_element\nbls12_381_G2_scalarMul :: Integer - bls12_381_G2_element - bls12_381_G2_element\nbls12_381_G2_scalarMul :: Integer -> bls12_381_G2_element -> bls12_381_G2_element\nbls12_381_G2_neg :: bls12_381_G2_element - bls12_381_G2_element\nbls12_381_G2_neg :: bls12_381_G2_element -> bls12_381_G2_element\nbls12_381_mulMlResult :: bls12_381_MlResult - bls12_381_MlResult - bls12_381_MlResult\nbls12_381_mulMlResult :: bls12_381_MlResult -> bls12_381_MlResult -> bls12_381_MlResult\nPairing operations: bls12_381_millerLoop :: bls12_381_G1_element - bls12_381_G2_element - bls12_381_MlResult bls12_381_finalVerify :: bls12_381_MlResult - bls12_381_MlResult - Bool This performs the final exponentiation (see section An important note on MlResult elements below).\nbls12_381_millerLoop :: bls12_381_G1_element - bls12_381_G2_element - bls12_381_MlResult\nbls12_381_millerLoop :: bls12_381_G1_element -> bls12_381_G2_element -> bls12_381_MlResult\nbls12_381_finalVerify :: bls12_381_MlResult - bls12_381_MlResult - Bool This performs the final exponentiation (see section An important note on MlResult elements below).\nbls12_381_finalVerify :: bls12_381_MlResult -> bls12_381_MlResult -> Bool\nAn important note on MlResult elements\nHash to curve. We include hash-to-curve functions, as per Hashing to Elliptic Curves internet draft. Refer to this section for further details: bls12_381_G1_hashToGroup :: ByteString - ByteString - bls12_381_G1_element bls12_381_G2_hashToGroup :: ByteString - ByteString - bls12_381_G2_element\nbls12_381_G1_hashToGroup :: ByteString - ByteString - bls12_381_G1_element\nbls12_381_G1_hashToGroup :: ByteString -> ByteString -> bls12_381_G1_element\nbls12_381_G2_hashToGroup :: ByteString - ByteString - bls12_381_G2_element\nbls12_381_G2_hashToGroup :: ByteString -> ByteString -> bls12_381_G2_element\nOn top of the elliptic curve operations, we also need to include deserialization functions, and equality definitions among the G1 and G2 types.\nDeserialisation (more information of the choice of compressed form over uncompressed form here): bls12_381_G1_compress :: bls12_381_G1_element - ByteString bls12_381_G1_uncompress :: ByteString - bls12_381_G1_element bls12_381_G2_compress :: bls12_381_G2_element - ByteString bls12_381_G2_uncompress :: ByteString - bls12_381_G2_element\nbls12_381_G1_compress :: bls12_381_G1_element - ByteString\nbls12_381_G1_compress :: bls12_381_G1_element -> ByteString\nbls12_381_G1_uncompress :: ByteString - bls12_381_G1_element\nbls12_381_G1_uncompress :: ByteString -> bls12_381_G1_element\nbls12_381_G2_compress :: bls12_381_G2_element - ByteString\nbls12_381_G2_compress :: bls12_381_G2_element -> ByteString\nbls12_381_G2_uncompress :: ByteString - bls12_381_G2_element\nbls12_381_G2_uncompress :: ByteString -> bls12_381_G2_element\nEquality functions: bls12_381_G1_equal :: bls12_381_G1_element - bls12_381_G1_element - Bool bls12_381_G2_equal :: bls12_381_G2_element - bls12_381_G2_element - Bool\nbls12_381_G1_equal :: bls12_381_G1_element - bls12_381_G1_element - Bool\nbls12_381_G1_equal :: bls12_381_G1_element -> bls12_381_G1_element -> Bool\nbls12_381_G2_equal :: bls12_381_G2_element - bls12_381_G2_element - Bool\nbls12_381_G2_equal :: bls12_381_G2_element -> bls12_381_G2_element -> Bool\nThis makes a total of 17 new functions and three new types.\nWe follow the ZCash Bls12-381 specification for the serialization of elements:\nFq elements are encoded in big-endian form. They occupy 48 bytes in this form.\nFq2 elements are encoded in big-endian form, meaning that the Fq2 element c0 + c1 * u is represented by the Fq element c1 followed by the Fq element c0. This means Fq2 elements occupy 96 bytes in this form.\nThe group G1 uses Fq elements for coordinates. The group G2 uses Fq2 elements for coordinates.\nG1 and G2 elements are encoded in compressed form (just the x-coordinate). G1 elements occupy 48 bytes and G2 elements occupy 96 bytes.\nThe most-significant three bits of a G1 or G2 encoding should be masked away before the coordinate(s) are interpreted. These bits are used to unambiguously represent the underlying element:\nThe most significant bit, when set, indicates that the point is in compressed form.\nThe second-most significant bit indicates that the point is at infinity. If this bit is set, the remaining bits of the group element's encoding should be set to zero.\nThe third-most significant bit is set if (and only if) this point is in compressed form, and it is not the point at infinity and its y-coordinate is the lexicographically largest of the two associated with the encoded x-coordinate.\nWe include the serialisation of the generator of G1 and the generator of G2:\ngenerator G1:\n[151, 241, 211, 167, 49, 151, 215, 148, 38, 149, 99, 140, 79, 169, 172, 15, 195, 104, 140, 79, 151, 116, 185, 5, 161,78, 58, 63, 23, 27, 172, 88, 108, 85, 232, 63, 249, 122, 26, 239, 251, 58, 240, 10, 219, 34, 198, 187]\n[151, 241, 211, 167, 49, 151, 215, 148, 38, 149, 99, 140, 79, 169, 172, 15, 195, 104, 140, 79, 151, 116, 185, 5, 161,78, 58, 63, 23, 27, 172, 88, 108, 85, 232, 63, 249, 122, 26, 239, 251, 58, 240, 10, 219, 34, 198, 187]\ngenerator G2:\n[147, 224, 43, 96, 82, 113, 159, 96, 125, 172, 211, 160, 136, 39, 79, 101, 89, 107, 208, 208, 153, 32, 182, 26, 181, 218, 97, 187, 220, 127, 80, 73, 51, 76, 241, 18, 19, 148, 93, 87, 229, 172, 125, 5, 93, 4, 43, 126, 2, 74, 162, 178, 240, 143, 10, 145, 38, 8, 5, 39, 45, 197, 16, 81, 198, 228, 122, 212, 250, 64, 59, 2, 180, 81, 11, 100, 122, 227, 209, 119, 11, 172, 3, 38, 168, 5, 187, 239, 212, 128, 86, 200, 193, 33, 189, 184]\n[147, 224, 43, 96, 82, 113, 159, 96, 125, 172, 211, 160, 136, 39, 79, 101, 89, 107, 208, 208, 153, 32, 182, 26, 181, 218, 97, 187, 220, 127, 80, 73, 51, 76, 241, 18, 19, 148, 93, 87, 229, 172, 125, 5, 93, 4, 43, 126, 2, 74, 162, 178, 240, 143, 10, 145, 38, 8, 5, 39, 45, 197, 16, 81, 198, 228, 122, 212, 250, 64, 59, 2, 180, 81, 11, 100, 122, 227, 209, 119, 11, 172, 3, 38, 168, 5, 187, 239, 212, 128, 86, 200, 193, 33, 189, 184]\nWe expose the hash-to-curve functions following the Hashing to Elliptic Curves internet draft. The function signature takes as input two ByteStrings and returns a point. The first ByteString is the message to be hashed, while the second ByteString is the Domain Separation Tag (DST). For more information on the DST, see section 3.1 of the internet draft. We limit the DST to be at most 255 bytes, following the standard specification. If applications require a domain separation tag that is longer than 255 bytes, they should convert it to a smaller DST following the instructions of the standard draft (see section 5.3.3).\nByteString\nByteString\nByteString\nSome libraries expose the possibility to use yet another ByteString when calling the hash-to-curve function. See for example the blst library. We choose not to include this extra ByteString in the function signature, because it is not part of the standard draft. In the case where we want to match a hash that did use this aug ByteString, one simply needs to prepend that value to the message. One can verify that by running the test-vector generation script introduced in cardano-base.\nByteString\nblst\nByteString\naug\nByteString\nTo recap, we have types bls12_381_G1_element and bls12_381_G2_element each of which is essentially a pair of values (x,y) that satisfy an equation of the form y 2 = x 3+ax+b. The blst library provides two serialisation formats for these:\nbls12_381_G1_element\nbls12_381_G2_element\n(x,y)\ny^2 = x^3+ax+b\nThe serialised format, where you have a bytestring encoding both the x and y coordinates of a point. A serialised bls12_381_G1_element takes up 96 bytes and a serialised bls12_381_G2_element takes up 192 bytes.\nx\ny\nbls12_381_G1_element\nbls12_381_G2_element\nThe compressed format, where you have a bytestring that only contains the x coordinate. When you uncompress a compressed point to get an in-memory point, the y coordinate has to be calculated from the equation of the curve. A compressed bls12_381_G1_element takes up 48 bytes and a compressed bls12_381_G2_element takes 92 bytes.\nx\ny\nbls12_381_G1_element\nbls12_381_G2_element\nThe PLC implementation currently uses the compressed format for serialising bls12_381_G1_element and bls12_381_G1_elements. There are at least three places where this is (or could be) used:\nbls12_381_G1_element\nbls12_381_G1_element\nStoring a group element as a bytestring inside a Data object which will be passed as a parameter to a script.\nDuring flat serialisation of PLC scripts (constants from G1 and G2 are converted into bytestrings and then flat deals with these as usual).\nIn the concrete PLC/UPLC syntax, where constants are written as hex strings representing compressed points.\nThe serialised format could also be used for all of these. The advantage of doing that is that deserialisation is much cheaper than uncompression (which involves calculating a square root in a finite field, which is expensive in general), but the disadvantage is that the serialised format requires twice as much space as the compressed form. We ran some benchmarks to determine CPU costs (in ExUnits) for both deserialisation, and uncompression and came up with the following:\nbls12_381_G1_deserialize : 701442 bls12_381_G1_uncompress : 16511372 bls12_381_G2_deserialize : 1095773 bls12_381_G2_uncompress : 33114723\nbls12_381_G1_deserialize : 701442 bls12_381_G1_uncompress : 16511372 bls12_381_G2_deserialize : 1095773 bls12_381_G2_uncompress : 33114723\nFor G1 uncompression is about 23 times more expensive than deserialisation, and for G2 uncompression is about 30 times more expensive than deserialisation. The maximum CPU budget per script is currently 1,000,000,000, so a single G2 uncompression is about 0.3 of the total allowance, whereas a G2 deserialisation is about 0.01 . This might seem like a compelling reason to prefer serialisation over compression, but our claim is that the time saving is not worthwhile because you can't fit enough serialised points into a script to make the speed gain significant. The bls12-381-costs program runs a number of benchmarks for execution costs of scripts that exercise the BLS builtins. One of these creates UPLC scripts which include varying numbers of compressed points and at run-time uncompresses them and adds them all together; bls-381-costs runs these scripts and prints out their costs as fractions of the maximum CPU budget and maximum script size (currently 16384). Here are the results:\nUncompress n G1 points and add the results\nn script size CPU usage Memory usage ---------------------------------------------------------------------- - 68 (0.4%) 100 (0.0%) 100 (0.0%) 10 618 (3.8%) 185801250 (1.9%) 45642 (0.3%) 20 1168 (7.1%) 371912820 (3.7%) 88002 (0.6%) 30 1718 (10.5%) 558024390 (5.6%) 130362 (0.9%) 40 2268 (13.8%) 744135960 (7.4%) 172722 (1.2%) 50 2818 (17.2%) 930247530 (9.3%) 215082 (1.5%) 60 3368 (20.6%) 1116359100 (11.2%) 257442 (1.8%) 70 3918 (23.9%) 1302470670 (13.0%) 299802 (2.1%) 80 4468 (27.3%) 1488582240 (14.9%) 342162 (2.4%) 90 5018 (30.6%) 1674693810 (16.7%) 384522 (2.7%) 100 5568 (34.0%) 1860805380 (18.6%) 426882 (3.0%) 110 6118 (37.3%) 2046916950 (20.5%) 469242 (3.4%) 120 6668 (40.7%) 2233028520 (22.3%) 511602 (3.7%) 130 7218 (44.1%) 2419140090 (24.2%) 553962 (4.0%) 140 7768 (47.4%) 2605251660 (26.1%) 596322 (4.3%) 150 8318 (50.8%) 2791363230 (27.9%) 638682 (4.6%)\nn script size CPU usage Memory usage ---------------------------------------------------------------------- - 68 (0.4%) 100 (0.0%) 100 (0.0%) 10 618 (3.8%) 185801250 (1.9%) 45642 (0.3%) 20 1168 (7.1%) 371912820 (3.7%) 88002 (0.6%) 30 1718 (10.5%) 558024390 (5.6%) 130362 (0.9%) 40 2268 (13.8%) 744135960 (7.4%) 172722 (1.2%) 50 2818 (17.2%) 930247530 (9.3%) 215082 (1.5%) 60 3368 (20.6%) 1116359100 (11.2%) 257442 (1.8%) 70 3918 (23.9%) 1302470670 (13.0%) 299802 (2.1%) 80 4468 (27.3%) 1488582240 (14.9%) 342162 (2.4%) 90 5018 (30.6%) 1674693810 (16.7%) 384522 (2.7%) 100 5568 (34.0%) 1860805380 (18.6%) 426882 (3.0%) 110 6118 (37.3%) 2046916950 (20.5%) 469242 (3.4%) 120 6668 (40.7%) 2233028520 (22.3%) 511602 (3.7%) 130 7218 (44.1%) 2419140090 (24.2%) 553962 (4.0%) 140 7768 (47.4%) 2605251660 (26.1%) 596322 (4.3%) 150 8318 (50.8%) 2791363230 (27.9%) 638682 (4.6%)\nUncompress n G2 points and add the results\nn script size CPU usage Memory usage ---------------------------------------------------------------------- - 68 (0.4%) 100 (0.0%) 100 (0.0%) 10 1098 (6.7%) 363545910 (3.6%) 45984 (0.3%) 20 2128 (13.0%) 728715130 (7.3%) 88704 (0.6%) 30 3158 (19.3%) 1093884350 (10.9%) 131424 (0.9%) 40 4188 (25.6%) 1459053570 (14.6%) 174144 (1.2%) 50 5218 (31.8%) 1824222790 (18.2%) 216864 (1.5%) 60 6248 (38.1%) 2189392010 (21.9%) 259584 (1.9%) 70 7278 (44.4%) 2554561230 (25.5%) 302304 (2.2%) 80 8308 (50.7%) 2919730450 (29.2%) 345024 (2.5%) 90 9338 (57.0%) 3284899670 (32.8%) 387744 (2.8%) 100 10368 (63.3%) 3650068890 (36.5%) 430464 (3.1%) 110 11398 (69.6%) 4015238110 (40.2%) 473184 (3.4%) 120 12428 (75.9%) 4380407330 (43.8%) 515904 (3.7%) 130 13458 (82.1%) 4745576550 (47.5%) 558624 (4.0%) 140 14488 (88.4%) 5110745770 (51.1%) 601344 (4.3%) 150 15518 (94.7%) 5475914990 (54.8%) 644064 (4.6%)\nn script size CPU usage Memory usage ---------------------------------------------------------------------- - 68 (0.4%) 100 (0.0%) 100 (0.0%) 10 1098 (6.7%) 363545910 (3.6%) 45984 (0.3%) 20 2128 (13.0%) 728715130 (7.3%) 88704 (0.6%) 30 3158 (19.3%) 1093884350 (10.9%) 131424 (0.9%) 40 4188 (25.6%) 1459053570 (14.6%) 174144 (1.2%) 50 5218 (31.8%) 1824222790 (18.2%) 216864 (1.5%) 60 6248 (38.1%) 2189392010 (21.9%) 259584 (1.9%) 70 7278 (44.4%) 2554561230 (25.5%) 302304 (2.2%) 80 8308 (50.7%) 2919730450 (29.2%) 345024 (2.5%) 90 9338 (57.0%) 3284899670 (32.8%) 387744 (2.8%) 100 10368 (63.3%) 3650068890 (36.5%) 430464 (3.1%) 110 11398 (69.6%) 4015238110 (40.2%) 473184 (3.4%) 120 12428 (75.9%) 4380407330 (43.8%) 515904 (3.7%) 130 13458 (82.1%) 4745576550 (47.5%) 558624 (4.0%) 140 14488 (88.4%) 5110745770 (51.1%) 601344 (4.3%) 150 15518 (94.7%) 5475914990 (54.8%) 644064 (4.6%)\nIt's clear from these figures that the limiting factor is the script size: about 300 G1 points or 150 G2 points can be processed in a single script before exceeding the script size limit, but the maximum CPU usage is only 55 of the the maximum CPU budget. If the serialisation (involving both x- and y-coordinates) was used instead then there would be some saving in execution time, but a single script would only be able to process about half as many points and it's unlikely that the time savings would compensate for that. For example, uncompressing 50 G2 points would cost about 1,655,000,000 CPU ExUnits and deserialising them would cost about 54,788,000, which is 1,600,000,000 ExUnits cheaper. At the time of writing, this equates to 0.27 Ada. However, 50 serialised G2 points take up 4800 more bytes than 50 compressed ones, and the extra bytes would cost 0.36 Ada. Thuis using serialisation would cost 0.09 Ada more than using compression for the same number of points.\nIn summary: even though uncompression is a lot more expensive per point than deserialisation, the size savings due to compression actually outweigh the speed gains due to serialisation because bytes per script are a lot more expensive than ExUnits per script in real terms. For this reason, we propose to support the compressed format and only the compressed format.\nWe intentionally limit what can be done with the MlResult element. In fact, the only way that one can generate a MlResult element is using the bls12_381_millerLoop function. Then these elements can only be used for operations among them (multiplication) and a final equality check (denoted finalVerify). In other words, MlResult elements are only there to eventually perform some equality checks. We thought of including the inverse function to allow division, but given that we simply allow for equality checks, if one needs to divide by A, then A can simply move to the other side of the equality sign. These limitations allow us for a performance trick, which is also used for the verification of BLS signatures. In a nutshell, a pairing is divided into two operations: (i) Miller loop, and (ii) final exponentiation. Both are expensive, but what we can do is first compute the Miller loop, which already maps two points from G1 and G2 to MlResult. Then we can use this result to perform the operations over these points (multiplications). Finally, when we want to check for equality, we invert one of the two points (or equivalently in this case we compute the conjugate), and multiply the result by the other point. Only now we compute the final exponentiation and verify that the result is the identity element. In other words:\nbls12_381_millerLoop\nfinalVerify\nA\nA\nthe 'partial pairing' function, bls12_381_millerLoop simply computes the Miller loop\nbls12_381_millerLoop\nthe equality check function, bls12_381_finalVerify, first computes an inverse, then a multiplication, a final exponentiation and checks that the element is the identity.\nbls12_381_finalVerify\nWhile the results of the Miller loop are already elements in Fp12, they are not necessarily part of the group. This is why we call the type used in the built-ins bls12_381_MlResult rather than bls12_381_GT.\nbls12_381_MlResult\nbls12_381_GT\nUsing the estimates in the section Costing of functions, we can see how this representation of GT elements is beneficial. Assume that we want to check the following relation: e(A,B) * e(C,D) =? e(E,F) * e(G,H). The Miller loop takes around 330us, and the final exponentiation around 370us. A full pairing would be 700us, and therefore checking this relation would cost around 2,8ms. If, instead, we break down the pairing into a Miller loop and a final exponentiation, and only compute the latter once, the cost of the relation above would be 330 * 4 + 370 = 1.7ms.\nCosting of functions\ne(A,B) * e(C,D) =? e(E,F) * e(G,H)\nFor this reason it is important to limit what can be done with bls12_381_MlResult, as the pairing really is not the full pairing operation, but only the Miller loop.\nbls12_381_MlResult\nBLST library, providing the algebraic operations.\ncardano-base with the haskell FFI to the BLST library.\nplutus\nOther libraries of interest\nEthereum support for BLS12-381. Not directly relevant as this is an Ethereum Improvement Proposal for a precompiled solidity contracts.\nWe present what would be the alternatives of using pairings in the different use cases presented above.\nSidechain bridges using the current technology would rely on either of the two possibilities: Require the bridge committee to interact during signature, or to rely on a precomputation phase. Current solutions only support non-robust signature schemes, meaning that if one signer misbehaves, the whole signature procedure needs to be restarted. This could seriously hinder sidechains. Non-aggregation of signatures. This would result in a linear \"checkpoint certificate\" with respect to the number of signers (both in communication and computation complexity). Basically, all committee members need to submit their signature, and the smart contract needs to verify all ed25519 signatures.\nRequire the bridge committee to interact during signature, or to rely on a precomputation phase. Current solutions only support non-robust signature schemes, meaning that if one signer misbehaves, the whole signature procedure needs to be restarted. This could seriously hinder sidechains.\nNon-aggregation of signatures. This would result in a linear \"checkpoint certificate\" with respect to the number of signers (both in communication and computation complexity). Basically, all committee members need to submit their signature, and the smart contract needs to verify all ed25519 signatures.\nZero Knowledge Proofs cannot be verified with current functions available in Plutus. There exist proofs that can be instantiated over non-pairing friendly curves, but these result in logarithmic sized proofs and linear verification with respect to the computation to prove, while solutions that rely on pairings can be represented more concisely, and are cheaper to verify.\nOne might be concerned of why we are exposing such low-level primitives, instead of exposing higher level protocol functions, such as VerifyBlsSignature or VerifyZKP. The motivation behind that is because pairings can enable a big number of use cases, and covering all of those can considerably extend the list of required functions.\nVerifyBlsSignature\nVerifyZKP\nThe BLS12-381 curve is fully defined by the following set of parameters (coefficient A=0 for all BLS12 curves). Taken from EIP 2537:\nBase field modulus = 0x1a0111ea397fe69a4b1ba7b6434bacd764774b84f38512bf6730d2a0f6b0f6241eabfffeb153ffffb9feffffffffaaab B coefficient = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004 Main subgroup order = 0x73eda753299d7d483339d80809a1d80553bda402fffe5bfeffffffff00000001 Extension tower Fp2 construction: Fp quadratic non-residue = 0x1a0111ea397fe69a4b1ba7b6434bacd764774b84f38512bf6730d2a0f6b0f6241eabfffeb153ffffb9feffffffffaaaa Fp6/Fp12 construction: Fp2 cubic non-residue c0 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001 Fp2 cubic non-residue c1 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001 Twist parameters: Twist type: M B coefficient for twist c0 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004 B coefficient for twist c1 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004 Generators: G1: X = 0x17f1d3a73197d7942695638c4fa9ac0fc3688c4f9774b905a14e3a3f171bac586c55e83ff97a1aeffb3af00adb22c6bb Y = 0x08b3f481e3aaa0f1a09e30ed741d8ae4fcf5e095d5d00af600db18cb2c04b3edd03cc744a2888ae40caa232946c5e7e1 G2: X c0 = 0x024aa2b2f08f0a91260805272dc51051c6e47ad4fa403b02b4510b647ae3d1770bac0326a805bbefd48056c8c121bdb8 X c1 = 0x13e02b6052719f607dacd3a088274f65596bd0d09920b61ab5da61bbdc7f5049334cf11213945d57e5ac7d055d042b7e Y c0 = 0x0ce5d527727d6e118cc9cdc6da2e351aadfd9baa8cbdd3a76d429a695160d12c923ac9cc3baca289e193548608b82801 Y c1 = 0x0606c4a02ea734cc32acd2b02bc28b99cb3e287e85a763af267492ab572e99ab3f370d275cec1da1aaa9075ff05f79be Pairing parameters: |x| (Miller loop scalar) = 0xd201000000010000 x is negative = true\nBase field modulus = 0x1a0111ea397fe69a4b1ba7b6434bacd764774b84f38512bf6730d2a0f6b0f6241eabfffeb153ffffb9feffffffffaaab B coefficient = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004 Main subgroup order = 0x73eda753299d7d483339d80809a1d80553bda402fffe5bfeffffffff00000001 Extension tower Fp2 construction: Fp quadratic non-residue = 0x1a0111ea397fe69a4b1ba7b6434bacd764774b84f38512bf6730d2a0f6b0f6241eabfffeb153ffffb9feffffffffaaaa Fp6/Fp12 construction: Fp2 cubic non-residue c0 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001 Fp2 cubic non-residue c1 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001 Twist parameters: Twist type: M B coefficient for twist c0 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004 B coefficient for twist c1 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004 Generators: G1: X = 0x17f1d3a73197d7942695638c4fa9ac0fc3688c4f9774b905a14e3a3f171bac586c55e83ff97a1aeffb3af00adb22c6bb Y = 0x08b3f481e3aaa0f1a09e30ed741d8ae4fcf5e095d5d00af600db18cb2c04b3edd03cc744a2888ae40caa232946c5e7e1 G2: X c0 = 0x024aa2b2f08f0a91260805272dc51051c6e47ad4fa403b02b4510b647ae3d1770bac0326a805bbefd48056c8c121bdb8 X c1 = 0x13e02b6052719f607dacd3a088274f65596bd0d09920b61ab5da61bbdc7f5049334cf11213945d57e5ac7d055d042b7e Y c0 = 0x0ce5d527727d6e118cc9cdc6da2e351aadfd9baa8cbdd3a76d429a695160d12c923ac9cc3baca289e193548608b82801 Y c1 = 0x0606c4a02ea734cc32acd2b02bc28b99cb3e287e85a763af267492ab572e99ab3f370d275cec1da1aaa9075ff05f79be Pairing parameters: |x| (Miller loop scalar) = 0xd201000000010000 x is negative = true\nOne should note that base field modulus is equal to 3 mod 4 that allows an efficient square root extraction.\nThe reason for choosing the BLS12-381 over the BN256 curve is that the former is claimed to provide 128 bits of security, while the latter was reduced to 100 bits of security after the extended number field sieve (a new algorithm to compute the discrete logarithm) was shown to reduce the security of these curves.\nAn EIP for precompiles of curve BLS12-381 already exists, but has been stagnant for a while. Nonetheless, Zcash, MatterLabs and Consensys support BLS12-381 curve, so it is certainly widely used in the space.\nFurther reading regarding curve BLS12-381 can be found here and the references thereof cited.\nBLST library audited by NCC Group and being formally verified by Galois.\nWe also have an analysis by Duncan Coutts for effects of including this library for continuous integration and long term maintainability:\nIn addition to security audits on the proposed implementation, it is important that we review the inclusion of the library for practical issues of build systems, and long term maintainability.\nIn particular in this case the library will be used in the chain format and in chain verification. This means we have special constraints that the format and verification logic must never change. Or more specifically: it must be possible forever to be able to verify the existing uses on the chain. So even if there are upgrades or format changes in future, it must still be possible to decode and verify the old uses. This is not a constraint that most software has, and so many libraries are not designed to work within that constraint.\nThe proposed implementation is https://github.com/supranational/blst\nThe implementation is in C and assembly for x86 and ARM v8. This is good for the existing Haskell implementation of the Cardano node because integrating with C libraries is relatively straightforward, it's well supported by the build and CI system and does not pull in too many extra dependencies. (Contrast for example if it were a Rust library where we would have serious practical problems on this front.)\nThe implementation appears to have been designed with blockchain implementations in mind. This is a good sign for the long term maintainability because it probably means that the library authors will continue to support the existing format and semantics even if there are changes or improvements.\nThe implementation is claimed to be compliant with draft IETF specifications. There is a risk that the specs may change before being declared final, and the library may be updated to follow. There is a risk that the Cardano community will have to support the old version forever. Though given the point above, it's probably the case that library updates would still provide compatibility with the current IETF drafts and serialisation formats.\nSo overall this seems like something the core development team and Cardano community could support long term without too high a cost. Though we should be aware of the risk that we may have to support an old version of the library, if the library gets changed in incompatible ways.\nTo ensure no compatibility surprises, it is worth considering forking the repository at a specific commit/version and building the node using that version. This is to guarantee the version remains available. Then for any future updates, the fork repo could be updated to a new version explicitly, with appropriate compatibility checks to ensure the existing on-chain uses are still compatible.\nWe did some preliminary costing of the BLS functions and the following cost of the new built-in functions:\nbls12_381_G1_compress : 3341914 bls12_381_G1_uncompress : 16511372 bls12_381_G1_add : 1046420 bls12_381_G1_equal : 545063 bls12_381_G1_hashToCurve : 66311195 + 23097*x bls12_381_G1_scalarMul : 94607019 + 87060*x (we use 94955259, with x = 4) bls12_381_G1_neg : 292890 bls12_381_G2_compress : 3948421 bls12_381_G2_uncompress : 33114723 bls12_381_G2_add : 2359410 bls12_381_G2_equal : 1102635 bls12_381_G2_hashToCurve : 204557793 + 23271*x bls12_381_G2_scalarMul : 190191402 + 85902*x bls12_381_G2_neg : 307813 bls12_381_GT_finalVerify : 388656972 bls12_381_GT_millerLoop : 402099373 bls12_381_GT_mul : 2533975 blake2b_256 : 358499 + 10186*x (521475, with x = 16) addInteger : 85664 + 712*max(x,y) (88512, with x = y = 4) multiplyInteger : 1000 + 55553*(x+y) (641924, with x = y = 4, and we include the price of modular reduction, as we need one per mult) divideInteger : if x>y then 809015 + 577*x*y else 196500 modInteger : 196500 expInteger : We estimate 32 mults and adds (23373952)\nbls12_381_G1_compress : 3341914 bls12_381_G1_uncompress : 16511372 bls12_381_G1_add : 1046420 bls12_381_G1_equal : 545063 bls12_381_G1_hashToCurve : 66311195 + 23097*x bls12_381_G1_scalarMul : 94607019 + 87060*x (we use 94955259, with x = 4) bls12_381_G1_neg : 292890 bls12_381_G2_compress : 3948421 bls12_381_G2_uncompress : 33114723 bls12_381_G2_add : 2359410 bls12_381_G2_equal : 1102635 bls12_381_G2_hashToCurve : 204557793 + 23271*x bls12_381_G2_scalarMul : 190191402 + 85902*x bls12_381_G2_neg : 307813 bls12_381_GT_finalVerify : 388656972 bls12_381_GT_millerLoop : 402099373 bls12_381_GT_mul : 2533975 blake2b_256 : 358499 + 10186*x (521475, with x = 16) addInteger : 85664 + 712*max(x,y) (88512, with x = y = 4) multiplyInteger : 1000 + 55553*(x+y) (641924, with x = y = 4, and we include the price of modular reduction, as we need one per mult) divideInteger : if x>y then 809015 + 577*x*y else 196500 modInteger : 196500 expInteger : We estimate 32 mults and adds (23373952)\nUsing these preliminary benchmarks, we performed some estimates of how much it would cost to verify Groth16 or Plonk proofs using the bindings. Details can be found here. The estimates for Groth16 (~23 of the execution budget required for a proof verification) were confirmed by the benchmarks shared above.\nIOG internal. PR open for Plutus bindings https://github.com/input-output-hk/plutus/pull/5231\nConfirmation from IOG Plutus Team that this curve support is included in a scheduled Plutus release. Included within the Chang #1 hardfork\nIncluded within the Chang #1 hardfork\nConfirmation from IOG Plutus Team that CIP-0035 Processes for changes to Plutus have been satisfied.\nThis CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-1694 | A First Step Towards On-Chain Decentralized Governance\n\nWe propose a revision of Cardano's on-chain governance system to support the new requirements for Voltaire. The existing specialized governance support for protocol parameter updates and MIR certificates will be removed, and two new fields will be added to normal transaction bodies for:\ngovernance actions votes\ngovernance actions\nvotes\nAny Cardano user will be allowed to submit a governance action. We also introduce three distinct governance bodies that have specific functions in this new governance framework:\na constitutional committee a group of delegated representatives (henceforth called DReps) the stake pool operators (henceforth called SPOs)\na constitutional committee\na group of delegated representatives (henceforth called DReps)\nthe stake pool operators (henceforth called SPOs)\nEvery governance action must be ratified by at least two of these three governance bodies using their on-chain votes. The type of action and the state of the governance system determines which bodies must ratify it.\nRatified actions are then enacted on-chain, following a set of well-defined rules.\nAs with stake pools, any Ada holder may register to be a DRep and so choose to represent themselves and/or others. Also, as with stake pools, Ada holders may, instead, delegate their voting rights to any other DRep. Voting rights will be based on the total Ada that is delegated, as a whole number of Lovelace.\nThe most crucial aspect of this proposal is therefore the notion of \"one Lovelace = one vote\".\nFor the many contributors to this proposal, see Acknowledgements.\nGoal\nCurrent design\nShortcomings of the Shelley governance design\nOut of scope\nWe're heading into the age of Voltaire, laying down the foundations for decentralized decision-making. This CIP describes a mechanism for on-chain governance that will underpin the Voltaire phase of Cardano. The CIP builds on and extends the original Cardano governance scheme that was based on a fixed number of governance keys. It aims to provide a first step that is both valuable and, importantly, is also technically achievable in the near term as part of the proposed Voltaire governance system.\nIt also seeks to act as a jumping-off point for continuing community input, including on appropriate threshold settings and other on-chain settings.\nSubsequent proposals may adapt and extend this proposal to meet emerging governance needs.\nThe on-chain Cardano governance mechanism that was introduced in the Shelley ledger era is capable of:\nmodifying the values of the protocol parameters (including initiating \"hard forks\") transferring Ada out of the reserves and the treasury (and also moving Ada between the reserves and the treasury)\nmodifying the values of the protocol parameters (including initiating \"hard forks\")\ntransferring Ada out of the reserves and the treasury (and also moving Ada between the reserves and the treasury)\nIn the current scheme, governance actions are initiated by special transactions that require Quorum-Many authorizations from the governance keys (5 out of 7 on the Cardano mainnet)1. Fields in the transaction body provide details of the proposed governance action: either i) protocol parameter changes; or ii) initiating funds transfers. Each transaction can trigger both kinds of governance actions, and a single action can have more than one effect (e.g. changing two or more protocol parameters).\nQuorum-Many\nProtocol parameter updates use transaction field n 6 of the transaction body.\nMovements of the treasury and the reserves use Move Instantaneous Rewards (abbrev. MIR) certificates.\nProperly authorized governance actions are applied on an epoch boundary (they are enacted).\nOne of the protocol parameters is sufficiently significant to merit special attention: changing the major protocol version enables Cardano to enact controlled hard forks. This type of protocol parameter update therefore has a special status, since stake pools must upgrade their nodes so they can support the new protocol version once the hard fork is enacted.\nThe Shelley governance design was intended to provide a simple, transitional approach to governance. This proposal aims to address a number of shortcomings with that design that are apparent as we move into Voltaire.\nThe Shelley governance design gives no room for active on-chain participation of Ada holders. While changes to the protocol are usually the results of discussions with selected community actors, the process is currently driven mainly by the founding entities. Ensuring that everyone can voice their concern is cumbersome, and can be perceived as arbitrary at times. Movements from the treasury constitute a critical and sensitive topic. However, they can be hard to track. It is important to have more transparency and more layers of control over these movements. While they need to be treated specially by SPOs, hard forks are not differentiated from other protocol parameter changes. Finally, while there is currently a somewhat common vision for Cardano that is shared by its founding entities and also by many community members, there is no clearly defined document where these guiding principles are recorded. It makes sense to leverage the Cardano blockchain to record the shared Cardano ethos in an immutable fashion, as a formal Cardano Constitution.\nThe Shelley governance design gives no room for active on-chain participation of Ada holders. While changes to the protocol are usually the results of discussions with selected community actors, the process is currently driven mainly by the founding entities. Ensuring that everyone can voice their concern is cumbersome, and can be perceived as arbitrary at times.\nThe Shelley governance design gives no room for active on-chain participation of Ada holders. While changes to the protocol are usually the results of discussions with selected community actors, the process is currently driven mainly by the founding entities. Ensuring that everyone can voice their concern is cumbersome, and can be perceived as arbitrary at times.\nMovements from the treasury constitute a critical and sensitive topic. However, they can be hard to track. It is important to have more transparency and more layers of control over these movements.\nMovements from the treasury constitute a critical and sensitive topic. However, they can be hard to track. It is important to have more transparency and more layers of control over these movements.\nWhile they need to be treated specially by SPOs, hard forks are not differentiated from other protocol parameter changes.\nWhile they need to be treated specially by SPOs, hard forks are not differentiated from other protocol parameter changes.\nFinally, while there is currently a somewhat common vision for Cardano that is shared by its founding entities and also by many community members, there is no clearly defined document where these guiding principles are recorded. It makes sense to leverage the Cardano blockchain to record the shared Cardano ethos in an immutable fashion, as a formal Cardano Constitution.\nFinally, while there is currently a somewhat common vision for Cardano that is shared by its founding entities and also by many community members, there is no clearly defined document where these guiding principles are recorded. It makes sense to leverage the Cardano blockchain to record the shared Cardano ethos in an immutable fashion, as a formal Cardano Constitution.\nThe following topics are considered to be out of the scope of this CIP.\nThis CIP focuses only on on-chain mechanisms. The provisions of the initial constitution are extremely important, as are any processes that will allow it to be amended. These merit their own separate and focused discussion.\nThis is an off-chain issue.\nAny potential legal enforcement of either the Cardano protocol or the Cardano Constitution are completely out of scope for this CIP.\nThe Cardano community must think deeply about the correct standards and processes for handling the creation of the governance actions that are specified in this CIP. In particular, the role of Project Catalyst in creating treasury withdrawal actions is completely outside the scope of this CIP.\nHow any private companies, public or private institutions, individuals etc. choose to hold or delegate their Ada, including delegation to stake pools or DReps, is outside the scope of this CIP.\nThe Cardano Constitution\nThe constitutional committee State of no-confidence Constitutional committee keys Replacing the constitutional committee Size of the constitutional committee Terms Guardrails Script\nState of no-confidence\nConstitutional committee keys\nReplacing the constitutional committee\nSize of the constitutional committee\nTerms\nGuardrails Script\nDelegated representatives (DReps) Pre-defined Voting Options Registered DReps New stake distribution for DReps Incentives for Ada holders to delegate voting stake DRep incentives\nPre-defined Voting Options\nRegistered DReps\nNew stake distribution for DReps\nIncentives for Ada holders to delegate voting stake\nDRep incentives\nGovernance actions Ratification Requirements Restrictions Enactment Lifecycle Content Protocol parameter groups\nRatification Requirements Restrictions\nRequirements\nRestrictions\nEnactment\nLifecycle\nContent\nProtocol parameter groups\nVotes Governance state Changes to the stake snapshot Definitions relating to voting stake\nGovernance state\nChanges to the stake snapshot\nDefinitions relating to voting stake\nThe Cardano Constitution is a text document that defines Cardano's shared values and guiding principles. At this stage, the Constitution is an informational document that unambiguously captures the core values of Cardano and acts to ensure its long-term sustainability. At a later stage, we can imagine the Constitution perhaps evolving into a smart-contract based set of rules that drives the entire governance framework. For now, however, the Constitution will remain an off-chain document whose hash digest value will be recorded on-chain. As discussed above, the Constitution is not yet defined and its content is out of scope for this CIP.\nWe define a constitutional committee which represents a set of individuals or entities (each associated with a Ed25519 or native or Plutus script credential) that are collectively responsible for ensuring that the Constitution is respected.\nThough it cannot be enforced on-chain, the constitutional committee is only supposed to vote on the constitutionality of governance actions (which should thus ensure the long-term sustainability of the blockchain) and should be replaced (via the no confidence action) if they overstep this boundary. Said differently, there is a social contract between the constitutional committee and the actors of the network. Although the constitutional committee could reject certain governance actions (by voting 'No' on them), they should only do so when those governance actions are in conflict with the Constitution.\nFor example, if we consider the hypothetical Constitution rule \"The Cardano network must always be able to produce new blocks\", then a governance action that would reduce the maximum block size to 0 would be, in effect, unconstitutional and so might not be ratified by the constitutional committee. The rule does not, however, specify the smallest acceptable maximum block size, so the constitutional committee would need to determine this number and vote accordingly.\n0\nThe constitutional committee is considered to be in one of the following two states at all times:\na normal state (i.e. a state of confidence) a state of no-confidence\na normal state (i.e. a state of confidence)\na state of no-confidence\nIn a state of no-confidence, the current committee is no longer able to participate in governance actions and must be replaced before any governance actions can be ratified (see below).\nThe constitutional committee will use a hot and cold key setup, similar to the existing \"genesis delegation certificate\" mechanism.\nThe constitutional committee can be replaced via a specific governance action (\"Update committee\", described below) that requires the approval of both the SPOs and the DReps. The threshold for ratification might be different depending on if the governance is in a normal state or a state of no confidence.\nThe new constitutional committee could, in principle, be identical to or partially overlap the outgoing committee as long as the action is properly ratified. This might happen, for example, if the electorate has collective confidence in all or part of the committee and wishes to extend its term of office.\nUnlike the Shelley governance design, the size of the constitutional committee is not fixed and can be any nonnegative number. It may be changed whenever a new committee is elected (\"Update committee\"). Likewise, the committee threshold (the fraction of committee Yes votes that are required to ratify governance actions) is not fixed and can also be varied by the governance action. This gives a great deal of flexibility to the composition of the committee. In particular, it is possible to elect an empty committee if the community wishes to abolish the constitutional committee entirely. Note that this is different from a state of no-confidence and still constitutes a governance system capable of enacting proposals.\nYes\nThere will be a new protocol parameter for the minimal size of the committee, itself a nonnegative number called committeeMinSize.\ncommitteeMinSize\nEach newly elected constitutional committee will have a term. Per-member terms allow for a rotation scheme, such as a third of the committee expiring every year. Expired members can no longer vote. Member can also willingly resign early, which will be marked on-chain as an expired member.\nIf the number of non-expired committee members falls below the minimal size of the committee, the constitutional committee will be unable to ratify governance actions. This means that only governance actions that don't require votes from the constitutional committee can still be ratified.\nFor example, a committee of size five with a threshold of 60 a minimum size of three and two expired members can still pass governance actions if two non-expired members vote Yes. However, if one more member expires then the constitutional committee becomes unable to ratify any more governance actions.\nYes\nThe maximum term is a governance protocol parameter, specified as a number of epochs. During a state of no-confidence, no action can be ratified, so the committee should plan for its own replacement if it wishes to avoid disruption.\nWhile the constitution is an informal, off-chain document, there will also be an optional script that can enforce some guidelines. This script acts to supplement the constitutional committee by restricting some proposal types. For example, if the community wishes to have some hard rules for the treasury that cannot be violated, a script that enforces these rules can be voted in as the guardrails script.\nThe guardrails script applies only to protocol parameter update and treasury withdrawal proposals.\nWarning CIP-1694 DReps should not be conflated with Project Catalyst DReps.\nIn order to participate in governance, a stake credential must be delegated to a DRep. Ada holders will generally delegate their voting rights to a registered DRep that will vote on their behalf. In addition, two pre-defined voting options are available:\nAbstain If an Ada holder delegates to Abstain, then their stake is actively marked as not participating in governance. The effect of delegating to Abstain on chain is that the delegated stake will not be considered to be a part of the active voting stake. However, the stake will be considered to be registered for the purpose of the incentives that are described in Incentives for Ada holders to delegate voting stake.\nAbstain\nAbstain\nIf an Ada holder delegates to Abstain, then their stake is actively marked as not participating in governance.\nAbstain\nThe effect of delegating to Abstain on chain is that the delegated stake will not be considered to be a part of the active voting stake. However, the stake will be considered to be registered for the purpose of the incentives that are described in Incentives for Ada holders to delegate voting stake.\nAbstain\nNo Confidence If an Ada holder delegates to No Confidence, then their stake is counted as a Yes vote on every No Confidence action and a No vote on every other action. The delegated stake will be considered part of the active voting stake. It also serves as a directly auditable measure of the confidence of Ada holders in the constitutional committee.\nNo Confidence\nNo Confidence\nIf an Ada holder delegates to No Confidence, then their stake is counted as a Yes vote on every No Confidence action and a No vote on every other action. The delegated stake will be considered part of the active voting stake. It also serves as a directly auditable measure of the confidence of Ada holders in the constitutional committee.\nNo Confidence\nYes\nNo Confidence\nNo\nNote The pre-defined voting options do not cast votes inside of transactions, their behavior is accounted for at the protocol level. The Abstain option may be chosen for a variety of reasons, including the desire to not participate in the governance system.\nAbstain\nNote Any Ada holder may register themselves as a DRep and delegate to themselves if they wish to actively participate in voting.\nNote Any wallet serving as the Registered Reward Wallet for a Stake Pool can be delegated to one of these Pre-defined Voting Options and doing so will serve as the default voting option selected by the SPO for all Governance Action votes, excepting Hard Fork Governance Actions. Due to the need for robust consensus around Hard Fork initiations, these votes must be met as a percentage of the stake held by all stake pools.\nIn Voltaire, existing stake credentials will be able to delegate their stake to DReps for voting purposes, in addition to the current delegation to stake pools for block production. DRep delegation will mimic the existing stake delegation mechanisms (via on-chain certificates). Similarly, DRep registration will mimic the existing stake registration mechanisms. Additionally, registered DReps will need to vote regularly to still be considered active. Specifically, if a DRep does not submit any votes for drepActivity-many epochs, the DRep is considered inactive, where drepActivity is a new protocol parameter. Inactive DReps do not count towards the active voting stake anymore, and can become active again for drepActivity-many epochs by voting on any governance actions or submitting a DRep update certificate. The reason for marking DReps as inactive is so that DReps who stop participating but still have stake delegated to them do not eventually leave the system in a state where no governance action can pass.\ndrepActivity\ndrepActivity\ndrepActivity\nRegistered DReps are identified by a credential that can be either:\nA verification key (Ed25519)\nA native or Plutus script\nThe blake2b-224 hash digest of a serialized DRep credential is called the DRep ID.\nThe following new types of certificates will be added for DReps: DRep registration certificates, DRep retirement certificates, and vote delegation certificates.\nDRep registration certificates include:\na DRep ID\na deposit\nan optional anchor\nAn anchor is a pair of:\na URL to a JSON payload of metadata\na hash of the contents of the metadata URL\nThe structure and format of this metadata is deliberately left open in this CIP. The on-chain rules will not check either the URL or the hash. Client applications should, however, perform the usual sanity checks when fetching content from the provided URL.\nDRep retirement certificates include:\na DRep ID\nNote that a DRep is retired immediately upon the chain accepting a retirement certificate, and the deposit is returned as part of the transaction that submits the retirement certificate (the same way that stake credential registration deposits are returned).\nVote delegation certificates include:\nthe DRep ID to which the stake should be delegated\nthe stake credential for the delegator\nNote\nDRep delegation always maps a stake credential to a DRep credential. This means that a DRep cannot delegate voting stake to another DRep.\nThe authorization scheme (i.e. which signatures are required for registration, retirement or delegation) mimics the existing stake delegation certificate authorization scheme.\nIn addition to the existing per-stake-credential distribution and the per-stake-pool distribution, the ledger will now also determine the per-DRep stake distribution. This distribution will determine how much stake each vote from a DRep is backed by.\nWarning\nUnlike the distribution that is used for block production, we will always use the most current version of the per-DRep stake distribution as given on the epoch boundary.\nThis means that for any topic which individual voters care deeply about, they have time to delegate to themselves as a DRep and vote directly. However, it means that there may be a difference between the stake that is used for block production and the stake that is used for voting in any given epoch.\nThere will be a short bootstrapping phase during which rewards will be earned for stake delegation etc. and may be withdrawn at any time. After this phase, although rewards will continue to be earned for block delegation etc., reward accounts will be blocked from withdrawing any rewards unless their associated stake credential is also delegated to a DRep or pre-defined voting option. This helps to ensure high participation, and so, legitimacy.\nNote\nEven though rewards cannot be withdrawn, they are not lost. As soon as a stake credential is delegated (including to a pre-defined voting option), the rewards can be withdrawn.\nDReps arguably need to be compensated for their work. Research on incentive models is still ongoing, and we do not wish to hold up implementation of this CIP while this is resolved.\nOur interim proposal is therefore to escrow Lovelace from the existing Cardano treasury until this extremely important decision can be agreed on by the community, through the on-chain governance mechanism that is being constructed.\nAlternatively, DReps could pay themselves through instances of the \"Treasury withdrawal\" governance action. Such an action would be auditable on-chain, and should reflect an off-chain agreement between DReps and delegators.\nWe define seven different types of governance actions. A governance action is an on-chain event that is triggered by a transaction and has a deadline after which it cannot be enacted.\nAn action is said to be ratified when it gathers enough votes in its favor (through the rules and parameters that are detailed below).\nAn action that fails to be ratified before its deadline is said to have expired.\nAn action that has been ratified is said to be enacted once it has been activated on the network.\nAny Ada holder can submit a governance action to the chain. They must provide a deposit of govActionDeposit Lovelace, which will be returned when the action is finalized (whether it is ratified or has expired). The deposit amount will be added to the deposit pot, similar to stake key deposits. It will also be counted towards the stake of the reward address it will be paid back to, to not reduce the submitter's voting power to vote on their own (and competing) actions.\ngovActionDeposit\nIf a guardrails script is present, the transaction must include that script in the witness set either directly, or via reference inputs, and any other requirements that the guardrails script makes must be satisfied.\nNote that a motion of no-confidence is an extreme measure that enables Ada holders to revoke the power that has been granted to the current constitutional committee.\nNote A single governance action might contain multiple protocol parameter updates. Many parameters are inter-connected and might require moving in lockstep.\nGovernance actions are ratified through on-chain voting actions. Different kinds of governance actions have different ratification requirements but always involve two of the three governance bodies, with the exception of a hard-fork initiation and security-relevant protocol parameters, which requires ratification by all governance bodies. Depending on the type of governance action, an action will thus be ratified when a combination of the following occurs:\nthe constitutional committee approves of the action (the number of members who vote Yes meets the threshold of the constitutional committee)\nYes\nthe DReps approve of the action (the stake controlled by the DReps who vote Yes meets a certain threshold of the total active voting stake)\nYes\nthe SPOs approve of the action (the stake controlled by the SPOs who vote Yes meets a certain threshold of the total active voting stake, excepting Hard Fork Governance Actions)\nYes\nWarning As explained above, different stake distributions apply to DReps and SPOs.\nA successful motion of no-confidence, update of the constitutional committee, a constitutional change, or a hard-fork, delays ratification of all other governance actions until the first epoch after their enactment. This gives an updated constitutional committee enough time to vote on current proposals, re-evaluate existing proposals with respect to a new constitution, and ensures that the in principle arbitrary semantic changes caused by enacting a hard-fork do not have unintended consequences in combination with other actions.\nThe following table details the ratification requirements for each governance action scenario. The columns represent:\nGovernance action type The type of governance action. Note that the protocol parameter updates are grouped into four categories.\nGovernance action type The type of governance action. Note that the protocol parameter updates are grouped into four categories.\nConstitutional committee (abbrev. CC) A value of indicates that the constitutional committee must approve this action. A value of - means that constitutional committee votes do not apply.\nConstitutional committee (abbrev. CC) A value of indicates that the constitutional committee must approve this action. A value of - means that constitutional committee votes do not apply.\nDReps The DRep vote threshold that must be met as a percentage of active voting stake.\nDReps The DRep vote threshold that must be met as a percentage of active voting stake.\nSPOs The SPO vote threshold which must be met as a certain threshold of the total active voting stake, excepting Hard Fork Governance Actions. Due to the need for robust consensus around Hard Fork initiations, these votes must be met as a percentage of the stake held by all stake pools. A value of - means that SPO votes do not apply.\nSPOs The SPO vote threshold which must be met as a certain threshold of the total active voting stake, excepting Hard Fork Governance Actions. Due to the need for robust consensus around Hard Fork initiations, these votes must be met as a percentage of the stake held by all stake pools. A value of - means that SPO votes do not apply.\nEach of these thresholds is a governance parameter. There is one additional threshold, Q5, related to security relevant protocol parameters, which is explained below. The initial thresholds should be chosen by the Cardano community as a whole. All thresholds for the Info action are set to 100 since setting it any lower would result in not being able to poll above the threshold.\nQ5\nSome parameters are relevant to security properties of the system. Any proposal attempting to change such a parameter requires an additional vote of the SPOs, with the threshold Q5.\nQ5\nThe security relevant protocol parameters are:\nmaxBlockBodySize\nmaxBlockBodySize\nmaxTxSize\nmaxTxSize\nmaxBlockHeaderSize\nmaxBlockHeaderSize\nmaxValueSize\nmaxValueSize\nmaxBlockExecutionUnits\nmaxBlockExecutionUnits\ntxFeePerByte\ntxFeePerByte\ntxFeeFixed\ntxFeeFixed\nutxoCostPerByte\nutxoCostPerByte\ngovActionDeposit\ngovActionDeposit\nminFeeRefScriptCostPerByte\nminFeeRefScriptCostPerByte\nNote It may make sense for some or all thresholds to be adaptive with respect to the Lovelace that is actively registered to vote. For example, a threshold could vary between 51 for a high level of registration and 75 for a low level registration. Moreover, the treasury threshold could also be adaptive, depending on the total Lovelace that is being withdrawn, or different thresholds could be set for different levels of withdrawal.\nNote To achieve legitimacy, the minimum acceptable threshold should be no less than 50 of the delegated stake.\nApart from Treasury withdrawals and Infos, we include a mechanism for ensuring that governance actions of the same type do not accidentally clash with each other in an unexpected way.\nEach governance action must include the governance action ID for the most recently enacted action of its given type. This means that two actions of the same type can be enacted at the same time, but they must be deliberately designed to do so.\nActions that have been ratified in the current epoch are prioritized as follows for enactment:\nMotion of no-confidence Update committee/threshold New Constitution or Guardrails Script Hard Fork initiation Protocol parameter changes Treasury withdrawals Info\nMotion of no-confidence\nUpdate committee/threshold\nNew Constitution or Guardrails Script\nHard Fork initiation\nProtocol parameter changes\nTreasury withdrawals\nInfo\nNote Info actions cannot be ratified or enacted, since they do not have any effect on the protocol.\nGovernance actions are enacted in order of acceptance to the chain. This resolves conflicts where, e.g. there are two competing parameter changes.\nGovernance actions are checked for ratification only on an epoch boundary. Once ratified, actions are staged for enactment.\nAll submitted governance actions will therefore either:\nbe ratified, then enacted or expire after a number of epochs\nbe ratified, then enacted\nor expire after a number of epochs\nIn all of those cases, deposits are returned immediately.\nAll governance actions are enacted on the epoch boundary after their ratification.\nEvery governance action will include the following:\na deposit amount (recorded since the amount of the deposit is an updatable protocol parameter)\na reward address to receive the deposit when it is repaid\nan anchor for any metadata that is needed to justify the action\na hash digest value to prevent collisions with competing actions of the same type (as described earlier)\nIn addition, each action will include some elements that are specific to its type:\nNote The new major protocol version must be precisely one greater than the current protocol version. Any two consecutive epochs will therefore either have the same major protocol version, or the later one will have a major protocol version that is one greater.\nNote There can be no duplicate committee members - each pair of credentials in a committee must be unique.\nEach governance action that is accepted on the chain will be assigned a unique identifier (a.k.a. the governance action ID), consisting of the transaction hash that created it and the index within the transaction body that points to it.\nWe have grouped the protocol parameter changes by type, allowing different thresholds to be set for each group.\nWe are not, however, restricting each protocol parameter governance action to be contained within one group. In case where a governance action carries updates for multiple parameters from different groups, the maximum threshold of all the groups involved will apply to any given such governance action.\nThe network, economic and technical parameter groups collect existing protocol parameters that were introduced during the Shelley, Alonzo and Babbage eras. In addition, we introduce a new governance group that is specific to the new governance parameters that will be introduced by CIP-1694.\nThe network group consists of:\nmaximum block body size (maxBlockBodySize)\nmaxBlockBodySize\nmaximum transaction size (maxTxSize)\nmaxTxSize\nmaximum block header size (maxBlockHeaderSize)\nmaxBlockHeaderSize\nmaximum size of a serialized asset value (maxValueSize)\nmaxValueSize\nmaximum script execution units in a single transaction (maxTxExecutionUnits)\nmaxTxExecutionUnits\nmaximum script execution units in a single block (maxBlockExecutionUnits)\nmaxBlockExecutionUnits\nmaximum number of collateral inputs (maxCollateralInputs)\nmaxCollateralInputs\nThe economic group consists of:\nminimum fee coefficient (txFeePerByte)\ntxFeePerByte\nminimum fee constant (txFeeFixed)\ntxFeeFixed\ndelegation key Lovelace deposit (stakeAddressDeposit)\nstakeAddressDeposit\npool registration Lovelace deposit (stakePoolDeposit)\nstakePoolDeposit\nmonetary expansion (monetaryExpansion)\nmonetaryExpansion\ntreasury expansion (treasuryCut)\ntreasuryCut\nminimum fixed rewards cut for pools (minPoolCost)\nminPoolCost\nminimum Lovelace deposit per byte of serialized UTxO (utxoCostPerByte)\nutxoCostPerByte\nprices of Plutus execution units (executionUnitPrices)\nexecutionUnitPrices\nThe technical group consists of:\npool pledge influence (poolPledgeInfluence)\npoolPledgeInfluence\npool retirement maximum epoch (poolRetireMaxEpoch)\npoolRetireMaxEpoch\ndesired number of pools (stakePoolTargetNum)\nstakePoolTargetNum\nPlutus execution cost models (costModels)\ncostModels\nproportion of collateral needed for scripts (collateralPercentage)\ncollateralPercentage\nThe governance group consists of all the new protocol parameters that are introduced in this CIP:\ngovernance voting thresholds (P1P_1P1 , P2aP_{2a}P2a , P2bP_{2b}P2b , P3P_3P3 , P4P_4P4 , P5aP_{5a}P5a , P5bP_{5b}P5b , P5cP_{5c}P5c , P5dP_{5d}P5d , P6P_6P6 , Q1Q_1Q1 , Q2aQ_{2a}Q2a , Q2bQ_{2b}Q2b , Q4Q_4Q4 , Q5Q_5Q5 )\ngovernance action maximum lifetime in epochs (govActionLifetime)\ngovActionLifetime\ngovernance action deposit (govActionDeposit)\ngovActionDeposit\nDRep deposit amount (dRepDeposit)\ndRepDeposit\nDRep activity period in epochs (dRepActivity)\ndRepActivity\nminimal constitutional committee size (committeeMinSize)\ncommitteeMinSize\nmaximum term length (in epochs) for the constitutional committee members (committeeMaxTermLength)\ncommitteeMaxTermLength\nEach vote transaction consists of the following:\na governance action ID\na role - constitutional committee member, DRep, or SPO\na governance credential witness for the role\nan optional anchor (as defined above) for information that is relevant to the vote\na 'Yes'/'No'/'Abstain' vote\nFor SPOs and DReps, the number of votes that are cast (whether 'Yes', 'No' or 'Abstain') is proportional to the Lovelace that is delegated to them at the point the action is checked for ratification. For constitututional committee members, each current committee member has one vote.\nWarning 'Abstain' votes are not included in the \"active voting stake\".\nNote that an explicit vote to abstain differs from abstaining from voting. Unregistered stake that did not vote behaves like an 'Abstain' vote, while registered stake that did not vote behaves like a 'No' vote. To avoid confusion, we will only use the word 'Abstain' from this point onward to mean an on-chain vote to abstain.\nThe governance credential witness will trigger the appropriate verifications in the ledger according to the existing UTxOW ledger rule (i.e. a signature check for verification keys, and a validator execution with a specific vote redeemer and new Plutus script purpose for scripts).\nUTxOW\nVotes can be cast multiple times for each governance action by a single credential. Correctly submitted votes override any older votes for the same credential and role. That is, the voter may change their position on any action if they choose. As soon as a governance action is ratified, voting ends and transactions containing further votes are invalid.\nWhen a governance action is successfully submitted to the chain, its progress will be tracked by the ledger state. In particular, the following will be tracked:\nthe governance action ID\nthe epoch that the action expires\nthe deposit amount\nthe rewards address that will receive the deposit when it is returned\nthe total 'Yes'/'No'/'Abstain' votes of the constitutional committee for this action\nthe total 'Yes'/'No'/'Abstain' votes of the DReps for this action\nthe total 'Yes'/'No'/'Abstain' votes of the SPOs for this action\nSince the stake snapshot changes at each epoch boundary, a new tally must be calculated when each unratified governance action is checked for ratification. This means that an action could be enacted even though the DRep or SPO votes have not changed (since the vote delegation could have changed).\nWe define a number of new terms related to voting stake:\nLovelace contained in a transaction output is considered active for voting (that is, it forms the \"active voting stake\"): It contains a registered stake credential. The registered stake credential has delegated its voting rights to a DRep.\nIt contains a registered stake credential.\nThe registered stake credential has delegated its voting rights to a DRep.\nRelative to some percentage P, a DRep (SPO) vote threshold has been met if the sum of the relative stake that has been delegated to the DReps (SPOs) that vote Yes to a governance action is at least P.\nP\nYes\nP\nRole of the constitutional committee\nIntentional omission of identity verification\nReducing the power of entities with large amounts of Ada\nPiggybacking on stake pool stake distribution\nSeparation of hard-fork initiation from standard protocol parameter changes\nThe purpose of the DReps\nRatification requirements table\nMotion of no-confidence\nUpdate committee/threshold (state of no-confidence)\nThe versatility of the info governance action\nHard-fork initiation\nNew metadata structures\nControlling the number of active governance actions\nNo AVST\nAt first sight, the constitutional committee may appear to be a special committee that has been granted extra power over DReps. However, because DReps can replace the constitutional committee at any time and DRep votes are also required to ratify every governance action, the constitutional committee has no more (and may, in fact, have less) power than the DReps. Given this, what role does the committee play, and why is it not superfluous? The answer is that the committee solves the bootstrapping problem of the new governance framework. Indeed, as soon as we pull the trigger and enable this framework to become active on-chain, then without a constitutional committee, there would rapidly need to be sufficient DReps, so that the system did not rely solely on SPO votes. We cannot yet predict how active the community will be in registering as DReps, nor how reactive other Ada holders will be regarding delegation of votes.\nThus, the constitutional committee comes into play to make sure that the system can transition from its current state into fully decentralized governance in due course. Furthermore, in the long run, the committee can play a mentoring and advisory role in the governance decisions by being a set of elected representatives who are put under the spotlight for their judgment and guidance in governance decisions. Above all, the committee is required at all times to adhere to the Constitution and to ratify proposals in accordance with the provisions of the Constitution.\nNote that this CIP does not mention any kind of identity validation or verification for the members of the constitutional committee or the DReps.\nThis is intentional.\nWe hope that the community will strongly consider only voting for and delegating to those DReps who provide something like a DID to identify themselves. However, enforcing identity verification is very difficult without some centralized oracle, which we consider to be a step in the wrong direction.\nVarious mechanisms, such as quadratic voting, have been proposed to guard against entities with a large amount of influence. In a system based on \"1 Lovelace, 1 vote\", however, it is trivially easy to split stake into small amounts and undo the protections. Without an on-chain identity verification system we cannot adopt any such measures.\nThe Cardano protocol is based on a Proof-of-Stake consensus mechanism, so using a stake-based governance approach is sensible. However, there are many ways that could be used to define how to record the stake distribution between participants. As a reminder, network addresses can currently contain two sets of credentials: one to identify who can unlock funds at an address (a.k.a. payment credentials) and one that can be delegated to a stake pool (a.k.a. delegation credentials).\nRather than defining a third set of credentials, we instead propose to re-use the existing delegation credentials, using a new on-chain certificate to determine the governance stake distribution. This implies that the set of DReps can (and likely will) differ from the set of SPOs, so creating balance. On the flip side, it means that the governance stake distribution suffers from the same shortcomings as that for block production: for example, wallet software providers must support multi-delegation schemes and must facilitate the partitioning of stake into sub-accounts should an Ada holder desire to delegate to multiple DReps, or an Ada holder must manually split their holding if their wallet does not support this.\nHowever, this choice also limits future implementation effort for wallet providers and minimizes the effort that is needed for end-users to participate in the governance protocol. The latter is a sufficiently significant concern to justify the decision. By piggybacking on the existing structure, the system remains familiar to users and reasonably easy to set up. This maximizes both the chance of success of, and the rate of participation in, the governance framework.\nIn contrast to other protocol parameter updates, hard forks (or, more correctly, changes to the protocol's major version number) require much more attention. Indeed, while other protocol parameter changes can be performed without significant software changes, a hard fork assumes that a super-majority of the network has upgraded the Cardano node to support the new set of features that are introduced by the upgrade. This means that the timing of a hard fork event must be communicated well ahead of time to all Cardano users, and requires coordination between stake pool operators, wallet providers, DApp developers, and the node release team.\nHence, this proposal, unlike the Shelley scheme, promotes hard fork initiations as a standalone governance action, distinct from protocol parameter updates.\nNothing in this proposal limits SPOs from becoming DReps. Why do we have DReps at all? The answer is that SPOs are chosen purely for block production and not all SPOs will want to become DReps. Voters can choose to delegate their vote to DReps without needing to consider whether they are also a good block producer, and SPOs can choose to represent Ada holders or not.\nThe requirements in the ratification requirement table are explained here. Most of the governance actions have the same kind of requirements: the constitutional committee and the DReps must reach a sufficient number of 'Yes' votes. This includes these actions:\nUpdate committee/threshold (normal state)\nNew Constitution\nProtocol parameter changes\nTreasury withdrawal\nA motion of no-confidence represents a lack of confidence by the Cardano community in the current constitutional committee, and hence the constitutional committee should not be included in this type of governance action. In this situation, the SPOs and the DReps are left to represent the will of the community.\nSimilar to the motion of no-confidence, electing a constitutional committee depends on both the SPOs and the DReps to represent the will of the community.\nWhile not binding on chain, the Info governance action could be useful in an number of situations. These include:\nratifying a CIP\ndeciding on the genesis file for a new ledger era\nrecording initial feedback for future governance actions\nRegardless of any governance mechanism, SPO participation is needed for any hard fork since they must upgrade their node software. For this reason, we make their cooperation explicit in the hard fork initiation governance action, by always requiring their vote. The constitutional committee also votes, signaling the constitutionality of a hard fork. The DReps also vote, to represent the will of every stake holder.\nThe governance actions, the votes and the certificates and the Constitution use new metadata fields, in the form of URLs and integrity hashes (mirroring the metadata structure for stake pool registration). The metadata is used to provide context. Governance actions need to explain why the action is needed, what experts were consulted, etc. Since transaction size constraints should not limit this explanatory data, we use URLs instead.\nThis does, however, introduce new problems. If a URL does not resolve, what should be the expectation for voting on that action? Should we expect everyone to vote 'No'? Is this an attack vector against the governance system? In such a scenario, the hash pre-image could be communicated in other ways, but we should be prepared for the situation. Should there be a summary of the justification on chain?\nInstead of specific dedicated fields in the transaction format, we could instead use the existing transaction metadata field.\nGovernance-related metadata can be clearly identified by registering a CIP-10 metadata label. Within that, the structure of the metadata can be determined by this CIP (exact format TBD), using an index to map the vote or governance action ID to the corresponding metadata URL and hash.\nThis avoids the need to add additional fields to the transaction body, at the risk of making it easier for submitters to ignore. However, since the required metadata can be empty (or can point to a non-resolving URL), it is already easy for submitters to not provide metadata, and so it is unclear whether this makes the situation worse.\nNote that transaction metadata is never stored in the ledger state, so it would be up to clients to pair the metadata with the actions and votes in this alternative, and would not be available as a ledger state query.\nSince governance actions are available for anyone to submit, we need some mechanism to prevent those individuals responsible for voting from becoming overwhelmed with a flood of proposals. A large deposit is one such mechanism, but this comes at the unfortunate cost of being a barrier for some people to submit an action. Note, however, that crowd-sourcing with a Plutus script is always an option to gather the deposit.\nWe could, alternatively, accept the possibility of a large number of actions active at any given time, and instead depend on off-chain socialization to guide voters' attention to those that merit it. In this scenario, the constitutional committee might choose to only consider proposals which have already garnered enough votes from the DReps.\nAn earlier draft of this CIP included the notion of an \"active voting stake threshold\", or AVST. The purpose of AVST was to ensure the legitimacy of each vote, removing the possibility that, for example, 9 out of 10 Lovelace could decide the fate of millions of entities on Cardano. There are really two concerns here, which are worth separating.\nThe first concern is that of bootstrapping the system, i.e. reaching the initial moment when sufficient stake is registered to vote. The second concern is that the system could lose participation over time. One problem with the AVST is that it gives an incentive for SPOs to desire a low voting registration (since their votes then hold more weight). This is absolutely not a slight on the existing SPOs, but an issue with bad incentives.\nWe have chosen, therefore, to solve the two concerns differently. We solve the bootstrapping problem as described in the section on bootstrapping. We solve the long-term participation problem by not allowing reward withdrawals (after the bootstrap phase) unless the stake is delegated to a DRep (including the two special cases, namely 'Abstain' and 'No confidence').\nThank the workshop attendees.\nWe have added Constitutional Committee terms.\nTwo new \"pre-defined\" voting options: abstain and no confidence.\nNew \"Info\" governance action.\nUse the most recent DRep stake distribution for ratification. This means that if ever your DRep votes how you do not like, you can immediately make yourself a DRep and vote how you want.\nEscrow some ADA from the current treasury for potential future DRep incentives.\nRemove the tiered treasury actions in favor of something adaptive (so the \"yes\" threshold would depend on: how much ada, how high the registered voting stake, and maybe how much ada is released every epoch\nhow much ada, how high the registered voting stake, and maybe how much ada is released every epoch\nhow much ada,\nhow high the registered voting stake, and maybe\nhow much ada is released every epoch\nSplit the protocol parameter updates into four groups: network, economic, technical, and governmental.\nMost governance actions can be enacted (upon ratification) right away. All but: protocol parameters and hard forks.\nRemove \"one action per type per epoch\" restriction in favor of tracking the last action ID of each type, and including this in the action.\nNo AVST.\nBootstrap phase: Until X of ADA is registered to vote or Y epochs have elapsed, only parameter changes and hard forks can happen. PP changes just need CC threshold, HFs need CC and SPOs. After the bootstrap phase, we put in place the incentive to keep low DReps, but this mechanism automatically relaxes.\nNew plutus script purpose for DReps.\nMultiple treasury withdrawals in one epoch.\nA section on the recursive problem of \"how do we ratify this CIP\".\nChanges to the local state-query protocol.\nNew ideas, time permitting: Weigh SPO stake vote by pledge somehow. DReps can specify which other DRep gets their delegators in the event that they retire. Reduced government action deposit if one member of the CC signs off on it (which presumably means it has gone through some process). Include hash of (future) genesis configuration within HF proposal.\nWeigh SPO stake vote by pledge somehow.\nDReps can specify which other DRep gets their delegators in the event that they retire.\nReduced government action deposit if one member of the CC signs off on it (which presumably means it has gone through some process).\nInclude hash of (future) genesis configuration within HF proposal.\nAdd guardrails script, which can control what treasury withdrawals and protocol parameter changes are allowed.\nRemove dropping of governance actions. The only effect this has is that in case a no confidence action passes, actions stay around. However, only new committee proposals that have been designed to build on top of that no confidence action can be enacted. If a new committee gets elected while some of those actions haven't expired, those actions can be ratified but the new committee has to approve them.\nAll governance actions are enacted one epoch after they are ratified.\nMove post-bootstrapping restrictions into 'Other Ideas'.\nAdd a section on different deposit amounts to 'Other Ideas'.\nAdd a section for a minimum AVS to 'Other Ideas'.\nRename some protocol parameters.\nRename TALLY to GOV.\nTALLY\nGOV\nTurn the Constitution into an anchor.\nRework which anchors are required and which are optional.\nClean up various inconsistencies and leftovers from older versions.\nGuard security-relevant changes behind SPO votes.\nThe system does not enter a state of no confidence with insufficient active CC members, the CC just becomes unable to act.\nClarify that CC members can use any kind of credential.\nUpdate the section on the bootstrap period.\nMention missing Q_5 parameter.\nQ_5\nVarious small fixes/consistency changes.\nA new ledger era is enabled on the Cardano mainnet, which implements the above specification.\nEnabled via Chang #1 hardfork\nThe features in this CIP require a hard fork.\nThis document describes an ambitious change to Cardano governance. We propose to implement the changes via two hard forks: the first one containing all new features but some being disabled for a bootstrap period and the second one enabling all features.\nIn the following sections, we give more details about the various implementation work items that have already been identified. In addition, the final section exposes a few open questions which will need to be finalized. We hope that those questions can be addressed through community workshops and discussions.\nThe ratification of this proposal is something of a circular problem: we need some form of governance framework in order to agree on what the final governance framework should be. As has been stated many times, CIPs are not authoritative, nor are they a governance mechanism. Rather, they describe technical solutions that have been deemed sound (from a technical standpoint) by community of experts.\nCIP-1694 arguably goes beyond the usual scope of the CIP process and there is a strong desire to ratify this CIP through some process. However, that process is yet to be defined and it remains an open question. The final ratification process is likely to be a blend of various ideas, such as:\nGather opinions from community-held workshops, akin to the Colorado workshop of February-March 2023.\nExercise voting actions on a public testnet, with sufficient participation.\nPoll the established SPOs.\nLeverage Project Catalyst to gather inputs from the existing voting community (albeit small in terms of active stake).\nNew elements will be added to the transaction body, and existing update and MIR capabilities will be removed. In particular, The governance actions and votes will comprise two new transaction body fields.\nNew elements will be added to the transaction body, and existing update and MIR capabilities will be removed. In particular,\nThe governance actions and votes will comprise two new transaction body fields.\nThree new kinds of certificates will be added in addition to the existing ones: DRep registration DRep de-registration Vote delegation And similarly, the current MIR and genesis certificates will be removed.\nThree new kinds of certificates will be added in addition to the existing ones:\nDRep registration\nDRep de-registration\nVote delegation\nAnd similarly, the current MIR and genesis certificates will be removed.\nA new Voting purpose will be added to Plutus script contexts. This will provide, in particular, the vote to on-chain scripts.\nA new Voting purpose will be added to Plutus script contexts. This will provide, in particular, the vote to on-chain scripts.\nVoting\nWarning As usual, we will provide a CDDL specification for each of those changes.\nThe PPUP transition rule will be rewritten and moved out of the UTxO rule and into the LEDGER rule as a new GOV rule. It will process and record the governance actions and votes.\nThe PPUP transition rule will be rewritten and moved out of the UTxO rule and into the LEDGER rule as a new GOV rule.\nPPUP\nUTxO\nLEDGER\nGOV\nIt will process and record the governance actions and votes.\nThe NEWEPOCH transition rule will be modified.\nThe NEWEPOCH transition rule will be modified.\nNEWEPOCH\nThe MIR sub-rule will be removed.\nThe MIR sub-rule will be removed.\nMIR\nA new RATIFY rule will be introduced to stage governance actions for enactment. It will ratify governance actions, and stage them for enactment in the current or next epoch, as appropriate.\nA new RATIFY rule will be introduced to stage governance actions for enactment.\nRATIFY\nIt will ratify governance actions, and stage them for enactment in the current or next epoch, as appropriate.\nA new ENACTMENT rule will be called immediately after the EPOCH rule. This rule will enact governance actions that have previously been ratified.\nA new ENACTMENT rule will be called immediately after the EPOCH rule. This rule will enact governance actions that have previously been ratified.\nENACTMENT\nEPOCH\nThe EPOCH rule will no longer call the NEWPP sub-rule or compute whether the quorum is met on the PPUP state.\nThe EPOCH rule will no longer call the NEWPP sub-rule or compute whether the quorum is met on the PPUP state.\nEPOCH\nNEWPP\nThe on-chain governance workload is large, but the off-chain workload for tools and applications will arguably be even larger. To build an effective governance ecosystem, the ledger will have to provide interfaces to various governance elements.\nWhile votes and DReps (de)registrations are directly visible in blocks and will, therefore, be accessible via the existing local-chain-sync protocols; we will need to upgrade the local-state-query protocol to provide extra insights on information which are harder to infer from blocks (i.e. those that require maintaining a ledger state). New state queries should cover (at least):\nGovernance actions currently staged for enactment\nGovernance actions under ratification, with the total and percentage of yes stake, no stake and abstain stake\nThe current constitutional committee, and constitution hash digest\nWe will need to be careful how we bootstrap this fledgling government. All the parties that are involved will need ample time to register themselves and to become familiar with the process.\nSpecial provisions will apply in the initial bootstrap phase. Firstly, during the bootstrap phase, a vote from the constitutional committee is sufficient to change the protocol parameters. Secondly, during the bootstrap phase, a vote from the constitutional committee, together with a sufficient SPO vote, is sufficient to initiate a hard fork. Thirdly, info actions will be available. No other actions other than those mentioned in this paragraph are possible during the bootstrap phase.\nThe bootstrap phase ends when the Constitutional Committee and SPOs ratify a subsequent hard fork, enabling the remaining governance actions and DRep participation. This is likely to be a number of months after the Chang hard fork. Although all features will be technically available at this point, additional requirements for using each feature may be specified in the constitution.\nMoreover, there will be an interim Constitutional committee with a set term, also specified in the next ledger era configuration file. The rotational schedule of the first non-interim committee could be included in the constitution itself. Note, however, that since the constitutional committee never votes on new committees, it cannot actually enforce the rotation.\nThe SPO vote could additionally be weighted by each SPO's pledge. This would provide a mechanism for allowing those with literal stake in the game to have a stronger vote. The weighting should be carefully chosen.\nA DRep could optionally list another DRep credential in their registration certificate. Upon retirement, all of the DRep's delegations would be automatically transferred to the given DRep credential. If that DRep had already retired, the delegation would be transfer to the 'Abstain' voting option.\nSince the DRep registration does not perform any necessary functions, the certificates for (de-)registering DReps could be removed. This makes the democracy more liquid since it removes some bureaucracy and also removes the need for the DRep deposit, at the cost of moving the anchor that is part of the DRep registration certificate into the transaction metadata.\nThe deposit that is attached to governance actions exists to prevent a flood of non-serious governance actions, each of which would require time and attention from the Cardano community. We could reduce this deposit for proposals which go through some agreed upon off-chain process. This would be marked on-chain by the endorsement of at least one constitutional committee member. The downside of this idea is that it gives more power to the constitutional committee.\nMultiple workshops for this CIP have proposed to introduce a different deposit amount for each type of governance action. It was not clear whether a majority was in favor of this idea, but this may be considered if it becomes clear that it is necessary.\nAs a further guarantee to ensure governance actions cannot be proposed right before a hard fork, be voted on by one DRep with a large amount of stake and be enacted immediately, there could be an additional requirement that a certain fixed absolute amount of stake needs to cast a 'Yes' vote on the action to be enacted.\nThis does not seem necessary in the current design, since the stake of all registered DReps behaves like a 'No' vote until they have actually cast a vote. This means that for this scenario to occur, the malicious actor needs at least to be in control of the fraction of DRep stake corresponding to the relevant threshold, at which point this might as well be considered a legitimate action.\nSome hard-forks require new genesis configurations. This has been the case for the Shelley and Alonzo hard forks (but not Allegra, Mary, Vasil or Valentine), may be the case in the future. At the moment, this proposal doesn't state anything about such a genesis configuration: it is implicitly assumed to be an off-chain agreement. We could however, enforce that (the hash of) a specific genesis configuration is also captured within a hard-fork governance action.\nAs discussed above, it may make sense for some or all thresholds to be adaptive with respect to the Lovelace that is actively registered to vote, so that the system provides greater legitimacy when there is only a low level of active voting stake. The bootstrapping mechanism that is proposed above may subsume this, however, by ensuring that the governance system is activated only when a minimum level of stake has been delegated to DReps.\nIt has been stated several times that \"DReps\" as presented here, might be confused with Project Catalst DReps. Similarly, some people have expressed confusion between the state of no-confidence, the motion of no-confidence and the no-confidence voting option.\nWe could imagine finding better terms for these concepts.\nNothing prevents money being taken out of the treasury other than the proposed votes and voting thresholds. Given that the Cardano treasury is a quite fundamental component of its monetary policy, we could imagine enforcing (at the protocol level) the maximum amount that can removed from the treasury over any period of time.\nMany people have stated that they believe that the actual voting turnout will not be so large as to be a strain on the throughput of the system. We also believe that this is likely to be the case, but when the bootstrap phase ends we might put one final, temporary safety measure in place (this will also allow us to justify a low DRep deposit amount).\nFor values of XXX and YYY that are still to be determined, as soon as the bootstrap phase has ended, when we calculate the DReps stake distribution for the next epoch boundary, we will consider only those DReps that are either in the top XXX-many DReps ranked by stake amount, or those DReps that have at least YYY Lovelace. Every epoch, the value of XXX will increase and the value of YYY will decrease, so that eventually XXX will be effectively infinite and YYY will be zero. Note that this is only an incentive, and nothing actually stops any DRep from casting their vote (though it will not be counted if it does not meet the requirements).\nIf the community decides at some point that there is indeed a problem with congestion, then a hard fork could be enacted that limits the number of DReps in a more restrictive way.\nReasonable numbers for the initial value of XXX are probably 5,000-10,000. Reasonable numbers for the initial value of YYY are probably the total number of Lovelace divided by the initial value of XXX.\nThe mechanism should be set to relax at a rate where the restriction is completely eliminated after a period of six months to one year.\nMany people have commented on and contributed to the first draft of this document, which was published in November 2022. We would especially like to thank the following people for providing their wisdom and insights:\nJack Briggs\nTim Harrison\nPhilip Lazos\nMichael Madoff\nEvangelos Markakis\nJoel Telpner\nThomas Upfield\nWe would also like to thank those who have commented via Github and other channels.\nIn addition, we would like to thank all the attendees of the workshop that was held in Longmont, Colorado on February 28th and March 1st 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nAdam Rusch, ADAO & Summon\nAddie Girouard\nAndrew Westberg\nDarlington Wleh, LidoNation\nEystein Hansen\nJames Dunseith, Gimbalabs\nJuana Attieh\nKenric Nelson\nLloyd Duhon, DripDropz\nMarcus Jay Allen\nMarek Mahut, 5 Binaries\nMarkus Gufler\nMatthew Capps\nMercy, Wada\nMichael Dogali\nMichael Madoff\nPatrick Tobler, NMKR\nPhilip Lazos\nLanningham, SundaeSwap\nRick McCracken\nRomain Pellerin\nSergio Sanchez Ferreros\nTim Harrison\nTsz Wai Wu\nIn addition, we would like to thank all the attendees of the workshop that was held in Mexico City, Mexico on May 20th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nDonovan Ria o\nCristian Jair Rojas\nVictor Hern ndez\nRam n Aceves\nSergio Andr s Cort s\nIsa as Alejandro Galv n\nAbigail Guzm n\nJorge Fernando Murgu a\nLuis Guillermo Santana\nIn addition, we would like to thank all the attendees of the workshop that was held in Buenos Aires, Argentina on May 20th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nLucas Macchiavelli\nAlejando Pestchanker\nJuan Manuel Castro Pippo\nFederico Weill\nJose Otegui\nMercedes Ruggeri\nMauro Andreoli\nElias Aires\nJorge Nasanovsky\nUlises Barreiro\nMartin Ochoa\nFacundo Lopez\nVanina Estrugo\nLuca Pestchanker\nIn addition, we would like to thank all the attendees of the workshop that was held in Johannesburg, South Africa on May 25th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nCeliwe Ngwenya\nBernard Sibanda\nDumo Mbobo\nShaolyn Dzwedere\nKunoshe Muchemwa\nSiphiwe Mbobo\nLucas Sibindi\nDayTapoya\nMdu Ngwenya\nLucky Khumalo\nSkhangele Malinga\nJoyce Ncube\nCosta Katenhe\nBramwell Kasanga\nPrecious Abimbola\nEthel Q Tshuma\nPanashe Sibanda\nRadebe Tefo\nKaelo Lentsoe\nRichmond Oppong\nIsrael Ncube\nSikhangele Malinga\nNana Safo\nNdaba Delsie\nCollen Tshepang\nDzvedere Shaolyn\nThandazile Sibanda\nNcube Joyce\nLucas Sibindi\nPinky Ferro\nIshmael Ntuta\nKhumalo Lucky\nFhulufelo\nThwasile Ngwenya\nKunashe Muchemwa\nDube Bekezela\nTinyiko Baloi\nDada Nomathemba\nIn addition, we would like to thank all the attendees of the workshop that was held in Bogota, Colombia on May 27th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nAlvaro Moncada\nJaime Andres Posada Castro\nJose Miguel De Gamboa\nNicolas Gomez\nLuis Restrepo (Moxie)\nJuanita Jaramillo R.\nDaniel Vanegas\nErnesto Rafael Pabon Moreno\nCarlos Eduardo Escobar\nManuel Fernando Brice o\nSebastian Pabon\nIn addition, we would like to thank all the attendees of the workshop that was held in Caracas, Venezuela on May 27th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nJean Carlo Aguilar\nWilmer Var n\nJos Erasmo Colmenares\nDavid Ja n\nF lix D vila\nYaneth Duarte\nNando Vitti\nWilmer Rojas\nAndreina Garc a\nCarmen Galban\nOsmarlina Ag ero\nEnder Linares\nCarlos A. Palacios R\nDewar Rodr guez\nLennys Blanco\nFrancys Garc a\nDavidson Arenas\nIn addition, we would like to thank all the attendees of the workshop that was held in Manizales, Colombia on May 27th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nYaris Cruz\nYaneth Duarte\nCiro Gelvez\nKevin Chacon\nJuan Sierra\nCaue Chianca\nSonia Malagon\nFacundo Ramirez\nHope R.\nIn addition, we would like to thank all the attendees of the workshop that was held in Addis Ababa, Ethiopia on May 27th and 28th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nKaleb Dori\nEyassu Birru\nMatthew Thornton\nTamir Kifle\nKirubel Tabu\nBisrat Miherete\nEmmanuel Khatchadourian\nTinsae Teka\nYoseph Ephrem\nYonas Eshetu\nHanna Kaleab\nTinsae Teka\nRobee Meseret\nMatias Tekeste\nEyasu Birhanu\nyonatan berihun\nNasrallah Hassan\nAndinet Assefa\nTewodros Sintayehu\nKIDUS MENGISTEAB\nDjibril Konate\nNahom Mekonnen\nEyasu Birhanu\nEyob Aschenaki\nTinsae Demissie\nYeabsira Tsegaye\nTihitna Miroche\nMearaf Tadewos\nYab Mitiku\nHabtamu Asefa\nDawit Mengistu\nNebiyu Barsula\nNebiyu Sultan\nNathan Samson\nIn addition, we would like to thank all the attendees of the workshop that was held in Kyoto and Fukuoka, Japan on May 27th and June 10th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nArimura\nHidemi\nNagamaru(SASApool)\nshiodome47(SODMpool)\nWakuda(AID1pool)\nYuta(Yuki Oishi)\nAndrew\nBANCpool\nMiyatake\nMuen\nRiekousagi\nSMAN8(SA8pool)\nTatsuya\nMako\nRirico\nBaku\nJUNO\nKinoko\nChikara\nET\nAkira555\nKent\nPpp\nShiodome47\nSam\nConcon\nSogame\nDemi\nNonnon\nbanC\nSMAN8(SA8pool)\nKensin\n/\nMUEN\nRanket\nA.yy\nN S\nKazuya\nDaikon\nIn addition, we would like to thank all the attendees of the workshop that was held in Monterey, California on May 28th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nShane Powser\nRodrigo Gomez\nAdam K. Dean\nJohn C. Valdez\nKyle Solomon\nErick \"Mag\" Magnana\nBryant Austin\nJohn Huthmaker\nAyori Selassie\nJosh Noriega\nMatthias Sieber\nIn addition, we would like to thank all the attendees of the workshop that was held in Tlaxcala, Mexico on June 1st 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nVictor Hern ndez\nCristian Jair Rojas\nMiriam Mejia\nJosmar Caba as\nLizbet Delgado\nJos Alberto S nchez\nF tima Valeria Zamora\nJulio C sar Montiel\nJes s P rez\nJos Adri n L pez\nLizbeth Calder n\nZayra Molina\nNayelhi P rez\nJosu Armas\nDiego Talavera\nDar an Guti rrez\nIn addition, we would like to thank all the attendees of the workshop that was held in LATAM Virtual on June 3rd 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nJuan Sierra\n@CaueChianca\nErnesto Rafael\nPabon Moreno\nSonia Malagon\nFacundo Ram rez\nMercedes Ruggeri\nHope R.\nYaris Cruz\nYaneth Duarte\nCiro G lvez\nKevin Chacon\nJuanita Jaramillo\nSebastian Pabon\nIn addition, we would like to thank all the attendees of the workshop that was held in Worcester, Massachusetts on June 8th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nCardanoSharp\nKenric Nelson\nMatthias Sieber\nRoberto Mayen\nIan Burzynski\nomdesign\nChris Gianelloni\nIn addition, we would like to thank all the attendees of the workshop that was held in Chicago, Illinois on June 10th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nAdam Rusch\nJose Martinez\nMichael McNulty\nVanessa Villanueva Collao\nMaaz Jedh\nIn addition, we would like to thank all the attendees of the workshop that was held virtually on June 12th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nRojo Kaboti\nTommy Frey\nTevo Saks\nSlate\nUBIO OBU\nIn addition, we would like to thank all the attendees of the workshop that was held in Toronto, Canada on June 15th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nJohn MacPherson\nLawrence Ley\nIn addition, we would like to thank all the attendees of the workshop that was held in Philadelphia, Pennsylvania on June 17th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nNOODZ\nJarhead\nJenny Brito\nShepard\nBONE Pool\ntype_biggie\nFLAWWD\nA.I. Scholars\nEddie\nJoker\nLex\nJerome\nJoey\nSwayZ\nCara Mia\nPHILLY 1694\nIn addition, we would like to thank all the attendees of the workshop that was held in Santiago de Chile on June 17th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nRodrigo Oyarsun\nSebasti n Aravena\nMusashi Fujio\nGeo Gavo\nLuc a Escobar\nJuan Cruz Franco\nNatalia Rosa\nCristian M. Garc a\nAlejandro Montalvo\nIn addition, we would like to thank all the attendees of the workshop that was held virtually on June 17th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nJuana Attieh\nNadim Karam\nAmir Azem\nRami Hanania\nLALUL Stake Pool\nHAWAK Stake Pool\nIn addition, we would like to thank all the attendees of the workshop that was held in Taipai, Taiwan on June 18th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nMichael Rogero\nTed Chen\nMic\nJeremy Firster\nEric Tsai\nDylan Chiang\nJohnsonCai\nDavidCHIEN\nZach Gu\nJimmy WANG\nJackTsai\nKatherine Hung\nWill Huang\nKwicil\nIn addition, we would like to thank all the attendees of the workshop that was held in Midgard Vikingcenter Horten, Norway on June 19th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nDaniel D. Johnsen\nThomas Lindseth\nEystein Hansen\nGudbrand Tokerud\nLally McClay\n$trym\nArne Rasmussen\nLise WesselTVVIN\nBjarne\nJostein Aanderaa\nKen-Erik lmheim\nDimSum\nIn addition, we would like to thank all the attendees of the workshop that was held virtually on June 19th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nNicolas Cerny\nNils Peuser\nRiley Kilgore\nAlejandro Almanza\nJenny Brito\nJohn C. Valdez\nRhys\nThyme\nAdam Rusch\nDevryn\nIn addition, we would like to thank all the attendees of the workshop that was held in New York City, New York on June 20th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nJohn Shearing\nGeoff Shearing\nDaniela Balaniuc\nSDuffy\nGarry Golden\nNewman\nEmmanuel Batse\nEbae\nMojira\nIn addition, we would like to thank all the attendees of the workshop that was held in La Cumbre, Argentina on June 23rd 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nUlises Barreiro\nDaniel F. Rodriguez\nDominique Gromez\nLeandro Chialvo\nClaudia Vogel\nGuillermo Lucero\nFunes, Brian Carrasco\nMelisa Carrasco\nCarlos Carrasco\nIn addition, we would like to thank all the attendees of the workshop that was held in Minneapolis, Minnesota on June 23rd 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nStephanie King\nDarlington Wleh\nIn addition, we would like to thank all the attendees of the workshop that was held in La Plata, Argentina on June 23rd 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nMauro Andreoli\nRodolfo Miranda\nAgustin Francella\nFederico Sting\nElias Aires\nLucas Macchiavelli\nPablo Hern n Mazzitelli\nIn addition, we would like to thank all the attendees of the workshop that was held in Puerto Madryn, Argentina on June 23rd 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nAndres Torres Borda\nFederico Ledesma Calatayud\nMaximiliano Torres\nFederico Prado\nDomingo Torres\nFloriana P rez Barria\nMartin Real\nFlorencia Garc a\nRoberto Neme\nIn addition, we would like to thank all the attendees of the workshop that was held in Accra, Ghana on June 24th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nWada\nLaurentine\nChristopher A.\nNathaniel D.\nEdufua\nMichael\nAugusta\nJeremiah\nBoaz\nMohammed\nRichmond O.\nEzekiel\nMegan\nJosue\nMichel T.\nBineta\nAfia O.\nMercy\nEnoch\nKofi\nAwura\nEmelia\nRichmond S.\nSolomon\nPhillip\nFaakor\nManfo\nJosh\nDaniel\nMermose\nIn addition, we would like to thank all the attendees of the workshop that was held virtually on June 24th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nJonas Riise\nThomas Lindseth\nAndr \"Eilert\" Eilertsen\nEystein Hansen\nIn addition, we would like to thank all the attendees of the workshop that was held in Seoul, South Korea on June 24th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nOscar Hong (JUNGI HONG)\nSPO_COOL (Kevin Kordano)\nSPO_KTOP (KT OH)\nWANG JAE LEE\nJAE HYUN AN\nINYOUNG MOON (Penny)\nHOJIN JEON\nSEUNG KYU BAEK\nSA SEONG MAENG\nJUNG MYEONG HAN\nBRIAN KIM\nJUNG HOON KIM\nSEUNG WOOK JUNG (Peter)\nHYUNG WOO PARK\nEUN JAE CHOI\nNA GYEONG KIM\nJADEN CHOI\nIn addition, we would like to thank all the attendees of the workshop that was held in Abu Dhabi, UAE on June 25th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nAmir Azem\nIan Arden\nMadina Abdibayeva\nBTBF (Yu Kagaya)\nTegegne Tefera\nRami Hanania\nTania Debs\nKhalil Jad\nMohamed Jamal\nRuslan Yakubov\nOUSHEK Mohamed eisa\nShehryar\nWael Ben Younes\nSantosh Ray\nJuana Attieh\nNadim Karam\nDubaistakePool\nHAWAK Pool\nLALKUL Stake Pools\nIn addition, we would like to thank all the attendees of the workshop that was held in Williamsburg, New York on June 25th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nPi\nJoseph\nSkyler\nForrest\nGabriel\nNewman\nIn addition, we would like to thank all the attendees of the workshop that was held in Lagos, Nigeria on June 28th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nJonah Benson\nAugusta\nUbio Obu\nOlumide Hrosuosegbe\nVeralyn Chinenye\nOna Ohimer\nWilliam Ese\nRuth Usoro\nWilliam P\nEsther Simi\nDaniel Effiom\nAkinkurai Toluwalase\nIn addition, we would like to thank all the attendees of the workshop that was held in Sao Paulo, Brazil on July 1st 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nOt vio Lima\nRodrigo Pacini\nMaria Carmo\nCau Chianca\nDaniela Alves\nJose Lins Dias\nFelipe Barcelos\nRosana Melo\nJohnny Oliveira\nLucas Ravacci\nCristofer Ramos\nWeslei Menck\nLeandro Tsutsumi\nIzaias Pessoa\nGabriel Melo\nYuri Nabeshima\nAlexandre Fernandes\nVinicius Ferreiro\nLucas Fernandes\nAlessandro Benicio\nMario Cielho\nLory Fernandes Lima\nLarissa Nogueira\nLatam Cardano Community\nIn addition, we would like to thank all the attendees of the workshop that was held in Brazil on July 4th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance. These include:\nLincon Vidal\nThiago da Silva Nunes\nRodrigo Pacini\nLivia Corcino de Albuquerque\nCau Chianca\nOt vio Lima\nThis CIP is licensed under CC-BY-4.0\nA formal description of the current rules for governance actions is given in the Shelley ledger specification. For protocol parameter changes (including hard forks), the PPUP transition rule (Figure 13) describes how protocol parameter updates are processed, and the NEWPP transition rule (Figure 43) describes how changes to protocol parameters are enacted. For funds transfers, the DELEG transition rule (Figure 24) describes how MIR certificates are processed, and the MIR transition rule (Figure 55) describes how treasury and reserve movements are enacted. Note The capabilities of the MIR transition rule were expanded in the Alonzo ledger specification There are many varying definitions of the term \"hard fork\" in the blockchain industry. Hard forks typically refer to non-backwards compatible updates of a network. In Cardano, we formalize the definition slightly more by calling any upgrade that would lead to more blocks being validated a \"hard fork\" and force nodes to comply with the new protocol version, effectively obsoleting nodes that are unable to handle the upgrade.\nA formal description of the current rules for governance actions is given in the Shelley ledger specification. For protocol parameter changes (including hard forks), the PPUP transition rule (Figure 13) describes how protocol parameter updates are processed, and the NEWPP transition rule (Figure 43) describes how changes to protocol parameters are enacted. For funds transfers, the DELEG transition rule (Figure 24) describes how MIR certificates are processed, and the MIR transition rule (Figure 55) describes how treasury and reserve movements are enacted. Note The capabilities of the MIR transition rule were expanded in the Alonzo ledger specification\nA formal description of the current rules for governance actions is given in the Shelley ledger specification.\nFor protocol parameter changes (including hard forks), the PPUP transition rule (Figure 13) describes how protocol parameter updates are processed, and the NEWPP transition rule (Figure 43) describes how changes to protocol parameters are enacted.\nFor protocol parameter changes (including hard forks), the PPUP transition rule (Figure 13) describes how protocol parameter updates are processed, and the NEWPP transition rule (Figure 43) describes how changes to protocol parameters are enacted.\nPPUP\nNEWPP\nFor funds transfers, the DELEG transition rule (Figure 24) describes how MIR certificates are processed, and the MIR transition rule (Figure 55) describes how treasury and reserve movements are enacted.\nFor funds transfers, the DELEG transition rule (Figure 24) describes how MIR certificates are processed, and the MIR transition rule (Figure 55) describes how treasury and reserve movements are enacted.\nDELEG\nMIR\nNote The capabilities of the MIR transition rule were expanded in the Alonzo ledger specification\nMIR\nThere are many varying definitions of the term \"hard fork\" in the blockchain industry. Hard forks typically refer to non-backwards compatible updates of a network. In Cardano, we formalize the definition slightly more by calling any upgrade that would lead to more blocks being validated a \"hard fork\" and force nodes to comply with the new protocol version, effectively obsoleting nodes that are unable to handle the upgrade.\nThere are many varying definitions of the term \"hard fork\" in the blockchain industry. Hard forks typically refer to non-backwards compatible updates of a network. In Cardano, we formalize the definition slightly more by calling any upgrade that would lead to more blocks being validated a \"hard fork\" and force nodes to comply with the new protocol version, effectively obsoleting nodes that are unable to handle the upgrade.\n2023 Cardano Foundation\n\n---\n\nCIP-1852 | HD (Hierarchy for Deterministic) Wallets for Cardano\n\nCardano extends the BIP44 by adding new chains used for different purposes. This document outlines how key derivation is done and acts as a registry for different chains used by Cardano wallets.\nFor Cardano, we use a new purpose field 1852' instead of 44' like in BIP44. There are three main reasons for this:\n1852'\n44'\nDuring the Byron-era, 44' was used. Since Byron wallets use a different algorithm for generating addresses from public keys, using a different purpose type allows software to easily know which address generation algorithm given just the derivation path (ex: given m / 44' / 1815' / 0' / 0 / 0, wallet software would know to handle this as a Byron-era wallet and not a Shelley-era wallet). Using a new purpose helps bring attention to the fact Cardano is using BIP32-Ed25519 and not standard BIP32. Using a new purpose allows us to extend this registry to include more Cardano-specific functionality in the future\nDuring the Byron-era, 44' was used. Since Byron wallets use a different algorithm for generating addresses from public keys, using a different purpose type allows software to easily know which address generation algorithm given just the derivation path (ex: given m / 44' / 1815' / 0' / 0 / 0, wallet software would know to handle this as a Byron-era wallet and not a Shelley-era wallet).\n44'\nm / 44' / 1815' / 0' / 0 / 0\nUsing a new purpose helps bring attention to the fact Cardano is using BIP32-Ed25519 and not standard BIP32.\nBIP32-Ed25519\nBIP32\nUsing a new purpose allows us to extend this registry to include more Cardano-specific functionality in the future\n1852 was chosen as it is the year of death of Ada Lovelace (following the fact that the coin_type value for Cardano is 1815 for her year of birth)\n1852\ncoin_type\n1815\nUsing 1852' as the purpose field, we defined the following derivation path:\n1852'\nm / purpose' / coin_type' / account' / role / index\nm / purpose' / coin_type' / account' / role / index\nExample: m / 1852' / 1815' / 0' / 0 / 0\nm / 1852' / 1815' / 0' / 0 / 0\nHere, role can be the following\nrole\n0\n1\n2\n3\n4\n5\nWallets MUST implement this new scheme using the master node derivation algorithm from Icarus with sequential addressing (see CIP3 for more information)\nCardano does not use BIP32 but actually uses BIP32-Ed25519. The -Ed25519 suffix is often dropped in practice (ex: we say the Byron release of Cardano supports BIP44 but in reality this is BIP44-Ed25519).\n-Ed25519\nThe Byron implementation of Cardano uses purpose = 44' (note: this was already a slight abuse of notation because Cardano implements BIP44-Ed25519 and not standard BIP44).\npurpose = 44'\nThere are two (incompatible) implementations of BIP32-Ed25519 in Cardano:\nHD Random (notably used initially in Daedalus) HD Sequential (notably used initially in Icarus)\nHD Random (notably used initially in Daedalus)\nHD Sequential (notably used initially in Icarus)\nThe difference is explained in more detail in CIP-0003.\nAs a general pattern, new wallet schemes should use a different purpose if they intend to piggy-back on the same structure but for a different use-case (see for instance CIP-1854).\nThe role can however be extending with new roles so long as they have no overlapping semantic with existing roles. If they do, then they likely fall into the first category of extension and would better be done via a new purpose.\nrole\nStandardisation of this derivation path among all wallets as of the Shelley ledger era.\nCommon agreement on the above Motivation, Rationale and Specification during the planning of Cardano's Shelley release.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-1853 | HD (Hierarchy for Deterministic) Stake Pool Cold Keys for Cardano\n\nCIP-1852 establishes how Shelley-era hierarchical deterministic (HD) wallets should derive their keys. This document is a follow-up of this CIP specifying how stake pool cold keys should be derived.\n(Hierarchical) deterministic derivation of stake pool cold keys enables their restorability from a seed and most importantly, their management on hardware wallet devices. This in turn mitigates man-in-the middle attacks to which pool operators would otherwise be vulnerable if they managed their stake pool cold keys on a device not specifically hardened against alteration of the data to be signed/serialized without operator's explicit consent.\nUsing 1853' as the purpose field, we define the following derivation path structure for stake pool cold keys:\n1853'\nm / purpose' / coin_type' / usecase' / cold_key_index'\nm / purpose' / coin_type' / usecase' / cold_key_index'\nExample: m / 1853' / 1815' / 0' / 0'\nm / 1853' / 1815' / 0' / 0'\nHere the usecase is currently fixed to 0'.\nusecase\n0'\nGiven that stake pool cold keys are cryptographically the same as wallet keys already covered in CIP-1852, the master node and subsequent child keys derivation MUST be implemented in the same way as specified for wallets in CIP-1852.\nStake pools are not wallets and the core concept of \"accounts\" is not applicable to them, nor are they supposed to be related to a user's wallet in any meaningful way. Therefore treating stake pool cold keys as another \"chain\" within CIP-1852 specification would rather be a deviation from CIP-1852 than its logical extension. Hence we establish a separate purpose and path structure for stake pool cold keys, having their specifics and differences from standard \"wallet\" keys in mind.\ncoin_type\ncoin_type is kept in order to remain consistent with the \"parent\" CIP-1852 and also to leave space for the possibility that some Cardano hard-fork/clone in the future would reuse this specification to derive their own stake pool cold keys.\ncoin_type\nusecase\nSimilarly as we have the chain path component in CIP-1852 paths for different types of wallet keys, it's plausible that in the future, there could be multiple varieties of stake pools. One such example of a possible future extension of this CIP could be pools managed by a group of operators instead of a single operator, for which a separate set of stake pool cold keys, driven by this parameter, could make sense both from logical and security perspective.\nchain\ncold_key_index\nEach stake pool is supposed to be managed separately so there is currently no incentive to connect them via a parent public key.\nusecase\nWe chose hardened derivation at the usecase index as there is no incentive to mix the stake pool cold keys with other potential usecases and if there was such incentive, it would most likely be more appropriate to create a separate usecase/purpose for that.\nStandardisation of this derivation path among three wallets as of the Shelley ledger era. Ledger App Cardano https://github.com/LedgerHQ/app-cardano\nLedger App Cardano https://github.com/LedgerHQ/app-cardano\nCommon agreement on the above Motivation, Rationale and Specification during the planning of Cardano's Shelley release.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-1854 | Multi-signatures HD Wallets\n\nThis document describes how to realize multi-parties transactions in Cardano. It is defined as an alternative to CIP-1852 for Cardano HD wallets. This specification does not cover the transport and sharing of partially signed transactions which is / will be covered in another document.\nMultisig wallets are Cardano wallets capable of providing and tracking keys involved in multi-parties transactions. Such transactions are used to move funds from addresses where (typically) more than one user must sign transactions for its funds to be spent. A simple analogue is a joint bank account where two account holders must approve withdrawals from the account. Cardano has native support for multi-signature schemes: funds at a multi-signature script address are owned by a monetary script, which specifies one or more combinations of cryptographic signatures which must be present to unlock funds at the address (see the Formal Ledger Spec, Figure 4: Multi-signature via Native Scripts). In a similar fashion, Cardano multisig scripts can also be used to capture stake rights on a particular address, shared between different parties.\nIn the Shelley era, Cardano offers six (partially overlapping) primitives for constructing monetary scripts which are summarized as the following grammar:\nSCRIPT = SIGNATURE KEY-HASH | ALL-OF 1*SCRIPT | ANY-OF 1*SCRIPT | N-OF UINT 1*SCRIPT | AFTER SLOT-NO | BEFORE SLOT-NO KEY-HASH = 28OCTET SLOT-NO = UINT\nSCRIPT = SIGNATURE KEY-HASH | ALL-OF 1*SCRIPT | ANY-OF 1*SCRIPT | N-OF UINT 1*SCRIPT | AFTER SLOT-NO | BEFORE SLOT-NO KEY-HASH = 28OCTET SLOT-NO = UINT\nScripts are thereby recursive structures which can contain scripts themselves. For example, a joint account between two parties which specifies that any of the two members can spend from it would be defined in pseudo-code as:\nJOINT-ACCOUNT := ANY-OF [SIGNATURE key-hash#1, SIGNATURE key-hash#2]\nJOINT-ACCOUNT := ANY-OF [SIGNATURE key-hash#1, SIGNATURE key-hash#2]\nIn order to spend from such script, one would have to provide a witness showing ownership of the key associated with either of key-hash#1 or key-hash#2 (or possibly, both).\nkey-hash#1\nkey-hash#2\nThe creation of a multisig address will not be covered in this document. Such addresses are described in [Shelley design specification - section 3.2 - Addresses and Credentials][delegation_design.pdf] and are obtained by serializing and hashing a multisig script. This functionality is assumed to be available through existing tooling or piece of software in a variety of ways.\nIn order to spend from a multisig address, one must provide a special witness in the spending transaction called a \"multisig witness\". Such witness must be the exact script used as an input for creating the hash part of the multisig address. Then, any additional required verification key signatures specified by the script must be provided as separate verification key witnesses (i.e. signatures of the hashed transaction body for each signing key).\nThis means that a wallet software has access to the full script to validate and also identify verification key hashes present in transactions, but only does so when funds are being spent from a multisig address! From the Allegra era and beyond, it is also possible to include script pre-image in transaction auxiliary data. Softwares may use this to communicate scripts ahead of time.\nWe consider the following HD derivation paths similarly to CIP-1852:\nm / purpose' / coin_type' / account_ix' / role / index\nm / purpose' / coin_type' / account_ix' / role / index\nTo associate multi-signature keys to a wallet, we reserve however purpose=1854' to distinguish multisig wallets from standard wallets. The coin type remains coin_type=1815' to identify Ada as registered in SLIP-0044. The account index (account_ix) may vary across the whole hardened domain. role=0 is used to identify payment keys, whereas role=2 identifies stake keys. role=1 is left unused for multisig wallets. Finally, the last index may vary across the whole soft domain, but according to the following rules:\npurpose=1854'\ncoin_type=1815'\naccount_ix\nrole=0\nrole=2\nrole=1\nindex\nWallet must derive multisig key indexes sequentially, starting from 0 and up to 2 31-1\nWallet must prevent the creation of new multisig keys before past keys are seen in an on-chain script.\nWallet should yet always provide up-to 20 consecutive unused multisig keys.\nWe can summarize the various paths and their respective domain in the following table:\npurpose\ncoin_type\naccount_ix\nrole\nindex\n1854'\n1815'\n[2^31 .. 2^32-1]\n0\n2\n[0 .. 2^31-1]\nm/1854 /1815 /0 /0/0\nm/1854’/1815’/0’/0/0\nm/1854 /1815 /0 /2/14\nm/1854’/1815’/0’/2/14\nm/1854 /1815 /0 /2/42\nm/1854’/1815’/0’/2/42\nm/1854 /1815 /0 /0/1337\nm/1854’/1815’/0’/0/1337\nMulti-signatures payment verification and signing keys (role=0) with chain code should be presented bech32-encoded using addr_shared_xvk and addr_shared_xsk prefixes respectively, as specified in CIP-5. When represented without chain-code, addr_shared_vk and addr_shared_sk should be used instead.\nrole=0\naddr_shared_xvk\naddr_shared_xsk\naddr_shared_vk\naddr_shared_sk\nSimilarly we use stake_shared_xvk, stake_shared_xsk, stake_shared_vk and stake_shared_sk for multi-signatures stake verification and signing keys (role=2).\nstake_shared_xvk\nstake_shared_xsk\nstake_shared_vk\nstake_shared_sk\nrole=2\n- base16: | 179be1ac9e63abb5fe4666df03007b07 33a3b63cdd08cd3635b8d364e16aaf26 49856f2ba513786afdbe5c7a07565c02 9ba7a4290d20aa3c80ecc1835841ed78 bech32: addr_shared_xvk1z7d7rty7vw4mtljxvm0sxqrmque68d3um5yv6d34hrfkfct24unynpt09wj3x7r2lkl9c7s82ewq9xa85s5s6g928jqwesvrtpq767qs66fnu\n- base16: | 179be1ac9e63abb5fe4666df03007b07 33a3b63cdd08cd3635b8d364e16aaf26 49856f2ba513786afdbe5c7a07565c02 9ba7a4290d20aa3c80ecc1835841ed78 bech32: addr_shared_xvk1z7d7rty7vw4mtljxvm0sxqrmque68d3um5yv6d34hrfkfct24unynpt09wj3x7r2lkl9c7s82ewq9xa85s5s6g928jqwesvrtpq767qs66fnu\nTo define a multisig wallet, participants must provide:\nA payment script template.\nA delegation script template (possibly null).\nAll the cosigners role=0 and role=2 (4th level) public keys involved in templates. Alternatively, cosigners may also share their account public key (3rd level) directly.\nA script template is a script where key hashes are replaced by a cosigner tag which captures the relation between the cosigners.\nNOTE 1: How an application represents tags is an implementation detail. Only the instantiated script appears on the blockchain.\nNOTE 2: As a reminder, a Cardano address is made of two parts: a payment part and a delegation part. The latter is optional and so is the delegation script template. If none is provided then the address does not contain any delegation part. Wallet softwares should decide whether they want to allow or forbid this but this proposal does not take position.\nThere must be a strict one-to-one mapping between the tags used in the template and the cosigner account keys provided. When a new address is needed, the software must instantiate the script by using keys derived from the relevant account. To instantiate a script from a template, a wallet software must abide by the following rules:\nWhen deriving new keys, the same indexes must be used for all account involved in the template. If a tag appears in multiple place in the script, then the same index must also be used for all instances of that tag.\nWhen deriving new keys, the same indexes must be used for all account involved in the template.\nIf a tag appears in multiple place in the script, then the same index must also be used for all instances of that tag.\n(1) and (2) implies that a given instance of a script is associated to one and only one derivation index.\nNOTE 3: Wallet software should not authorize users to update the script templates after an address has been used.\nFor example, considering the joint-account example from above, an example of payment template can simply be:\nJOINT-ACCOUNT-TEMPLATE := ANY-OF [ SIGNATURE cosigner#1 , SIGNATURE cosigner#2 ]\nJOINT-ACCOUNT-TEMPLATE := ANY-OF [ SIGNATURE cosigner#1 , SIGNATURE cosigner#2 ]\nand an instantiation of that template for e.g. an index equals to 1 could be:\nJOINT-ACCOUNT := ANY-OF [ SIGNATURE addr_shared_vkh1rcw47lrnczf8d4gt7d8vp6z95l8effslyejszez7069z6fa469v , SIGNATURE addr_shared_vkh1f8vd0s83hr7y2r96gct785e4d0d77ep9jn5wqfa4d6r4kqzxd65 ]\nJOINT-ACCOUNT := ANY-OF [ SIGNATURE addr_shared_vkh1rcw47lrnczf8d4gt7d8vp6z95l8effslyejszez7069z6fa469v , SIGNATURE addr_shared_vkh1f8vd0s83hr7y2r96gct785e4d0d77ep9jn5wqfa4d6r4kqzxd65 ]\nKeys used in the instantiated template all come from the same derivation index for all cosigners. This allows all wallets to easily find back what indexes other cosigners used to derive a particular key (the same as they use for a particular address).\nIn case the wallet needs a change address internally, it must use the smallest unused indexes known, in ascending order. An index is considered unused if there's no transaction in the ledger with a script using its corresponding key hash. When constructing a transaction, a wallet should use UTxOs associated with script addresses only (hybrid transactions using non-shared UTxOs are forbidden).\nOnce constructed, a transaction must be signed with all required private keys from the initiator. The initiator must also include all instances of scripts necessary (typically one per input) and then either broadcast the transaction to its cosigners or submits it to the network if valid. The mean by which the transaction is broadcast is out of the scope of this specification.\nUpon receiving a partially signed transaction, wallets must for each input:\nFind the script associated with the input in the transaction witnesses Verify that the script matches the wallet's template(s) Identify the derivation index used to instantiate the script from its known keys Derive the public keys of each other co-signers from that same derivation index and the cosigners parent keys Verify that there's at least one signature from another co-signer Verify all signatures provided by co-signers with their associated public keys\nFind the script associated with the input in the transaction witnesses\nVerify that the script matches the wallet's template(s)\nIdentify the derivation index used to instantiate the script from its known keys\nDerive the public keys of each other co-signers from that same derivation index and the cosigners parent keys\nVerify that there's at least one signature from another co-signer\nVerify all signatures provided by co-signers with their associated public keys\nNOTE The step (5) is crucial and implies that wallets initiating transactions have to sign them before broadcasting them as a proof of provenance. Otherwise it is possible for attackers to broadcast a \"fake\" transaction to all cosigners who may not pay attention to its details.\nWhen valid, the wallet should prompt the user for signing. The transaction can be submitted by any of the cosigners who deems it valid (it could be that only a subset of the cosigners is required to sign). Several cosigners may submit the transaction concurrently without issue, the ledger will ensure that only one transaction eventually get through.\nNOTE: The management of change addresses is an implementation detail of the wallet so long as all change addresses abide by the rules described earlier in this section. A wallet may choose to generate a single change address per transaction, while another may chose to generate many.\nWallets should warn users when discovering known keys in scripts which non-matching templates. Such addresses should not be included as part of the wallet's UTxO and should be treated as anomalies in the wallet. As a matter of facts, everything described in this document is a mere convention between wallets. There's however nothing at the protocol level that enforces that this specification is followed. It is therefore very much possible for advanced users to use their multisig keys in scripts that are different from the template. So long as indexes used are discoverable by the wallet, then such scripts are also discoverable.\nHowever, because such scripts / addresses would not have been created by the wallet, they are considered not being part of the it altogether and would not count towards the wallet's balance. Software should however as much as possible alert users about the existence of such anomalies.\nFor example, if Alice and Bob wants to share a wallet by requiring a signature from each other on every payment, they can define the following payment script template:\nALL-OF (SIGNATURE alice) (SIGNATURE bob)\nALL-OF (SIGNATURE alice) (SIGNATURE bob)\nAfter exchanging their corresponding public keys, both wallets should be initialized and present to Alice and Bob exactly 20 identical addresses, where each address is associated with exactly one derivation index. The first address will use one key from Alice's wallet at path m/1854 /1815 /0 /0/0 and a key from Bob's at exactly the same path. The next address will use keys at path m/1854 /1815 /0 /0/1 and so forth.\n20\nm/1854’/1815’/0’/0/0\nm/1854’/1815’/0’/0/1\nWhen Alice initiates a transaction from this wallet, she'll construct the transaction body, sign it with her corresponding private key, include an instance of the script as witness and broadcast the transaction to Bob via Telegram. Upon reception, Bob is able to verify that the transaction was indeed signed by Alice using her private key at index #0 (Bob has indeed Alice's parent public key in its possession and is therefore able to derive a child key at index #0 to verify the signature on the transaction). Bob proceeds with the payment by signing the transaction in return and submitting it.\nMultisig keys are scoped to accounts, which allows wallet's owners to separate their activity easily.\nMultisig keys are scoped to accounts, which allows wallet's owners to separate their activity easily.\nWe use a different purpose for mainly two reasons: It prevents mixing up standard wallets with shared wallets, which would be undesirable and become rapidly a nightmare for software to maintain. In particular, it also makes it possible to share an intermediary account key with co-signers without disclosing any information about non-shared keys in our wallet. It makes it easier to extend any of the 1852' or 1854' in a similar manner. The addition of a new role can be done in both scheme very consistently. Using a different purpose also fits well the use-case on hardware wallets who can still rely on a single root seed to manage many types of wallets.\nWe use a different purpose for mainly two reasons:\nIt prevents mixing up standard wallets with shared wallets, which would be undesirable and become rapidly a nightmare for software to maintain. In particular, it also makes it possible to share an intermediary account key with co-signers without disclosing any information about non-shared keys in our wallet.\nIt prevents mixing up standard wallets with shared wallets, which would be undesirable and become rapidly a nightmare for software to maintain. In particular, it also makes it possible to share an intermediary account key with co-signers without disclosing any information about non-shared keys in our wallet.\nIt makes it easier to extend any of the 1852' or 1854' in a similar manner. The addition of a new role can be done in both scheme very consistently.\nIt makes it easier to extend any of the 1852' or 1854' in a similar manner. The addition of a new role can be done in both scheme very consistently.\nUsing a different purpose also fits well the use-case on hardware wallets who can still rely on a single root seed to manage many types of wallets.\nOne or many keys can be created from a parent public key via soft-derivation. This allows participant to easily share their multisig keys to participate in a multisig script without having to fetch for their hardware device. The device is still required for signing.\nOne or many keys can be created from a parent public key via soft-derivation. This allows participant to easily share their multisig keys to participate in a multisig script without having to fetch for their hardware device. The device is still required for signing.\nDocument at least one case of a community adopted CIP-1852 compliant multisig wallet: Round Table wallet\nRound Table wallet\nCommunity developed reference implementation: github:ADAOcommunity/round-table\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-1855 | Forging policy keys for HD Wallets\n\nThis document describes how to derive forging policy keys used for minting/burning tokens.\nForging tokens is derived from a script policy. The script policy includes hashes of keys needed to forge new tokens and must be witnessed by these keys in such a way as the script stipulates. This CIP defines the derivation path at wich parties are expected to derive such keys.\nWe consider the following HD derivation paths similarly to CIP-1852:\nm / purpose' / coin_type' / policy_ix'\nm / purpose' / coin_type' / policy_ix'\nTo associate policy keys to a wallet, we reserve however purpose=1855' for policy keys for forging tokens. The coin type remains coin_type=1815' to identify Ada as registered in SLIP-0044. We use a hardened index for each policy key as derivation is not needed.\npurpose=1855'\ncoin_type=1815'\nWe can summarize the various paths and their respective domain in the following table:\npurpose\ncoin_type\npolicy_ix\n1855'\n1815'\n[2^31 .. 2^32-1]\nTo distinguish such keys & derived material in the human readable prefix of the bech32 representation, we introduce the following prefixes for insertion into CIP-0005:\npolicy_sk\npolicy_vk\npolicy_vkh\nm/1855 /1815 /0\nm/1855’/1815’/0’\nm/1855 /1815 /1\nm/1855’/1815’/1’\nm/1855 /1815 /2\nm/1855’/1815’/2’\nERC20 Converter IOHK is developing needs to keep track of policy keys. Rather than having randomly generated policy keys, a policy key can be associated with a mnemonic which is easier to backup.\nERC20 Converter IOHK is developing needs to keep track of policy keys. Rather than having randomly generated policy keys, a policy key can be associated with a mnemonic which is easier to backup.\nA 3rd party may want to have multiple tokens tied to same mnemonic, so we allow an index to specify the token.\nA 3rd party may want to have multiple tokens tied to same mnemonic, so we allow an index to specify the token.\nContrary to CIP 1852, we don't use the role and index levels of the derivation path, since index is expressed at the 3rd level and no roles for policy signing keys are currently anticipated.\nContrary to CIP 1852, we don't use the role and index levels of the derivation path, since index is expressed at the 3rd level and no roles for policy signing keys are currently anticipated.\nrole\nindex\nNo prefixes are defined for extended keys, since currently this CIP does not define further derivations.\nNo prefixes are defined for extended keys, since currently this CIP does not define further derivations.\nWe use a different purpose for mainly two reasons: It prevents mixing up standard wallets with policy keys used for forging. Using a different purpose also fits well the use-case on hardware wallets who can still rely on a single root seed to manage many types of wallets.\nWe use a different purpose for mainly two reasons:\nIt prevents mixing up standard wallets with policy keys used for forging.\nIt prevents mixing up standard wallets with policy keys used for forging.\nUsing a different purpose also fits well the use-case on hardware wallets who can still rely on a single root seed to manage many types of wallets.\nUsing a different purpose also fits well the use-case on hardware wallets who can still rely on a single root seed to manage many types of wallets.\nStandardisation of this derivation path among three wallets as of the Shelley ledger era.\nCommon agreement on the above Motivation, Rationale and Specification during the planning of Cardano's Shelley release.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-9999 | Cardano Problem Statements\n\nA Cardano Problem Statement (CPS) is a formalized document for the Cardano ecosystem and the name of the process by which such documents are produced and listed. CPSs are meant to complement CIPs and live side-by-side in the CIP repository as first-class citizens.\nNote Read this CIP's number as \"CIP minus 1\"\nA common friction point regarding complex CIPs is how their main problems are stated. For example, the 'Motivation' section in CIPs is sometimes not sufficient -- or simply underused -- to describe the various aspects of a problem, its scope, and its constraints in the necessary detail. This lack of clarity leads, in the end, to poorly defined issues and unfruitful debates amongst participants who understand problems differently.\nThe introduction of the Cardano Problem Statements (CPSs) addresses this gap by defining a formal template and structure around the description of problems. CPSs are meant to replace the more elaborate motivation of complex CIPs. However, they may also exist on their own as requests for proposals from ecosystem actors who've identified a problem but are yet to find any suitable solution.\nOver time, CPSs may complement grant systems that want to target well-known problems of the ecosystem; they can, for example, serve as the foundation for RFP (Request For Proposals) documents. We hope they may also help make some discussions more fluid by capturing a problem and its various constraints well.\nCPSs are, first and foremost, documents that capture a problem and a set of constraints and hypotheses. Documents are Markdown files with a front matter Preamble and pre-defined sections. CPS authors must abide by the general structure, though they are free to organize each section as they see fit.\nThe structure of a CPS file is summarized in the table below:\nEach CIP must begin with a YAML key:value style header preamble (also known as 'front matter data'), preceded and followed by three hyphens (---).\n---\nCPS\nTitle\nStatus\nCategory\nAuthors\nProposed Solutions\nDiscussions\nCreated\nLicense\nFor example:\n--- CPS: 1 Title: The Blockchain Trilemma Status: Open Category: Consensus Authors: - Alice <alice@domain.org> - Bob <bob@domain.org> Proposed Solutions: [] Discussions: - https://forum.cardano.org/t/solving-the-blockchain-trilemma/107720 - https://github.com/cardano-foundation/cips/pulls/9999 Created: 2009-02-14 ---\n--- CPS: 1 Title: The Blockchain Trilemma Status: Open Category: Consensus Authors: - Alice <alice@domain.org> - Bob <bob@domain.org> Proposed Solutions: [] Discussions: - https://forum.cardano.org/t/solving-the-blockchain-trilemma/107720 - https://github.com/cardano-foundation/cips/pulls/9999 Created: 2009-02-14 ---\nNote A reference template is available in .github/CPS-TEMPLATE.md\nA CPS must be stored in a specific folder named after its number and in a file called README.md. Before a number is assigned, use ???? as a placeholder name (thus naming new folders as CPS-????). After a number has been assigned, rename the folder.\nREADME.md\n????\nCPS-????\nAdditional supporting files (such as diagrams, binary specifications, dialect grammars, JSON schemas etc.) may be added to the CPS's folder under freely chosen names.\nFor example:\nCPS-0001 ├── README.md └── requirements.toml\nCPS-0001 ├── README.md └── requirements.toml\nFrom its creation onwards, a problem statement evolves around the following statuses.\nSolved: by <CIP-XXXX>[,<CIP-YYYY>,...]\nNote There is no \"draft\" status: a proposal which has not been merged (and hence exists in a PR) is a draft CPS. Draft CPSs should include the status they aim for on acceptance, typically but not always; this will be 'Open'.\nAs defined in CIP-0001.\nCPSs are licensed in the public domain. More so, they must be licensed under one of the following licenses. Each new CPS must identify at least one acceptable license in its preamble. In addition, each license must be referenced by its respective abbreviation below in the \"Copyright\" section.\nWarning\nAll licenses not explicitly included in the above lists are not acceptable terms for a Cardano Problem Statement unless a later CIP extends this one to add them.\nA statement must be well-formulated (i.e. unambiguous) and demonstrate an existing problem (for which use cases exist with no suitable alternatives). When related to a current project, the problem statement must also have been acknowledged by its respective project maintainers. In some cases, problem statements may be written after the facts and be merged directly as 'Solved' should they document in more depth what motivated an existing solution.\nProblem statements deemed unclear, for which alternatives exist with no significant drawbacks or establish unrealistic goals, shall be rejected (i.e. pull request closed without merge) with justifications or withdrawn by their authors.\nSimilarly, problems that appear abandoned by their authors shall also be rejected until resurrected by their authors or another community member.\nOnce merged, authors, project maintainers, or any other ecosystem actors may propose solutions addressing the problem in the form of CIP. They add their CIP to the 'Proposed Solutions' field in the CPS' 'Preamble' once a solution has been fully implemented and reaches the goals fixed in the original statement.\nOf course, a solution may only partially address a problem. In this case, one can alter the problem statement to incorporate the partial solutions and reflect the remaining issue(s) to solve.\nAs defined in CIP-0001.\nGoals make it easier to assess whether a solution solves a problem. Goals also give a direction for projects to follow and can help navigate the design space. The section is purposely flexible -- which we may want to make more rigid in the future if it is proven hard for authors to articulate their intents. Ideally, goals capture high-level requirements.\nUse cases are essential to understanding a problem and showing that a problem addresses a need. Without use cases, there is, in fact, no problem, and merely disliking a design doesn't make it problematic. A use case is also generally user-driven, which encourages the ecosystem to open a dialogue with users to build a system that is useful to others and not only well-designed for the mere satisfaction of engineers.\nThis section is meant to save time, especially for problem statement authors who will likely be the ones who end up reviewing proposed solutions. Open questions allow authors to state upfront elements they have already thought of and that any solution should consider in its design. Moreso, it is an opportunity to mention, for example, security considerations or common pitfalls that solutions should avoid.\nReview this proposal with existing actors of the ecosystem\nFormulate at least one problem statement following this process CPS-0001: Metadata Discoverability & Trust CPS-0002: Pointer Addresses\nCPS-0001: Metadata Discoverability & Trust\nCPS-0002: Pointer Addresses\nConfirm after repeated cycles of CPS submissions, reviews, and merges that the CPS process is both effective and accessible to the community.\nThis CIP is licensed under CC-BY-4.0.\nA problem may be only partially solved, in which case it remains in status 'Open'. Authors are encouraged to amend the document to explain what part of the problem remains to be solved. Consequently, CPS that are 'Solved' are considered fully addressed.\nA problem may be only partially solved, in which case it remains in status 'Open'. Authors are encouraged to amend the document to explain what part of the problem remains to be solved. Consequently, CPS that are 'Solved' are considered fully addressed.\nA problem may be only partially solved, in which case it remains in status 'Open'. Authors are encouraged to amend the document to explain what part of the problem remains to be solved. Consequently, CPS that are 'Solved' are considered fully addressed.\n2023 Cardano Foundation\n\n---\n\nCIP-7 | Curve Pledge Benefit\n\nModifying the current rewards calculation equation by substituting a n-root curved relationship between pledge and pledge benefit rewards for the current linear relationship will better achieve the original design goal of incentivizing pledge to help prevent Sybil attacks. This also reduces the unfortunate side effect in the current equation that over rewards private pools which provide no additional security benefit.\nThere are two main reasons for changing the current linear a0 pledge benefit factor in the rewards equation.\nPools pledging less than 1 million ADA see very little reward benefit. This is not a strong incentive for pool operators as at current prices that is approximately $150,000 USD. Private pools get massive reward benefit without providing any additional protection against Sybil attacks. Why should a private pool make 29 more rewards than a pool with 5m ADA pledge while doing the same work?\nPools pledging less than 1 million ADA see very little reward benefit. This is not a strong incentive for pool operators as at current prices that is approximately $150,000 USD.\nPools pledging less than 1 million ADA see very little reward benefit. This is not a strong incentive for pool operators as at current prices that is approximately $150,000 USD.\nPrivate pools get massive reward benefit without providing any additional protection against Sybil attacks. Why should a private pool make 29 more rewards than a pool with 5m ADA pledge while doing the same work?\nPrivate pools get massive reward benefit without providing any additional protection against Sybil attacks. Why should a private pool make 29 more rewards than a pool with 5m ADA pledge while doing the same work?\nThis is a modification of the maxPool function defined in section 11.8 Rewards Distribution Calculation of A Formal Specification of the Cardano Ledger .\nmaxPool = (R / (1 + a0)) * (o + (s * a0 * ((o - (s * ((z0 - o) / z0))) / z0)))\nwhere: R = ((reserve * rho) + fees) * (1 - tau) o = min(pool_stake / total_stake, z0) = z0 for fully saturated pool s = pledge / total_stake z0 = 1 / k and the following are current protocol parameters: k = 150 rho = 0.0022 a0 = 0.3 tau = .05\nThe idea is to replace s in the above equation with an n-root curve expression of pledge rather than the linear pledge value.\nWe use an expression called crossover to represent the point where the curve crosses the line and the benefit in the new and original equations is identical. Because the a0 pledge benefit is spread over the pledge range from 0 to saturation there is a dependence on k and total_stake. Since k and total_stake will likely change over time it is best to express crossover in terms of k and total_stake as follows:\ncrossover = total_stake / (k * crossover_factor)\nwhere crossover_factor is any real number greater than or equal to 1. So crossover_factor is essentially a divisor of the pool saturation amount. For example, setting crossover_factor to 20 with k = 150 and total_stake = 31 billion gives a crossover of approximately 10.3 million.\nAlso, we can parameterize the n-root curve exponent. This gives us:\ns = pow(pledge, (1 / curve_root)) * pow(crossover, ((curve_root - 1) / curve_root)) / total_stake\nThe curve_root could be set to any integer greater than 0 and when set to 1 produces the current rewards equation. The curve_root is n in n-root. For example, 1 = linear, 2 = square root, 3 = cube root, 4 = fourth root, etc.\nBy making this modification to the rewards equation we introduce two new protocol parameters, crossover_factor and curve_root, that need to be set thoughtfully.\nSee rewards.php for some simple PHP code that allows you to try different values for crossover_factor and curve_root and compare the resulting rewards to the current equation. For usage, run \"php -f rewards.php help\".\nAn interesting set of parameters as an example is:\ncurve_root = 3 crossover_factor = 8\nRunning \"php -f rewards.php 3 8\" produces:\nAssumptions Reserve: 14b Total stake: 31.7b Tx fees: 0 Fully Saturated Pool Rewards available in epoch: 29.3m Pool saturation: 211.3m\nCurve root: 3 Crossover factor: 8 Crossover: 26.4m\nPledge Rewards Benefit Alt Rwd Alt Bnft 0k 150051 0 150051 0 10k 150053 0 150458 0.27 50k 150062 0.01 150747 0.46 100k 150073 0.01 150928 0.58 200k 150094 0.03 151156 0.74 500k 150158 0.07 151551 1 1m 150264 0.14 151941 1.26 2m 150477 0.28 152432 1.59 5m 151116 0.71 153282 2.15 10m 152181 1.42 154122 2.71 20m 154311 2.84 155180 3.42 50m 160702 7.1 157012 4.64 100m 171352 14.2 158821 5.84 211.3m 195067 30 161305 7.5\nAs you can see this gives meaningful pledge benefit rewards to pools pledging less than 1m ADA.\nUsing the n-root curve pledge benefit shows a much more reasonable distribution of pledge related rewards which will encourage meaningful pledges from more pool operators thus making the network more secure against Sybil attacks. It also provides higher rewards for higher pledge without disproportionately rewarding a very few private pool operators who provide no additional security value to the network. This modification maintains the general principles of the current rewards equation and does not introduce any hard limits. It improves the incentives that were originally designed to make them more meaningful for the majority of pool operators.\nThis proposal is backwards compatible with the current reward function by setting the curve_root parameter to 1.\nThe new equation is implemented in the ledger and enacted through a hard-fork.\nAgreement by the Ledger team as defined in CIP-0084 under Expectations for ledger CIPs including \"expert opinion\" on changes to rewards & incentives.\nAgreement by the Ledger team as defined in CIP-0084 under Expectations for ledger CIPs including \"expert opinion\" on changes to rewards & incentives.\nAuthor has offered to produce an implementation of this change as a pull request if shown where the current maxPool reward equation is implemented in the code.\nAuthor has offered to produce an implementation of this change as a pull request if shown where the current maxPool reward equation is implemented in the code.\n2020 Shawn McMurdo. This CIP is licensed under Apache-2.0.\n2023 Cardano Foundation\n\n---\n\nCIP-30 | Cardano dApp-Wallet Web Bridge\n\nThis documents describes a webpage-based communication bridge allowing webpages (i.e. dApps) to interface with Cardano wallets. This is done via injected javascript code into webpages. This specification defines the manner that such code is to be accessed by the webpage/dApp, as well as defining the API for dApps to communicate with the user's wallet. This document currently concerns the Shelley-Mary era but will have a second version once Plutus is supported. This specification is intended to cover similar use cases as web3 for Ethereum or EIP-0012 for Ergo. The design of this spec was based on the latter.\nIn order to facilitate future dApp development, we will need a way for dApps to communicate with the user's wallet. While Cardano does not yet support smart contracts, there are still various use cases for this, such as NFT management. This will also lay the groundwork for an updated version of the spec once the Alonzo hardfork is released which can extend it to allow for Plutus support.\nA string representing an address in either bech32 format, or hex-encoded bytes. All return types containing Address must return the hex-encoded bytes format, but must accept either format for inputs.\nAddress\nA hex-encoded string of the corresponding bytes.\ncbor<T>\nA hex-encoded string representing CBOR corresponding to T defined via CDDL either inside of the Shelley Multi-asset binary spec or, if not present there, from the CIP-0008 signing spec. This representation was chosen when possible as it is consistent across the Cardano ecosystem and widely used by other tools, such as cardano-serialization-lib, which has support to encode every type in the binary spec as CBOR bytes.\nT\ntype DataSignature = {| signature: cbor<COSE_Sign1>, key: cbor<COSE_Key>, |};\ntype DataSignature = {| signature: cbor<COSE_Sign1>, key: cbor<COSE_Key>, |};\nIf we have CBOR specified by the following CDDL referencing the Shelley-MA CDDL:\ntransaction_unspent_output = [ input: transaction_input, output: transaction_output, ]\ntransaction_unspent_output = [ input: transaction_input, output: transaction_output, ]\nthen we define\ntype TransactionUnspentOutput = cbor<transaction_unspent_output>\ntype TransactionUnspentOutput = cbor<transaction_unspent_output>\nThis allows us to use the output for constructing new transactions using it as an output as the transaction_output in the Shelley Multi-asset CDDL does not contain enough information on its own to spend it.\ntransaction_output\ntype Paginate = {| page: number, limit: number, |};\ntype Paginate = {| page: number, limit: number, |};\nUsed to specify optional pagination for some API calls. Limits results to {limit} each page, and uses a 0-indexing {page} to refer to which of those pages of {limit} items each. dApps should be aware that if a wallet is modified between paginated calls that this will change the pagination, e.g. some results skipped or showing up multiple times but otherwise the wallet must respect the pagination order.\nAn extension is an object with a single field \"cip\" that describes a CIP number extending the API (as a plain integer, without padding). For example:\n\"cip\"\n{ \"cip\": 30 }\n{ \"cip\": 30 }\nAPIErrorCode { InvalidRequest: -1, InternalError: -2, Refused: -3, AccountChange: -4, } APIError { code: APIErrorCode, info: string }\nAPIErrorCode { InvalidRequest: -1, InternalError: -2, Refused: -3, AccountChange: -4, } APIError { code: APIErrorCode, info: string }\nInvalidRequest - Inputs do not conform to this spec or are otherwise invalid.\nInternalError - An error occurred during execution of this API call.\nRefused - The request was refused due to lack of access - e.g. wallet disconnects.\nAccountChange - The account has changed. The dApp should call wallet.enable() to reestablish connection to the new account. The wallet should not ask for confirmation as the user was the one who initiated the account change in the first place.\nwallet.enable()\nDataSignErrorCode { ProofGeneration: 1, AddressNotPK: 2, UserDeclined: 3, } type DataSignError = { code: DataSignErrorCode, info: String }\nDataSignErrorCode { ProofGeneration: 1, AddressNotPK: 2, UserDeclined: 3, } type DataSignError = { code: DataSignErrorCode, info: String }\nProofGeneration - Wallet could not sign the data (e.g. does not have the secret key associated with the address)\nAddressNotPK - Address was not a P2PK address and thus had no SK associated with it.\nUserDeclined - User declined to sign the data\ntype PaginateError = {| maxSize: number, |};\ntype PaginateError = {| maxSize: number, |};\n{maxSize} is the maximum size for pagination and if the dApp tries to request pages outside of this boundary this error is thrown.\nTxSendErrorCode = { Refused: 1, Failure: 2, } type TxSendError = { code: TxSendErrorCode, info: String }\nTxSendErrorCode = { Refused: 1, Failure: 2, } type TxSendError = { code: TxSendErrorCode, info: String }\nRefused - Wallet refuses to send the tx (could be rate limiting)\nFailure - Wallet could not send the tx\nTxSignErrorCode = { ProofGeneration: 1, UserDeclined: 2, } type TxSignError = { code: TxSignErrorCode, info: String }\nTxSignErrorCode = { ProofGeneration: 1, UserDeclined: 2, } type TxSignError = { code: TxSignErrorCode, info: String }\nProofGeneration - User has accepted the transaction sign, but the wallet was unable to sign the transaction (e.g. not having some of the private keys)\nUserDeclined - User declined to sign the transaction\nIn order to initiate communication from webpages to a user's Cardano wallet, the wallet must provide the following javascript API to the webpage. A shared, namespaced cardano object must be injected into the page if it did not exist already. Each wallet implementing this standard must then create a field in this object with a name unique to each wallet containing a wallet object with the following methods. The API is split into two stages to maintain the user's privacy, as the user will have to consent to cardano.{walletName}.enable() in order for the dApp to read any information pertaining to the user's wallet with {walletName} corresponding to the wallet's namespaced name of its choice.\ncardano\nwallet\ncardano.{walletName}.enable()\n{walletName}\ncardano.{walletName}.enable({ extensions: Extension[] } = {}): Promise<API>\nErrors: APIError\nAPIError\nThis is the entrypoint to start communication with the user's wallet. The wallet should request the user's permission to connect the web page to the user's wallet, and if permission has been granted, the full API will be returned to the dApp to use. The wallet can choose to maintain a whitelist to not necessarily ask the user's permission every time access is requested, but this behavior is up to the wallet and should be transparent to web pages using this API. If a wallet is already connected this function should not request access a second time, and instead just return the API object.\nAPI\nUpon start, dApp can explicitly request a list of additional functionalities they expect as a list of CIP numbers capturing those extensions. This is used as an extensibility mechanism to document what functionalities can be provided by the wallet interface. CIP-0030 provides a set of base interfaces that every wallet must support. Then, new functionalities are introduced via additional CIPs and may be all or partially supported by wallets.\nDApps are expected to use this endpoint to perform an initial handshake and ensure that the wallet supports all their required functionalities. Note that it's possible for two extensions to be mutually incompatible (because they provide two conflicting features). While we may try to avoid this as much as possible while designing CIPs, it is also the responsability of wallet providers to assess whether they can support a given combination of extensions, or not. Hence wallets aren't expected to fail should they not recognize or not support a particular combination of extensions. Instead, they should decide what they enable and reflect their choice in the response to api.getExtensions() in the Full API. As a result, dApps may fail and inform their users or may use a different, less-efficient, strategy to cope with a lack of functionality.\napi.getExtensions()\nIt is at the extension author's discretion if they wish to separate their endpoints from the base API via namespacing. Although, it is highly recommend that authors do namespace all of their extensions. If namespaced, endpoints must be preceded by .cipXXXX. from the API object, without any leading zeros.\n.cipXXXX.\nAPI\nFor example; CIP-0123's endpoints should be accessed by:\napi.cip123.endpoint1() api.cip123.endpoint2()\napi.cip123.endpoint1() api.cip123.endpoint2()\nAuthors should be careful when omitting namespacing. Omission should only be considered when creating endpoints to override those defined in this specification or other extensions. Even so when overriding; the new functionality should not prevent dApps from accessing past functionality thus overriding must ensure backwards compatibility.\nAny namespace omission needs to be fully justified via the proposal's Rationale section, with explanation to why it is necessary. Any potential backwards compatibility considerations should be noted to give wallets and dApps a clear unambiguous direction.\nExtensions that are draft, in development, or prototyped should not use extension naming nor should they use official namspacing until assigned a CIP number. Draft extension authors are free to test their implementation endpoints by using the Experimental API. Once a CIP number is assigned implementors should move functionality out of the experimental API.\nYes. Extensions may have other extensions as pre-requisite. Some newer extensions may also invalidate functionality introduced by earlier extensions. There's no particular rule or constraints in that regards. Extensions are specified as CIP, and will define what it entails to enable them.\nYes. They all are CIPs.\nYes. Extensions may introduce new endpoints or error codes, and modify existing ones. Although, it is recommended that endpoints are namespaced. Extensions may even change the rules outlined in this very proposal. The idea being that wallet providers should start off implementing this CIP, and then walk their way to implementing their chosen extensions.\nNo. It's up to wallet providers to decide which extensions they ought to support.\ncardano.{walletName}.isEnabled(): Promise<bool>\nErrors: APIError\nAPIError\nReturns true if the dApp is already connected to the user's wallet, or if requesting access would return true without user confirmation (e.g. the dApp is whitelisted), and false otherwise. If this function returns true, then any subsequent calls to wallet.enable() during the current session should succeed and return the API object.\nwallet.enable()\nAPI\ncardano.{walletName}.apiVersion: String\nThe version number of the API that the wallet supports. Set to 1.\n1\ncardano.{walletName}.supportedExtensions: Extension[]\nA list of extensions supported by the wallet. Extensions may be requested by dApps on initialization. Some extensions may be mutually conflicting and this list does not thereby reflect what extensions will be enabled by the wallet. Yet it informs on what extensions are known and can be requested by dApps if needed.\ncardano.{walletName}.name: String\nA name for the wallet which can be used inside of the dApp for the purpose of asking the user which wallet they would like to connect with.\ncardano.{walletName}.icon: String\nA URI image (e.g. data URI base64 or other) for img src for the wallet which can be used inside of the dApp for the purpose of asking the user which wallet they would like to connect with.\nUpon successful connection via cardano.{walletName}.enable(), a javascript object we will refer to as API (type) / api (instance) is returned to the dApp with the following methods. All read-only methods (all but the signing functionality) should not require any user interaction as the user has already consented to the dApp reading information about the wallet's state when they agreed to cardano.{walletName}.enable(). The remaining methods api.signTx() and api.signData() must request the user's consent in an informative way for each and every API call in order to maintain security.\ncardano.{walletName}.enable()\nAPI\napi\ncardano.{walletName}.enable()\napi.signTx()\napi.signData()\nThe API chosen here is for the minimum API necessary for dApp - Wallet interactions without convenience functions that don't strictly need the wallet's state to work. The API here is for now also only designed for Shelley's Mary hardfork and thus has NFT support. When Alonzo is released with Plutus support this API will have to be extended.\napi.getExtensions(): Promise<Extension[]>\nErrors: APIError\nAPIError\nRetrieves the list of extensions enabled by the wallet. This may be influenced by the set of extensions requested in the initial enable request.\nenable\napi.getNetworkId(): Promise<number>\nErrors: APIError\nAPIError\nReturns the network id of the currently connected account. 0 is testnet and 1 is mainnet but other networks can possibly be returned by wallets. Those other network ID values are not governed by this document. This result will stay the same unless the connected account has changed.\napi.getUtxos(amount: cbor<value> = undefined, paginate: Paginate = undefined): Promise<TransactionUnspentOutput[] | null>\nErrors: APIError, PaginateError\nAPIError\nPaginateError\nIf amount is undefined, this shall return a list of all UTXOs (unspent transaction outputs) controlled by the wallet. If amount is not undefined, this request shall be limited to just the UTXOs that are required to reach the combined ADA/multiasset value target specified in amount, and if this cannot be attained, null shall be returned. The results can be further paginated by paginate if it is not undefined.\namount\nundefined\namount\nundefined\namount\nnull\npaginate\nundefined\napi.getCollateral(params: { amount: cbor<Coin> }): Promise<TransactionUnspentOutput[] | null>\nErrors: APIError\nAPIError\nThe function takes a required object with parameters. With a single required parameter for now: amount. (NOTE: some wallets may be ignoring the amount parameter, in which case it might be possible to call the function without it, but this behavior is not recommended!). Reasons why the amount parameter is required:\namount\namount\nDapps must be motivated to understand what they are doing with the collateral, in case they decide to handle it manually. Depending on the specific wallet implementation, requesting more collateral than necessarily might worsen the user experience with that dapp, requiring the wallet to make explicit wallet reorganisation when it is not necessary and can be avoided. If dapps don't understand how much collateral they actually need to make their transactions work - they are placing more user funds than necessary in risk.\nDapps must be motivated to understand what they are doing with the collateral, in case they decide to handle it manually.\nDepending on the specific wallet implementation, requesting more collateral than necessarily might worsen the user experience with that dapp, requiring the wallet to make explicit wallet reorganisation when it is not necessary and can be avoided.\nIf dapps don't understand how much collateral they actually need to make their transactions work - they are placing more user funds than necessary in risk.\nSo requiring the amount parameter would be a by-spec behavior for a wallet. Not requiring it is possible, but not specified, so dapps should not rely on that and the behavior is not recommended.\namount\nThis shall return a list of one or more UTXOs (unspent transaction outputs) controlled by the wallet that are required to reach AT LEAST the combined ADA value target specified in amount AND the best suitable to be used as collateral inputs for transactions with plutus script inputs (pure ADA-only utxos). If this cannot be attained, an error message with an explanation of the blocking problem shall be returned. NOTE: wallets are free to return utxos that add up to a greater total ADA value than requested in the amount parameter, but wallets must never return any result where utxos would sum up to a smaller total ADA value, instead in a case like that an error message must be returned.\namount\namount\nThe main point is to allow the wallet to encapsulate all the logic required to handle, maintain, and create (possibly on-demand) the UTXOs suitable for collateral inputs. For example, whenever attempting to create a plutus-input transaction the dapp might encounter a case when the set of all user UTXOs don't have any pure entries at all, which are required for the collateral, in which case the dapp itself is forced to try and handle the creation of the suitable entries by itself. If a wallet implements this function it allows the dapp to not care whether the suitable utxos exist among all utxos, or whether they have been stored in a separate address chain (see https://github.com/cardano-foundation/CIPs/pull/104), or whether they have to be created at the moment on-demand - the wallet guarantees that the dapp will receive enough utxos to cover the requested amount, or get an error in case it is technically impossible to get collateral in the wallet (e.g. user does not have enough ADA at all).\nThe amount parameter is required, specified as a string (BigNumber) or a number, and the maximum allowed value must be agreed to be something like 5 ADA. Not limiting the maximum possible value might force the wallet to attempt to purify an unreasonable amount of ADA just because the dapp is doing something weird. Since by protocol the required collateral amount is always a percentage of the transaction fee, it seems that the 5 ADA limit should be enough for the foreseeable future.\namount\nstring\nnumber\napi.getBalance(): Promise<cbor<value>>\nErrors: APIError\nAPIError\nReturns the total balance available of the wallet. This is the same as summing the results of api.getUtxos(), but it is both useful to dApps and likely already maintained by the implementing wallet in a more efficient manner so it has been included in the API as well.\napi.getUtxos()\napi.getUsedAddresses(paginate: Paginate = undefined): Promise<Address[]>\nErrors: APIError\nAPIError\nReturns a list of all used (included in some on-chain transaction) addresses controlled by the wallet. The results can be further paginated by paginate if it is not undefined.\npaginate\nundefined\napi.getUnusedAddresses(): Promise<Address[]>\nErrors: APIError\nAPIError\nReturns a list of unused addresses controlled by the wallet.\napi.getChangeAddress(): Promise<Address>\nErrors: APIError\nAPIError\nReturns an address owned by the wallet that should be used as a change address to return leftover assets during transaction creation back to the connected wallet. This can be used as a generic receive address as well.\napi.getRewardAddresses(): Promise<Address[]>\nErrors: APIError\nAPIError\nReturns the reward addresses owned by the wallet. This can return multiple addresses e.g. CIP-0018.\napi.signTx(tx: cbor<transaction>, partialSign: bool = false): Promise<cbor<transaction_witness_set>>\nErrors: APIError, TxSignError\nAPIError\nTxSignError\nRequests that a user sign the unsigned portions of the supplied transaction. The wallet should ask the user for permission, and if given, try to sign the supplied body and return a signed transaction. If partialSign is true, the wallet only tries to sign what it can. If partialSign is false and the wallet could not sign the entire transaction, TxSignError shall be returned with the ProofGeneration code. Likewise if the user declined in either case it shall return the UserDeclined code. Only the portions of the witness set that were signed as a result of this call are returned to encourage dApps to verify the contents returned by this endpoint while building the final transaction.\npartialSign\npartialSign\nTxSignError\nProofGeneration\nUserDeclined\napi.signData(addr: Address, payload: Bytes): Promise<DataSignature>\nErrors: APIError, DataSignError\nAPIError\nDataSignError\nThis endpoint utilizes the CIP-0008 signing spec for standardization/safety reasons. It allows the dApp to request the user to sign a payload conforming to said spec. The user's consent should be requested and the message to sign shown to the user. The payment key from addr will be used for base, enterprise and pointer addresses to determine the EdDSA25519 key used. The staking key will be used for reward addresses. This key will be used to sign the COSE_Sign1's Sig_structure with the following headers set:\naddr\nCOSE_Sign1\nSig_structure\nalg (1) - must be set to EdDSA (-8)\nalg\nEdDSA\nkid (4) - Optional, if present must be set to the same value as in the COSE_key specified below. It is recommended to be set to the same value as in the \"address\" header.\nkid\nCOSE_key\n\"address\"\n\"address\" - must be set to the raw binary bytes of the address as per the binary spec, without the CBOR binary wrapper tag\n\"address\"\nThe payload is not hashed and no external_aad is used.\nexternal_aad\nIf the payment key for addr is not a P2Pk address then DataSignError will be returned with code AddressNotPK. ProofGeneration shall be returned if the wallet cannot generate a signature (i.e. the wallet does not own the requested payment private key), and UserDeclined will be returned if the user refuses the request. The return shall be a DataSignature with signature set to the hex-encoded CBOR bytes of the COSE_Sign1 object specified above and key shall be the hex-encoded CBOR bytes of a COSE_Key structure with the following headers set:\naddr\nDataSignError\nAddressNotPK\nProofGeneration\nUserDeclined\nDataSignature\nsignature\nCOSE_Sign1\nkey\nCOSE_Key\nkty (1) - must be set to OKP (1)\nkty\nOKP\nkid (2) - Optional, if present must be set to the same value as in the COSE_Sign1 specified above.\nkid\nCOSE_Sign1\nalg (3) - must be set to EdDSA (-8)\nalg\nEdDSA\ncrv (-1) - must be set to Ed25519 (6)\ncrv\nEd25519\nx (-2) - must be set to the public key bytes of the key used to sign the Sig_structure\nx\nSig_structure\napi.submitTx(tx: cbor<transaction>): Promise<hash32>\nErrors: APIError, TxSendError\nAPIError\nTxSendError\nAs wallets should already have this ability, we allow dApps to request that a transaction be sent through it. If the wallet accepts the transaction and tries to send it, it shall return the transaction id for the dApp to track. The wallet is free to return the TxSendError with code Refused if they do not wish to send it, or Failure if there was an error in sending it (e.g. preliminary checks failed on signatures).\nTxSendError\nRefused\nFailure\nMultiple experimental namespaces are used:\nunder api (ex: api.experimental.myFunctionality).\napi\napi.experimental.myFunctionality\nunder cardano.{walletName} (ex: window.cardano.{walletName}.experimental.myFunctionality)\ncardano.{walletName}\nwindow.cardano.{walletName}.experimental.myFunctionality\nThe benefits of this are:\nWallets can add non-standardized features while still following the CIP30 structure dApp developers can use these functions explicitly knowing they are experimental (not stable or standardized) New features can be added to CIP30 as experimental features and only moved to non-experimental once multiple wallets implement it It provides a clear path to updating the CIP version number (when functions move from experimental - stable)\nWallets can add non-standardized features while still following the CIP30 structure\ndApp developers can use these functions explicitly knowing they are experimental (not stable or standardized)\nNew features can be added to CIP30 as experimental features and only moved to non-experimental once multiple wallets implement it\nIt provides a clear path to updating the CIP version number (when functions move from experimental - stable)\nSee justification and explanations provided with each API endpoint.\nExtensions provide an extensibility mechanism and a way to negotiate (possibly conflicting) functionality between a DApp and a wallet provider. There's rules enforced as for what extensions a wallet decide to support or enable. The current mechanism only gives a way for wallets to communicate their choice back to a DApp.\nWe use object as extensions for now to leave room for adding fields in the future without breaking all existing interfaces. At this point in time however, objects are expected to be singleton.\nExtensions can be seen as a smart versioning scheme. Except that, instead of being a monotonically increasing sequence of numbers, they are multi-dimensional feature set that can be toggled on and off at will. This is a versioning \" -la-carte\" which is useful in a context where:\nThere are multiple concurrent standardization efforts on different fronts to accommodate a rapidly evolving ecosystem; Not everyone agrees and has desired to support every existing standard; There's a need from an API consumer standpoint to clearly identify what features are supported by providers.\nThere are multiple concurrent standardization efforts on different fronts to accommodate a rapidly evolving ecosystem;\nNot everyone agrees and has desired to support every existing standard;\nThere's a need from an API consumer standpoint to clearly identify what features are supported by providers.\nBy encouraging the explicit namespacing of each extension we aim to improve the usability of extensions for dApps. By allowing special cases where namespacing can be dropped we maintain good flexibility in extension design.\nThe interface is implemented and supported by various wallet providers. See also: cardano-caniuse.\nThe interface is used by DApps to interact with wallet providers. Few examples: https://www.jpg.store/ https://app.minswap.org/ https://muesliswap.com/ https://exchange.sundaeswap.finance/ https://app.indigoprotocol.io/\nhttps://www.jpg.store/\nhttps://app.minswap.org/\nhttps://muesliswap.com/\nhttps://exchange.sundaeswap.finance/\nhttps://app.indigoprotocol.io/\nProvide some reference implementation of wallet providers Berry-Pool/nami-wallet Emurgo/yoroi-wallet\nProvide some reference implementation of wallet providers\nBerry-Pool/nami-wallet\nEmurgo/yoroi-wallet\nProvide some reference implementation of the dapp connector cardano-foundation/connect-with-wallet\nProvide some reference implementation of the dapp connector\ncardano-foundation/connect-with-wallet\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCPS-0013 | Better builtin data structures in Plutus\n\nPlutus Core lacks builtin data structures with good asymptotic performance for some use cases.\nPlutus Core has a few builtin data structures, but these are mostly used to make a minimally adequate representation of the Data type. It does not have builtin data structures optimized for performance.\nData\nUsers can implement their own data structures (since Plutus Core is an expressive programming language), but in practice this has not happened much. In particular, we will focus on two examples here:\nArrays with constant-time lookup Maps with logarithmic-time lookup (also Sets, but we can treat them as a special case of Maps)\nArrays with constant-time lookup\nMaps with logarithmic-time lookup (also Sets, but we can treat them as a special case of Maps)\nBoth of these are difficult to implement in Plutus Core:\nArrays are (we believe) impossible without some kind of primitive with constant-time lookup Maps are possible but are typically moderately complex data structures which require a lot of code, and this has not been done in practice\nArrays are (we believe) impossible without some kind of primitive with constant-time lookup\nMaps are possible but are typically moderately complex data structures which require a lot of code, and this has not been done in practice\nA common pattern in DEXs is to have a list of inputs/outputs to match up in a datum. In some cases the order is highly significant, e.g. earlier orders should be processed first, and the outcome of processing an earlier order may affect later ones.\nFor example, we might have:\ninputIdxs :: BuiltinList Integer outputIdxs :: BuiltinList Integer\ninputIdxs :: BuiltinList Integer outputIdxs :: BuiltinList Integer\nWe then want to go through these lists, looking up the corresponding inputs and outputs and check some property (e.g. that the value is directly transferred from one to the other).\nThis requires a quadratic amount of work, which puts a low ceiling on how many orders can be processed at once. Empirically, many are capped at about 30, whereas if they were limited only by the amount of space in the transaction for inputs and outputs the limit would be hundreds.\nIf we had arrays with constant time indexing, we could make this linear instead. Note that unless we also implemented the \"Data fields\" suggestion below we would still need to do a linear amount of work to create arrays for the transaction inputs and outputs from the lists in the script context.\nThe Data type has a Constr alternative which is used for encoding datatype constructors. This is used for encoding the script context, and is used by languages such as Aiken extensively for representing user-defined datatypes also.\nData\nConstr\nThe fields of the constructor are encoded in a list; hence to access a particular field the compiled code needs to do a linear amount of work. If the arguments to a Constr were an array, we could access the fields in constant time.\nConstr\nSimilarly, the List and Map constructors of Data could use arrays.\nList\nMap\nData\nValue\nThe Value type is a nested map: it is a map from bytestrings (representing policy IDs) to maps from bytestrings (representing token names) to integers (representing quantities). Since map operations are currently linear, this means that even simple operations like checking whether one value is less than another can have quadratic cost.\nValue\nThis would be much better if map operations were logarithmic cost.\nMany applications have a known set of participants identified by some bytestring, typically a public key. It is therefore natural to store per-party state in a map indexed by the party identifier.\nSince map operations currently have much worse complexity than a good map data structure (often linear/quadratic instead of logarithmic/linear), this is needlessly expensive and imposes a limit on the number of parties.\nReduce the cost of operations on Value by a factor of 2-10 Reduce the cost of a matching algorithm such that we can handle hundreds of matches for the same cost it currently takes to do 30.\nReduce the cost of operations on Value by a factor of 2-10\nValue\nReduce the cost of a matching algorithm such that we can handle hundreds of matches for the same cost it currently takes to do 30.\nCan we implement a set/map data structure in Plutus Core code that has acceptable performance and doesn t require too much size overhead?\nDo we need generic maps or is a map-from-bytestring sufficient? What about map-from-integer? Generic maps are harder since we typically need to know how to order the key type\nGeneric maps are harder since we typically need to know how to order the key type\nIs an array type useful even if it is immutable? We are unlikely to be able to offer mutable arrays\nWe are unlikely to be able to offer mutable arrays\nAre builtin data structures useful enough even if they can only contain builtin types? This would mean that complex data structures would have to be stored inside arrays as Data, rather than using Scott encoding or sum-of-products representation\nThis would mean that complex data structures would have to be stored inside arrays as Data, rather than using Scott encoding or sum-of-products representation\nData\nCan we feasibly change the structure of the builtin Data type so that Constr arguments are in an array? We would need to retain both versions for backwards compatibility\nData\nConstr\nWe would need to retain both versions for backwards compatibility\nhttps://x.com/Quantumplation/status/1733298551571038338?s=20\nThis CPS is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-3 | Wallet Key Generation\n\nMany wallets utilize some way of mapping a sentence of words (easy to read and write for humans) uniquely back and forth to a sized binary data (harder to remember).\nThis document outlines the various mapping algorithms used in the Cardano ecosystem.\nThe philosophy of cryptocurrencies is that you are in charge of your own finances. Therefore, it is very anti-thematic for wallet software to lock in a user by explicitly describing the algorithm used to derive keys for a wallet (both the master key and key derivation)\nTo this end, this document outlines all the relevant key generation algorithms used in the Cardano ecosystem.\nConversion from a recovery phrase to entropy is the same as described in BIP39.\nIn Cardano, hierarchical deterministic (abbrev. HD) wallets are similar to those described in BIP-0032. Notably, we use a variation called ED25519-BIP32. A reference implementation can be found here.\nThe master key generation is the mean by which on turns an initial entropy into a secure cryptographic key.\nMore specifically, the generation is a function from an initial seed to an extended private key (abbrev. XPrv) composed of:\n64 bytes: an extended Ed25519 secret key composed of: 32 bytes: Ed25519 curve scalar from which few bits have been tweaked according to ED25519-BIP32 32 bytes: Ed25519 binary blob used as IV for signing\n32 bytes: Ed25519 curve scalar from which few bits have been tweaked according to ED25519-BIP32\n32 bytes: Ed25519 binary blob used as IV for signing\n32 bytes: chain code for allowing secure child key derivation\nThroughout the years, Cardano has used different styles of master key generation:\nThis CIP is merely to document the existing standards and not to provide rationales for the various methods used.\nHowever, you can learn more at the following links:\nAdrestia documentation\nSLIP-0010\nSLIP-0023\nEach generation method is documented and provides test vectors in a language-agnostic way.\nThere exists reference implementations in various languages for each method.\nAt least 2 Cardano wallets (e.g. Yoroi & Daedalus) implement these methods.\nImplementation of each algorithm will be carried out in Yoroi and Daedalus (via cardano-wallet) by Emurgo and Input Output respectively.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation\n\n---\n\nCIP-1 | CIP Process\n\nA Cardano Improvement Proposal (CIP) is a formalised design document for the Cardano community and the name of the process by which such documents are produced and listed. A CIP provides information or describes a change to the Cardano ecosystem, processes, or environment concisely and in sufficient technical detail. In this CIP, we explain what a CIP is, how the CIP process functions, the role of the CIP Editors, and how users should go about proposing, discussing, and structuring a CIP.\nThe Cardano Foundation intends CIPs to be the primary mechanisms for proposing new features, collecting community input on an issue, and documenting design decisions that have gone into Cardano. Plus, because CIPs are text files in a versioned repository, their revision history is the historical record of significant changes affecting Cardano.\nCIPs aim to address two challenges mainly:\nThe need for various parties to agree on a common approach to ease the interoperability of tools or interfaces. The need to propose and discuss changes to the protocol or established practice of the ecosystem.\nThe need for various parties to agree on a common approach to ease the interoperability of tools or interfaces.\nThe need to propose and discuss changes to the protocol or established practice of the ecosystem.\nThe CIP process does not by itself offer any form of governance. For example, it does not govern the process by which proposed changes to the Cardano protocol are implemented and deployed. Yet, it is a crucial, community-driven component of the governance decision pipeline as it helps to collect thoughts and proposals in an organised fashion. Additionally, specific projects may choose to actively engage with the CIP process for some or all changes to their project.\nThis document outlines the technical structure of the CIP and the technical requirements of the submission and review process. The history, social features and human elements of the CIP process are described the CIP repository Wiki.\nDocument Structure Header Preamble Translations Repository Organization Versioning Licensing Statuses Status: Proposed Status: Active Status: Inactive Path to Active Categories Project Enlisting\nStructure Header Preamble Translations Repository Organization Versioning Licensing\nHeader Preamble\nTranslations\nRepository Organization\nVersioning\nLicensing\nStatuses Status: Proposed Status: Active Status: Inactive\nStatus: Proposed\nStatus: Active\nStatus: Inactive\nPath to Active\nCategories\nProject Enlisting\nProcess 1. Early Stage 1.a. Authors open a pull request Naming CIPs with similar subjects 1.b. Authors seek feedback 2. Editors' role 2.a. Triage in bi-weekly meetings 2.b. Reviews 3. Merging CIPs in the repository 4. Implementors work towards Active status following their 'Implementation Plan'\n1. Early Stage 1.a. Authors open a pull request Naming CIPs with similar subjects 1.b. Authors seek feedback\n1.a. Authors open a pull request Naming CIPs with similar subjects\nNaming CIPs with similar subjects\n1.b. Authors seek feedback\n2. Editors' role 2.a. Triage in bi-weekly meetings 2.b. Reviews\n2.a. Triage in bi-weekly meetings\n2.b. Reviews\n3. Merging CIPs in the repository\n4. Implementors work towards Active status following their 'Implementation Plan'\nEditors Missions Reviews Nomination\nMissions\nReviews\nNomination\nA CIP is, first and foremost, a document which proposes a solution to a well-defined problem. Documents are Markdown files with a Preamble and a set of pre-defined sections. CIPs authors must abide by the general structure, though they are free to organise each section as they see fit.\nThe structure of a CIP file is summarised in the table below:\nMotivation\nN/A\nNote Each of these section titles (Abstract onward) should be an H2 heading (beginning with markdown ##). Subsections like Versioning or Acceptance Criteria should be H3 headings (e.g. ### Versioning). Don't include a H1 title heading (markdown #): for web friendly contexts, this will be generated from the Preamble.\n##\n### Versioning\n#\nEach CIP must begin with a YAML key:value style header preamble (also known as front matter data), preceded and followed by three hyphens (---).\n---\nCIP\nTitle\n-\nStatus\nCategory\nAuthors\nImplementors\nN/A\n[]\nDiscussions\nSolution-To\nCreated\nLicense\nFor example:\n--- CIP: 1 Title: CIP Process Status: Active Category: Meta Authors: - Frederic Johnson <frederic.johnson@cardanofoundation.org> - Sebastien Guillemot <sebastien@dcspark.io> - Matthias Benkort <matthias.benkort@cardanofoundation.org> - Duncan Coutts <duncan.coutts@iohk.io> Implementors: N/A Discussions: - https://github.com/cardano-foundation/CIPs/pull/366 Created: 2020-03-21 License: CC-BY-4.0 ---\n--- CIP: 1 Title: CIP Process Status: Active Category: Meta Authors: - Frederic Johnson <frederic.johnson@cardanofoundation.org> - Sebastien Guillemot <sebastien@dcspark.io> - Matthias Benkort <matthias.benkort@cardanofoundation.org> - Duncan Coutts <duncan.coutts@iohk.io> Implementors: N/A Discussions: - https://github.com/cardano-foundation/CIPs/pull/366 Created: 2020-03-21 License: CC-BY-4.0 ---\nEspecially because Markdown link syntax is not supported in the header preamble, labels can be added to clarify list items; e.g.:\nDiscussions: - Original-PR: https://github.com/cardano-foundation/CIPs/pull/366\nDiscussions: - Original-PR: https://github.com/cardano-foundation/CIPs/pull/366\nNote A reference template is available in .github/CIP-TEMPLATE.md\nA CIP must be stored in a specific folder named after its number (4-digit, left-padded with 0) and in a file called README.md. Before a number is assigned, a placeholder folder name should be used (e.g. CIP-my-new-feature). After a number has been assigned, rename the folder.\n0\nREADME.md\nCIP-my-new-feature\nAdditional supporting files (such as diagrams, binary specifications, dialect grammars, JSON schemas etc.) may be added to the CIP's folder under freely chosen names.\nFor example:\nCIP-0010 ├── README.md ├── registry.json └── registry.schema.json\nCIP-0010 ├── README.md ├── registry.json └── registry.schema.json\nWhile CIPs are mainly technical documents meant to be read primarily by developers -- and thus often written in English; some may be translated into various languages to increase their outreach. Any file in a CIP folder may also include translated content satisfying the following rules:\nAny translated file shall share a common basename with its original source.\nAny translated file shall share a common basename with its original source.\nTranslation file basenames must have a suffix using an ISO 639-1 language code, separated by a dot . character. (e.g. README.fr.md).\nTranslation file basenames must have a suffix using an ISO 639-1 language code, separated by a dot . character. (e.g. README.fr.md).\n.\nREADME.fr.md\nWhen no language code is provided as suffix, the default language for the document is assumed to be English (UK/US).\nWhen no language code is provided as suffix, the default language for the document is assumed to be English (UK/US).\nTranslated CIPs (i.e. README files), must not include the original preamble. They must, however, include the following preamble as yaml frontmatter data:\nTranslated CIPs (i.e. README files), must not include the original preamble. They must, however, include the following preamble as yaml frontmatter data:\nREADME\nCIP\nSource\nTitle\nRevision\n12ab34c\nTranslators\nJohn Doe <john.doe@email.domain>\nLanguage\nfr\nTranslated CIPs inherit the same licensing terms as their original sources.\nCIPs must indicate how the defined Specification is versioned. Note this does not apply to the CIP text, for which annotated change logs are automatically generated and available through the GitHub UI as a history of CIP files and directories.\nAuthors are free to describe any approach to versioning that allows versioned alterations to be added without author oversight. Stipulating that the proposal must be superseded by another is also considered to be valid versioning.\nA single Versioning scheme can be placed either as a subsection of the Specification section or in an optional Versioning top-level section near the end. If the Specification contains multiple specification subsections, each of these can have a Versioning subsection within it.\nCIPs are licensed in the public domain. More so, they must be licensed under one of the following licenses. Each new CIP must identify at least one acceptable license in its preamble. In addition, each license must be referenced by its respective abbreviation below in the \"Copyright\" section.\nWarning\nAll licenses not explicitly included in the above lists are not acceptable terms for a Cardano Improvement Proposal unless a later CIP extends this one to add them.\nCIPs can have three statuses: Proposed, Active or Inactive. The CIP Process section highlights how CIPs move through these statuses; no CIP should be given one of these statuses without satisfying the criteria described here below.\nProposed\nActive\nInactive\nNote There is no \"draft\" status: a proposal which has not been merged (and hence exists in a PR) is a draft CIP. Draft CIPs should include the status they are aiming for on acceptance. Typically, but not always, this will be 'Proposed'.\nA 'Proposed' CIP is any CIP that meets the essential CIP criteria but is not yet 'Active'. The criteria that must meet a CIP to be merged as 'Proposed' are:\nIt must contain all the sections described in Structure.\nThe quality of the content must be to the Editors satisfaction. That means it must be grammatically sound, well-articulated and demonstrates noticeable efforts in terms of completeness and level of detail.\nIts technical soundness should have been established. Where necessary, this may require review by particular experts and addressing their concerns. Note that the requirement is that the proposal makes sense (i.e. be technically sound), yet no consulted experts need to think it is a good idea.\nIt must have a valid Path to Active as defined below.\nAn 'Active' CIP has taken effect according to the criteria defined in its 'Path to Active' section. Said differently, each CIP defines by which criteria it can become 'Active' and those criteria are included in the review process. Exact criteria thereby depends on the nature of the CIP, typically:\nFor CIPs that relate to particular projects or pieces of technology, it becomes 'Active' by being implemented and released;\nFor changes to the Cardano protocol, a CIP becomes 'Active' by being live on the Cardano mainnet;\nFor ecosystem standards, it means gaining sufficient and visible adoption in the community.\nIt must have a valid Path to Active as defined below: even the CIP is already acknowledged as Active or being documented retroactively (after acceptance and implementation).\nActive\nA proposal that is 'Active' is considered complete and is synonymous with \"production readiness\" when it comes to the maturity of a solution. 'Active' CIPs will not be updated substantially (apart from minor edits, proofreading and added precisions). They can, nevertheless, be challenged through new proposals if need be.\nAn 'Inactive' CIP describes any proposal that does not fit into the other types. A CIP can therefore be 'Inactive' for various reasons (e.g. obsolete, superseded, abandoned). Hence the status must indicate a justification between parentheses; for example:\nStatus: Inactive (superseded by CIP-0001)\nStatus: Inactive (superseded by CIP-0001)\nThis must be subdivided into two sub-sections:\n'Acceptance Criteria' This sub-section must define a list of criteria by which the proposal can become active. Criteria must relate to observable metrics or deliverables and be reviewed by editors and project maintainers when applicable. For example: \"The changes to the ledger rules are implemented and deployed on Cardano mainnet by a majority of the network\", or \"The following key projects have implemented support for this standard\".\n'Acceptance Criteria'\nThis sub-section must define a list of criteria by which the proposal can become active. Criteria must relate to observable metrics or deliverables and be reviewed by editors and project maintainers when applicable. For example: \"The changes to the ledger rules are implemented and deployed on Cardano mainnet by a majority of the network\", or \"The following key projects have implemented support for this standard\".\n'Implementation Plan' This sub-section should define the plan by which a proposal will meet its acceptance criteria, when applicable. More, proposals that require implementation work in a specific project may indicate one or more implementors. Implementors must sign off on the plan and be referenced in the document's preamble. In particular, an implementation that requires a hard-fork should explicitly mention it in its 'Implementation Plan'.\n'Implementation Plan'\nThis sub-section should define the plan by which a proposal will meet its acceptance criteria, when applicable. More, proposals that require implementation work in a specific project may indicate one or more implementors. Implementors must sign off on the plan and be referenced in the document's preamble.\nIn particular, an implementation that requires a hard-fork should explicitly mention it in its 'Implementation Plan'.\nNote the statuses of Proposed and Active both require a Path to Active section, making this a required section for all viable proposals. Even if a CIP is edited or submitted with an Inactive status, it may still be helpful to have a Path to Active if there are conditions that might lead to its acceptance or implementation.\nProposed\nActive\nInactive\nPath to Active\nCIPs are classified into distinct categories that help organise (and thus, find) them. Categories are meant to be flexible and evolve as new domains emerge. Authors may leave the category as ? should they not be sure under which category their proposal falls; editors will eventually assign one or reject the proposal altogether should it relate to an area where the CIP process does not apply.\n?\nSubmissions can be made to these categories whenever relevant, without following any particular submission guidelines:\nAdditionally, representatives of ecosystem categories may explicitly enlist their categories (see next section) to suggest a closer relationship with the CIP process. The following categories are confirmed as enlisted according to CIPs which define that relationship:\nThese tentatively enlisted categories await CIPs to describe any enlistment relationship:\nProject representatives intending to follow an \"enlisted\" category above agree to coordinate with related development by sharing efforts to review and validate new proposals. It should be noted that single organisations can no longer represent any ecosystem or development category, which makes these enlistment guidelines both decentralised and cooperative, including whenever possible:\nallocating time to review proposals from actors of the community when solicited by editors (i.e. after one first round of reviews);\ndefining additional rules and processes whereby external actors can engage with their project as part of the CIP process;\ndefining boundaries within their project for which the CIP process does apply;\nestablishing points of contact and any designated reviews for a category;\nagreeing upon how proposals move from the state of idea (i.e. CIP) to actual implementation work;\nwriting CIPs for significant changes introduced in their projects when it applies.\nAny guidelines for this cooperation should be described by a dedicated CIP whenever possible. When such a CIP is posted or supersedes another one, it will be entered into the above table in the Categories section. Participants of enlisted categories should follow the requirements outlined in that CIP and should update such proposals whenever these requirements or relationships change.\nWarning A positive review by any enlisted project representative does not constitute a commitment to implement the CIP. It is still the CIP author's responsibility to create an implementation plan and identify implementors.\nEditors occasionally invite representatives from enlisted categories to speak during review meetings and solicit them for ultimate approvals of proposals in their area of expertise.\nNote Optionally, projects may show their enlisting using the following badge on their introductory README:\n![https://github.com/cardano-foundation/CIPs](https://raw.githubusercontent.com/cardano-foundation/CIPs/master/.github/badge.svg)\n![https://github.com/cardano-foundation/CIPs](https://raw.githubusercontent.com/cardano-foundation/CIPs/master/.github/badge.svg)\nProposals must be submitted to the cardano-foundation/CIPs repository as a pull request named after the proposal's title. The pull request title should not include a CIP number (and use ? instead as number); the editors will assign one. Discussions may precede a proposal. Early reviews and discussions streamline the process down the line.\n?\nNote Pull requests should not include implementation code: any code bases should instead be provided as links to a code repository.\nNote Proposals addressing a specific CPS should also be listed in the corresponding CPS header, in 'Proposed Solutions', to keep track of ongoing work.\nWhen a CIP title and subject matter share a common element, begin the CIP title with that common element and end it with the specific portion, delimited with the - character. Example (CIP-0095):\n-\nWeb-Wallet Bridge - Governance\nCIP editors will help determine these common elements and, whenever necessary, rename both CIP document titles and PR titles accordingly. The objective is to provide commonly recognisable names for similar developments (e.g. multiple extensions to another CIP or scheme).\nIn the original comment for your pull request, please include a link to the directory or the README.md for the CIP in your working branch, so readers and reviewers can easily follow your work. This makes it easier for editors and the community to read and review your proposal.\nREADME.md\nNote If this link changes (e.g. from the CIP directory being renamed), please keep this link updated.\nAuthors shall champion their proposals. The CIP process is a collaborative effort, which implies discussions between different groups of individuals. While editors may provide specific inputs and help reach out to experts, authors shall actively seek feedback from the community to help move their proposal forward.\nDiscussions and comments shall mainly happen on Github in pull requests. When discussed on other mediums, we expect authors or participants to report back a summary of their discussions to the original pull request to keep track of the most critical conversations in a written form and all in one place.\nAs much as possible, commenters/reviewers shall remain unbiased in their judgement and assess proposals in good faith. Authors have the right to reject comments or feedback but are strongly encouraged to address concerns in their 'Rationale' section. Ultimately, CIP editors shall make the last call concerning the various statements made on a proposal and their treatment by the author(s).\nBy opening pull requests or posting comments, commenters and authors agree to our Code of Conduct. Any comment infringing this code of conduct shall be removed or altered without prior notice.\nNote For acceptability guidelines, including a concise review checklist, see CIP Wiki CIPs for Reviewers & Authors.\nCIP editors meet regularly in a public Discord server to go through newly proposed ideas in a triage phase. As a result of a triage, editors acknowledge new CIPs, and briefly review their preamble. Editors also assign numbers to newly proposed CIPs during this phase. Incidentally, the triage allows new CIPs to get visibility for potential reviews.\nIn every meeting, editors will also review in more depth some chosen CIPs (based on their readiness and the stability of the discussions) and assess if they meet the criteria to be merged in their aimed status.\nDuring review sessions, editors will regularly invite project maintainers or actors from the ecosystem who are deemed relevant to the meeting's agenda. However, meetings are open and held in public to encourage anyone interested in participating.\nA dedicated Discord channel may also be created for some long-running discussions to support quick chats and progress on particular topics (while still being regularly summarised on the repository).\nOnce a proposal has reached all requirements for its target status (as explained in Statuses) and has been sufficiently and faithfully discussed by community members, it is merged with its target status.\nWarning Ideas deemed unsound shall be rejected with justifications or withdrawn by the authors. Similarly, proposals that appear abandoned by their authors shall be rejected until resurrected by their authors or another community member.\nCIPs are generally merged with the status 'Proposed' until they meet their 'Path to Active' requirements. In some rare cases (mainly when written after the facts and resulting in a broad consensus), proposals may be merged as 'Active' immediately.\nEach proposal is unique and has a bespoke 'Path to Active', which must be reviewed case-by-case. There must be sufficient time between the first appearance of a proposal and its merge into the repository to ensure enough opportunity for community members to review it.\nOnce merged, implementors shall execute the CIP's 'Implementation Plan', if any. If a proposal has no implementors or no 'Implementation Plan', it may simply remain as 'Proposed' in the repository.\nWarning It is perfectly fine to submit ideas in the repository with no concrete implementation plan, yet they should be treated as such: ideas.\nBesides, once all of the 'Path to Active' requirements have been met, authors shall make another pull request to change their CIP's status to 'Active'. Editors may also do this on occasion.\nFor a full, current description of Editor workflow, see CIP Wiki CIPs for Editors.\nCIP Editors safeguard the CIP process: they form a group enforcing the process described in this document and facilitating conversations between community actors. CIP editors should strive to keep up to date with general technical discussions and Cardano proposals. For each new draft proposal submitted on cardano-foundation/CIPs an editor might review it as follows:\nRead the proposal to check if it is ready, sound, and complete.\nCheck if it has been properly formatted.\nCheck if sufficient time has been allowed for proper discussion amongst the community.\nEnsure the motivation behind the CIP is valid and that design choices have relevant justifications or rationale.\nConfirm licensing terms are acceptable.\nAssign a CIP number\nAssign a given category to help with searching\nRequest wording/grammar adjustments\nCIPs that do not meet a sufficient level of quality or don't abide by the process described in this document will be rejected until their authors address review comments.\nNote that editors may provide technical feedback on proposals in some cases, although they aren't expected to be the sole technical reviewers of proposals. CIPs are, before anything, a community-driven effort. While editors are here to facilitate the discussion and mediate debates, they aren't necessarily technical experts on all subjects covered by CIPs.\nTherefore, CIPs authors are encouraged to reach out to known experts to demonstrate their good faith and openness when they champion a proposal. Editors may help with such efforts but cannot be expected to do this alone.\nExisting editors or the community may nominate new editors, provided they have proven to be already existing and active contributors to the CIP process and are ready to commit some of their time to the CIP process regularly.\nThe missions of an editor include, but aren't exclusively limited to, any of the tasks listed above. Active members that seek to become listed editors may also come forth and let it be known. Any application will take the form of a pull request updating this document with a justification as the pull request's description.\nCurrent editors are listed here below:\nEmeritus editors:\nFrederic Johnson - @crptmppt\nSebastien Guillemot - @SebastienGllmt\nMatthias Benkort - @KtorZ\nDuncan Coutts - @dcoutts\nA significant friction point regarding complex CIPs is often how the main problem is stated. The 'Motivation' is often insufficient (or simply underused) to describe various aspects of a problem, its scope, and its constraints. This lack of clarity leads, in the end, to poorly defined issues and debates over solutions that feel unclear amongst different participants.\nThe introduction of CIP-9999: Cardano Problem Statements addresses this gap by introducing a formal template and structure around problem statements. However, requiring any CIP to be preceded by a CPS would likely be overkill and become an obstacle to the overall adoption of the CIP process for more straightforward problems. At this stage, it is reasonable to think either (a) that CIP authors would foresee the complexity and state their problem as a CPS beforehand or (b) that editors or reviewers will require authors to write a CPS to clarify a perhaps ambiguous motivation on complex CIPs.\nWe also anticipate project maintainers or community actors will write standalone CPS to document well-known issues for which the design space is still to be explored.\nA recurring pain point with the previous CIP process was the lack of clear ownership/accountability of some proposals affecting remote parts of the ecosystem. On several occasions, proposals from community members have concerned, for example, core components of the Cardano architecture. However, those proposals have been hard to move forward with and to either reject or turn into concrete action steps. Authors usually do not have the technical proficiency required to implement them and rely on the core engineering team in charge of projects to do so. Thus, explicit compliance and collaboration of those engineering teams are necessary to propose changes affecting their work.\nBy asking teams to explicitly state their compliance with the CIP process with clear, accountable owners (as individuals), it becomes plausible to now establish a dialogue between community members and technical leadership responsible for specific ecosystem projects. Furthermore, projects that, on the contrary, do not seek to participate in CIP or receive contributions in the form of CIP/CPS are automatically taken out of this process, saving time and energy for both reviewers and authors.\nThe 'Editors' section now details how to become a CIP editor. The process aims to be simple and define those involved the most with editorship tasks as editors. Thus, being an active contributor to the CIP process as a prerequisite only makes sense. We want to leave the room open for either existing editors to refer an existing community as an editor or for community members to formulate that request explicitly.\nThere are no delays or number of contributions necessary to pretend to become an editor. Those criteria are often less relevant than others and more subjective, such as the quality of one's participation or their relevance. Since editors also need to work with one another in the end, it seems logical that existing editors have their final say about whom they intend to work with.\ntype\nThe type field in the header has shown to be:\ntype\nconfusing (often authors are getting it wrong);\nnot-too-useful (as a type tells you very little about the nature of the CIP).\ntype\nAn ad-hoc classification by non-rigid categories, which may evolve over time to reflect ecosystem areas, seems better suited. Therefore, we do not require authors to categorise their CIPs; instead, categories will be added and maintained by editors as a side task atop the CIP process.\nOver time we've learnt that the valuable information a status should convey is really about the readiness of a CIP, especially regarding standards. For a long time, many CIPs have lived as Draft despite some being used in dozens of systems. Consequently, the status has lost a bit of its meaning. The frontier between Draft and Proposed hasn't always been clear, and it has proven challenging to come up with good statuses to describe all possible rejections. So instead, the current division of statuses is as simple-as-it-can-be and remains flexible regarding rejections.\nDraft\nstatus\nDraft\nProposed\nThe choice of a code of conduct follows other popular open source initiatives. It serves as a good, unilaterally accepted foundation which can be later revisited if necessary.\nThe proposal has been reviewed by the community and sufficiently advertised on various channels. Cardano Forum IOG Technical Community Discord Twitter Reddit Cardano Summit 2022 IO ScotFest 2022\nThe proposal has been reviewed by the community and sufficiently advertised on various channels.\nCardano Forum\nIOG Technical Community Discord\nTwitter\nReddit\nCardano Summit 2022\nIO ScotFest 2022\nAll major concerns or feedback have been addressed. The document is as unambiguous as it can be and it outlines a process that a supermajority of reviewers is happy to follow.\nAll major concerns or feedback have been addressed. The document is as unambiguous as it can be and it outlines a process that a supermajority of reviewers is happy to follow.\nRework existing draft CIPs to adopt this new format and process. In particular, CIPs affecting enlisted projects should be brought to the attention of the respective project maintainers.\nEdit / align old CIPs preambles and sections to at least reflect also this new format.\nThis CIP is licensed under CC-BY-4.0.\n2023 Cardano Foundation"
}